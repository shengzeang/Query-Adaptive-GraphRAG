{"Question": "\"The following problem illustrates the way memory aliasing can cause unexpected \n program behavior. Consider the following procedure to swap two values:\n 1 /* Swap value x at xp with value y at yp */\n 2 void swap(int *xp, int *yp)\n 3{\n 4 *xp = *xp + *yp; /* x+y */\n 5 *yp = *xp - *yp; /* x+y-y=x*/\n 6 *xp = *xp - *yp; /* x+y-x=y*/\n 7}\n If this procedure is called with xp equal to yp, what effect will it have?\"", "Level-1 Topic": "Programming fundamentals", "Level-2 Topic": "Function (computer programming)", "Rationale": "If xp equals yp, the arithmetic operations in the swap function result in the value being set to zero. The addition and subtraction steps, meant to swap distinct values, cancel out when both pointers reference the same memory location.", "Answer": "The value at xp to zero"}
{"Question": "Why does every C program need a routine called main?", "Level-1 Topic": "Programming fundamentals", "Level-2 Topic": "Function (computer programming)", "Rationale": "In C, the main function is the entry point where the program starts execution. The C runtime system initializes the program and then jumps to the main function, making it essential for every C program to have a main function.", "Answer": "Every program needs a main function, because the C startup code, which is \n common to every C program, jumps to a function called main."}
{"Question": "Computational thinking should come before programming. Why?", "Level-1 Topic": "Programming fundamentals", "Level-2 Topic": "Algorithm", "Rationale": "Computational thinking precedes programming because it ensures a clear understanding of the problem and logically structured steps. This process simplifies the translation of these steps into code, making the programming process more efficient and effective.", "Answer": "Make sure the problem is well understood and the steps of the solution are logically articulated so that the steps can easily and properly transformed into programming code."}
{"Question": "Assuming current points to the next-to-last link in a singly linked list, what statement will delete the last link from the list by Java?", "Level-1 Topic": "Programming languages and compilers", "Level-2 Topic": "Language paradigms (imperative, functional, logic)", "Rationale": "A satisfactory answer explains how to remove the last link by setting the next-to-last link's 'next' reference to null, thus detaching the last node.\n", "Answer": "current.next=null;"}
{"Question": "When all references to a link are changed to refer to something else by Java, what happens to the link?", "Level-1 Topic": "Programming languages and compilers", "Level-2 Topic": "Language paradigms (imperative, functional, logic)", "Rationale": "A satisfactory answer should explain that when Java no longer has any references to an object, it becomes eligible for garbage collection, which reclaims its memory.", "Answer": "Java's garbage collection process destroys it."}
{"Question": "How many \"null\" references in a binary tree of n nodes?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "Each of the \\( n \\) nodes has 2 pointers, totaling \\( 2n \\). Subtract \\( n-1 \\) edges, leaving \\( n+1 \\) null references.", "Answer": "n+1"}
{"Question": "How many \"null\" references in a doubly linked list of n nodes?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "In a doubly linked list, the first node's previous and the last node's next pointers are null, totaling 2 null references.", "Answer": "2"}
{"Question": "A binary tree of characters has the preorder sequence \u201calgorithm\u201d and the inorder sequence \u201cgloraihtm.\u201d In the level-wise traversal of this tree, the fourth element is (start counting from 1)", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "Level-order traversal sequence is \"a\", \"l\", \"i\", \"g\", ... so 'g' is fourth.", "Answer": "g"}
{"Question": "This subject has 327 registered students. Suppose I keep your IDs in a sorted array. If I use binary search to search for a student in this array, and I find her record in three steps. The possible indices for her ID in the array are:", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm", "Rationale": "In a sorted array of 327 elements, binary search in 3 steps can narrow down to one of the four regions, specifically leading to indices around , 327/4, 327/2 and 33*27/4, i.e., 81, 163, or 245.", "Answer": "81, 163, 245"}
{"Question": "Arrange the following functions in increasing order of growth rate (if f(n) precedes g(n), then f(n) = O(g(n)):11log(n^2) - 5logn, n/2011, 2^2011, 2011^2, n^2 - 2011n", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Time and space complexity analysis", "Rationale": "Constants (201122011^220112, 220112^{2011}22011) grow the least. Logarithmic (11log\u2061(n2)\u22125log\u2061n11\\log(n^2) - 5\\log n11log(n2)\u22125logn) grows slower than linear (n2011\\frac{n}{2011}2011n\u200b), which in turn grows slower than quadratic (n2\u22122011nn^2 - 2011nn2\u22122011n). Hence, the order reflects increasing growth rates.", "Answer": "20112,22011,11log(n2)\u22125logn,2011n\u200b,n2\u22122011n"}
{"Question": "We use bubble sort to sort the array [2011, 3011, 11, 11, 2, 30]. How many major iterations will the sorting take?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm", "Rationale": "Bubble sort requires n\u22121n-1n\u22121 major iterations to sort an array of nnn elements. With 6 elements in the array [2011,3011,11,11,2,30][2011, 3011, 11, 11, 2, 30][2011,3011,11,11,2,30], it takes 5 iterations to complete the sorting.", "Answer": "5"}
{"Question": "What is the invariant in the selection sort?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm", "Rationale": "The invariant in selection sort is that the subarray to the left of the \"outer\" index is always sorted. This ensures that as the algorithm progresses, the sorted portion of the array grows, and items with indices less than or equal to \"outer\" are sorted.", "Answer": "Items with indices less than or equal to outer are sorted."}
{"Question": "Suppose you push 10, 20, 30, and 40 onto the stack. Then you pop three items. Which one is left on the stack?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "In a stack, last in, first out (LIFO) principle applies. Popping three items (30, 40, 20) leaves 10, the first item pushed, on top.", "Answer": "10"}
{"Question": "What do LIFO and FIFO mean?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "LIFO means Last-In-First-Out, as the most recent item is removed first. FIFO means First-In-First-Out, as the oldest item is removed first. These terms describe the order of item removal in data structures like stacks and queues.", "Answer": "Last-In-First-Out; and First-In-First-Out"}
{"Question": "As other items are inserted and removed, does a particular item in a queue\nmove along the array from lower to higher indices, or higher to lower?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "In a queue, items are enqueued at the rear and dequeued from the front. They remain stationary at their original indices; only the indices of the front and rear pointers change.", "Answer": "It doesn't move at all."}
{"Question": "Suppose you insert 15, 25, 35, and 45 into a queue. Then you remove three items. Which one is left?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "In a queue, items are processed in First-In-First-Out (FIFO) order. Removing three items (15, 25, 35) leaves the last inserted, 45, as the remaining item.", "Answer": "45"}
{"Question": "Inserting an item into a typical priority queue takes what big O time?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Time and space complexity analysis", "Rationale": "Insertion in a typical priority queue takes O(N) time because it may require shifting elements to maintain heap property after inserting a new item.", "Answer": "O(N)"}
{"Question": "How many references must you change to insert a link in the middle of a singly linked list?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm design techniques", "Rationale": "Inserting a link in the middle of a singly linked list requires changing 2 references: the new node's next and the previous node's next.", "Answer": "2"}
{"Question": "How many references must you change to insert a link at the end of a singly linked list?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm design techniques", "Rationale": "To insert a link at the end of a singly linked list, you only need to change the last node's next reference to point to the new node.", "Answer": "1"}
{"Question": "Assuming a copy takes longer than a comparison, is it faster to delete an item with a certain key from a linked list or from an unsorted array?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Time and space complexity analysis", "Rationale": "Deleting from a linked list is faster because it requires changing one reference, whereas an unsorted array requires shifting elements, which involves multiple comparisons and copies.", "Answer": "a linked list"}
{"Question": "How many times would you need to traverse a singly linked list to delete the item with the largest key?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm design techniques", "Rationale": "With a singly linked list that includes previous references, you can delete the item with the largest key in one traversal, as you can directly access the last node.", "Answer": "once, if the links include a previous reference"}
{"Question": "Which do you think would be a better choice to implement a stack: a singly\nlinked list or an array?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "A singly linked list is usually better for implementing a stack because it dynamically adjusts memory usage, unlike a fixed-size array, which can waste space. Both allow O(1) time complexity for push and pop operations.", "Answer": "Usually, the list. They both do push and pop in O(1) time, but the list uses memory more efficiently."}
{"Question": "Insertion and deletion in a tree require what big O time?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Time and space complexity analysis", "Rationale": "Insertion and deletion in a balanced tree require O(logN) time because these operations involve traversing the tree's height, which is logN for a balanced tree.", "Answer": "O(logN)"}
{"Question": "In a complete binary tree with 20 nodes, and the root considered to be at level 0, how many nodes are there at level 4?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Data structure", "Rationale": "In a complete binary tree, each level doubles the number of nodes. With 20 nodes, level 4 has 5 nodes because 2^4 = 16 and the last 4 nodes are distributed across levels 0-3.", "Answer": "5"}
{"Question": "Using big O notation, say how long it takes (ideally) to find an item in a hash table.", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Time and space complexity analysis", "Rationale": "In a hash table, finding an item ideally takes O(1) time because the hash function directly maps keys to indices, bypassing the need for search or traversal.", "Answer": "O(1)"}
{"Question": "What are the first five step sizes in quadratic probing?", "Level-1 Topic": "Data structures and algorithms", "Level-2 Topic": "Algorithm", "Rationale": "In quadratic probing, step sizes are calculated as  i^2  where i starts from 1. The first five values are 1, 4, 9, 16, 25 because they represent the squares of the first five natural numbers.", "Answer": "1, 4, 9, 16, 25"}
{"Question": "How is a data warehouse di\ufb00erent from a database?", "Level-1 Topic": "Database", "Level-2 Topic": "Date warehourse", "Rationale": "A data warehouse differs from a database in that it aggregates historical data from multiple sources for analysis and decision support, using a unified schema. In contrast, a database manages current, interrelated data with support for ad-hoc queries and transaction processing, often with varying schemas.", "Answer": "Di\ufb00erences between a data warehouse and a database: A data warehouse is a repository of information collected from multiple sources, over a history of time, stored under a uni\ufb01ed schema, and used for data analysis and decision support; whereas a database, is a collection of interrelated data that \n represents the current status of the stored data. There could be multiple heterogeneous databases \n where the schema of one database may not agree with the schema of another. A database system \n supports ad-hoc query and on-line transaction processing."}
{"Question": "Discuss issues to consider during data integration.", "Level-1 Topic": "Database", "Level-2 Topic": "Data integration", "Rationale": "These concerns are crucial in the database domain as they focus on ensuring that data from multiple sources is combined accurately and consistently.", "Answer": "Data integration involves combining data from multiple sources into a coherent data store. Issues that \n must be considered during such integration include:\n \u2022 Schema integration: The metadata from the di\ufb00erent data sources must be integrated in order \n to match up equivalent real-world entities. This is referred to as the entity identi\ufb01cation problem.\n \u2022 Handling redundant data: Derived attributes may be redundant, and inconsistent attribute \n naming may also lead to redundancies in the resulting data set. Also, duplications at the tuple \n level may occur and thus need to be detected and resolved.\n \u2022 Detection and resolution of data value con\ufb02icts: Di\ufb00erences in representation, scaling or \n encoding may cause the same real-world entity attribute values to di\ufb00er in the data sources being \n integrated."}
{"Question": "Robust data loading poses a challenge in database systems because the input data are often dirty. In many \n cases, an input record may miss multiple values, some records could be contaminated, with some data values \n out of range or of a di\ufb00erent data type than expected. Work out an automated data cleaning and loading \n algorithm so that the erroneous data will be marked, and contaminated data will not be mistakenly inserted \n into the database during data loading. How can we apply automated procedure on these problems?", "Level-1 Topic": "Database", "Level-2 Topic": "Data processing", "Rationale": "It is addressed that how to handle dirty data during loading by using metadata, rules, and domain knowledge, which are essential for maintaining data integrity. This approach aligns with database management practices, where ensuring accurate and clean data is critical for reliable database operations.", "Answer": "\u2022 Use metadata (e.g., domain, range, dependency, distribution).\n \u2022 Check unique rule, consecutive rule and null rule.\n \u2022 Check \ufb01eld overloading.\n \u2022 Spell-checking. \u2022 Detect di\ufb00erent attribute names which actually have the same meaning.\n \u2022 Use domain knowledge to detect errors and make corrections."}
{"Question": "Brie\ufb02y compare the following concepts. Discovery-driven cube, multifeature cube, virtual warehouse.", "Level-1 Topic": "Database", "Level-2 Topic": "Date warehourse", "Rationale": "", "Answer": "A discovery-driven cub e uses precomputed measures and visual cues to indicate data exceptions \n at all levels of aggregation, guiding the user in the data analysis process. A multi-feature cub e \n computes complex queries involving multiple dependent aggregates at multiple granularities (e.g., \n to \ufb01nd the total sales for every item having a maximum price, we need to apply the aggregate \n function SUM to the tuple set output by the aggregate function MAX). A virtual warehouse is a \n set of views (containing data warehouse schema, dimension, and aggregate measure de\ufb01nitions) \n over operational databases."}
{"Question": "Suppose that a data warehouse consists of the four dimensions, date, spectator, location, and game, \n and the two measures, count and charge, where charge is the fare that a spectator pays when watching \n a game on a given date. Spectators may be students, adults, or seniors, with each category having its \n own charge rate. Starting with the base cuboid [date, spectator, location, g ame], what speci\ufb01c OLAP operations \n should one perform in order to list the total charge paid by student spectators at GM Place in \n 2010?", "Level-1 Topic": "Database", "Level-2 Topic": "Date warehourse", "Rationale": "The operations described\u2014roll-up and dice\u2014are standard OLAP techniques used for querying and aggregating data in data warehouses. They involve manipulating dimensions and measures to derive specific insights, which is a key aspect of database management.", "Answer": "\u2022 Roll-up on date from date id to year.\n \u2022 Roll-up on game from game id to all.\n \u2022 Roll-up on location from location id to location name.\n \u2022 Roll-up on spectator from spectator id to status.\n \u2022 Dice with status=\u201cstudents\u201d, location name=\u201cGM Place\u201d, and year = 2010."}
{"Question": "Suppose that a data warehouse contains 20 dimensions, each with about \ufb01ve levels of granularity. Users are mainly interested in four particular dimensions, each having three frequently accessed \n levels for rolling up and drilling down. How would you design a data cube structure to support \n this preference e\ufb03ciently?", "Level-1 Topic": "Database", "Level-2 Topic": "Date warehourse", "Rationale": "The approach of using partial materialization to optimize data cube structure involves selecting specific cuboids to improve efficiency.", "Answer": "An e\ufb03cient data cube structure to support this preference would be to use partial materialization, \n or selected computation of cuboids. By computing only the proper subset of the whole set of \n possible cuboids, the total amount of storage space required would be minimized while maintaining \n a fast response time and avoiding redundant computation."}
{"Question": "Iceberg conditions, such as \u201ccount(\u2217) \u2265 min sup\u201d, has been used to speed up the computation of iceberg cubes by pushing iceberg conditions deeply into the cube computation. Such iceberg conditions are antimonotonic constraints, i.e., if a cell cannot satisfy the constraint, its descendant cells can never  satisfy the same constraint. However, in practice, iceberg conditions may not always be antimonotonic. For example, one may like to give the following conditions in computing a Sales Iceberg cube: (1) AVG(price) \u2265 800, and (2) COUNT(\u2217) \u2265 50. State why constraint (1) cannot be pushed deeply into a typical iceberg cube computation process.", "Level-1 Topic": "Database", "Level-2 Topic": "Date warehourse", "Rationale": "The question discusses iceberg conditions and their application in cube computation, particularly focusing on constraints like average price which do not exhibit the antimonotonic property. ", "Answer": "An anti-monotionic iceberg cube can be computed e\ufb03ciently by exploring the Apriori property: \n if a given cell does not satisfy minimum support, then no descendant (i.e., more specialized or \n detailed version) of the cell will satisfy minimum support either. However, since the constraint \n involves the non-anti-monotonic measure average, it does not have the Apriori property."}
{"Question": "A database schema is given below.\n \n EMPLOYEES(ENO, FNAME, M, INIT, LNAME, BYEAR, SEX, SALARY,DNO)\n DEPARTMENT(DNO, DNAME, MGR_ENO, ENO)\n \n PLOC are referring to names of cities, such as Hong Kong, Shanghai, etc. DYEAR is in the format of YYYY (integer).\n \n Write an SQL statement that retrieves the first name and birthyear of all employees born in the 50s.", "Level-1 Topic": "Database", "Level-2 Topic": "SQL", "Rationale": "The SQL statement uses BETWEEN to filter employees whose birth year (BYEAR) falls within the 1950s range. This efficiently retrieves the desired data, ensuring only employees born between 1950 and 1959 are selected.", "Answer": "SELECT fname, byear\n FROM employees \n WHERE 1950<= byear AND byear <=1959"}
{"Question": "A database schema is given below.\n \n EMPLOYEES(ENO, FNAME, M, INIT, LNAME, BYEAR, SEX, SALARY,DNO)\n DEPARTMENT(DNO, DNAME, MGR_ENO, ENO)\n \n PLOC are referring to names of cities, such as Hong Kong, Shanghai, etc. DYEAR is in the format of YYYY (integer).\n \n Write a SQL statement to provide the department number for departments only have male employees.", "Level-1 Topic": "Database", "Level-2 Topic": "SQL", "Rationale": "The SQL statement first finds departments with any non-male employees and excludes these departments from the results. The remaining departments are those with only male employees. This approach ensures departments with mixed or female employees are filtered out.", "Answer": "SELECT dno \n FROM department\n WHERE dno not IN (SELECT distinct dno FROM employees where sex = \u2018f\u2019)"}
{"Question": "Keyword queries used in Web search are quite different from database queries. List key differences between the two, in terms of the way the queries are specified, and in terms of what is the result of a query", "Level-1 Topic": "Database", "Level-2 Topic": "SQL", "Rationale": "A satisfactory answer should identify differences in query language (structured vs. unstructured), specificity (precise vs. ambiguous), and result format (exact matches vs. ranked results), explaining these aspects clearly with examples from web search and database queries.", "Answer": "Queries used in the Web are specified by providing a list of keywords with no specific syntax. The result is typically an ordered list of URLs, along with snippets of information about the content of the URLs. In contrast, database queries have a specific syntax allowing complex queries to be specified. And in the relational world the result of a query is always a table"}
{"Question": "Suppose end system A wants to send a large file to end system B. At a very high level, describe how end system A creates packets from the file. When one of these packets arrives to a router, what information in the packet does the router use to determine the link onto which the packet is forwarded?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "End system A divides the file into packets, each with a header containing the destination IP address. Routers use this IP address to decide the next link for forwarding the packet, ensuring it reaches end system B.", "Answer": "End system A breaks the large file into chunks. It adds header to each chunk, thereby generating multiple packets from the file. The header in each packet includes the IP address of the destination (end system B). The packet switch uses the destination IP address in the packet to determine the outgoing link."}
{"Question": "Consider an application that transmits data at a steady rate (for example, the sender generates an N-bit unit of data every k time units, where k is small and fixed). Also, when such an application starts, it will continue running for a relatively long period of time. Answer the following questions, briefly justifying your answer: Would a packet-switched network or a circuit-switched network be more appropriate for this application? Why?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Routing and switching", "Rationale": "Even if link capacities exceed the total data rates, congestion control is important for handling unforeseen traffic patterns or failures. It ensures robustness and maintains performance under varying network conditions.", "Answer": "A circuit-switched network would be well suited to the application, because the application involves long sessions with predictable smooth bandwidth requirements. Since the transmission rate is known and not bursty, bandwidth can be reserved for each application session without significant waste. In addition, the overhead costs of setting up and tearing down connections are amortized over the lengthy duration of a typical application session."}
{"Question": "Consider an application that transmits data at a steady rate (for example, the sender generates an N-bit unit of data every k time units, where k is small and fixed). Also, when such an application starts, it will continue running for a relatively long period of time. Answer the following questions, briefly justifying your answer: Suppose that a packet-switched network is used and the only traffic in this network \n comes from such applications as described above. Furthermore, assume that the sum of the application data rates is less than the capacities of each and every link. Is some form of congestion control needed? Why?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Routing and switching", "Rationale": "Despite having enough bandwidth, congestion control remains important for managing unexpected traffic spikes, network failures, and dynamic conditions. It helps maintain network stability and performance even under varying circumstances.", "Answer": "In the worst case, all the applications simultaneously transmit over one or more network links. However, since each link has sufficient bandwidth to handle the sum of all of the applications' data rates, no congestion (very little queuing) will occur. Given such generous link capacities, the network does not need congestion control mechanisms."}
{"Question": "Suppose you would like to urgently deliver 40 terabytes data from Boston to Los Angeles. You have available a 100 Mbps dedicated link for data transfer. Would you prefer to transmit the data via this link or instead use FedEx over-night delivery? Explain.", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Wireless and mobile networks", "Rationale": "Sending 40 terabytes over a 100 Mbps link would take about 37 days, whereas FedEx overnight delivery guarantees arrival in one day, making it a faster and potentially cheaper option for urgent data transfers.", "Answer": "40 terabytes = 40 * 1012 * 8 bits. So, if using the dedicated link, it will take 40 * 1012 * 8 / (100 *106 ) =3200000 seconds = 37 days. But with FedEx overnight delivery, you can guarantee the data arrives in one day, and it should cost less than $100."}
{"Question": "What is the difference between network architecture and application architecture?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "Network architecture involves the structure and layers of communication protocols, such as the OSI model, to facilitate network operations. Application architecture refers to the design of software applications, defining their structure and interactions, such as client-server or peer-to-peer.", "Answer": "2. Network architecture refers to the organization of the communication process into layers (e.g., the five-layer Internet architecture). Application architecture, on the other hand, is designed by an application developer and dictates the broad structure of the application (e.g., client-server or P2P)."}
{"Question": "For a communication session between a pair of processes, which process is the client and which is the server?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Network programming", "Rationale": "The client initiates communication by sending a request to the server, which is designed to wait and respond to incoming requests. This distinction defines the roles in a client-server model.", "Answer": "The process which initiates the communication is the client; the process that waits to be contacted is the server."}
{"Question": "Suppose you wanted to do a transaction from a remote client to a server as fast as possible. Would you use UDP or TCP? Why?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "UDP is preferred for fast transactions due to its low latency, as it avoids the connection establishment and teardown overhead required by TCP. This reduces the total roundtrip time compared to TCP, which needs at least two RTTs.", "Answer": "You would use UDP. With UDP, the transaction can be completed in one roundtrip time (RTT) - the client sends the transaction request into a UDP socket, and the server sends the reply back to the client's UDP socket. With TCP, a minimum of two RTTs are needed - one to set-up the TCP connection, and another for the client to send the request, and for the server to send back the reply."}
{"Question": "What is meant by a handshaking protocol?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "Handshaking involves initial exchanges of control packets to establish communication settings before data transfer. SMTP uses this method to negotiate and confirm communication parameters, whereas HTTP starts data transfer immediately without this formal handshake.", "Answer": "A protocol uses handshaking if the two communicating entities first exchange control packets before sending data to each other. SMTP uses handshaking at the application layer whereas HTTP does not."}
{"Question": "Why do HTTP, SMTP, and POP3 run on top of TCP rather than on UDP?", "Level-1 Topic": "Computer Networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "TCP ensures reliable, ordered, and gap-free data delivery, which is crucial for protocols like HTTP, SMTP, and POP3. These protocols need to ensure that all data is correctly received, making TCP a better fit than UDP, which lacks these guarantees.", "Answer": "The applications associated with those protocols require that all application data be received in the correct order and without gaps. TCP provides this service whereas UDP does not."}
{"Question": "From a user\u2019s perspective, what is the difference between the download-and-delete mode and the download-and-keep mode in POP3?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "Download-and-delete mode removes emails after retrieval, which is inconvenient for users accessing from multiple devices. Download-and-keep mode retains emails on the server, causing potential inefficiencies by transferring old messages each time a new device accesses the email.", "Answer": "With download and delete, after a user retrieves its messages from a POP server, the messages are deleted. This poses a problem for the nomadic user, who may want to access the messages from many different machines (office PC, home PC, etc.). In the download and keep configuration, messages are not deleted after the user retrieves the messages. This can also be inconvenient, as each time the user retrieves the stored messages from a new machine, all of non-deleted messages will be transferred to the new machine (including very old messages)."}
{"Question": "In BitTorrent, suppose Alice provides chunks to Bob throughout a 30-second interval. Will Bob necessarily return the favor and provide chunks to Alice in this same interval? Why or why not?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "In BitTorrent, Bob will only provide chunks to Alice if Alice is among his top neighbors, which is not guaranteed. Thus, even if Alice provides chunks to Bob, Bob might not reciprocate within the same interval due to peer selection dynamics.", "Answer": "It is not necessary that Bob will also provide chunks to Alice. Alice has to be in the top 4 neighbors of Bob for Bob to send out chunks to her; this might not occur even if Alice provides chunks to Bob throughout a 30-second interval."}
{"Question": "What is an overlay network? Does it include routers? What are the edges in the overlay network?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "An overlay network is a network built on top of another network, consisting of nodes and logical links between them. Edges represent logical connections, such as TCP links, and routers are not part of the overlay network itself.", "Answer": "The overlay network in a P2P file sharing system consists of the nodes participating in the file sharing system and the logical links between the nodes. There is a logical link (an \u201cedge\u201d in graph theory terms) from node A to node B if there is a semi-permanent TCP connection between A and B. An overlay network does not include routers."}
{"Question": "Besides network-related considerations such as delay, loss, and bandwidth performance, there are other important factors that go into designing a CDN server selection strategy. What are they?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network programming", "Rationale": "CDN server selection must consider load balancing to avoid overloading servers, diurnal traffic patterns, DNS server differences, availability of rare content, and hotspots from popular content. These factors ensure efficient and reliable content delivery beyond just network performance.", "Answer": "Other than network-related factors, there are some important factors to consider, such as load-balancing (clients should not be directed to overload clusters), diurnal effects, variations across DNS servers within a network, limited availability of rarely accessed video, and the need to alleviate hot-spots that may arise due to popular video content."}
{"Question": "What is the difference between MAIL FROM : in SMTP and From : in the mail message itself?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "MAIL FROM: is an SMTP command used to specify the sender to the SMTP server. In contrast, From: is an email header in the message body that indicates the sender to recipients. They serve different roles in the email transmission process.", "Answer": "The MAIL FROM: in SMTP is a message from the SMTP client that identifies the sender of the mail message to the SMTP server. The From: on the mail message itself is NOT an SMTP message, but rather is just a line in the body of the mail message."}
{"Question": "What is a whois database?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "The WHOIS database helps locate details about domain names, IP addresses, or network administrators, including registrar and DNS server information. It\u2019s crucial for managing and understanding domain and network ownership and configuration.", "Answer": "For a given input of domain name (such as ccn.com), IP address or network administrator name, the whois database can be used to locate the corresponding registrar, whois server, DNS server, and so on."}
{"Question": "What are the advantages and disadvantages of having a large number of simultaneous TCP connections?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network programming", "Rationale": "A large number of simultaneous TCP connections can speed up file downloads by allowing parallel transfers. However, it may also lead to high bandwidth consumption, potentially degrading the performance for other users sharing the same network resources.", "Answer": "The advantage is that you will you potentially download the file faster. The disadvantage is that you may be hogging the bandwidth, thereby significantly slowing down the downloads of other users who are sharing the same physical links."}
{"Question": "We have seen that Internet TCP sockets treat the data being sent as a byte stream but UDP sockets recognize message boundaries. What are one advantage and one disadvantage of byte-oriented API versus having the API explicitly recognize and preserve application-defined message boundaries?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network programming", "Rationale": "Byte-oriented APIs (TCP) simplify applications without inherent message boundaries but require manual boundary encoding for message-oriented applications. UDP, with its message-oriented nature, naturally preserves message boundaries, reducing the need for additional boundary indicators at the application level.", "Answer": "For an application such as remote login (telnet and ssh), a byte-stream oriented protocol is very natural since there is no notion of message boundaries in the application. When a user types a character, we simply drop the character into the TCP connection. \n In other applications, we may be sending a series of messages that have inherent boundaries between them. For example, when one SMTP mail server sends another SMTP mail server several email messages back to back. Since TCP does not have a mechanism to indicate the boundaries, the application must add the indications itself, so that receiving side of the application can distinguish one message from the next. If each message were instead put into a distinct UDP segment, the receiving end would be able to distinguish the various messages without any indications added by the sending side of the application."}
{"Question": "We have said that an application may choose UDP for a transport protocol because UDP offers finer application control (than TCP) of what data is sent in a segment and when. Why does an application have more control of what data is sent in a segment?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network programming", "Rationale": "UDP provides more control to the application by directly encapsulating the application message in a segment, preserving boundaries. In contrast, TCP handles segmentation and reassembly, potentially combining or splitting messages across multiple TCP segments, thus giving the application less control over segment boundaries.", "Answer": "Consider sending an application message over a transport protocol. With TCP, the application writes data to the connection send buffer and TCP will grab bytes without necessarily putting a single message in the TCP segment; TCP may put more or less than a single message in a segment. UDP, on the other hand, encapsulates in a segment whatever the application gives it; so that, if the application gives UDP an application message, this message will be the payload of the UDP segment. Thus, with UDP, an application has more control of what data is sent in a segment."}
{"Question": "We have said that an application may choose UDP for a transport protocol because UDP offers finer application control (than TCP) of what data is sent in a segment and when. Why does an application have more control on when the segment is sent?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network programming", "Rationale": "UDP provides applications with greater control over transmission timing because it lacks TCP's flow and congestion control. These TCP mechanisms can delay data transmission, whereas UDP sends data immediately, giving applications more direct control over when segments are sent.", "Answer": "With TCP, due to flow control and congestion control, there may be significant delay from the time when an application writes data to its send buffer until when the data is given to the network layer. UDP does not have delays due to flow control and congestion control."}
{"Question": "Why is it necessary for the server to use a special initial sequence number in the \n SYNACK?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "The server uses a special initial sequence number (ISN) in the SYN-ACK to ensure unique connection identification and proper packet sequencing. This also mitigates SYN flood attacks by making it harder for attackers to predict ISNs and hijack sessions.", "Answer": "In order to defend itself against SYN FLOOD attack."}
{"Question": "Host A is sending an enormous file to Host B over a TCP connection. Over this connection there is never any packet loss and the timers never expire. Denote the transmission rate of the link connecting Host A to the Internet by R bps. Suppose that the process in Host A is capable of sending data into its TCP socket at a rate S bps, where S=10\u22c5R. Further suppose that the TCP receive buffer is large enough to hold the entire file, and the send buffer can hold only one percent of the file. What would prevent the process in Host A from continuously passing data to its TCP socket at rate S bps? TCP flow control? TCP congestion control? Or something else? Elaborate.", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "The process in Host A is throttled by the limited send buffer capacity. Once the buffer is full, the data transmission rate reduces to match the link rate R, as the process cannot send more data until space frees up in the buffer.", "Answer": "In this problem, there is no danger in overflowing the receiver since the receiver\u2019s receive buffer can hold the entire file. Also, because there is no loss and acknowledgements are returned before timers expire, TCP congestion control does not throttle the sender. However, the process in host A will not continuously pass data to the socket because the send buffer will quickly fill up. Once the send buffer becomes full, the process will pass data at an average rate or R << S."}
{"Question": "What is the fundamental difference between a router and link-layer switch?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "A router forwards packets using the IP address at the network layer (Layer 3), enabling inter-network communication. In contrast, a link-layer switch forwards packets using the MAC address at the data link layer (Layer 2) for intra-network communication.", "Answer": "A router forwards a packet based on the packet\u2019s IP (layer 3) address. A link-layer switch forwards a packet based on the packet\u2019s MAC (layer 2) address."}
{"Question": "What are the key differences between routing and forwarding?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "Forwarding is the immediate, hardware-based process of moving packets between router interfaces, operating on short timescales. Routing, however, involves the long-term, software-based process of determining packet paths across the network, operating on longer timescales.", "Answer": "3. The key differences between routing and forwarding is that forwarding is a router\u2019s local action of transferring packets from its input interfaces to its output interfaces, and forwarding takes place at very short timescales (typically a few nanoseconds), and thus is typically implemented in hardware. Routing refers to the network-wide process that determines the end-to-end paths that packets take from sources to destinations. Routing takes place on much longer timescales (typically seconds), and is often implemented in software."}
{"Question": "What is the role of the forwarding table within a router?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "The forwarding table in a router holds entries that map destination addresses to specific outgoing interfaces. This table is used to quickly determine where to send incoming packets, ensuring efficient packet routing through the router\u2019s switching fabric.", "Answer": "The role of the forwarding table within a router is to hold entries to determine the outgoing link interface to which an arriving packet will be forwarded via switching fabric."}
{"Question": "Discuss why each input port in a high-speed router stores a shadow copy of the forwarding table.", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "Each input port in a high-speed router stores a shadow copy of the forwarding table to enable local, decentralized lookups. This approach prevents bottlenecks at the centralized routing processor, ensuring faster and more efficient packet forwarding.", "Answer": "With the shadow copy, the forwarding lookup is made locally, at each input port, without invoking the centralized routing processor. Such a decentralized approach avoids creating a lookup processing bottleneck at a single point within the router."}
{"Question": "Suppose that an arriving packet matches two or more entries in a router\u2019s forwarding table. With traditional destination-based forwarding, what rule does a router apply to determine whichof these rules should be applied to determine the output port to which the arriving packet should be switched?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "When a packet's destination matches multiple entries, a router uses the longest prefix matching rule. This means the router selects the forwarding entry with the longest prefix that matches the destination address, ensuring the most specific route is used.", "Answer": "A router uses longest prefix matching to determine which link interface a packet will be forwarded to if the packet\u2019s destination address matches two or more entries in the forwarding table. That is, the packet will be forwarded to the link interface that has the longest prefix match with the packet\u2019s destination."}
{"Question": "Suppose Host A sends Host B a TCP segment encapsulated in an IP datagram. When Host B receives the datagram, how does the network layer in Host B know it should pass the segment (that is, the payload of the datagram) to TCP rather than to UDP or to some other upper-layer protocol?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "The 8-bit protocol field in the IP datagram header indicates the transport layer protocol used by the encapsulated segment. This field informs the network layer in Host B which protocol (e.g., TCP or UDP) should handle the segment's payload.", "Answer": "The 8-bit protocol field in the IP datagram contains information about which transport layer protocol the destination host should pass the segment to."}
{"Question": "What is meant by the term \u201croute aggregation\u201d? Why is it useful for a router to perform route aggregation?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "Route aggregation allows an ISP to advertise a single prefix for multiple networks, reducing the number of routes advertised and simplifying routing tables. This helps manage routing efficiency and decreases the load on routers by minimizing the number of routes they need to process.", "Answer": "Route aggregation means that an ISP uses a single prefix to advertise multiple networks. Route aggregation is useful because an ISP can use this technique to advertise to the rest of the Internet a single prefix address for the multiple networks that the ISP has."}
{"Question": "How does generalized forwarding differ from destination-based forwarding?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "Destination-based forwarding relies solely on the destination IP address to route packets to a specific output port. Generalized forwarding, however, uses multiple header fields for matching and supports a variety of actions, including load balancing, header rewriting, and advanced processing.", "Answer": "32. Forwarding has two main operations: match and action. With destination-based forwarding, the match operation of a router looks up only the destination IP address of the to-be-forwarded datagram, and the action operation of the router involves sending the packet into the switching fabric to a specified output port. With generalized forwarding, the match can be made over multiple header fields associated with different protocols at different layers in the protocol stack, and the action can include forwarding the packet to one or more output ports, load-balancing packets across multiple outgoing interfaces, rewriting header values (as in NAT), purposefully blocking/dropping a packet (as in a firewall), sending a packet to a special server for further processing and action, and more."}
{"Question": "Describe how a network administrator of an upper-tier ISP can implement policy when configuring BGP.", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network security", "Rationale": "To enforce policy in BGP, ISP B can filter routes to avoid carrying transit traffic between other tier-1 ISPs. By not advertising routes to and from ISPs A and C if they pass through each other, ISP B prevents unwanted transit.", "Answer": "A tier-1 ISP B may not to carry transit traffic between two other tier-1 ISPs, say A and C, with which B has peering agreements. To implement this policy, ISP B would not advertise to A routes that pass through C; and would not advertise to C routes that pass through A."}
{"Question": "What is the purpose of the service abstraction layer in the OpenDaylight SDN controller?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network programming", "Rationale": "The service abstraction layer facilitates communication between network service applications and components by providing a uniform interface and enabling interactions through various protocols, simplifying integration and event handling in the OpenDaylight SDN controller.", "Answer": "The service abstraction layer allows internal network service applications to communicate with each other. It allows controller components and applications to invoke each other\u2019s services and to subscribe to events they generate. This layer also provides a uniform abstract interface to the specific underlying communications protocols in the communication layer, including OpenFlow and SNMP."}
{"Question": "Describe how loops in paths can be detected in BGP.", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Routing and switching", "Rationale": "BGP detects loops by checking the AS path attribute. If a route advertisement includes the AS number of the receiving router, it signifies a loop, as the route has traversed through the same AS more than once.", "Answer": "Since full AS path information is available from an AS to a destination in BGP, loop detection is simple \u2013 if a BGP peer receives a route that contains its own AS number in the AS path, then using that route would result in a loop."}
{"Question": "Why would the token-ring protocol be inefficient if a LAN had a very large perimeter?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Network protocols and architectures", "Rationale": "The token-ring protocol becomes inefficient in a large LAN because the time required for a frame to circulate the ring (tprop) is long compared to the time to send and receive frames (L/R). This delays token release and reduces network efficiency.", "Answer": "When a node transmits a frame, the node has to wait for the frame to propagate around the entire ring before the node can release the token. Thus, if L/R is small as compared to tprop, then the protocol will be inefficient."}
{"Question": "As a mobile node gets farther and farther away from a base station, what are two actions that a base station could take to ensure that the loss probability of a transmitted frame does not increase?", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Wireless and mobile networks", "Rationale": "To counteract increased loss probability as a mobile node moves farther from a base station, increasing transmission power can enhance signal strength, and reducing transmission rate can improve reliability by extending time for error correction.", "Answer": "a) Increasing the transmission power\n b) Reducing the transmission rate"}
{"Question": "Describe the role of the beacon frames in 802.11.", "Level-1 Topic": "Computer networks", "Level-2 Topic": "Wireless and mobile networks", "Rationale": "In 802.11 networks, beacon frames broadcast by APs allow nearby stations to detect and identify the AP, facilitating network discovery and aiding in the organization and synchronization of wireless communication within the network.", "Answer": "APs transmit beacon frames. An AP\u2019s beacon frames will be transmitted over one of the 11 channels. The beacon frames permit nearby wireless stations to discover and identify the AP."}
{"Question": "What are three approaches that can be taken to avoid having a single wireless link degrade the performance of an end-to-end transport-layer TCP connection?", "Level-1 Topic": "Computer Network", "Level-2 Topic": "Transport Layer", "Rationale": "These methods are specific to enhancing network reliability and performance, directly addressing how to mitigate issues caused by wireless link degradation.", "Answer": "a) Local recovery\nb) TCP sender awareness of wireless links\nc) Split-connection approaches"}
{"Question": "The OSPF routing protocol uses a MAC rather than digital signatures to provide message integrity. Why do you think a MAC was chosen over digital signatures?", "Level-1 Topic": "Computer Network", "Level-2 Topic": "Network Layer", "Rationale": "OSPF uses a MAC instead of digital signatures because digital signatures require a complex PKI. With all routers in the same domain, a MAC with symmetric key distribution is simpler and sufficient for ensuring message integrity.", "Answer": "Digital signatures require an underlying Public Key Infrastructure (PKI) with certification authorities. For OSPF, all routers are in a same domain, so the administrator can easily deploy the symmetric key on each router, without the need of a PKI."}
{"Question": "What is the difference between end-to-end delay and packet jitter? What are the causes of packet jitter?", "Level-1 Topic": "Computer Network", "Level-2 Topic": "Network Layer", "Rationale": "End-to-end delay measures the total time for a packet to travel from source to destination, while packet jitter refers to the variation in this delay between successive packets. Causes of packet jitter include network congestion, route changes, and variable processing delays.", "Answer": "End-to-end delay is the time it takes a packet to travel across the network from source to destination. Delay jitter is the fluctuation of end-to-end delay from packet to the next packet."}
{"Question": "Register $t1 contains the address 0x10000100. Write the instruction (using lw) that loads the four bytes that precede this address into register $t2:", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Instruction set architecture", "Rationale": "The instruction lw $t2, -4($t1) uses an offset of -4 to load the four bytes located at the address immediately preceding 0x10000100 stored in $t1 into register $t2, correctly accessing the desired memory location.", "Answer": "lw $t2, -4($t1)"}
{"Question": "Assuming 12-bit 2\u2019s complement representation, the hexadecimal number FD2 is equal to the decimal number:", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Digital logic and boolean algebra", "Rationale": "The 12-bit 2's complement hexadecimal number FD2 is negative. Inverting the bits and adding 1 yields 46. Thus, FD2 equals -46 in decimal, confirming the answer is correct.", "Answer": "-46"}
{"Question": "It is common to \ufb01nd assembly code lines of the form\nxorl %edx,%edx\nin code that was generated from C where no Exclusive-Or operations were \npresent. Explain the effect of this particular Exclusive-Or instruction and what \nuseful operation it implements.", "Level-1 Topic": "Computer Architecture", "Level-2 Topic": "Instruction set architecture", "Rationale": "The xorl %edx, %edx instruction efficiently clears the %edx register by exploiting the property that any value XORed with itself yields zero. This corresponds to setting a variable to zero in C, optimizing performance over mov operations.", "Answer": "This instruction is used to set register %edx to zero, exploiting the property \nthat x ^ x = 0 for any x . It corresponds to the C statement x=0."}
{"Question": "The following code fragment occurs often in the compiled version of library \nroutines:\n1 call next\n2 next:\n3 popl %eax Explain why there is no matching ret instruction to this call.", "Level-1 Topic": "Computer Architecture", "Level-2 Topic": "Instruction set architecture", "Rationale": "This sequence uses call to push the address of next onto the stack and then popl to retrieve it into %eax. It avoids ret because the goal is to capture the instruction pointer, not return from a function.", "Answer": "This is not a true procedure call, since the control follows the same ordering \nas the instructions and the return address is popped from the stack."}
{"Question": "Have you ever wondered why a C main routine can end with a call to exit,a \n return statement, or neither, and yet the program still terminates properly? \n Explain.", "Level-1 Topic": "Operating systems", "Level-2 Topic": "Process management and scheduling", "Rationale": "The main function can terminate with or without a return statement, or by calling exit, as all methods ultimately invoke _exit. This transfers control back to the operating system, ensuring the program terminates correctly.", "Answer": "If main terminates with a return statement, then control passes back to \n the startup routine, which returns control to the operating system by calling \n _exit. The same behavior occurs if the user omits the return statement. If \n main terminates with a call to exit, then exit eventually returns control to \n the operating system by calling _exit. The net effect is the same in all three \n cases: when main has \ufb01nished, control passes back to the operating system."}
{"Question": "Describe a reference pattern that results in severe external fragmentation in an \n allocator based on simple segregated storage.", "Level-1 Topic": "Operating systems", "Level-2 Topic": "Memory management", "Rationale": "The described pattern causes severe external fragmentation because memory blocks are allocated in various size classes, but are never reclaimed or reused. The lack of coalescing leads to fragmented free space, preventing efficient memory utilization for future allocations.", "Answer": "Here is one pattern that will cause external fragmentation: The application makes \n numerous allocation and free requests to the \ufb01rst size class, followed by numer- \n ous allocation and free requests to the second size class, followed by numerous \n allocation and free requests to the third size class, and so on. For each size class, \n the allocator creates a lot of memory that is never reclaimed because the allocator \n doesn\u2019t coalesce, and because the application never requests blocks from that size \n class again."}
{"Question": "What are the general categories of operations in instruction set architectures like MIPS?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Instruction set architecture", "Rationale": "MIPS supports various operation categories, which enhance its versatility in handling different tasks. By focusing on these operations, MIPS achieves a streamlined design, making it efficient for pipelining and execution in RISC architectures.", "Answer": "The general categories of operations are data transfer, arithmetic logical, control, and floating point. MIPS exemplifies RISC architectures, emphasizing simplicity and efficient pipelining for performance."}
{"Question": "How do control flow instructions differ among MIPS, 80x86, and ARM regarding procedure calls and return addresses?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Instruction set architecture", "Rationale": "Understanding these differences helps in grasping how various ISAs implement control flow instructions, impacting efficiency and design choices in program execution and memory management.", "Answer": "MIPS and ARM store return addresses in a register, while 80x86 uses the stack in memory for return addresses, highlighting differences in their control flow handling."}
{"Question": "What are the three key aspects conveyed by a data dependence in pipelining and parallelism", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Pipelining and parallelism", "Rationale": "Understanding these aspects helps in managing hazards and optimizing execution order, crucial for maximizing instruction-level parallelism and enhancing overall performance in pipelined architectures.", "Answer": "A data dependence conveys the possibility of a hazard, the order of result calculations, and an upper bound on the potential parallelism that can be exploited."}
{"Question": "What is an antidependence between two instructions, and how does it affect instruction execution in pipelining?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Pipelining and parallelism", "Rationale": "Recognizing antidependences helps in scheduling instructions to avoid hazards, ensuring efficient execution in pipelined architectures and maximizing performance by reducing stalls and improving parallelism.", "Answer": "Antidependence occurs when instruction j writes to a register or memory location that instruction i reads, potentially causing hazards if not managed properly during execution."}
{"Question": "How does way prediction reduce conflict misses while maintaining the hit speed of direct-mapped caches?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Memory hierarchy", "Rationale": "This technique optimizes cache performance by minimizing delays associated with conflict misses, enhancing efficiency in accessing data while maintaining high hit speeds in direct-mapped cache architectures.", "Answer": "Way prediction reduces conflict misses by using extra bits to predict the block within a set, allowing early multiplexor selection and parallel tag comparison, thus improving access speed."}
{"Question": "How do the techniques of critical word first and early restart improve processor performance when handling cache misses?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Memory hierarchy", "Rationale": "By minimizing idle time during cache misses, these strategies optimize data retrieval processes, improving the efficiency of memory access and maintaining processor throughput.", "Answer": "Both techniques prioritize sending the requested word to the processor immediately, allowing execution to continue without waiting for the entire block, thus enhancing overall performance during cache misses."}
{"Question": "How do write buffers and write merging optimize performance in write-through and write-back caches?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Memory hierarchy", "Rationale": "These optimizations minimize delays caused by memory writes, enhancing overall system performance by ensuring the processor remains active and reducing the frequency of memory access operations.", "Answer": "Write buffers allow processors to continue executing while data is written to memory. Write merging combines new data with existing entries, reducing memory access and improving efficiency."}
{"Question": "What are the differences between register prefetch and cache prefetch in compiler-inserted prefetching instructions?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Memory hierarchy", "Rationale": "Understanding these distinctions helps in optimizing memory access patterns and managing potential exceptions, enhancing the efficiency of data retrieval processes in memory hierarchies.", "Answer": "Register prefetch loads data directly into a register, while cache prefetch loads data only into the cache. Both can be faulting or nonfaulting regarding exceptions."}
{"Question": "What factors have contributed to the increased importance of multiprocessing in modern computing?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Thread-Level Parallelism", "Rationale": "These factors highlight the shift towards multiprocessing as a scalable solution for performance enhancement, addressing the limitations of single-threaded processing in handling large datasets and parallel workloads.", "Answer": "Key factors include inefficiencies in exploiting ILP, growth in cloud computing, data-intensive applications, acceptable desktop performance, improved multiprocessor utilization, and the benefits of design replication."}
{"Question": "How do multiprocessors exploit thread-level parallelism, and what role does the operating system play in this process?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Thread-Level Parallelism", "Rationale": "This coordination allows efficient resource utilization and synchronization of threads, maximizing performance by enabling concurrent execution of multiple threads in shared memory environments.", "Answer": "Multiprocessors exploit thread-level parallelism through tightly coupled processors coordinated by a single operating system that manages shared memory and thread execution in a shared address space."}
{"Question": "What distinguishes multicomputers from multiprocessors and warehouse-scale systems, and what is their primary application?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Thread-Level Parallelism", "Rationale": "This distinction allows multicomputers to balance performance and flexibility, making them suitable for complex scientific tasks that demand significant parallel processing capabilities across distributed resources.", "Answer": "Multicomputers are less tightly coupled than multiprocessors but more so than warehouse-scale systems, primarily used for high-end scientific computation requiring substantial computational resources."}
{"Question": "What is distributed shared memory (DSM) in the context of multiprocessors, and why is it important for supporting larger processor counts?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Thread-Level Parallelism", "Rationale": "This design optimizes memory access for scalability, enabling efficient communication and performance for systems with many processors, thereby addressing bandwidth limitations inherent in centralized memory architectures.", "Answer": "Distributed shared memory (DSM) allows memory to be physically distributed among processors, ensuring sufficient bandwidth and reducing access latency for larger processor counts compared to centralized memory systems."}
{"Question": "How does a warehouse-scale computer (WSC) prevent downtime in the event of a power loss, and what systems are employed to maintain power continuity?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism", "Rationale": "The UPS system, comprising generators and batteries, prevents total WSC failure by providing immediate power during outages, with necessary equipment often housed separately to save space.", "Answer": "A WSC uses an uninterruptible power supply (UPS) with diesel engines, batteries, or flywheels to maintain power during outages, ensuring that IT equipment remains operational."}
{"Question": "What is power utilization effectiveness (PUE) and how is it calculated in the context of warehouse-scale computers (WSC)?", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism", "Rationale": "PUE is a critical metric for evaluating energy efficiency in large-scale data centers, directly tied to WSC design principles.", "Answer": "Power Usage Effectiveness (PUE) is a measure of how efficiently a data center uses energy. It compares the total energy used by the entire facility to the energy used just by the computing equipment. The formula is simple: PUE = Total energy used by the data center / Energy used by IT equipment. Why it matters: Lower PUE = less wasted energy = lower costs, Energy is about 40% of a big data center's expenses, Helps reduce environmental impact"}
{"Question": "PUE is a metric that measures datacenter efficiency, calculated as the total facility power divided by the IT equipment power.", "Level-1 Topic": "Computer architecture", "Level-2 Topic": "Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism", "Rationale": "A higher PUE indicates lower efficiency, reflecting how effectively a WSC utilizes power for IT equipment compared to overall facility power consumption.", "Answer": "Power Usage Effectiveness (PUE) is a key metric measuring data center energy efficiency, calculated by dividing total facility power consumption by IT equipment power consumption (PUE = Total Facility Power / IT Equipment Power), where an ideal PUE of 1.0 indicates all power goes to IT equipment. Typical hyperscale data centers achieve 1.1-1.3 PUE through optimization strategies like free cooling (using outside air), hot/cold aisle containment, high-voltage power distribution, and AI-driven thermal management, while conventional data centers range from 1.5-2.0. For warehouse-scale computers (WSCs), PUE critically impacts operational costs (energy constitutes ~40% of expenses) and influences design decisions like rack density, cooling system selection (liquid vs. air), and geographical siting based on ambient temperatures, with leading hyperscale operators like Google and Facebook achieving state-of-the-art PUEs around 1.1 through these advanced optimizations."}
{"Question": "Let X be a binomial random variable with mean Np and variance Np(1 \u2212 p). Show that the ratio X/N also has a binomial distribution with mean p and \nvariance p(1 \u2212 p)/N.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and statistics", "Rationale": "The mean and variance of r=X/N are correctly derived as p and p(1\u2212p)/N, respectively. However, X/N is not binomial but rather a scaled version of it. The binomial distribution is discrete, while X/N yields a continuous value between 0 and 1, which is not binomial in nature.", "Answer": "Let r = X/N. Since X has a binomial distribution, r also has the same distribution. The mean and variance for r can be computed as follows: Mean, E[r] = E[X/N] = E[X]/N = (Np)/N = p; Variance $\\begin{aligned} E\\left[(r-E[r])^2\\right] & =E\\left[(X / N-E[X / N])^2\\right] \\\\ & =E\\left[(X-E[X])^2\\right] / N^2 \\\\ & =N p(1-p) / N^2 \\\\ & =p(1-p) / N\\end{aligned}$"}
{"Question": "Suppose a crime has been committed. Blood is found at the scene for which there is \n no innocent explanation. It is of a type which is present in 1% of the population. The prosecutor claims: \u201cThere is a 1% chance that the defendant would have the crime blood type if he \n were innocent. Thus there is a 99% chance that he guilty\u201d. This is known as the prosecutor\u2019s fallacy. \n What is wrong with this argument?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and statistics", "Rationale": "The prosecutor's fallacy mistakenly equates a low probability of evidence under innocence with a high probability of guilt. Proper assessment should involve Bayes' theorem, accounting for prior probabilities, not just the rarity of the blood type.", "Answer": "The prosecutor's fallacy confuses the probability of finding the blood type if the defendant were innocent (1%) with the probability that the defendant is innocent given the blood type match. The correct interpretation requires considering the base rate of guilt, not just the rarity of the blood type. Even if the blood type is rare, it does not directly imply a high probability of guilt."}
{"Question": "Suppose a crime has been committed. Blood is found at the scene for which there is \nno innocent explanation. It is of a type which is present in 1% of the population. The defender claims: \u201cThe crime occurred in a city of 800,000 people. The blood type would be \nfound in approximately 8000 people. The evidence has provided a probability of just 1 in 8000 that \nthe defendant is guilty, and thus has no relevance.\u201d This is known as the defender\u2019s fallacy. What is \nwrong with this argument?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and statistics", "Rationale": "The defender\u2019s fallacy overlooks the importance of combining the blood type evidence with other factors. While the blood type is rare, its relevance depends on how it interacts with other evidence, which influences the overall probability of guilt.", "Answer": "The defender\u2019s fallacy mistakenly argues that the rarity of the blood type (1 in 8000) makes the evidence irrelevant. However, this ignores the base rate of the crime and other contextual factors. The blood type evidence needs to be evaluated in conjunction with all other evidence and the overall probability of guilt, not in isolation."}
{"Question": "Show that the variance of a sum is $\\operatorname{var}[X+Y]=\\operatorname{var}[X]+\\operatorname{var}[Y]+2 \\operatorname{cov}[X, Y]$, where $\\operatorname{cov}[X, Y]$ is the covariance between $X$ and $Y$", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and statistics", "Rationale": "To show that var[X+Y]=var[X]+var[Y]+2cov[X,Y], expand var[X+Y] using E[(X+Y)^2] and simplify with variance and covariance definitions.", "Answer": "Calculate this straightforwardly:\n $$\n \\begin{aligned}\n \\operatorname{var}[X+Y] & =\\mathbb{E}\\left[(X+Y)^2\\right]-\\mathbb{E}^2[X+Y] \\\\\n & =\\mathbb{E}\\left[X^2\\right]-\\mathbb{E}^2[X]+\\mathbb{E}\\left[Y^2\\right]-\\mathbb{E}^2[Y]+2 \\mathbb{E}[X Y]-2 \\mathbb{E}[X][Y] \\\\\n & =\\operatorname{var}[X]+\\operatorname{var}[Y]+2 \\operatorname{cov}[X, Y] .\n \\end{aligned}\n $$\n \n Using the definition of operators var, cov and the linearity of expectation should yield this result easily."}
{"Question": "How do you determine if a set of vectors is linearly independent?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Linear Algebra", "Rationale": "Linear independence means that the vectors do not lie in the same plane or line.", "Answer": "A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others."}
{"Question": "Explain the process of finding the inverse of a matrix.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Linear Algebra", "Rationale": "The inverse exists only if the determinant is non-zero, and row reduction provides a systematic method.", "Answer": "To find the inverse, use the formula A^(\u22121)=1/det(A)*\u200badj(A)  if det(A)!=0, or apply row reduction to transform the matrix into the identity matrix."}
{"Question": "What is the significance of eigenvalues and eigenvectors in linear algebra?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Linear Algebra", "Rationale": "They provide insights into the behavior of linear transformations.", "Answer": "Eigenvalues and eigenvectors reveal important properties of matrices, such as stability and modes of transformation, and are used in various applications like differential equations and quantum mechanics."}
{"Question": "Describe the Gram-Schmidt process.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Linear Algebra", "Rationale": "This process is used to simplify problems involving orthogonal projections and to construct orthonormal bases.", "Answer": "The Gram-Schmidt process orthogonalizes a set of vectors in an inner product space, producing an orthogonal (or orthonormal) set."}
{"Question": "How can you use matrix decomposition techniques in practical applications?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Linear Algebra", "Rationale": "These techniques simplify complex matrix operations and are essential in numerical analysis and machine learning.", "Answer": "Matrix decomposition techniques like LU, QR, and SVD are used in solving linear systems, optimizing computations, and analyzing data."}
{"Question": "These techniques simplify complex matrix operations and are essential in numerical analysis and machine learning.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and Statistics", "Rationale": "This ratio provides a measure of the likelihood of the event.", "Answer": "The probability is calculated as the ratio of the number of favorable outcomes to the total number of possible outcomes."}
{"Question": "Explain the difference between discrete and continuous random variables.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and Statistics", "Rationale": "The distinction affects how probabilities are calculated and interpreted.", "Answer": "Discrete random variables take on a countable number of distinct values, while continuous random variables take on an infinite number of values within a range."}
{"Question": "What is the Central Limit Theorem and why is it important?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and Statistics", "Rationale": "It justifies the use of normal distribution in inferential statistics.", "Answer": "The Central Limit Theorem states that the distribution of the sample mean approaches a normal distribution as the sample size grows, regardless of the population's distribution."}
{"Question": "How do you interpret a confidence interval?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and Statistics", "Rationale": "It quantifies the uncertainty in estimating population parameters.", "Answer": "A confidence interval provides a range of values within which the true population parameter is likely to fall, with a specified level of confidence."}
{"Question": "Describe the process of hypothesis testing.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Probability and Statistics", "Rationale": "It provides a systematic method for making inferences about population parameters.", "Answer": "Hypothesis testing involves formulating null and alternative hypotheses, calculating a test statistic, and comparing it to a critical value to decide whether to reject the null hypothesis."}
{"Question": "How do you find the critical points of a function?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Calculus and Optimization", "Rationale": "Critical points indicate where the function's slope is zero, which could be maxima, minima, or saddle points.", "Answer": "Critical points are found by setting the first derivative of the function to zero and solving for the variable."}
{"Question": "Explain the concept of a limit in calculus.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Calculus and Optimization", "Rationale": "Limits are fundamental in defining continuity, derivatives, and integrals.", "Answer": "A limit describes the value that a function approaches as the input approaches a certain point."}
{"Question": "What is the significance of the Fundamental Theorem of Calculus?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Calculus and Optimization", "Rationale": "It provides a way to evaluate definite integrals using antiderivatives.", "Answer": "The Fundamental Theorem of Calculus links differentiation and integration, showing that they are inverse processes."}
{"Question": "How do you use Lagrange multipliers to find constrained extrema?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Calculus and Optimization", "Rationale": "This method finds extrema of functions subject to equality constraints.", "Answer": "Lagrange multipliers involve setting up a system of equations using the gradient of the function and the gradient of the constraint, then solving for the variables."}
{"Question": "Describe the process of optimization in calculus.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Calculus and Optimization", "Rationale": "Optimization is used in various fields to find the best possible solutions under given conditions.", "Answer": "Optimization involves finding the maximum or minimum values of a function by analyzing its critical points and boundary values."}
{"Question": "How do you determine if a graph is connected?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Discrete Mathematics", "Rationale": "Connectivity indicates that all vertices are reachable from any other vertex.", "Answer": "How do you determine if a graph is connected?"}
{"Question": "Explain the concept of a combinatorial proof.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Discrete Mathematics", "Rationale": "It provides an intuitive and often simpler way to prove combinatorial identities.", "Answer": "A combinatorial proof involves demonstrating the equality of two expressions by counting the same set in different ways."}
{"Question": "What is the significance of Euler's formula in graph theory?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Discrete Mathematics", "Rationale": "It provides a fundamental relationship in the study of polyhedra and planar graphs.", "Answer": "Euler's formula V\u2212E+F=2 relates the number of vertices, edges, and faces in a planar graph."}
{"Question": "How do you use recurrence relations to solve problems in discrete mathematics?", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Discrete Mathematics", "Rationale": "They model many natural and computational processes.", "Answer": "Recurrence relations express sequences in terms of previous terms, and solving them involves finding a closed-form solution or using iterative methods."}
{"Question": "Describe the principle of mathematical induction.", "Level-1 Topic": "Mathematics", "Level-2 Topic": "Discrete Mathematics", "Rationale": "It is a powerful method for proving statements about integers.", "Answer": "Mathematical induction involves proving a base case and then showing that if the statement holds for an arbitrary case, it holds for the next case."}
{"Question": "In the RSA public-key encryption scheme, each user has a public key, e, and a private key, d. Suppose Bob leaks his private key d. Rather than generating a new modulus, he decides to generate a new public key and a new private key. Is this safe? Explain.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "If Bob leaks his private key but keeps n the same, an attacker can calculate \u03d5(n) and derive the new private key d\u2032 from the new public key e\u2032because the modulus n remains unchanged, preserving the relationship \u03d5(n).", "Answer": "No. If n = p * q is not changed, for any new e\u2019 not equal e, adversary will be able to calculate the new corresponding d\u2019 easily by the following steps:\n 1. Public key = (n, e) is known to adversary at any time. \n 2. After Bob leaks his secret key d, adversary will be able to obtain extra information and calculate phi(n). The number of possible values of phi(n) is greatly reduced, i.e., for k a positive integer, phi(n) = (e*d \u2013 1)/k and k < min{e, d}. By fixing e\u2019, the possible values of d\u2019 can be obtained. The adversary can simply check if m = (m^e\u2019)^d\u2019 mod n to determine whether phi (n) and d\u2019 is the correct one.\n 3. For any new public key e\u2019, the property for e\u2019 * d\u2019 = 1 mod phi (n) still holds, since Bob used this to generate his new d\u2019.\n 4. Since both phi(n) and e\u2019 are known, adversary can derive the value of new secret key d\u2019 easily by extended Euclidean algorithm"}
{"Question": "A bank uses a biometric system to authenticate employees entering the safe where the money is stored overnight. To get in the room, one has to type in the username and put his/her finger on the sensor. The fingerprint is then digitalized and sent to the authentication server, which accepts or rejects access to the room. The authentication server relates the username with the digital version of the fingerprint. Statistical analysis show that the authentication server has a false-reject rate of 10% and a false-accept rate of 0.5%. The user is allowed to try five attempts, after which security guards are called and the user is intercepted.\n Explain what false-accepts and false-rejects are. Are the above-mentioned rates suitable for this kind of application?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "False Acceptance occurs when unauthorized users are incorrectly granted access, while False Rejection happens when authorized users are denied. In critical applications like securing a safe, a low FAR is crucial to prevent unauthorized access, while a high FRR might be acceptable if users have multiple attempts to authenticate successfully.", "Answer": "If a non-authorized person is successfully authenticated to the server and given access to the room, we talk about False Acceptance. A false rejection means that the fingerprint of an authorized person (employee) was wrongly rejected as unauthorized. Since the False Acceptance Rate and the False Rejection Rate are negatively related (increasing one decreases the other and vice versa), it is essential in the current application that the FAR be much less than the FRR, and low enough in general. The fact that the user is allowed to attempt five times partially compensates for the given relatively high FRR."}
{"Question": "A bank uses a biometric system to authenticate employees entering the safe where the money is stored overnight. To get in the room, one has to type in the username and put his/her finger on the sensor. The fingerprint is then digitalized and sent to the authentication server, which accepts or rejects access to the room. The authentication server relates the username with the digital version of the fingerprint. Statistical analysis show that the authentication server has a false-reject rate of 10% and a false-accept rate of 0.5%. The user is allowed to try five attempts, after which security guards are called and the user is intercepted.\n If Tom finds a way to manipulate the fingerprint-reader as he wants, what interesting data would he be able to collect? How can he exploit what he collects?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Ethical hacking and penetration testing", "Rationale": "If an attacker manipulates the fingerprint-reader, they could collect digital fingerprints of employees. By using these stolen fingerprints, they could potentially bypass the authentication system, illustrating the need for robust security measures beyond just biometric data.", "Answer": "If Tom can hack the fingerprint-reader as he wants, then he could basically read and copy all digital fingerprints of the employees using this reader. Having the digital fingerprints available, he could then potentially counter the authentication server by directly sending the fingerprints of other employees and access the room."}
{"Question": "For Monoalphabetic Cipher, how can we obtain the plaintext if the key is unknown and brute force attack is not possible?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "When brute force is not feasible, statistical cryptanalysis is used for monoalphabetic ciphers. By analyzing letter frequencies and common patterns in the ciphertext, one can infer the likely letter mappings and recover the plaintext.", "Answer": "English texts contain letters with non-equal frequency of occurrence. \n Statistical cryptanalysis can be attempted to first locate frequently occurred letters, bigrams and trigrams and find out possible plaintext-letter-to-ciphertext-letter mappings (the key)."}
{"Question": "Consider a one-way authentication technique based on asymmetric encryption:\n A \u2192 B: IDA\n B \u2192 A: E(PKA, R)\n A \u2192 B: R\n Explain the protocol.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "This protocol verifies A\u2019s identity to B using asymmetric encryption. B encrypts a random challenge with A's public key, which only A can decrypt. When A returns the correct challenge, B is assured of A's identity.", "Answer": "A sends her ID to B. B uses A\u2019s public key to encrypt a random picked message R. This is a means of authenticating A to B. Only A can decrypt the second message, to recover R."}
{"Question": "Consider the following simple password-based authentication system. A secret key X is embedded inside the application software. The application software requires access to a file called userlist.txt for its functioning. userlist.txt is a text file recording the list of legitimate users. Each line in userlist.txt records the login credential of a user and is of the following format: USERID#SALT#SH#HMAC, where the character # is used as a separator. The fields are explained below.\n USERID: ID of the user\n SALT: A random string of 64 bits\n SH: Hash of the salt concatenated with the user\u2019s password. SHA-1 is used. \n HMAC: HMAC of salt concatenated with SH using X as the secret key. Again, SHA-1 is used and HMAC is 160 bits.\n How can the program verify the user\u2019s password?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Web application security", "Rationale": "To verify a user's password, the system retrieves the stored record and recalculates SH and HMAC based on the provided credentials. The user is authenticated if these recalculated values match the stored SH and HMAC, ensuring both correct password and integrity.", "Answer": "The user supplies a user id and a password PW. \n The system looks for the record for id of the form USERID#SALT#SH#HMAC. \n The system accepts the user if and only if \n SH = H(SALT||PW) and HMAC = H(SALT||SH||X)"}
{"Question": "Consider the following simple password-based authentication system. A secret key X is embedded inside the application software. The application software requires access to a file called userlist.txt for its functioning. userlist.txt is a text file recording the list of legitimate users. Each line in userlist.txt records the login credential of a user and is of the following format: USERID#SALT#SH#HMAC, where the character # is used as a separator. The fields are explained below.\n USERID: ID of the user\n SALT: A random string of 64 bits\n SH: Hash of the salt concatenated with the user\u2019s password. SHA-1 is used. \n HMAC: HMAC of salt concatenated with SH using X as the secret key. Again, SHA-1 is used and HMAC is 160 bits.\n What is the purpose of SALT? Compare the security if SH is only hash of the user\u2019s password", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Web application security", "Rationale": "SALT enhances security by ensuring unique hash values for identical passwords and preventing efficient attacks using precomputed hashes. Without SALT, attackers could use precomputed tables to quickly break multiple passwords, but SALT forces individual computations, slowing down such attacks.", "Answer": "Comparing to the case of storing the hash of password directly, the attacker can hash and check once for one user at a time only, instead of hashing once and checking against all users\u2019 hashes. Hence, salt slows down the attacker."}
{"Question": "Consider the following simple password-based authentication system. A secret key X is embedded inside the application software. The application software requires access to a file called userlist.txt for its functioning. userlist.txt is a text file recording the list of legitimate users. Each line in userlist.txt records the login credential of a user and is of the following format: USERID#SALT#SH#HMAC, where the character # is used as a separator. The fields are explained below.\n USERID: ID of the user\n SALT: A random string of 64 bits\n SH: Hash of the salt concatenated with the user\u2019s password. SHA-1 is used. \n HMAC: HMAC of salt concatenated with SH using X as the secret key. Again, SHA-1 is used and HMAC is 160 bits.\n Discuss the conditions under which this authentication system is secure.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Web application security", "Rationale": "The security of the authentication system depends on protecting userlist.txt, ensuring SHA-1\u2019s one-way property, and safeguarding the secret key X. Without access to userlist.txt, attacks are limited to online guessing. If the file is accessible, strong hashing and high entropy passwords defend against offline attacks. If the file can be modified, securing the secret key and using HMAC prevents unauthorized modifications.", "Answer": "Case I: Attacker cannot read userlist.txt. Then only online password guessing is possible. Restricted attempts should be sufficient.\n Case II: Attacker can read userlist.txt. Offline dictionary attack is possible. Require SHA-1 to be one-way and passwords are of high entropy.\n Case III: Attacker can modify userlist.txt. Attacker may create a new entry. In this case, we need to ensure the attacker cannot get X from the implementation. That is why we need HMAC = H(SALT||SH||X) to prevent the attacker from forging this value."}
{"Question": "In key exchange protocols, Perfect Forward Secrecy (PFS) is a security property that protects past sessions against future compromise of the private key(s). Explain why the Diffie-Hellman Key Exchange protocol satisfies PFS. State the assumption(s).", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Diffie-Hellman Key Exchange achieves Perfect Forward Secrecy by generating new private keys for each session, which ensures that past sessions remain secure even if current session keys are compromised. The security relies on using different private keys for different sessions and the assumption that private keys are not reused.", "Answer": "In DH Key exchange, Alice and Bob generate their private keys, a and b, randomly to calculate g^a mod p and g^b mod p respectively for each session. If a and b of one session \n are comprised, the attacker can only decrypt the messages of that session, but not those in the past, when another sets of a and b are used. Assumption: Alice and Bob must use a new a and b as their private keys respectively for each session. (It is ok to have new private keys generated because they are not used to encrypt/decrypt of the messages directly, but the generation of the session keys)"}
{"Question": "Let H be a one-way hash function. Alice chooses an initial seed K0. She computes Ki = H(Ki-1) for i = 1 to N, where N is a system parameter. Each Ki will be used as a one-time password.\n Which of the following approaches should be adopted for proper authentication of Alice? Describe the mechanism.\n 1. From K1 to KN\n 2. From KN to K1", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Using OTPs in reverse order (from KN to K1) ensures that each OTP can only be used once. The one-way nature of the hash function means knowing Ki allows an attacker to compute Ki+1 but not Ki\u22121. This ensures secure authentication while maintaining the integrity of the OTP sequence.", "Answer": "1. Anyone can compute Ki, given Ki-1. \n 2. Setup \u2013\n Alice sends KN to Bob. \n Bob initializes the database entry. \n Authentication \u2013\n Alice sends KN-1 to Bob. \n Bob checks if KN = H(KN-1).\n If yes, Bob stores KN-1 and discards KN."}
{"Question": "Define the following security terms: Disclosure", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "Disclosure is the act of revealing or exposing sensitive information without proper authorization, while unauthorized access refers to gaining entry to systems or data without permission. Disclosure involves making information accessible, not just accessing it.", "Answer": "Unauthorized access to information"}
{"Question": "Define the following security terms: Masquerading", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "Masquerading is when one entity pretends to be another to deceive systems or individuals. It involves impersonation, aiming to gain unauthorized access or privileges, often by mimicking legitimate credentials or identifiers.", "Answer": "Impersonation of one entity by another"}
{"Question": "Define the following security terms: Threat", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "A threat refers to any potential or actual event that can exploit vulnerabilities to cause harm or disruption to a system or its assets. It encompasses potential dangers that could compromise security, not just a violation.", "Answer": "A threat is a potential violation of security"}
{"Question": "What is the difference between security policy and security mechanism?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "A security policy defines the rules and guidelines for protecting assets, whereas a security mechanism implements and enforces these rules. The policy sets the standards, and the mechanism ensures they are followed, using tools, methods, or procedures.", "Answer": "A security policy is a statement of what is, and what is not allowed when assessing computer assets (e.g., programs, documents, hardware and services). \n A security mechanism is a method, tool, or procedure for enforcing a security policy. It can be technical or non-technical."}
{"Question": "What is Principle of Effectiveness in security principles?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "The Principle of Effectiveness asserts that security controls must be properly implemented and functional to effectively protect against threats. They should be efficient, user-friendly, and appropriately suited to the security needs they address.", "Answer": "Controls must be used properly to be effective\nControls should be efficient, easy to use and appropriate"}
{"Question": "In Playfair cipher, why are \u2018I\u2019 and \u2018J\u2019 put in the same slot of the key matrix?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "In the Playfair cipher, the 5x5 key matrix cannot fit all 26 letters, so 'I' and 'J' are combined in one slot. This choice minimizes confusion during decryption, as 'I' is common and 'J' is less frequent, making the combination practical.", "Answer": "The key matrix consists of only 25 slots and so two alphabets must be put in the same slot. \n I/J are normally be chosen to put in the same slot because I is a vowel and J is comparatively rare in English words. It introduces less ambiguity to the reader after decryption."}
{"Question": "Why is autokey cipher more resistant to statistical cryptanalysis than monoalphabetic cipher?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "The autokey cipher uses a changing key derived from the plaintext, unlike the fixed substitutions of the monoalphabetic cipher. This variability disrupts frequency patterns, making the autokey cipher more resistant to statistical cryptanalysis by producing a more varied frequency distribution.", "Answer": "This is because monoalphabetic cipher always maps one letter to another letter. The frequency distribution of the letter remains unchanged after encryption. For autokey cipher, the plaintext is used as part of the key when the master key is exhausted. One letter has a much higher chance to map to different letters, given different letter key. It gives a flatter frequency distribution of the ciphertext letters."}
{"Question": "Describe how stream cipher can approximate the property of one-time pad.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Stream ciphers approximate the one-time pad by generating a pseudorandom key stream from a given key. This key stream is used to encrypt the plaintext, mimicking the one-time pad's property of providing a random key for each bit of plaintext, though not achieving the same level of security.", "Answer": "Given a key as the input, the deterministic function generates a pseudorandom bit sequence that is to be used as the key stream to encrypt the plaintext. The pseudorandom bit sequence mimics the property of one-time-pad."}
{"Question": "What are the aims of confusion and diffusion in Feistel Cipher Structure?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Diffusion spreads plaintext patterns throughout the ciphertext to obscure statistical structures, while confusion makes the relationship between ciphertext and key intricate. Together, they enhance security by making decryption without the key more difficult.", "Answer": "Diffusion\ndissipates statistical structure of plaintext over bulk of ciphertext\n Confusion\n makes relationship between ciphertext and key as complex as possible"}
{"Question": "In Diffie-Hellman Key exchange, why is a passive eavesdropper unable to obtain the agreed secret key between Alice and Bob?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "The eavesdropper sees only public values Ya and Yb. Due to the discrete logarithm problem's complexity, they can't efficiently compute the shared secret g^ab mod\u2009p from these values alone.", "Answer": "The passive eavesdropper is only able to obtain Ya = g^a mod p and Yb = g^b mod p. \n Due to the difficulty of Discrete Log Problem (or Computational Diffie\u2013Hellman assumption). \n The attacker is unable to calculate efficiently g^ab mod p, which is the secret key."}
{"Question": "Under DES, The programmer adopts electronic codebook (ECB) as the mode of operation. Comment on the performance and security of his choice.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "ECB mode's parallelizable nature enhances performance, but its failure to obscure patterns in plaintext blocks weakens security, making it vulnerable to attacks that exploit repetitive structures in the encrypted data.", "Answer": "For performance, each block can be encrypted/decrypted separately and thus can be performed in a parallel manner.\n \n For security, the same plaintext always produces the same ciphertext. Statistical analysis is made easier if there exists non-evenly distributed occurrence of the plaintext information."}
{"Question": "The RSA key agreement protocol uses the recipient\u2019s public key to encrypt a session key. Under what condition is the RSA key agreement scheme susceptible to man_x0002_-in-the-middle attack? Explain.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "RSA key agreement is vulnerable to man-in-the-middle attacks when it lacks identity verification. An attacker can intercept and modify key exchanges, tricking both parties into using a session key controlled by the attacker.", "Answer": "If the session key is always generated by Alice or Bob. Trudy can intercept the encrypted key (or the digital envelop) but cannot extract the key generated by Alice (or Bob). However, if Trudy can fool Bob that he is Alice and Alice that she is Bob, he can trick them into using a session key of his choice. This is possible as the basic RSA key agreement protocol does not provide for the verification of the identities (i.e., authentication) of the participants."}
{"Question": "Assume an encryption algorithm in which the effort for the good guys (the ones that know the key) grows linearly with the bit length of the key, and for which the only way to break it is a brute-force attack of trying all possible keys. Then suppose advances in computer technology make computers twice as fast. Given that both the good guys and the bad guys get these faster computers, does this advance in computer speed work to the advantage of the good guys, the bad guys, or does it not make any difference? Illustrate using a concrete example.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Advancements in computer speed favor the good guys in a brute-force encryption scenario. Doubling the key length exponentially increases the number of possible keys, which makes brute-forcing impractical for the bad guys, even if both parties benefit from faster computers.", "Answer": "The advance in computer speed works to the advantage of the good guys. If the performance of the good guys grows linearly with the bit length of the key, then doubling the computer speed would allow for doubling the length of the key without any performance penalty. Doubling the length of the key would have a significant impact on the bad guys, since the number of keys that much be checked grows exponentially with the length of the key. \n For example, if the key were originally 8 bits, the bad guys would need to check 256 keys (2^8). If both good and bad guys get computers that run twice as fast, then, for the same amount of processing time, the good guys can use a 16-bit key and the bad guys can check 512 keys. However, because the number of keys grows exponentially, there will now be (2^16) or 65,536 keys. Checking this many keys will take the bad guys 128 times longer to than the original 8-bit key (65,536 / 512 = 128)."}
{"Question": "What is the difference between strong collision resistance and weak collision resistance for hash functions?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Weak collision resistance ensures that it's hard to find a collision for a specific input, whereas strong collision resistance ensures that it's hard to find any collision for the hash function overall. Strong collision resistance is a stricter requirement.", "Answer": "Weak collision resistant: For any given block x, it is computationally infeasible to find y\u2260x with H(y) = H(x).\n Strong collision resistant: It is computationally infeasible to find any pair (x, y) such that H(x) = H(y)."}
{"Question": "Consider the use of CRC (or checksum) to detect message errors in communication protocols. Can similar mechanism be used to detect message tampering (violation of message integrity) by an attacker? Explain.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "CRCs and checksums may not reliably detect message tampering because they can fail to catch changes that preserve the checksum value. Secure hash functions are designed to detect tampering by ensuring that even small changes in the message result in a different hash, making tampering detectable.", "Answer": "It depends on the schemes being used.\n Suppose a secure hash function is used to ensure message integrity. If the function is secure against collision-companion attack, it is infeasible for the attacker to find a new message with the same hash value as the message being attacked.\n \n However, in some CRC (or checksum) algorithms, only the total number of 1s or 0s in the message is checked. The attacker can alter the message so that the new tampered message still passes the CRC checking. For example, \n Original Message: 1010 -> Checksum = 2 (two \u20181\u2019 bits)\n Tampered Message: 0110 -> Checksum = 2 (two \u20181\u2019 bits)\n As we see from the above example, the checksum is not able to detect any tampering of the message."}
{"Question": "In what way does the public-key encrypted message hash provide a better digital signature than the public-key encrypted message?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "A public-key encrypted message hash is preferable for digital signatures because it involves encrypting a smaller message digest, which is computationally more efficient than encrypting the entire message.", "Answer": "A public-key signed message digest is \u201cbetter\u201d in that one need only encrypt (using the private key) a short message digest, rather than the entire message. Since public key encryption with a technique like RSA is expensive, it\u2019s desirable to have to sign (encrypt) a smaller amount of data than a larger amount of data."}
{"Question": "Briefly explain how the symmetric encryption can provide the confidentiality mechanisms.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "Symmetric encryption ensures confidentiality by using a shared secret key. The sender encrypts the message with this key, and only the receiver, who possesses the same key, can decrypt it. This means the message remains confidential between the communicating parties.", "Answer": "When both the sender and receiver share a secret key K, message M can be encrypted using this K. Thus, the message confidentiality is provided because only the receiver can decrypt it using the K."}
{"Question": "Briefly explain how the public-key encryption can provide the confidentiality and authentication \n mechanisms.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "NAN", "Answer": "The sender use the public key for encryption, only the receiver can use his private key for decryption. Thus, confidentiality is guaranteed. If the sender use his private key for encryption, the receiver can use sender\u2019s public key for decryption. Thus, authentication is guaranteed."}
{"Question": "Can the Diffie-Hellman algorithm provide the authentication mechanism? Show your reasons.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "NAN", "Answer": "The Diffie-Hellman algorithm cannot provide the authentication mechanism. It is used for key exchange. Both sides can get the same secret symmetric key for message encryption/decryption, but not for message authentication."}
{"Question": "Explain the collision resistance and one-wayness of cryptographic hash functions.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "NAN", "Answer": "Collision-resistance: Hard to find two messages (pre-images) hashes to the same hash (image)\n One-wayness: Hard to find the pre-image given an image"}
{"Question": "For Diffie-Hellman algorithm, given prime p, we need to choose g to be its primitive root. Please explain the meaning of the primitive root and the reason to choose such g.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "NAN", "Answer": "A primitive root of \ud835\udc5d and an integer \ud835\udc54: all the numbers from 1 to \ud835\udc5d \u2212 1 can be generated by repeatedly raising \ud835\udc54 to various powers modulo \ud835\udc5d. More formally, if \ud835\udc54 is a primitive \n root modulo \ud835\udc54, then for every number \ud835\udc4e in the range 1 to \ud835\udc5d \u2212 1, there exists an integer \ud835\udc58 such that \ud835\udc54^\ud835\udc58 = \ud835\udc4e ( mod \ud835\udc5d). By choosing a primitive root g, it ensures that the \n discrete logarithm of any residue modulo p can be expressed as a power of g. This property is crucial for the security of the algorithm."}
{"Question": "Explain the collision resistance and one-wayness of cryptographic hash functions.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography,Application Layer", "Rationale": "NAN", "Answer": "Collision-resistance: Hard to find two messages (pre-images) hashes to the same hash (image)\n One-wayness: Hard to find the pre-image given an image"}
{"Question": "What is the difference between a virus and a worm?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Malware analysis", "Rationale": "A virus needs user action to propagate, like opening an infected email, whereas a worm self-replicates, spreading through networks without human interaction, often exploiting vulnerabilities automatically.", "Answer": "Virus: Requires some form of human interaction to spread. Classic example: E-mail viruses. Worm: No user replication needed. Worm in infected host scans IP addresses and port numbers, looking for vulnerable processes to infect."}
{"Question": "Describe how a botnet can be created and how it can be used for a DDoS attack.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "A botnet is created by exploiting system vulnerabilities and infecting multiple hosts, which can then be remotely controlled by an attacker. For a DDoS attack, the botnet floods a target with overwhelming traffic, causing service disruption.", "Answer": "Creation of a botnet requires an attacker to find vulnerability in some application or system (e.g. exploiting the buffer overflow vulnerability that might exist in an application). After finding the vulnerability, the attacker needs to scan for hosts that are vulnerable. The target is basically to compromise a series of systems by exploiting that particular vulnerability. Any system that is part of the botnet can automatically scan its environment and propagate by exploiting the vulnerability. An important property of such botnets is that the originator of the botnet can remotely control and issue commands to all the nodes in the botnet. Hence, it becomes possible for the attacker to issue a command to all the nodes, that target a single node (for example, all nodes in the botnet might be commanded by the attacker to send a TCP SYN message to the target, which might result in a TCP SYN flood attack at the target)."}
{"Question": "How is cryptanalysis different from brute-force attack?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Cryptanalysis focuses on discovering vulnerabilities in cryptographic algorithms using analytical techniques, while a brute-force attack exhaustively attempts every possible key. The former often leverages knowledge of the algorithm, whereas the latter relies solely on sheer computational brute force.", "Answer": "Cryptanalysis involves analyzing and breaking cryptographic algorithms and protocols to find weaknesses or vulnerabilities that can be exploited. It often uses mathematical techniques and known plaintexts.\nBrute-force attack is a method where an attacker systematically tries every possible key until the correct one is found. It does not require knowledge of the algorithm's weaknesses."}
{"Question": "What is a one-way hash function?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "A one-way hash function transforms input data into a fixed-size hash, ensuring that the original data cannot be feasibly retrieved. This property enhances security by allowing data integrity checks without revealing the underlying input, crucial for applications like password storage.", "Answer": "A one-way hash function is a cryptographic function that takes an input and produces a fixed-size string of characters (the hash). It is designed to be irreversible, meaning that it is computationally infeasible to derive the original input from the hash."}
{"Question": "What advantage might elliptic curve cryptography (ECC) have over RSA?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Cryptography", "Rationale": "Elliptic Curve Cryptography (ECC) achieves equivalent security to RSA using much smaller key sizes, enhancing performance through faster computations. This efficiency reduces storage and bandwidth requirements, making ECC particularly advantageous for mobile and resource-constrained environments.", "Answer": "ECC offers the same level of security as RSA with smaller key sizes, resulting in faster computations and reduced storage and bandwidth requirements."}
{"Question": "What is the difference between data integrity and system integrity?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "Data integrity ensures the accuracy and consistency of data throughout its lifecycle, protecting it from unauthorized changes. In contrast, system integrity encompasses the reliable operation of hardware and software, ensuring systems function as intended without compromise or failure.", "Answer": "Data Integrity: Refers to the accuracy and consistency of data over its lifecycle, ensuring that data remains unaltered and reliable.\nSystem Integrity: Refers to the overall functioning of a system, ensuring that both hardware and software are operating as intended and have not been compromised"}
{"Question": "List and briefly define the kinds of threat consequences and the types of threat actions which cause these consequences.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "Threat consequences include loss of confidentiality, integrity, and availability. Unauthorized access leads to confidentiality loss, data manipulation causes integrity loss, and Denial of Service (DoS) attacks disrupt availability. Each threat action directly contributes to specific vulnerabilities and risks.", "Answer": "Threat Consequences:\nLoss of Confidentiality: Unauthorized access to sensitive information.\nLoss of Integrity: Unauthorized modification of data.\nLoss of Availability: Disruption of access to resources.\nThreat Actions:\nUnauthorized Access: Gaining access to systems or data without permission.\nData Manipulation: Altering data to mislead or cause damage.\nDenial of Service (DoS): Making services unavailable to legitimate users."}
{"Question": "Differentiate between a network attack surface and a software attack surface.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Network security", "Rationale": "The network attack surface comprises vulnerabilities across network components, such as open ports and services, while the software attack surface focuses on vulnerabilities within an application\u2019s code, libraries, and interfaces. Both surfaces represent different areas of potential exploitation.", "Answer": "Network Attack Surface: Refers to the total sum of vulnerabilities in a network that can be exploited, including open ports, services, and protocols.\nSoftware Attack Surface: Refers to the vulnerabilities within an application, including its code, libraries, and interfaces that can be exploited."}
{"Question": "List and briefly define the fundamental security design principles.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Web application security", "Rationale": "The fundamental security design principles include Least Privilege, ensuring minimal access; Defense in Depth, employing multiple security layers; Fail-Safe Defaults, defaulting to secure states; Separation of Duties, distributing critical tasks; and Accountability, logging actions for monitoring and responsibility.", "Answer": "Least Privilege: Users should have the minimum access necessary to perform their tasks.\nDefense in Depth: Implement multiple layers of security controls to protect systems.\nFail-Safe Defaults: Systems should default to a secure state, denying access unless explicitly allowed.\nSeparation of Duties: Divide critical tasks among different individuals to reduce risk.\nAccountability: Ensure that actions taken on a system are logged and monitored for accountability."}
{"Question": "Explain how the proactive password checker approach can improve password security.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Web application security", "Rationale": "A proactive password checker improves security by analyzing proposed passwords for common patterns, dictionary words, and previously compromised passwords. By rejecting weak options, it encourages users to create stronger, more secure passwords, thereby reducing the risk of unauthorized access.", "Answer": "A proactive password checker evaluates passwords against common patterns, dictionaries, and known compromised passwords before they are set. This helps users create stronger passwords by rejecting weak ones, thereby enhancing overall password security."}
{"Question": "Define the terms database, database management system, and query language.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Web application security", "Rationale": "A database is a structured collection of electronic data. A Database Management System (DBMS) is software facilitating database creation and management. A query language, like SQL, allows users to interact with databases by making structured queries to retrieve or manipulate data.", "Answer": "Database: A structured collection of data that is stored and accessed electronically.\nDatabase Management System (DBMS): Software that enables the creation, manipulation, and administration of databases.\nQuery Language: A formal language used to make queries in a database, such as SQL (Structured Query Language)."}
{"Question": "What is a Trojan horse attack?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Malware analysis", "Rationale": "A Trojan horse attack involves malware that masquerades as a legitimate application, tricking users into installation. Once executed, it can carry out harmful actions, such as data theft or granting unauthorized access, compromising system security and user privacy.", "Answer": "A Trojan horse attack is a type of malware that disguises itself as a legitimate program or file to deceive users into installing it. Once activated, it can perform malicious actions, such as stealing data or providing unauthorized access to the attacker."}
{"Question": "List a few characteristics to classify rootkits.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Malware analysis", "Rationale": "Rootkits are classified by their characteristics: Persistence ensures they remain on a system despite reboots, Stealthiness allows them to hide files and processes from detection, and Privilege Escalation enables exploitation of vulnerabilities for elevated access, facilitating undetected manipulation.", "Answer": "Persistence: Rootkits often employ techniques to maintain their presence on a system, surviving reboots and updates.\nStealthiness: They are designed to hide their presence, making detection difficult by concealing files, processes, and system modifications.\nPrivilege Escalation: Many rootkits exploit vulnerabilities to gain higher privileges, allowing them to manipulate the system undetected."}
{"Question": "Briefly describe the elements of a GD scanner.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Malware analysis", "Rationale": "A GD scanner includes a Signature Database for known malware patterns, Heuristic Analysis for identifying unknown threats, File System Monitoring for unauthorized changes, Process Monitoring for suspicious activities, and a Reporting Mechanism to alert users, enhancing overall malware detection and response.", "Answer": "A GD (Generic Detection) scanner typically includes the following elements:\n1.Signature Database: A repository of known malware signatures and patterns that the scanner uses to compare against files and processes.\n2.Heuristic Analysis: Techniques that analyze behavior and characteristics of files to identify potentially harmful software, even if it is not in the signature database.\n3.File System Monitoring: Continuous scanning of the file system to detect unauthorized changes or new file installations indicative of malware activity.\n4.Process Monitoring: Observing running processes for unusual behavior or known malicious activity patterns.\n5.Reporting Mechanism: Providing alerts and reports to users or administrators about detected threats and their potential impacts.\nThese elements work together to enhance the scanner's ability to detect and respond to various forms of malware, including rootkits."}
{"Question": "What functions can a professional code of conduct serve to fulfill?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Ethical hacking and penetration testing", "Rationale": "A professional code of conduct provides guidance for ethical behavior, establishes standards of practice, ensures accountability, builds trust, aids in conflict resolution, promotes professional development, and enhances public awareness. These functions collectively foster integrity and responsibility within the profession.", "Answer": "A professional code of conduct serves several important functions, including:\n1. Guidance: It provides clear guidelines for ethical behavior and decision-making within a profession, helping professionals navigate complex situations.\n2.Standards of Practice: The code establishes standards for acceptable behavior, which can help to maintain the integrity and reputation of the profession.\n3. Accountability: It holds professionals accountable for their actions, allowing for disciplinary measures in cases of misconduct or violations.\n4. Trust Building: By adhering to a code of conduct, professionals can build trust with clients, colleagues, and the public, fostering confidence in their services.\n5. Conflict Resolution: The code can offer frameworks for resolving ethical dilemmas and disputes, providing a basis for fair and consistent handling of issues.\n6. Professional Development: It encourages ongoing education and self-improvement, prompting professionals to stay informed about ethical standards and best practices.\n7. Public Awareness: A code of conduct can inform the public about the ethical commitments of a profession, enhancing transparency and understanding.\nThese functions collectively contribute to the professionalism and ethical standards within a field, promoting a culture of integrity and responsibility."}
{"Question": "How do \u201cThe Rules\u201d differ from a professional code of ethics?", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Ethical hacking and penetration testing", "Rationale": "\u201cThe Rules\u201d are specific, formal regulations focused on compliance and prescriptive behavior, often legally binding. In contrast, a professional code of ethics outlines broader moral principles, promoting flexibility in ethical decision-making and emphasizing values like integrity, beyond mere compliance.", "Answer": "\u201cThe Rules\u201d and a professional code of ethics differ in several key aspects:\n1. Nature and Purpose\nThe Rules: Typically refer to specific, often formalized regulations or guidelines that dictate acceptable behavior within a profession or organization. They are often legally binding and focus on compliance.\nProfessional Code of Ethics: A broader set of principles that outline the moral standards and ethical responsibilities of professionals. It emphasizes values such as integrity, honesty, and respect.\n2. Flexibility\nThe Rules: Generally less flexible and more prescriptive, providing clear dos and don\u2019ts that must be followed.\nProfessional Code of Ethics: More flexible, allowing for interpretation and adaptation to varying situations and ethical dilemmas.\n3. Focus\nThe Rules: Focus on specific behaviors and actions that are permissible or prohibited, often related to legal or organizational standards.\nProfessional Code of Ethics: Focuses on guiding professionals in making ethical decisions, promoting a culture of ethical awareness and responsibility.\n4. Scope\nThe Rules: Often limited to regulatory compliance and may not address broader ethical considerations.\nProfessional Code of Ethics: Broader in scope, addressing general principles that guide the conduct of professionals beyond mere compliance.\n5. Enforcement\nThe Rules: Typically enforced through specific penalties or disciplinary actions for violations.\nProfessional Code of Ethics: While violations can lead to disciplinary actions, enforcement is often based on peer review and self-regulation rather than formal legal repercussions.\nIn summary, while \u201cThe Rules\u201d provide specific guidelines for behavior, a professional code of ethics offers a framework of moral principles that help professionals navigate complex ethical situations."}
{"Question": "Explain the \u2018WannaCry\u2019 cyberattack considering it from an ethics-of-cybersecurity viewpoint.", "Level-1 Topic": "Cybersecurity", "Level-2 Topic": "Ethical hacking and penetration testing", "Rationale": "From an ethics-of-cybersecurity viewpoint, WannaCry highlights the responsibility of organizations to secure vulnerabilities and the ethical implications of exploiting discovered flaws. The incident underscores the need for transparency, accountability, and proactive measures to protect users from malicious cyber threats.", "Answer": "The WannaCry ransomware attack occurred in May 2017, affecting hundreds of thousands of computers worldwide. It exploited a vulnerability in Microsoft Windows, known as EternalBlue, which had been developed by the U.S. National Security Agency (NSA) and later leaked by a hacking group. The ransomware encrypted users' files and demanded payment in Bitcoin for decryption."}
{"Question": "Why eyes can perceive color?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Perception", "Rationale": "There are three different types of cone, each sensitive to a different color (blue, green and red).", "Answer": "The eye perceives color because the cones are sensitive to light of different wavelengths."}
{"Question": "A person\u2019s interaction with the outside world occurs through information being received and sent: input and output. Input in the human occurs mainly through the senses and output through the motor control of the effectors. What are the five major senses?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Perception", "Rationale": "There are five major senses: sight, hearing, touch, taste and smell. The first three are the most important to HCI. Taste and smell do not currently play a significant role in HCI,", "Answer": "There are five major senses: sight, hearing, touch, taste and smell."}
{"Question": "Ergonomics (or human factors) is traditionally the study of the physical characteristics of the interaction, which includes:", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Ergonomics", "Rationale": "Ergonomics (or human factors) is traditionally the study of the physical characteristics of the interaction: how the controls are designed, the physical environment in which the interaction takes place, and the layout and physical qualities of the screen.", "Answer": "How the controls are designed, the physical environment in which the interaction takes place, and the layout and physical qualities of the screen."}
{"Question": "Describe more about ergonomics and HCI.", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Ergonomics", "Rationale": "Ergonomics is a huge area, which is distinct from HCI but sits alongside it. Its contribution to HCI is in determining constraints on the way we design systems and suggesting detailed and specific guidelines and standards. Ergonomic factors are in general well established and understood and are therefore used as the basis for standardizing hardware designs.", "Answer": "Ergonomics is a huge area, which is distinct from HCI but sits alongside it. Its contribution to HCI is in determining constraints on the way we design systems and suggesting detailed and specific guidelines and standards. Ergonomic factors are in general well established and understood and are therefore used as the basis for standardizing hardware designs."}
{"Question": "How does adding an escape key to a dialog system complicate the State Transition Network (STN) and its hierarchical structure?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Dialogue", "Rationale": "Incorporating an escape key into an STN necessitates creating numerous transitions from every state to the main menu, complicating the diagram and potentially ruining the hierarchical organization of the dialog.", "Answer": "Adding an escape key requires connecting every state in the STN back to the main menu, which disrupts the hierarchical structure and introduces complexity in managing state transitions."}
{"Question": "What are the three approaches for linking dialog semantics to application functionality, and how does each approach address the relationship between dialog and presentation?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Dialogue", "Rationale": "Each approach provides a method for connecting dialog design with application functionality, either through specific semantic forms, code integration, or formal specifications, addressing how dialog elements are translated into actions.", "Answer": "The three approaches are:\n\nNotation-specific semantics: Uses special-purpose semantic forms within the dialog notation to define how dialog elements interact with application functionality.\nLinks to programming languages: Attaches pieces of programming code to the dialog to directly control application behavior based on user input.\nLinks to specification notations: Utilizes formal specification notations to define how dialog elements should function and interact with the application, ensuring consistency between dialog and semantics."}
{"Question": "Why is evaluation considered a continuous process throughout the design life cycle, and how does it relate to early design modifications and prototyping?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Evaluation", "Rationale": "Continuous evaluation integrates feedback throughout the design process, facilitating early changes and improvements. This iterative approach helps address issues early, leveraging both expert analysis and user participation to refine the design efficiently.", "Answer": "Evaluation should be continuous to ensure designs meet user requirements and expectations. It allows for early modifications based on feedback, using analytic and informal techniques to refine prototypes before extensive implementation."}
{"Question": "What are the three main goals of evaluation in the design of interactive systems?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Evaluation", "Rationale": "Evaluation aims to ensure the system functions as intended, meets user needs effectively, and addresses any issues, providing a comprehensive assessment of both performance and user experience.", "Answer": "The three main goals of evaluation are to assess the system's functionality and accessibility, to evaluate users\u2019 experience of the interaction, and to identify specific problems within the system."}
{"Question": "Why is it advantageous to perform the first evaluation of a system before implementation, and what are some limitations of expert analysis methods in evaluation?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Evaluation", "Rationale": "Early evaluation prevents expensive design errors by enabling adjustments before implementation. Expert analysis is useful but limited, as it assesses adherence to usability principles without evaluating actual user experience.", "Answer": "Performing the first evaluation before implementation helps avoid costly mistakes by allowing design alterations early. Expert analysis methods, while flexible and cost-effective, don\u2019t assess actual user interaction or real-world use."}
{"Question": "How can Cognitive Complexity Theory (CCT) be used to predict the difficulty of the interaction between a user\u2019s goals and a system\u2019s model?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Cognitive models", "Rationale": "By aligning hierarchical descriptions of user goals and system models, CCT identifies mismatches, which helps predict and address potential difficulties in user-system interactions.", "Answer": "CCT predicts interaction difficulty by comparing hierarchical descriptions of the user\u2019s goals and the system\u2019s model. Mismatches between these hierarchies indicate areas of potential dissonance."}
{"Question": "How can pre-existing manual procedures be used to improve goal hierarchies in Cognitive Complexity Theory (CCT), and what example illustrates the importance of goal closure?", "Level-1 Topic": "Human-computer interaction", "Level-2 Topic": "Cognitive models", "Rationale": "Using real procedures to build goal hierarchies ensures they reflect user behavior accurately. The ATM case demonstrates that failing to complete all subgoals can lead to significant user issues.", "Answer": "Pre-existing manual procedures can create natural goal hierarchies for CCT, offering early insights into user goals. The ATM example highlights the need for goal closure to avoid user errors, like forgetting cards."}
{"Question": "Assume that there is a classification problem with $6$ classes. Each instance has $8$ features. What is the total number of parameters, if you are solving it by using linear logistic regression and one-vs-all approach? Remember to include $\\theta_0$.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Supervised learning", "Rationale": "Each instance has 8 features. A logistic regression model has one parameter for each feature plus one bias term \u03b80\\theta_0\u03b80\u200b. Thus, for one class, we have: Number\u00a0of\u00a0parameters=8(features)+1(\u03b80)=9. So in total we have 6*9 = 54", "Answer": "54"}
{"Question": "The following attributes are measured for members of a herd of Asian ele- \nphants: weight, height, tusk length, trunk length, and ear area. Based on these \nmeasurements, what sort of similarity measure would you \nuse to compare or group these elephants? Justify your answer and explain \nany special circumstances.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Data mining", "Rationale": "The attributes are numerical with varying scales, requiring standardization before applying Euclidean distance. Cosine and correlation measures are unsuitable due to their focus on angle or linear relationships rather than magnitude. Euclidean distance properly captures the magnitude differences.", "Answer": "These attributes are all numerical, but can have widely varying ranges of values, depending on the scale used to measure them. Furthermore, the attributes are not asymmetric and the magnitude of an attribute matters. These \nlatter two facts eliminate the cosine and correlation measure. Euclidean distance, applied after standardizing the attributes to have a mean of 0 and a \nstandard deviation of 1, would be appropriate."}
{"Question": "You are given a set of m ob jects that is divided into K groups, where the ith \ngroup is of size mi . If the goal is to obtain a sample of size n<m, what is \nthe di\ufb00erence between the following two sampling schemes? (Assume sampling \nwith replacement.) (a) We randomly select n \u2217 mi /m elements from each group.\n(b) We randomly select n elements from the data set, without regard for the \ngroup to which an ob ject belongs.", "Level-1 Topic": "Data Science", "Level-2 Topic": "Sampling Techniques", "Rationale": "The first sampling scheme ensures proportional representation from each group, while the second is random, resulting in variable group representation. The first scheme is deterministic, whereas the second scheme is probabilistic, guaranteeing only expected proportionality.", "Answer": "The first scheme is guaranteed to get the same number of objects from each \ngroup, while for the second scheme, the number of objects from each group \nwill vary. More specifically, the second scheme only guarantees that, on average, the number of objects from each group will be n \u2217 mi/m."}
{"Question": "Consider a document-term matrix, where tfij is the frequency of the ith word \n(term) in the j th document and m is the number of documents. Consider the \nvariable transformation that is de\ufb01ned by $t f_{i j}^{\\prime}=t f_{i j} * \\log \\frac{m}{d f_i}$ where df i is the number of documents in which the ith term appears, which \nis known as the document frequency of the term. This transformation is \nknown as the inverse document frequency transformation. What is the e\ufb00ect of this transformation if a term occurs in one document? \nIn every document? What might be the purpose of this transformation?", "Level-1 Topic": "Information Retrieval", "Level-2 Topic": "Tf-idf", "Rationale": "The inverse document frequency (IDF) transformation reduces the weight of common terms (present in all documents) to zero, emphasizing rare terms that better distinguish documents. This enhances information retrieval by focusing on uniquely informative words.", "Answer": "Terms that occur in every document have 0 weight, while those that \noccur in one document have maximum weight, i.e., log m. This normalization reflects the observation that terms that occur in \nevery document do not have any power to distinguish one document \nfrom another, while those that are relatively rare do."}
{"Question": "Describe the steps involved in data mining when viewed as a process of knowledge discovery.", "Level-1 Topic": "Data Science", "Level-2 Topic": "Knowledge discovery", "Rationale": "The steps outlined cover the full process of knowledge discovery in data mining, from initial data cleaning to presenting the final results. This comprehensive approach ensures that data is prepared, analyzed, and interpreted effectively, leading to actionable insights and patterns.", "Answer": "\u2022 Data cleaning, a process that removes or transforms noise and inconsistent data\n \u2022 Data integration, where multiple data sources may be combined\n \u2022 Data selection, where data relevant to the analysis task are retrieved from the database\n \u2022 Data transformation, where data are transformed or consolidated into forms appropriate \n for mining\n \u2022 Data mining, an essential process where intelligent and e\ufb03cient methods are applied in \n order to extract patterns\n \u2022 Pattern evaluation, a process that identi\ufb01es the truly interesting patterns representing \n knowledge based on some interestingness measures\n \u2022 Knowledge presentation, where visualization and knowledge representation techniques are \n used to present the mined knowledge to the user"}
{"Question": "How is a quantile-quantile plot di\ufb00erent from a quantile plot?", "Level-1 Topic": "Data Science", "Level-2 Topic": "Statistical Plotting", "Rationale": "A quantile plot shows the percentage of values below a given value in a single distribution, while a quantile-quantile plot compares quantiles from two distributions. The latter helps assess how well the distributions match by plotting one distribution's quantiles against another's.", "Answer": "Quantile plot is a graphical method used to show the approximate percentage of values below \n or equal to the independent variable in a univariate distribution. Thus, it displays quantile \n information for all the data, where the values measured for the independent variable are plotted \n against their corresponding quantile.\n A quantile-quantile plot however, graphs the quantiles of one univariate distribution against the \n corresponding quantiles of another univariate distribution. Both axes display the range of values \n measured for their corresponding distribution, and points are plotted that correspond to the \n quantile values of the two distributions. A line (y = x) can be added to the graph along with \n points representing where the \ufb01rst, second and third quantiles lie, in order to increase the graph\u2019s \n informational value. Points that lie above such a line indicate a correspondingly higher value for \n the distribution plotted on the y-axis, than for the distribution plotted on the x-axis at the same \n quantile. The opposite e\ufb00ect is true for points lying below this line."}
{"Question": "Brie\ufb02y outline how to compute the dissimilarity between objects described by ratio-scaled variables.", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data Analytics", "Rationale": "", "Answer": "\u2022 Treat ratio-scaled variables as interval-scaled variables, so that the Minkowski, Manhattan, or \n Euclidean distance can be used to compute the dissimilarity.\n \u2022 Apply a logarithmic transformation to a ratio-scaled variable f having value xif for ob ject i by \n using the formula y_if = log(x_if ). The y_if values can be treated as interval-valued,\n \u2022 Treat x_if as continuous ordinal data, and treat their ranks as interval-scaled variables."}
{"Question": "ChiMerge [Ker92] is a supervised, bottom-up (i.e., merge-based) data discretization method. It relies \n on \u03c7 2 analysis: adjacent intervals with the least \u03c72 values are merged together till the chosen stopping \n criterion satis\ufb01es. Brie\ufb02y describe how ChiMerge works.", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data Analytics", "Rationale": "", "Answer": "The ChiMerge algorithm consists of an initialization step and a bottom-up merging process, where \n intervals are continuously merged until a termination condition is met. Chimerge is initialized by \n \ufb01rst sorting the training examples according to their value for the attribute being discretized and \n then constructing the initial discretization, in which each example is put into its own interval (i.e., \n place an interval boundary before and after each example). The interval merging process contains \n two steps, repeated continuously: (1) compute the \u03c72 value for each pair of adjacent intervals, (2) \n merge (combine) the pair of adjacent intervals with the lowest \u03c72 value. Merging continues until \n a prede\ufb01ned stopping criterion is met."}
{"Question": "Suppose that frequent itemsets are saved for a large transactional database, DB . Discuss how to \n e\ufb03ciently mine the (global) association rules under the same minimum support threshold, if a set of new transactions, denoted as \u2206DB , is (incrementally) added in ?", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data mining", "Rationale": "The response outlines an efficient approach for mining association rules when new transactions are incrementally added to a database. This involves updating counts of itemsets based on the new data and re-evaluating their frequency. ", "Answer": "\u2022 For itemsets that are frequent in DB , scan \u2206DB once and add their counts to see if they are still \n frequent in the updated database.\n \u2022 For itemsets that are frequent in \u2206DB but not in DB , scan DB once to add their counts to see \n if they are frequent in the updated DB."}
{"Question": "Why is tree pruning useful in decision tree induction?", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Decision Trees", "Rationale": "Tree pruning is used in decision tree induction to improve the generalization of the model by removing branches that may overfit the training data. This involves statistical measures to eliminate unreliable branches, resulting in a more compact and reliable tree. ", "Answer": "There could be too many branches, some of which may re\ufb02ect anomalies in the training data due to noise or outliers. Tree pruning addresses this \n issue of over\ufb01tting the data by removing the least reliable branches (using statistical measures). This \n generally results in a more compact and reliable decision tree that is faster and more accurate in its \n classi\ufb01cation of data."}
{"Question": "Given a decision tree, you have the option of (a) converting the decision tree to rules and then pruning \n the resulting rules, or (b) pruning the decision tree and then converting the pruned tree to rules. What \n advantage does (a) have over (b)?", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Decision Trees", "Rationale": "Method (a) allows for more flexible pruning by removing specific preconditions of rules rather than entire subtrees, as in method (b). This approach can be less restrictive and potentially more nuanced, leading to better handling of overfitting and enhancing model performance.", "Answer": "If pruning a subtree, we would remove the subtree completely with method (b). However, with method \n (a), if pruning a rule, we may remove any precondition of it. The latter is less restrictive."}
{"Question": "It is important to calculate the worst-case computational complexity of the decision tree algorithm. \n Given data set D, the number of attributes n, and the number of training tuples |D|, show that the \n computational cost of growing a tree is at most n \u00d7 |D| \u00d7 log (|D|).", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Decision Trees", "Rationale": "The response accurately outlines the worst-case computational complexity of the decision tree algorithm by breaking down the steps involved in growing the tree. It considers the depth of the tree and the cost associated with each level, combining these factors to derive the overall complexity as O(n\u00d7\u2223D\u2223\u00d7log(\u2223D\u2223)). ", "Answer": "The worst-case scenario occurs when we have to use as many attributes as possible before being able \n to classify each group of tuples. The maximum depth of the tree is log (|D|). At each level we will \n have to compute the attribute selection measure O(n) times (one per attribute). The total number of \n tuples on each level is |D| (adding over all the partitions). Thus, the computation per level of the tree \n is O(n \u00d7 |D|). Summing over all of the levels we obtain O(n \u00d7 |D| \u00d7 log (|D|))."}
{"Question": "Outline methods for addressing the class imbalance problem. Suppose a bank would like to develop \n a classi\ufb01er that guards against fraudulent credit card transactions. Illustrate how you can induce a \n quality classi\ufb01er based on a large set of non-fraudulent examples and a very small set of fraudulent \n cases.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Algorithm", "Rationale": "The response outlines methods for addressing class imbalance, specifically tailored for a scenario involving fraudulent credit card transactions. It discusses techniques like oversampling, undersampling, threshold-moving, and ensemble methods, which are commonly used in machine learning to handle imbalanced datasets.", "Answer": "The \ufb01rst one is oversampling, which works by resampling the positive tuples so that the resulting \n training set contains an equal number of positive and negative tuples. So for the data in the question, \n we can replicate tuples of the fraudulent cases to form a training set with size comparable to non- \n fraudulent examples.\n The second one is undersampling, which works by decreasing the number of negative tuples. It \n randomly eliminates tuples from the ma jority (negative) class until there are an equal number of ositive and negative tuples. Thus for the fraudulent credit card transactions problem, we need to \n randomly remove the non-fraudulent examples until the size is equal to the fraudulent cases.\n The third one is threshold-moving, which applies to classi\ufb01ers that, given an input tuple, return a \n continuous output value. Rather than manipulating the training tuples, this method returns classi\ufb01- \n cation decision based on the output values. By tuning the threshold of the output value deciding the \n exact predictions of the input, rare class tuples like the fraudulent cases in the question are easier to \n classify. And hence, there is less chance of costly false negative errors. In other words, the classi\ufb01er \n will hardly misclassify the fraudulent cases.\n The \ufb01nal one is Ensemble methods, applied to algorithms like boosting and random forest. The \n individual classi\ufb01ers within these algorithms may include versions of the approaches described above, \n such as oversampling and threshold-moving."}
{"Question": "The support vector machine (SVM) is a highly accurate classi\ufb01cation method. However, SVM classi\ufb01ers \n su\ufb00er from slow processing when training with a large set of data tuples. Discuss how to overcome this \n di\ufb03culty and develop a scalable SVM algorithm for e\ufb03cient SVM classi\ufb01cation in large datasets.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Algorithm", "Rationale": "Support Vector Machine (SVM) classification involves using microclusters, which are created with a CF-Tree, and training the SVM on these microclusters' centroids to reduce computational complexity. By focusing on representative data points and iteratively refining the model with additional entries, the approach effectively manages large datasets and improves processing efficiency. ", "Answer": "1. Construct the microclusters using a CF-Tree.\n 2. Train an SVM on the centroids of the microclusters.\n 3. Decluster entries near the boundary.\n 4. Repeat the SVM training with the additional entries.\n 5. Repeat the above until convergence."}
{"Question": "Compare and contrast associative classi\ufb01cation and discriminative frequent pattern-based classi\ufb01cation. \n Why is classi\ufb01cation based on frequent patterns able to achieve higher classi\ufb01cation accuracy in many \n cases than a classical decision-tree method?", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Algorithm", "Rationale": "", "Answer": "Association-based classi\ufb01cation is a method where association rules are generated and analyzed for \n use in classi\ufb01cation. We \ufb01rst search for strong associations between frequent patterns (conjunctions of \n attribute-value pairs) and class labels. Using such strong associations we classify new examples. \n Discriminative frequent pattern-based classi\ufb01cation \ufb01rst \ufb01nds discriminative frequent patterns accord- \n ing to certain measures such as pattern length and pattern frequency and uses these patterns as \n features. Then it trains classi\ufb01ers based on these features.\n Frequent pattern-based classi\ufb01cation can achieve higher accuracy than a classical decision tree because \n it overcomes the constraint of decision trees, which consider only one attribute at a time. Some frequent \n patterns could have very high discriminative power because they combine multiple attributes."}
{"Question": "Compare the advantages and disadvantages of eager classi\ufb01cation (e.g., decision tree, Bayesian, neural \n network) versus lazy classi\ufb01cation (e.g., k -nearest neighbor, case-based reasoning).", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Algorithm", "Rationale": "Eager classification (e.g., decision trees, Bayesian, neural networks) benefits from faster classification and the ability to assign attribute weights, but it can be less flexible and requires more training time. In contrast, lazy classification (e.g., k-nearest neighbor, case-based reasoning) benefits from a richer hypothesis space and quicker training but suffers from higher storage costs, slower classification times, and potential issues with attribute weighting.", "Answer": "Eager classi\ufb01cation is faster at classi\ufb01cation than lazy classi\ufb01cation because it constructs a general- \n ization model before receiving any new tuples to classify. Weights can be assigned to attributes, which \n can improve classi\ufb01cation accuracy. Disadvantages of eager classi\ufb01cation are that it must commit to \n a single hypothesis that covers the entire instance space, which can decrease classi\ufb01cation, and more \n time is needed for training.\n Lazy classi\ufb01cation uses a richer hypothesis space, which can improve classi\ufb01cation accuracy. It \n requires less time for training than eager classi\ufb01cation. A disadvantages of lazy classi\ufb01cation is that all \n training tuples need to be stored, which leads to expensive storage costs and requires e\ufb03cient indexing \n techniques. Another disadvantage is that it is slower at classi\ufb01cation because classi\ufb01ers are not built \n until new tuples need to be classi\ufb01ed. Furthermore, attributes are all equally weighted, which can \n decrease classi\ufb01cation accuracy. (Problems may arise due to irrelevant attributes in the data.)"}
{"Question": "Both k-means and k -medoids algorithms can perform e\ufb00ective clustering. Illustrate the strength and weakness of k -means in comparison with the k -medoids algorithm.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Clustering", "Rationale": "K-means is noted for its efficiency but is sensitive to noise and outliers, which can degrade the quality of clustering. In contrast, k-medoids is more robust to noise and outliers but generally less efficient. ", "Answer": "k-means vs. k medoids: more e\ufb03cient but quality deteriorates by noise and outliers."}
{"Question": "Why is it that BIRCH encounters di\ufb03culties in \ufb01nding clusters of arbitrary shape but OPTICS does not? Can you propose some modi\ufb01cations to BIRCH to help it \ufb01nd clusters of arbitrary shape?", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Clustering", "Rationale": "It contrasts this with OPTICS, which uses a density-based approach, allowing for the detection of arbitrary shapes. The proposed modification for BIRCH to adopt a density-based and connectivity-based distance measure is logical as it aligns with the strategy that enables OPTICS to handle arbitrary cluster shapes effectively.", "Answer": "BIRCH uses Euclidean distance and inter-cluster proximity as distance measure, which leads to spher- \n ical shaped clusters.\n OPTICS uses density-based (connectivity) measure, which grows clusters based on the connected points \n within a de\ufb01ned radius, and thus can \ufb01nd arbitrary shaped clusters. There could be several ways to modify BIRCH. One is to use density-based and connectivity-based dis- \n tance measure to cluster low-level B+-trees and build levels of CF-tree which will lead to connectivity- \n based (arbitrary-shaped) cluster."}
{"Question": "Traditional clustering methods are rigid in that they require each ob ject to belong exclusively to only \n one cluster. Explain why this is a special case of fuzzy clustering. You may use k -means as an example.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Clustering", "Rationale": "In traditional clustering, each object is assigned entirely to a single cluster, resulting in a partition matrix where each object has a membership value of 1 for one cluster and 0 for others. This matrix still meets the criteria for a partition matrix in fuzzy clustering,", "Answer": "k -means can be regarded as a special case of fuzzy clustering. The corresponding partition matrix is \n that, if ob ject oi belongs to cluster Cj , then wij = 1 and the other entries on the i-th row are 0. It \n is easy to verify that the partition matrix M de\ufb01ned as such satis\ufb01es all the three requirements for a \n partition matrix in fuzzy clustering."}
{"Question": "In a large sparse graph where on average each node has a low degree, is the similarity matrix using \n SimRank still sparse? If so, in what sense? If not, why? Deliberate on your answer.", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data Analytics", "Rationale": "This acknowledges that practical approximations can lead to sparsity by setting small values to zero, particularly when the graph's k-neighborhoods are small, which is common in graphs with low average degree. ", "Answer": "Theoretically, even when the average degree is low, as long as the graph is connected, the similarity \n matrix is not sparse at all, since every entry in the matrix has a non-zero value.\n When an approximation is used wherein entries of very small values are set to 0, the similarity matrix \n using SimRank may be sparse when the average degree is low. In fact, the sparsity of the matrix is \n related to the average size of the k -neighborhoods of nodes, since if two nodes are k steps or more \n away, the SimRank between them can be set to 0, where k is a small number, such as 2 or 3."}
{"Question": "Compare the SCAN algorithm with DBSCAN. What are their similarities \n and di\ufb00erences?", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data Analytics", "Rationale": "Both algorithms are density-based and use \u03f5-neighborhoods and core objects to grow clusters, a key feature of density-based clustering methods.  SCAN is designed for graph data and can identify hubs and outliers, while DBSCAN is tailored to point data and does not identify hubs. ", "Answer": "Similarities: Both methods are density-based, and follow similar frameworks. Both use the mecha- \n nisms of \u03f5-neighborhoods and core ob jects/vertices to grow clusters.\n Di\ufb00erences: The similarity between points in DBSCAN and that between vertices in SCAN are dif- \n ferent, due to the di\ufb00erent types of data. SCAN identi\ufb01es hubs and outliers. DBSCAN does not \n \ufb01nd any hub points."}
{"Question": "What is the di\ufb00erence between row scalability and column scalability ?", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data Analytics", "Rationale": "Row scalability refers to the system's ability to handle an increase in the number of rows without a disproportionate increase in execution time, while column scalability pertains to the system's ability to handle an increase in the number of columns linearly. ", "Answer": "A data mining system is row scalable if, when the number of rows is enlarged 10 times, it takes \n no more than 10 times to execute the same data mining queries.\n A data mining system is column scalable if the mining query execution time increases approx- \n imately linearly with the number of columns (i.e., attributes or dimensions)."}
{"Question": "Discuss why a document-term matrix is an example of a dataset that has asymmetric discrete or asymmetric continuous features.", "Level-1 Topic": "Data Science", "Level-2 Topic": "Data Analytics", "Rationale": "It correctly identifies that a document-term matrix is an example of a dataset with asymmetric discrete features because most terms do not appear in most documents, leading to many zero entries that are not meaningful for comparison. The explanation also discusses how applying TFIDF and normalization techniques converts the matrix into one with asymmetric continuous features, yet the asymmetry persists due to the presence of zeros that remain uninformative.", "Answer": "The ijth entry of a document-term matrix is the number of times that term \n j occurs in document i. Most documents contain only a small fraction of \n all the possible terms, and thus, zero entries are not very meaningful, either \n in describing or comparing documents. Thus, a document-term matrix has \n asymmetric discrete features. If we apply a TFIDF normalization to terms \n and normalize the documents to have an L2 norm of 1, then this creates a \n term-document matrix with continuous features. However, the features are \n still asymmetric because these transformations do not create non-zero entries \n for any entries that were previously 0, and thus, zero entries are still not very \n meaningful."}
{"Question": "Consider the problem of \ufb01nding the K nearest neighbors of a data object. A \n programmer designs Algorithm for this task. \n1: for i =1 to number of data objects do\n 2: Find the distances of the ith object to all other objects.\n 3: Sort these distances in decreasing order.\n (Keep track of which ob ject is associated with each distance.)\n 4: return the ob jects associated with the \ufb01rst K distances of the sorted list \n 5: end for$ Describe the potential problems with this algorithm if there are duplicate \n ob jects in the data set. Assume the distance function will only return a \n distance of 0 for ob jects that are the same.", "Level-1 Topic": "Machine Learning", "Level-2 Topic": "Algorithm", "Rationale": "It correctly points out that the order of duplicates in the nearest neighbor list may be inconsistent and dependent on implementation details, potentially leading to non-deterministic results. The response also mentions that if duplicates are abundant, the nearest neighbor list may be dominated by them, which could be undesirable in many contexts. Lastly, it accurately identifies that an object may not be listed as its own nearest neighbor. ", "Answer": "First, the order of duplicate objects on a nearest neighbor list will depend on details of the algorithm and the \n order of objects in the data set. Second, if there are enough duplicates, \n the nearest neighbor list may consist only of duplicates. Third, an object \n may not be its own nearest neighbor."}
{"Question": "In what cases can the intercept \ud835\udc4f in the linear regression $$f(x)=w^{\\mathrm{T}}+b$$ be ignored?", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Supervised learning", "Rationale": "Standardization to zero mean implies \ud835\udc4f is not needed to fit the data.", "Answer": "When the feature values are standardized to have zero mean, the intercept \ud835\udc4f can be ignored."}
{"Question": "In generalized linear models, does the link function need to be linear? If not, what conditions must it satisfy? Does log-linear regression belong to generalized linear models?", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Supervised learning", "Rationale": "Log-linear regression is a special case of generalized linear models where the link function is $$g(\\dot{})=ln(\\dot{} )$$.", "Answer": "The link function does not need to be linear; it must be a monotonic and continuous function. This targets to ensure a one-to-one correspondence between the linear predictor and the mean of the response variable, allowing proper model fitting and interpretation. Log-linear regression is a type of generalized linear model."}
{"Question": "Explain the basic principle of Bayesian decision theory in multi-class tasks.", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Supervised learning", "Rationale": "The Bayes decision rule minimizes overall risk by choosing the class label that has the lowest conditional risk \ud835\udc45(\ud835\udc50\u2223\ud835\udc65) for each sample", "Answer": "Given \ud835\udc41 possible class labels $$y=\\{c_{1},c_{2},...,c_{N}\\}$$ and \u03bbij as the loss of misclassifying a true label $$c_{i}$$ as $$c_{j}$$ , and using the posterior probability \n$$\ud835\udc43(\ud835\udc50_{\ud835\udc57}\u2223\ud835\udc65)$$ the expected loss or \"conditional risk\" for classifying sample \ud835\udc65 as $$\ud835\udc50_{\ud835\udc56}$$ is given by: \u200b$$R(c_{i}|x)= \\sum_{j=1}^{N} \\lambda _{ij}P(c_{j}|x)$$. Our task is to find a decision rule\u210e:\ud835\udc65\u2192\ud835\udc66 to minimize the overall risk: $$R(h)= \\mathbb{E}_{x}[R(h(x)|x)]$$. To minimize the overall risk, the Bayes decision rule is to select the class label that minimizes the conditional risk for each sample: $$h^{*}= arg_{c\\in \\mathcal{Y}} min R(c|x) $$. The Bayes decision rule minimizes overall risk by choosing the class label that has the lowest conditional risk \ud835\udc45(\ud835\udc50\u2223\ud835\udc65) for each sample."}
{"Question": "What method is used to solve the problem when we do not know the dependency network structure between the data?", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Supervised learning", "Rationale": "Score function is very important to estimate bayesian networks and introduces our inductive preferences for the Bayesian network. ", "Answer": "The primary task in Bayesian network learning is to find the most appropriate Bayesian network structure based on training data. Scoring search is a common approach to solving this problem. It involves defining a scoring function to assess how well the Bayesian network fits the training data, and then using this function to find the optimal network structure."}
{"Question": "Please briefly introduce the three main prototype clustering algorithms and explain their principles.", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Unsupervised learning", "Rationale": "--", "Answer": "1.K-Means: Partitions data into \ud835\udc58 clusters by minimizing the variance within each cluster, using centroids as prototypes. 2. Learning vector quantization also characterizes clusters by finding a set of prototype vectors, but it uses class labels as supervisory information to aid clustering. 3. Gaussian Mixture Clustering uses a probabilistic model to express cluster prototypes, assuming sample generation follows a Gaussian mixture distribution. Clustering is determined by the posterior probabilities of the prototypes."}
{"Question": "In hierarchical clustering, what is the difference between using maximum distance and minimum distance?", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Unsupervised learning", "Rationale": "Maximum Distance tends to produce more compact clusters with well-separated boundaries, as it considers the farthest distance between clusters.Minimum Distance results in more elongated or chaining clusters, as it considers only the closest points, which might lead to \"chaining\" of clusters. ", "Answer": "In hierarchical clustering, maximum distance (complete linkage) measures the distance between the furthest points of two clusters, while minimum distance (single linkage) measures the distance between the closest points."}
{"Question": "Please analyze the reasons why SVM is sensitive to noise.", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Supervised learning", "Rationale": "--", "Answer": "Support Vector Machines (SVM) can be sensitive to noise for several reasons:Margin Sensitivity: SVM aims to find a hyperplane with the maximum margin. Noise can affect the position of this hyperplane, especially if noisy points are close to the margin. Support Vector Dependence: The decision boundary is determined by the support vectors. If noisy data points become support vectors, they can disproportionately influence the model. Overfitting Risk: In high-dimensional spaces, SVM might overfit noisy data, especially if the regularization parameter is not properly tuned. Kernel Sensitivity: The choice of kernel can affect how noise influences the decision boundary. Some kernels may amplify the effects of noise."}
{"Question": "Please explain the connection between Gaussian kernel SVM and RBF neural networks.", "Level-1 Topic": "Machine learning", "Level-2 Topic": "Supervised learning", "Rationale": "They share similar underlying principles but are applied differently: SVM is a classification method, while RBF networks are used for both classification and regression. ", "Answer": "Gaussian kernel SVM and Radial Basis Function (RBF) neural networks are closely related: Kernel Function: The Gaussian kernel used in SVM is equivalent to the radial basis function used in RBF networks. Both functions measure similarity based on distance from a center. Feature Space Mapping: Both methods implicitly map input data into a high-dimensional space where linear separation or approximation is easier. Modeling Non-linearity: Both Gaussian kernel SVM and RBF networks handle non-linear data by transforming it into a space where linear methods can be applied."}
{"Question": "Please list some methods for data discretization.", "Level-1 Topic": "Data science and big data", "Level-2 Topic": "Data mining", "Rationale": "Discretization, where the raw values of a numeric attribute (e.g., age) are replaced by\ninterval labels (e.g., 0\u201310, 11\u201320, etc.) or conceptual labels (e.g., youth, adult, senior).", "Answer": "Binning,  Histogram Analysis, Cluster, Decision Tree,\nand Correlation Analyses."}
{"Question": "Please simply discribe the Concept hierarchy generation for nominal data.", "Level-1 Topic": "Data science and big data", "Level-2 Topic": "Data mining", "Rationale": "Concept hierarchy generation for nominal data is one of the strategy of data transofrmation. ", "Answer": "In this step of data pre-processing, attributes such as street can be generalized to higher-level concepts, like city or country. Many hierarchies for nominal attributes are implicit within the database schema and can be automatically defined at the schema definition level."}
{"Question": "Please discribe the general procedure for applying a hierarchical pyramid algorithm in discrete wavelet transform that halves the data at each iteration.", "Level-1 Topic": "Data science and big data", "Level-2 Topic": "Data mining", "Rationale": "--", "Answer": "The length, L, of the input data vector must be an integer power of  2. This conditioncan be met by padding the data vector with zeros as necessary (L \u2265 n).\n2. Each transform involves applying two functions. The first applies some data smoothing, such as a sum or weighted average. The second performs a weighted difference, which acts to bring out the detailed features of the data. \n3. The two functions are applied to pairs of data points in X, that is, to all pairs of measurements (x2i ,x2i+1). This results in two data sets of length L/2. In general, these represent a smoothed or low-frequency version of the input data and the high_x0002_frequency content of it, respectively.\n4. The two functions are recursively applied to the data sets obtained in the previous loop, until the resulting data sets obtained are of length 2. \n5. Selected values from the data sets obtained in the previous iterations are designated the wavelet coefficients of the transformed  data."}
{"Question": "Use flowchart to discribe the process of Forward selection and Backward elimination.", "Level-1 Topic": "Data science and big data", "Level-2 Topic": "Data mining", "Rationale": "Forward selection and bacward elimination have different order. ", "Answer": "Forward selection: Initial attribute set: {A1, A2, A3, A4, A5, A6} Initial reduced set: {}  => {A1}  => {A1, A4}  => Reduced attribute set:   {A1, A4, A6}; backward elimination: Initial attribute set: {A1, A2, A3, A4, A5, A6} => {A1, A3, A4, A5, A6} => {A1, A4, A5, A6} => Reduced attribute set:   {A1, A4, A6}"}
{"Question": "In outlier detection by semi-supervised learning, what is the advantage of using objects\nwithout labels in the training data set?", "Level-1 Topic": "Data science and big data", "Level-2 Topic": "Data mining", "Rationale": "Unlabeled data could be good chances to extract more information not only for outliers but also for normal nodes. ", "Answer": "In semi-supervised learning for outlier detection, using unlabeled objects in the training data helps to better model the data distribution, improving the detection of outliers by leveraging additional information about the structure and distribution of normal data."}
{"Question": "Because clusters may form a hierarchy, outliers may belong to different granularity\nlevels. Propose a clustering-based outlier detection method that can find outliers at\ndifferent levels.", "Level-1 Topic": "Data science and big data", "Level-2 Topic": "Data mining", "Rationale": "This method helps in detecting outliers by leveraging the hierarchical nature of clusters and considering different levels of cluster granularity.", "Answer": "Hierarchical Clustering: Perform hierarchical clustering on the dataset to generate a dendrogram, which represents clusters at various  granularity levels.  Granularity Levels: Extract clusters at multiple levels of the dendrogram, identifying different cluster sizes and hierarchies.  Outlier Detection:  At Each Level: Calculate outlier scores for data points at each level based on distance from cluster centroids or other metrics.  Aggregate Scores: Combine outlier scores across different levels to identify points that consistently deviate from clusters at multiple granularities.  Final Evaluation: Use thresholds or statistical methods to determine which points are outliers based on their aggregated scores."}
{"Question": "Several commonly used search engines allow usersto formulate their query as a natural language question rather than lists of search terms.Choose one of these and try asking a few questions (for example \u2018What is the height ofthe Eiffel Tower in metres?\u2019). What do you notice about the answers? Now try to answerthe same questions with a normal web search (by entering a set of query terms instead ofa natural language question). Are the results of this search any better or worse? Wouldit have been easier for you to \ufb01nd the answer to your query by asking a question or bysearching in the usual way? Try a range of different questions and see whether this makesany difference to the results.", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Web search", "Rationale": "The performance of search engines indeed varies based on question complexity and user experience. Preferences for natural language or search terms depend on factors like query type, ease of use, and individual familiarity with search engines, affecting retrieval effectiveness.", "Answer": "Performance of the search engine is likely to vary widely for a range of questions. Whether theuser prefers formulating queries as natural language questions or using search terms is likely tobe in\ufb02uenced by several factors, including query type, experience of using search engines andpersonal preference."}
{"Question": "Think of some polysemouswords (examples include \u2018ball\u2019, \u2018bank\u2019, \u2018port\u2019, but there are many others). Use these asqueries in a search engine and comment on what you notice about the results.", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Web search", "Rationale": "When searching for polysemous words like \"bank,\" one meaning often dominates the results. This occurs due to factors like search engine algorithms prioritizing the most common usage. Additionally, unexpected results may arise from names or abbreviations, complicating retrieval.", "Answer": "It is likely that one of the possible meanings of the polysemous term will dominate the searchresults and other meanings may not appear at all. In addition, the queries may retrieve unexpectedweb pages, for example where the terms are used as a name or abbreviation."}
{"Question": "Contrast evaluation using judged queries vs evaluation using click logs. Where/how canmismatch between evaluation and real users arise?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Information retrieval models", "Rationale": "Evaluating using judged queries can misinterpret user intent, as relevance assessments may not align with actual user needs. In contrast, click logs capture real user behavior but can be shallow and context-dependent, leading to discrepancies in perceived relevance.", "Answer": "Real users may be performing navigational, informational or transactional search. They have a richinformation need, but describe their query in a few words. Based on this short query, a relevancejudge under lab conditions might pick a very unlikely interpretation of the query. For example,judging the query \u2018amazon\u2019 according to the criterion: \u2018The user is searching for documents aboutthe history of Amazon and the factors that led to its success in the online arena.\u2019 Some users mightwant this, but the most likely intent is actually that the user wants to go shopping. If the relevancejudgments systematically disagree with real usage, then the evaluation will reward systems thatfail to satisfy real users.In summary, click data comes from real users, whereas judgments are made in lab conditions.Clicks are made in the context of a list (the probability of click depends on what other documentsappear), whereas judgments are usually independent of a list. Clicks are affected by summaries,whereas judgments are usually made on document full text. Clicks are shallow because most usersonly look at a few documents, whereas judgments can be made on a large number of documentsfor each query. It may be necessary to combine click information over many users to get a reliableindication of relevance and even then a bad document may be highly clicked, because clicks areonly an implicit indication of relevance, whereas relevance judgments are explicit.\nFinally, evaluations based on independent document-level judgments alone, cannot penalise sys-tems which retrieve overlapping or duplicate documents."}
{"Question": "Consider a scenario where it is necessary to do some categorisation. This could be workrelated or personal, e.g. \ufb01ling applications, photo album, etc. Describe in detail the purpose of the categorisation.", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Information retrieval models", "Rationale": "Categorization serves to organize and manage information efficiently based on its intended use. In work-related contexts, categorization ensures quick retrieval and dissemination of relevant data for specific tasks. In personal scenarios, such as photo albums, categorization aids in easy browsing, sharing, and preserving memories effectively tailored to individual preferences and needs.", "Answer": "It should consider the argument that text categorisation involves a qualitative decision. Consider the reason, need, and purpose of this decision for your chosen scenario.Categorisation is part of a wider context of use. The main points can be summarised as follows:1. Categorisation is intended to make a set of documents/objects easier to manage in some way.2. Effective management of documents/objects depends on its intended purpose, for both senderand recipient.3. The intended purpose of a document/object is re\ufb02ected in its structure and layout, as well asin its use of language.\nFor example, for a photograph categorisation scenario involving your personal collection considerwhy you tend to retrieve or wish to retrieve photographs (e.g., to share with others, browseyourself). Consider how you are currently storing them (e.g., chronologically, event based, peoplebased, a combination, random) and its effect on your retrieval. Consider how you have retrievedyour photos in the past and how you think this may be in the future. Now, imagine you have aphotograph collection but it is for work, say a news agency. How might some of these issues bedifferent? For example, consider the role of captions, the circumstances in which you may needto retrieve various images in the future, and so on."}
{"Question": "Search engine users have been found to enter only a small number of query terms. Outline two dif\ufb01culties arising from this fact associated with current search engine technology.Explain brie\ufb02y how the semantic search approach aims to overcome these.", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Information retrieval models", "Rationale": "Semantic search enhances traditional search engines by understanding the context and meaning of words beyond their literal interpretation. By annotating documents with semantic information, ambiguity due to synonymy and polysemy can be reduced, leading to more precise search results that align with user intent.", "Answer": "Two dif\ufb01culties are index term synonymy and query term polysemy. The use of semantic anno-tation of documents, that is annotation based on the concepts in the documents, not simply thecharacter strings used to represent the concepts, can overcome these dif\ufb01culties by modelling therelationship between the meanings of terms."}
{"Question": "What dif\ufb01culties prevent a realistic analytical model being derived for parallel computingas applied to information retrieval tasks?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Information retrieval models", "Rationale": "Developing a realistic analytical model for parallel computing in information retrieval is challenging due to the complexity of modeling tasks. The introduction of multiple variables, such as task variations and distribution methods, increases the difficulty, making precise modeling nearly impossible.", "Answer": "The \ufb01rst problem is modelling a task, which involves a signi\ufb01cant complexity problem in itsown right. Adding more variables such as more tasks and different distribution methods makesthe problem very dif\ufb01cult if not impossible."}
{"Question": "With regard to the synthetic model, what are the limitations preventing the syntheticmodel from actually predicting the relative difference between different types of inverted\ufb01le distribution methods?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Information retrieval models", "Rationale": "The synthetic model faces limitations in accurately predicting the performance of different inverted file distribution methods due to the complexities of inter-process communication in parallel computing. Non-deterministic interactions among processes can lead to variable outcomes, complicating performance predictions and hindering effective modeling.", "Answer": "The problem is the limitations of modelling communication between machines in a parallel com-puter. This is due to the non-deterministic nature of the interaction between processes completinga given task, and is still and outstanding research issue in parallelism."}
{"Question": "For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it is not.", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Information retrieval models", "Rationale": "Processing postings lists in order of size is not always optimal for conjunctive queries. For instance, starting with a smaller list can quickly reveal no matches, potentially reducing unnecessary comparisons. The optimal order depends on expected intersections rather than list sizes alone.", "Answer": "Processing postings lists in order of size for a conjunctive query is not guaranteed to be optimal. For example, if the lists are:\nTerm A: [1, 2, 3, 4, 5, 6] (size 6)\nTerm B: [4, 5, 6] (size 3)\nTerm C: [1, 7, 8] (size 3)\nProcessing A first may lead to unnecessary comparisons. Starting with C could reveal no matches earlier, allowing for quicker termination. Thus, the optimal order depends on expected intersections, not just size."}
{"Question": "Why is the idf of a term always \ufb01nite?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Tf\u2013idf", "Rationale": "The IDF of a term is finite because it is calculated using the total number of documents and the number of documents containing the term. Since both quantities are finite in any document collection, the resulting IDF value remains well-defined and manageable.", "Answer": "IDF is always finite because it is derived from finite quantities related to the document collection, ensuring that the calculations remain well-defined and manageable."}
{"Question": "What is the idf of a term that occurs in every document? Compare this with the use of stop word lists.", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Tf\u2013idf", "Rationale": "The IDF (inverse document frequency) of a term present in every document is 0, indicating it provides no discriminative power for retrieval. Stop word lists similarly remove common terms, improving search relevance by emphasizing more meaningful, distinctive terms in queries.", "Answer": "In essence, the IDF of a term that occurs in every document is 0, rendering it ineffective for retrieval purposes. Stop word lists serve to eliminate such terms, enhancing the relevance and precision of search results by prioritizing more distinctive terms."}
{"Question": "Can the tf\u2013 idf weight of a term in a document exceed 1?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Tf\u2013idf", "Rationale": "The tf-idf weight of a term can exceed 1 when term frequency (tf) is high and inverse document frequency (idf) is low. This occurs in specific contexts, particularly when a term is frequent in a document but not widely used across the corpus.", "Answer": "If both tf is sufficiently high and idf is not too low, the tf-idf weight can indeed exceed 1. Therefore, while it's common for tf-idf values to be under 1, it is entirely possible for them to exceed this threshold in certain cases."}
{"Question": "How would you create the dictionary in blocked sort-based indexing on the \ufb02y to avoid an extra pass through the data?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Inverted index", "Rationale": "The outlined approach effectively creates a dictionary in blocked sort-based indexing without an extra data pass. By processing data in blocks, maintaining a dynamic dictionary, and merging sorted terms, the overall indexing process becomes efficient and streamlined.", "Answer": "To create the dictionary in blocked sort-based indexing on the fly and avoid an extra pass through the data, you can follow these steps:\nRead and Process Data in Blocks: As you read the input data, divide it into manageable blocks. For each block, extract the terms and their associated document IDs.\nMaintain a Dynamic Dictionary: While processing each block, maintain a dictionary (or a hash table) that records the unique terms encountered along with their corresponding postings lists (document IDs).\nSort and Merge: After processing each block, sort the terms within that block. You can then merge the sorted terms into the overall dictionary dynamically, combining postings lists for terms that appear in multiple blocks.\nOutput the Dictionary: Once all blocks have been processed, the dictionary will be complete, and you will have avoided an extra pass through the data."}
{"Question": "Assume that machines in MapReduce have 100 GB of disk space each. Assume further that the postings list of the term the has a size of 200 GB. Then the MapReduce algorithm as described cannot be run to construct the index. How would you modify MapReduce so that it can handle this case?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Inverted index", "Rationale": "The proposed modifications enable handling postings lists larger than available disk space. By partitioning data, utilizing distributed storage, merging results, and employing streaming techniques, the MapReduce framework can efficiently process large datasets without exceeding resource limits.", "Answer": "To handle the case where a postings list exceeds the available disk space in a MapReduce environment, you can modify the process as follows:\nData Partitioning: Split the postings list into smaller chunks that fit within the 100 GB limit. Each chunk can be processed independently by different machines.\nIntermediate Storage: Use a distributed storage system (like HDFS) to store these chunks temporarily. During the Map phase, each machine processes its assigned chunk and writes the results back to HDFS.\nMerging Results: After all chunks are processed, implement a secondary MapReduce job to merge the intermediate results into a complete postings list.\nStreaming Processing: Consider using streaming techniques to process data as it is read, minimizing the need for large intermediate results."}
{"Question": "For optimal load balancing, the inverters in MapReduce must get segmented postings \ufb01les of similar sizes. For a new collection, the distribution of key-value pairs may not be known in advance. How would you solve this problem?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Inverted index", "Rationale": "The proposed strategies effectively address the challenge of balancing load in MapReduce. Dynamic partitioning, sampling, adaptive load balancing, and hash-based partitioning can help ensure even distribution of key-value pairs, optimizing performance despite unknown distributions.", "Answer": "To solve the problem of optimal load balancing in MapReduce when the distribution of key-value pairs is unknown, you can use the following strategies:\nDynamic Partitioning: Implement a dynamic partitioning strategy that allows the system to adjust the size of the segments based on runtime statistics. Monitor the sizes of the postings files as they are being created and adjust the splits accordingly.\nSampling: Conduct a sampling phase before the main processing to estimate the distribution of key-value pairs. Use this information to create initial splits that are balanced in size.\nAdaptive Load Balancing: Use an adaptive approach during the Map phase where tasks can be reassigned based on current load. If some inverters finish early, they can take on additional tasks from those that are still processing.\nHash-Based Partitioning: Use a hash-based method to distribute key-value pairs across segments. This can help ensure that pairs are distributed more evenly even if the overall distribution is not known."}
{"Question": "How do collaborative filtering and content-based filtering differ in recommender systems?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Recommender system", "Rationale": "Understanding these differences is crucial for designing effective recommendation systems that cater to diverse user preferences and behavior patterns.", "Answer": "Collaborative filtering relies on user-item interactions, while content-based filtering uses item attributes for recommendations."}
{"Question": "What challenges are associated with developing hybrid recommendation systems that combine collaborative and content-based filtering approaches?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Recommender system", "Rationale": "Overcoming these challenges is essential for creating hybrid systems that provide accurate and personalized recommendations to users.", "Answer": "Challenges include data sparsity, cold start problems, and effectively blending the strengths of different filtering methods."}
{"Question": "How can matrix factorization techniques such as Singular Value Decomposition (SVD) enhance the quality of recommendations in collaborative filtering?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Recommender system", "Rationale": "Employing advanced techniques like matrix factorization improves the recommendation process by capturing nuanced user preferences and behaviors.", "Answer": "SVD helps identify latent factors in user-item interactions, enabling precise and personalized recommendations."}
{"Question": "In what ways can contextual information like temporal or spatial data be integrated into recommender systems to enhance recommendation accuracy?", "Level-1 Topic": "Information retrieval", "Level-2 Topic": "Recommender system", "Rationale": "Incorporating contextual cues enriches recommendation algorithms by tailoring suggestions to specific user contexts, leading to more relevant and timely recommendations.", "Answer": "Context-aware recommendation systems adapt suggestions based on time, location, or other contextual factors, improving relevance and user satisfaction."}
{"Question": "What is the difference between a morpheme and a word? Provide examples to illustrate your answer.", "Level-1 Topic": "NLP", "Level-2 Topic": "Morphological Analysis", "Rationale": "This answer highlights the fundamental distinction between morphemes and words, illustrating the concept with clear examples that demonstrate both free and bound morphemes.", "Answer": "A morpheme is the smallest unit of meaning in a language, while a word is a complete unit of meaning that can stand alone. Morphemes can be classified as free morphemes (which can stand alone as words, e.g., \"book,\" \"run\") and bound morphemes (which cannot stand alone and must attach to other morphemes, e.g., \"un-\" in \"undo,\" \"-ed\" in \"walked\")."}
{"Question": "Explain the concepts of stemming and lemmatization in morphological analysis. How do they differ?", "Level-1 Topic": "NLP", "Level-2 Topic": "Morphological Analysis", "Rationale": "This answer explains both processes clearly, emphasizing their purposes and differences, which is crucial for understanding morphological analysis in NLP.", "Answer": "Stemming is the process of reducing a word to its base or root form by removing prefixes and suffixes, often without regard to the actual meaning (e.g., \"running\" becomes \"run\"). Lemmatization, on the other hand, reduces a word to its base form (lemma) based on its meaning and context (e.g., \"better\" becomes \"good\"). The key difference is that stemming may produce non-words, while lemmatization always results in valid words."}
{"Question": "Describe the role of parsing in syntactic analysis and differentiate between constituency parsing and dependency parsing.", "Level-1 Topic": "NLP", "Level-2 Topic": "Syntactic Analysis", "Rationale": "This answer provides a comprehensive overview of parsing and distinguishes between the two main types, highlighting their unique characteristics and purposes in syntactic analysis.", "Answer": "Parsing in syntactic analysis involves analyzing the grammatical structure of a sentence to understand how words are organized and related. Constituency parsing breaks a sentence down into its constituent parts (phrases), creating a hierarchical tree structure that represents the grammatical relationships. Dependency parsing, in contrast, focuses on the relationships between individual words, establishing a tree structure where nodes represent words and edges represent dependencies."}
{"Question": "What are some common challenges faced in syntactic analysis, and how can they be addressed?", "Level-1 Topic": "NLP", "Level-2 Topic": "Syntactic Analysis", "Rationale": "This answer identifies key challenges and suggests practical solutions, demonstrating an understanding of the complexities involved in syntactic analysis.", "Answer": "Common challenges in syntactic analysis include ambiguity (e.g., sentences that can be parsed in multiple ways), variations in sentence structure, and the presence of ungrammatical sentences. These challenges can be addressed by using probabilistic parsing techniques that consider context and likelihood, incorporating machine learning models trained on large datasets, and employing context-sensitive grammars to better handle variations."}
{"Question": "What is semantic ambiguity, and how does it affect natural language understanding? Provide examples.", "Level-1 Topic": "NLP", "Level-2 Topic": "Semantic Analysis", "Rationale": "This answer defines semantic ambiguity and illustrates its impact on NLP with relevant examples, emphasizing the importance of context in semantic analysis.", "Answer": "Semantic ambiguity occurs when a word or phrase has multiple meanings, leading to confusion in interpretation. For example, the word \"bank\" can refer to a financial institution or the side of a river. This ambiguity affects natural language understanding by making it difficult for algorithms to determine the intended meaning without additional context."}
{"Question": "Discuss the significance of word embeddings in semantic analysis. How do they enhance the understanding of word meanings?", "Level-1 Topic": "NLP", "Level-2 Topic": "Semantic Analysis", "Rationale": "This answer explains the concept of word embeddings and their significance in semantic analysis, highlighting their practical applications and benefits in understanding word meanings.", "Answer": "Word embeddings are vector representations of words that capture their meanings based on context and usage in large corpora. They enhance semantic analysis by allowing algorithms to understand relationships between words, such as synonyms and analogies (e.g., \"king\" - \"man\" + \"woman\" = \"queen\"). This representation helps in tasks like sentiment analysis, machine translation, and information retrieval by providing a more nuanced understanding of word meanings."}
{"Question": "What are some key applications of Natural Language Processing in real-world scenarios? Discuss at least three applications.", "Level-1 Topic": "NLP", "Level-2 Topic": "Applications of Natural Language Processing", "Rationale": "This answer identifies and explains several significant applications of NLP, demonstrating its relevance and impact in various fields.", "Answer": "Key applications of NLP include: Chatbots and Virtual Assistants: These systems use NLP to understand user queries and provide relevant responses, enhancing customer service and user interaction. Sentiment Analysis: This application analyzes text data (e.g., social media posts, reviews) to determine the sentiment expressed, helping businesses gauge public opinion and customer satisfaction. Machine Translation: NLP enables automatic translation of text from one language to another, facilitating communication across language barriers (e.g., Google Translate)."}
{"Question": "How does NLP contribute to improving information retrieval systems? Explain the techniques involved.", "Level-1 Topic": "NLP", "Level-2 Topic": "Applications of Natural Language Processing", "Rationale": "This answer outlines how NLP improves information retrieval, detailing specific techniques that contribute to enhanced performance and user satisfaction.", "Answer": "NLP enhances information retrieval systems by improving the relevance and accuracy of search results. Techniques involved include: Query Expansion: Using synonyms and related terms to broaden search queries, increasing the chances of retrieving relevant documents. Natural Language Understanding: Analyzing user queries to understand intent and context, allowing for more precise matching with documents. Text Summarization: Providing concise summaries of documents to help users quickly assess relevance. These techniques enable systems to better interpret user needs and deliver more relevant results."}
{"Question": "Explain the concept of overfitting in artificial neural networks and describe at least two techniques used to prevent it.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Artificial Neural Network", "Rationale": "These techniques help create a model that generalizes better to new data, addressing the problem of overfitting by simplifying the model or making it more robust.", "Answer": "Overfitting occurs when an artificial neural network learns the training data too well, capturing noise and outliers instead of generalizing well to unseen data. This typically results in high accuracy on the training set but poor performance on validation or test sets. To prevent overfitting, several techniques can be employed: Regularization: Techniques such as L1 and L2 regularization add a penalty to the loss function to discourage overly complex models by limiting the size of the weights. Dropout: During training, dropout randomly sets a proportion of the neurons to zero, effectively making the model less reliant on any one feature and promoting redundancy."}
{"Question": "Discuss the importance of the architecture of an artificial neural network and how it affects the network's performance.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Artificial Neural Network", "Rationale": "Various tasks require different architectural designs, and understanding the importance of this allows practitioners to tailor models to their specific problem, ensuring efficient learning and better performance.", "Answer": "The architecture of an artificial neural network, including the number of layers, the types of layers (convolutional, recurrent, dense, etc.), and the number of neurons in each layer, plays a crucial role in determining the network's capability to learn complex patterns. A well-designed architecture can capture the relevant features from the input data effectively, while a poorly designed one may struggle to represent the data accurately. For instance, convolutional layers are essential in image processing tasks because they are capable of detecting spatial hierarchies in images, while recurrent layers are better suited for sequential data like time series or natural language. The choice of architecture directly affects performance metrics such as accuracy, training time, and model complexity."}
{"Question": "Describe the role of feature matching in the process of 3D reconstruction and the common algorithms used for this purpose.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "3D Reconstruction", "Rationale": "Feature matching is essential as it allows the reconstruction algorithms to understand spatial relationships and depth, forming the foundation for creating accurate 3D representations.", "Answer": "Feature matching is a critical step in 3D reconstruction where corresponding features are identified across multiple 2D images taken from different viewpoints. This process allows for the establishment of ties between points in various images, enabling the extraction of depth information and the construction of 3D models from 2D data. Common algorithms used for feature matching include: SIFT (Scale-Invariant Feature Transform): Detects and describes local features in images, providing a basis for matching features across images. ORB (Oriented FAST and Rotated BRIEF): An efficient alternative to SIFT that works well for real-time applications, known for being fast and less computationally expensive."}
{"Question": "What is the significance of multi-view geometry in 3D reconstruction, and what are its main challenges?", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "3D Reconstruction", "Rationale": "Understanding the significance and challenges of multi-view geometry helps practitioners improve the accuracy and robustness of 3D reconstruction processes by addressing these critical issues.", "Answer": "Multi-view geometry is significant in 3D reconstruction as it utilizes multiple images taken from different angles to compute the 3D structure of a scene. It provides a robust framework for understanding how different views relate to each other, allowing for more accurate depth estimation and reconstruction of complex scenes. The main challenges include: Camera Calibration: Accurate calibration is essential for ensuring that the relative positions and orientations of the cameras are known, which can be difficult in practice. Handling Occlusions: Occlusions can cause certain features to be visible in some views but not in others, complicating the matching and reconstruction processes."}
{"Question": "Discuss how convolutional operations are utilized in image processing and their impact on feature extraction.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Image Processing Operations", "Rationale": "Convolution is foundational for extracting and transforming features from images, leading to better performance in tasks such as object detection and image classification.", "Answer": "Convolutional operations involve applying a kernel or filter to an image to produce a transformed output known as a feature map. These operations help in extracting important features such as edges, textures, and patterns by highlighting variations in pixel intensity. The impact on feature extraction is profound; for example, by using different filters, such as Sobel for edge detection or Gaussian for blurring, one can enhance specific aspects of the image for further analysis. Deeper layers in a convolutional neural network can learn increasingly complex features as a result of multiple convolutions, allowing for the effective recognition of objects or patterns. Convolution is foundational for extracting and transforming features from images, leading to better performance in tasks such as object detection and image classification."}
{"Question": "Explain the process and benefits of histogram equalization in the context of image enhancement.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Image Processing Operations", "Rationale": "Histogram equalization is widely used because it can significantly improve visual quality and is particularly effective in medical imaging and other applications where detail discernment is critical.", "Answer": "Histogram equalization is an image enhancement technique that aims to improve the contrast in an image by redistributing the intensity levels more uniformly across the available range. The process involves calculating the histogram of the image, determining the cumulative distribution function (CDF), and using this to map the pixel values to new values in a way that spreads them out more evenly. The benefits of histogram equalization include enhanced contrast, which emphasizes the differences between adjacent regions in the image, making details more visible. Additionally, it improves visibility by making useful information in dark or bright regions more discernible, providing a clearer representation of the image content. Histogram equalization is widely used because it can significantly improve visual quality and is particularly effective in medical imaging and other applications where detail discernment is critical."}
{"Question": "What are the key differences between Two-Stage and One-Stage object detection algorithms, and provide examples of each.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Object Detection", "Rationale": "Understanding the differences helps practitioners choose the appropriate algorithm based on specific requirements, such as whether speed or accuracy is more critical for their application.", "Answer": "Two-Stage object detection algorithms involve separating the detection process into two distinct phases: first identifying potential bounding boxes and then classifying those boxes. Examples of Two-Stage methods include Faster R-CNN and R-CNN. In contrast, One-Stage algorithms consolidate these processes into a single step, predicting bounding boxes and class probabilities simultaneously, which generally results in faster processing times. An example of this is YOLO (You Only Look Once) or SSD (Single Shot MultiBox Detector). Understanding the differences between these two types of algorithms helps practitioners choose the appropriate method based on specific requirements, such as whether speed or accuracy is more critical for their application."}
{"Question": "How do Keypoint-based methods differ from Region-based methods in object detection, and what are the advantages of each?", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Object Detection", "Rationale": "Recognizing the strengths and weaknesses of both methods allows for better decision-making in selecting approaches based on the specific requirements and constraints of object detection tasks.", "Answer": "Keypoint-based methods focus on detecting and describing specific key points or landmarks within a target object, such as SIFT or ORB, allowing for robust matching and recognition in varying poses, illuminations, or occlusions. Their advantage lies in their ability to recognize objects based on distinctive features, making them suitable for applications where the objects appear in diverse conditions. On the other hand, Region-based methods, such as R-CNN, first propose candidate regions likely containing objects and then classify these regions. Their advantage is that they often achieve higher accuracy by examining localized areas in detail, but this comes at the cost of slower processing times compared to keypoint-based methods. Recognizing the strengths and weaknesses of both methods allows for better decision-making in selecting approaches based on the specific requirements and constraints of object detection tasks."}
{"Question": "Describe how computer vision technologies are applied in autonomous vehicles and the benefits they provide.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Applications of Computer Vision", "Rationale": "Computer vision is crucial for enabling autonomous vehicles to understand and interact with their surroundings, significantly enhancing their functionality and safety.", "Answer": "In autonomous vehicles, computer vision technologies are used for various tasks, including object detection and recognition, which involves identifying other vehicles, pedestrians, traffic signs, and obstacles to enable safe navigation. Additionally, lane detection is another critical application, where the system recognizes road markings to help maintain the vehicle in the correct lane. The benefits of these technologies include improved safety through the reduction of human errors, enhanced situational awareness for the vehicle, and the ability to operate in complex environments. By leveraging computer vision, autonomous vehicles can make informed decisions in real-time, significantly enhancing their functionality and safety."}
{"Question": "Explain the role of computer vision in medical imaging and how it contributes to healthcare.", "Level-1 Topic": "Computer Vision", "Level-2 Topic": "Applications of Computer Vision", "Rationale": "By enhancing the efficiency and precision of medical image analysis, computer vision directly contributes to better patient outcomes and more effective healthcare delivery.", "Answer": "Computer vision plays a vital role in medical imaging by automating the analysis of images obtained from modalities such as X-rays, CT scans, and MRIs. Techniques such as image segmentation are used to isolate regions of interest, such as tumors, for more focused analysis, while feature extraction helps in recognizing patterns that indicate diseases or abnormalities. The contributions of computer vision to healthcare include improving diagnostic accuracy, reducing the time required for image analysis, and assisting radiologists in identifying conditions that may be missed during manual reviews. By enhancing the efficiency and precision of medical image analysis, computer vision directly contributes to better patient outcomes and more effective healthcare delivery."}
{"Question": "How does forward chaining ensure that facts are not redundantly processed when they are merely renamings of known facts?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Forward chaining", "Rationale": "Forward chaining identifies that renamings of facts do not contribute new information, thus ensuring efficiency by not re-processing equivalent facts.", "Answer": "Forward chaining avoids redundant processing by recognizing that renamings of known facts (e.g., Likes(xIceCream) vs. Likes(yIceCream)) do not add new information, thus preventing unnecessary repetition."}
{"Question": "What distinguishes a fixed point in forward chaining with first-order definite clauses from a fixed point in propositional forward chaining?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Forward chaining", "Rationale": "First-order fixed points include universal quantifiers, extending beyond the explicit facts found in propositional fixed points, thus capturing broader logical conclusions.", "Answer": "In first-order forward chaining, fixed points can include universally quantified atomic sentences, unlike propositional forward chaining, which only includes explicitly stated facts without quantifiers."}
{"Question": "Why does backward chaining with backtracking work only for finite-domain Constraint Satisfaction Problems (CSPs) in Prolog, and what algorithms are used for infinite-domain CSPs?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Backward chaining", "Rationale": "Backward chaining with backtracking requires a finite number of variable assignments to enumerate all possibilities, while infinite-domain CSPs need algorithms designed for handling unbounded variable ranges.", "Answer": "Backward chaining with backtracking is limited to finite-domain CSPs because it enumerates all possible variable assignments. For infinite-domain CSPs, algorithms like bounds propagation or linear programming are used instead."}
{"Question": "How do Constraint Logic Programming (CLP) systems enhance standard logic programming queries, and what types of algorithms do they use to solve constraints?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Backward chaining", "Rationale": "CLP systems improve query efficiency and flexibility by using diverse algorithms for constraints and query processing, blending techniques from constraint satisfaction, logic programming, and deductive databases.", "Answer": "CLP systems enhance logic programming queries by integrating advanced algorithms like heuristic conjunct ordering and backjumping. They use specific constraint-solving methods, such as linear programming, for efficient constraint resolution."}
{"Question": "What role does the knowledge base (KB) play in a knowledge-based agent, and how does its definition as a \"set of sentences\" differ from natural language sentences?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "The KB provides structured information for reasoning in agents, and its \"sentences\" are formal constructs for logic, differing from the more nuanced sentences of natural languages.", "Answer": "The KB is crucial for a knowledge-based agent as it stores the agent's information and rules. Unlike natural language sentences, these technical sentences are formal representations used for logical inference."}
{"Question": "How do the operations TELL and ASK function within a knowledge base, and why is inference important for these operations?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "Inference allows the knowledge base to derive new information consistently from existing facts, ensuring queries (ASK) are answered accurately based on what has been added (TELL).", "Answer": "TELL adds new information to the knowledge base, while ASK queries existing knowledge. Inference ensures that queries (ASK) are answered based on previously added facts (TELL) without fabricating information."}
{"Question": "How does a knowledge-based agent utilize its knowledge base (KB) in its operation, and what role does the initial background knowledge play?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "The knowledge base equips the agent with essential background information, allowing it to effectively process percepts and make decisions based on both existing knowledge and new inputs.", "Answer": "A knowledge-based agent uses its KB to process percepts and decide on actions. The initial background knowledge in the KB provides foundational context for interpreting new inputs and making informed decisions."}
{"Question": "How does the declarative approach to building a knowledge-based agent differ from the procedural approach, and what role does the TELL operation play in this approach?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "The declarative method involves explicitly adding knowledge to the agent's KB with TELL, making the agent's operation flexible and adjustable. In contrast, the procedural approach hardcodes behaviors, lacking this flexibility.", "Answer": "The declarative approach builds a knowledge-based agent by adding knowledge using TELL operations, while the procedural approach encodes behaviors directly into the code. TELL allows gradual knowledge accumulation."}
{"Question": "How does the concept of axioms relate to the use of a knowledge representation language in a knowledge base?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge representation language", "Rationale": "Axioms are foundational and undisputed truths in the knowledge representation language, serving as the basis for deriving other information and constructing the knowledge base.", "Answer": "Axioms in a knowledge representation language are fundamental assertions assumed to be true without derivation, forming the base from which other sentences and inferences are built within the knowledge base."}
{"Question": "What potential benefits could arise from uncovering the rules of natural languages for use in knowledge representation and reasoning systems?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge representation language", "Rationale": "Natural languages are rich and expressive, so understanding their rules could significantly improve knowledge representation systems, making them more capable of handling and reasoning with real-world information.", "Answer": "Uncovering natural language rules could integrate vast existing textual knowledge into systems, enhancing their ability to represent and reason with information, and leveraging extensive human-written content."}
{"Question": "How does multiple inheritance complicate the process of inheritance reasoning in knowledge representation systems?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Artificial intelligence introduction", "Rationale": "Multiple inheritance introduces complexity by making it challenging to resolve conflicting or overlapping properties from different categories, complicating the inheritance process.", "Answer": "Multiple inheritance complicates inheritance reasoning because an object belonging to multiple categories or categories overlapping can create conflicts or ambiguities in determining inherited properties."}
{"Question": "How does procedural attachment address limitations in expressive power within semantic network systems?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "Procedural attachment enhances expressive power by providing tailored procedures for specific relations, filling gaps left by general inference algorithms and offering more flexibility in handling complex queries.", "Answer": "Procedural attachment addresses limitations by using special procedures for specific relations, allowing queries or assertions to bypass general inference algorithms and handle complex cases directly."}
{"Question": "How do semantic networks handle default values for categories, and how does this differ from a strictly logical knowledge base?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "Semantic networks use default values to infer typical attributes, allowing flexibility in representing common cases, whereas a strictly logical KB treats conflicting information as a contradiction, demanding consistency.", "Answer": "Semantic networks handle default values by assuming a typical value unless contradicted by specific information, unlike a strictly logical KB, which would treat conflicting information as a contradiction."}
{"Question": "How does the inheritance algorithm in semantic networks manage default values and overrides, and how can one incorporate exceptions to maintain strictly logical semantics?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "The algorithm applies default values from general categories unless overridden by specific data. To maintain logical consistency, exceptions can be detailed explicitly, ensuring accurate representation in a strictly logical framework.", "Answer": "The inheritance algorithm manages defaults by following upward links and applying the most specific value found. Exceptions can be incorporated by explicitly stating conditions, such as specific rules for individual cases."}
{"Question": "How does classical planning differ from scheduling in the context of managing actions and time?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "Classical planning addresses action order but omits time details. Scheduling adds timing constraints and durations, crucial for managing real-world tasks like airport operations.", "Answer": "Classical planning focuses on the order and sequence of actions without considering time, whereas scheduling involves specifying how long actions take and their precise timing, such as departure and arrival times."}
{"Question": "How does the Critical Path Method (CPM) help in determining the scheduling of actions in a partial-order plan?", "Level-1 Topic": "Artificial intelligence introduction", "Level-2 Topic": "Knowledge base", "Rationale": "CPM optimizes scheduling by calculating the longest sequence of dependent actions, helping to determine the earliest and latest start times while adhering to constraints in a partial-order plan.", "Answer": "CPM identifies the earliest start and finish times for actions by analyzing the longest path through a graph, ensuring optimal scheduling within the constraints of a partial-order plan."}
