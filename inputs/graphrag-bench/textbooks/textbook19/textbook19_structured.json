[
    {
        "content": "1 \nIntroduction \nLa dernie`re chose qu’on trouve en faisant un ouvrage est de savoir celle qu’il faut mettre la premie`re. [The last thing you figure out in writing a book is what to put first.] Pascal \n2 \nRegular Expressions, Text Normalization, Edit Distance \nELIZA \nUser: I am unhappy.   \nELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY User: I need some help, that much seems certain.   \nELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP   \nUser: Perhaps I could learn to get along with my mother. ELIZA: TELL ME MORE ABOUT YOUR FAMILY   \nUser: My mother takes care of me.   \nELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU   \nUser: My father.   \nELIZA: YOUR FATHER   \nUser: You are like my father in some ways. \nWeizenbaum (1966) \nThe dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like “I need X” and translate them into suitable outputs like “What would it mean to you if you got X?”. This simple technique succeeds in this domain because ELIZA doesn’t actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA’s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA’s abilities even after the program’s operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion. \nchatbots \nOf course modern conversational agents are much more than a diversion; they can answer questions, book flights, or find restaurants, functions for which they rely on a much more sophisticated understanding of the user’s intent, as we will see in Chapter 24. Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play a crucial role in natural language processing. \ntext normalization \nWe’ll begin with the most important tool for describing text patterns: the regular expression. Regular expressions can be used to specify strings we might want to extract from a document, from transforming “I need X” in ELIZA above, to defining strings like $$ 199$ or $$ 24.99$ for extracting tables of prices from a document. \ntokenization \nWe’ll then turn to a set of tasks collectively called text normalization, in which regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock ’n’ roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we’ll need to separate $I ^ { prime } m$ into the two words $I$ and am. For processing tweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.",
        "chapter": "Introduction",
        "section": "N/A",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "lemmatization \nSome languages, like Japanese, don’t have spaces between words, so word tokenization becomes more difficult. \nstemming sentence segmentation \nAnother part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points. \nFinally, we’ll need to compare words and other strings. We’ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. \n2.1 Regular Expressions \nregular expression \ncorpus \nOne of the unsung successes in standardization in computer science has been the regular expression (RE), a language for specifying text search strings. This practical language is used in every computer language, word processor, and text processing tools like the Unix tools grep or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. They are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. \nA search can be designed to return every match on a line, if there are more than one, or just the first match. In the following examples we generally underline the exact part of the pattern that matches the regular expression and show only the first match. We’ll show regular expressions delimited by slashes but note that slashes are not part of the regular expressions. \nRegular expressions come in many variants. We’ll be describing extended regular expressions; different regular expression parsers may only recognize subsets of these, or treat some expressions slightly differently. Using an online regular expression tester is a handy way to test out your expressions and explore these variations. \nconcatenation \n2.1.1 Basic Regular Expression Patterns \nThe simplest kind of regular expression is a sequence of simple characters; putting characters in sequence is called concatenation. To search for woodchuck, we type /woodchuck/. The expression /Buttercup/ matches any string containing the substring Buttercup; grep with that expression would return the line I’m called little Buttercup. The search string can consist of a single character (like $/ ! / rangle$ ) or a sequence of characters (like /urgl/). \nRegular expressions are case sensitive; lower case /s/ is distinct from upper case /S/ (/s/ matches a lower case $s$ but not an upper case $S$ ). This means that \nrange \nthe pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either $w$ or $W$ . \nThe regular expression /[1234567890]/ specifies any single digit. While such classes of characters as digits or letters are important building blocks in expressions, they can get awkward (e.g., it’s inconvenient to specify \n/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/ \nto mean “any capital letter”). In cases where there is a well-defined sequence associated with a set of characters, the brackets can be used with the dash $( - )$ to specify any one character in a range. The pattern /[2-5]/ specifies any one of the characters 2, 3, 4, or 5. The pattern $/ [ b - 9 ] /$ specifies one of the characters $b , c , d , e , f ,$ or g. Some other examples are shown in Fig. 2.3. \nThe square braces can also be used to specify what a single character cannot be, by use of the caret ˆ. If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated. For example, the pattern $/ [ hat { textbf { a } } ] /$ matches any single character (including special characters) except $a$ . This is only true when the caret is the first symbol after the open square brace. If it occurs anywhere else, it usually stands for a caret; Fig. 2.4 shows some examples. \nHow can we talk about optional elements, like an optional $s$ in woodchuck and woodchucks? We can’t use the square brackets, because while they allow us to say “s or $S ^ { prime prime }$ , they don’t allow us to say “s or nothing”. For this we use the question mark $/ ? /$ , which means “the preceding character or nothing”, as shown in Fig. 2.5. \n\nKleene * \nWe can think of the question mark as meaning “zero or one instances of the previous character”. That is, it’s a way of specifying how many of something that we want, something that is very important in regular expressions. For example, consider the language of certain sheep, which consists of strings that look like the following: \nbaa! baaa! baaaa! baaaaa! . . . \nThis language consists of strings with a $b$ , followed by at least two a’s, followed by an exclamation point. The set of operators that allows us to say things like “some number of as” are based on the asterisk or *, commonly called the Kleene * (generally pronounced “cleany star”). The Kleene star means “zero or more occurrences of the immediately previous character or regular expression”. So $mu { * } /$ means “any string of zero or more as”. This will match $a$ or aaaaaa, but it will also match $o f f$ Minor since the string Off Minor has zero $a$ ’s. So the regular expression for matching one or more $a$ is $/ mathtt { a a } ^ { * } /$ , meaning one $a$ followed by zero or more as. More complex patterns can also be repeated. So $/ [ mathsf { a b } ] ^ { * } /$ means “zero or more $a$ ’s or b’s” (not “zero or more right square braces”). This will match strings like aaaa or ababab or bbbb. \nKleene $^ +$ \nFor specifying multiple digits (useful for finding prices) we can extend $big / [ 0 - 9 ] big /$ , the regular expression for a single digit. An integer (a string of digits) is thus $/ [ 0 - 9 ] [ 0 - 9 ] ^ { * } /$ . (Why isn’t it just $/ left[ 0 - 9 right] ^ { * } / ? )$ \nSometimes it’s annoying to have to write the regular expression for digits twice, so there is a shorter way to specify “at least one” of some character. This is the Kleene $^ +$ , which means “one or more occurrences of the immediately preceding character or regular expression”. Thus, the expression $/ [ 0 - 9 ] + 1$ is the normal way to specify “a sequence of digits”. There are thus two ways to specify the sheep language: $/ 6 a a a \" !$ or $/ 6 a a + 1 /$ . \nOne very important special character is the period $( / . / )$ , a wildcard expression that matches any single character (except a carriage return), as shown in Fig. 2.6. \nThe wildcard is often used together with the Kleene star to mean “any string of characters”. For example, suppose we want to find any line in which a particular word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.*aardvark/. \nanchors \nAnchors are special characters that anchor regular expressions to particular places in a string. The most common anchors are the caret ˆ and the dollar sign $$ 5$ . The caret ˆ matches the start of a line. The pattern /ˆThe/ matches the word The only at the start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to indicate a negation inside of square brackets, and just to mean a caret. (What are the contexts that allow grep or Python to know which function a given caret is supposed to have?) The dollar sign $$ 5$ matches the end of a line. So the pattern $boldsymbol { lrcorner } boldsymbol { mathfrak { S } }$ is a useful pattern for matching a space at the end of a line, and $nearrow$ The dog. $$ 7$ matches a line that contains only the phrase The dog. (We have to use the backslash here since we want the . to mean “period” and not the wildcard.) \n\nThere are also two other anchors: b matches a word boundary, and B matches a non-boundary. Thus, /btheb/ matches the word the but not the word other. More technically, a “word” for the purposes of a regular expression is defined as any sequence of digits, underscores, or letters; this is based on the definition of “words” in programming languages. For example, $langle rangle langle boldsymbol { mathsf { b 9 9 } } backslash boldsymbol { mathsf { b } } /$ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $$ 99$ (since 99 follows a dollar sign $( $ )$ , which is not a digit, underscore, or letter). \n2.1.2 Disjunction, Grouping, and Precedence \ndisjunction \nSuppose we need to search for texts about pets; perhaps we are particularly interested in cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can’t use the square brackets to search for “cat or dog” (why can’t we say /[catdog]/?), we need a new operator, the disjunction operator, also called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or the string dog. \nprecedence \nSometimes we need to use this disjunction operator in the midst of a larger sequence. For example, suppose I want to search for information about pet fish for my cousin David. How can I specify both guppy and guppies? We cannot simply say /guppy|ies/, because that would match only the strings guppy and ies. This is because sequences like guppy take precedence over the disjunction operator |. To make the disjunction operator apply only to a specific pattern, we need to use the parenthesis operators ( and ). Enclosing a pattern in parentheses makes it act like a single character for the purposes of neighboring operators like the pipe | and the Kleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunction only to apply to the suffixes y and ies. \nThe parenthesis operator ( is also useful when we are using counters like the Kleene*. Unlike the | operator, the Kleene* operator applies by default only to a single character, not to a whole sequence. Suppose we want to match repeated instances of a string. Perhaps we have a line that has column labels of the form Column 1 Column 2 Column 3. The expression $/ mathsf { C o l u m n _ } [ Theta - 9 ] + mathrel { lrcorner } mathrel { ast } /$ will not match any number of columns; instead, it will match a single column followed by any number of spaces! The star here applies only to the space ␣ that precedes it, not to the whole sequence. With the parentheses, we could write the expression $/ ( cos mu mathrm { m n } _ { mathsf { L } } [ Theta - 9 ] + hphantom { m } _ { mathsf { L } } ^ { * } ) ^ { * } /$ to match the word Column, followed by a number and optional spaces, the whole pattern repeated zero or more times.",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "Basic Regular Expression Patterns",
        "subsubsection": "N/A"
    },
    {
        "content": "There are also two other anchors: b matches a word boundary, and B matches a non-boundary. Thus, /btheb/ matches the word the but not the word other. More technically, a “word” for the purposes of a regular expression is defined as any sequence of digits, underscores, or letters; this is based on the definition of “words” in programming languages. For example, $langle rangle langle boldsymbol { mathsf { b 9 9 } } backslash boldsymbol { mathsf { b } } /$ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $$ 99$ (since 99 follows a dollar sign $( $ )$ , which is not a digit, underscore, or letter). \n2.1.2 Disjunction, Grouping, and Precedence \ndisjunction \nSuppose we need to search for texts about pets; perhaps we are particularly interested in cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can’t use the square brackets to search for “cat or dog” (why can’t we say /[catdog]/?), we need a new operator, the disjunction operator, also called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or the string dog. \nprecedence \nSometimes we need to use this disjunction operator in the midst of a larger sequence. For example, suppose I want to search for information about pet fish for my cousin David. How can I specify both guppy and guppies? We cannot simply say /guppy|ies/, because that would match only the strings guppy and ies. This is because sequences like guppy take precedence over the disjunction operator |. To make the disjunction operator apply only to a specific pattern, we need to use the parenthesis operators ( and ). Enclosing a pattern in parentheses makes it act like a single character for the purposes of neighboring operators like the pipe | and the Kleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunction only to apply to the suffixes y and ies. \nThe parenthesis operator ( is also useful when we are using counters like the Kleene*. Unlike the | operator, the Kleene* operator applies by default only to a single character, not to a whole sequence. Suppose we want to match repeated instances of a string. Perhaps we have a line that has column labels of the form Column 1 Column 2 Column 3. The expression $/ mathsf { C o l u m n _ } [ Theta - 9 ] + mathrel { lrcorner } mathrel { ast } /$ will not match any number of columns; instead, it will match a single column followed by any number of spaces! The star here applies only to the space ␣ that precedes it, not to the whole sequence. With the parentheses, we could write the expression $/ ( cos mu mathrm { m n } _ { mathsf { L } } [ Theta - 9 ] + hphantom { m } _ { mathsf { L } } ^ { * } ) ^ { * } /$ to match the word Column, followed by a number and optional spaces, the whole pattern repeated zero or more times. \n\nThis idea that one operator may take precedence over another, requiring us to sometimes use parentheses to specify what we mean, is formalized by the operator precedence hierarchy for regular expressions. The following table gives the order of RE operator precedence, from highest precedence to lowest precedence. \nThus, because counters have a higher precedence than sequences, $/ mathrm { t h e \" / }$ matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, /the|any/ matches the or any but not thany or theny. \nPatterns can be ambiguous in another way. Consider the expression $/ [ a - z ] ^ { * } /$ when matching against the text once upon a time. Since $/ [ a - z ] ^ { ast } /$ matches zero or more letters, this expression could match nothing, or just the first letter $o$ , on, onc, or once. In these cases regular expressions always match the largest string they can; we say that patterns are greedy, expanding to cover as much of a string as they can. \nThere are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator $ast ? { }$ is a Kleene star that matches as little text as possible. The operator $mathbf { + } mathbf { ? }$ is a Kleene plus that matches as little text as possible. \n2.1.3 A Simple Example \nSuppose we wanted to write a RE to find cases of the English article the. A simple (but incorrect) pattern might be: \n/the/ \nOne problem is that this pattern will miss the word when it begins a sentence and hence is capitalized (i.e., The). This might lead us to the following pattern: \n/[tT]he/ \nBut we will still incorrectly return texts with the embedded in other words (e.g., other or theology). So we need to specify that we want instances with a word boundary on both sides: \n/b[tT]heb/ \nSuppose we wanted to do this without the use of $left. mathsfit { b } / right.$ . We might want this since /b/ won’t treat underscores and numbers as word boundaries; but we might want to find the in some context where it might also have underlines or numbers nearby (the or the25). We need to specify that we want instances in which there are no alphabetic letters on either side of the the: \nBut there is still one more problem with this pattern: it won’t find the word the when it begins a line. This is because the regular expression [ˆa-zA-Z], which we used to avoid embedded instances of the, implies that there must be some single (although non-alphabetic) character before the the. We can avoid this by specifying that before the the we require either the beginning-of-line or a non-alphabetic character, and the same at the end of the line:",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "Disjunction, Grouping, and Precedence",
        "subsubsection": "N/A"
    },
    {
        "content": "This idea that one operator may take precedence over another, requiring us to sometimes use parentheses to specify what we mean, is formalized by the operator precedence hierarchy for regular expressions. The following table gives the order of RE operator precedence, from highest precedence to lowest precedence. \nThus, because counters have a higher precedence than sequences, $/ mathrm { t h e \" / }$ matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, /the|any/ matches the or any but not thany or theny. \nPatterns can be ambiguous in another way. Consider the expression $/ [ a - z ] ^ { * } /$ when matching against the text once upon a time. Since $/ [ a - z ] ^ { ast } /$ matches zero or more letters, this expression could match nothing, or just the first letter $o$ , on, onc, or once. In these cases regular expressions always match the largest string they can; we say that patterns are greedy, expanding to cover as much of a string as they can. \nThere are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator $ast ? { }$ is a Kleene star that matches as little text as possible. The operator $mathbf { + } mathbf { ? }$ is a Kleene plus that matches as little text as possible. \n2.1.3 A Simple Example \nSuppose we wanted to write a RE to find cases of the English article the. A simple (but incorrect) pattern might be: \n/the/ \nOne problem is that this pattern will miss the word when it begins a sentence and hence is capitalized (i.e., The). This might lead us to the following pattern: \n/[tT]he/ \nBut we will still incorrectly return texts with the embedded in other words (e.g., other or theology). So we need to specify that we want instances with a word boundary on both sides: \n/b[tT]heb/ \nSuppose we wanted to do this without the use of $left. mathsfit { b } / right.$ . We might want this since /b/ won’t treat underscores and numbers as word boundaries; but we might want to find the in some context where it might also have underlines or numbers nearby (the or the25). We need to specify that we want instances in which there are no alphabetic letters on either side of the the: \nBut there is still one more problem with this pattern: it won’t find the word the when it begins a line. This is because the regular expression [ˆa-zA-Z], which we used to avoid embedded instances of the, implies that there must be some single (although non-alphabetic) character before the the. We can avoid this by specifying that before the the we require either the beginning-of-line or a non-alphabetic character, and the same at the end of the line: \n\nfalse positives false negatives \nThe process we just went through was based on fixing two kinds of errors: false positives, strings that we incorrectly matched like other or there, and false negatives, strings that we incorrectly missed, like The. Addressing these two kinds of errors comes up again and again in implementing speech and language processing systems. Reducing the overall error rate for an application thus involves two antagonistic efforts: \n• Increasing precision (minimizing false positives) • Increasing recall (minimizing false negatives) \nWe’ll come back to precision and recall with more precise definitions in Chapter 4. \n2.1.4 More Operators \nFigure 2.8 shows some aliases for common ranges, which can be used mainly to save typing. Besides the Kleene * and Kleene $^ +$ we can also use explicit numbers as counters, by enclosing them in curly brackets. The regular expression $/ { 3 } /$ means “exactly 3 occurrences of the previous character or expression”. So $/ { tt a } backslash . { 2 4 } z /$ will match $a$ followed by 24 dots followed by $z$ (but not $a$ followed by 23 or 25 dots followed by a z). \nA range of numbers can also be specified. So $/ { boldsymbol { mathrm { n } } , boldsymbol { mathrm { m } } } /$ specifies from $n$ to $m$ occurrences of the previous char or expression, and $/ { { boldsymbol { mathbf { n } } } , { boldsymbol { mathbf { mathit { 3 } } } } /$ means at least $n$ occurrences of the previous expression. REs for counting are summarized in Fig. 2.9. \nnewline \nFinally, certain special characters are referred to by special notation based on the backslash () (see Fig. 2.10). The most common of these are the newline character n and the tab character t. To refer to characters that are special themselves (like ., *, [, and ), precede them with a backslash, (i.e., $/ backslash . / , / backslash ^ { * } / , / backslash [ /$ , and $/ backslash backslash / )$ .",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "A Simple Example",
        "subsubsection": "N/A"
    },
    {
        "content": "false positives false negatives \nThe process we just went through was based on fixing two kinds of errors: false positives, strings that we incorrectly matched like other or there, and false negatives, strings that we incorrectly missed, like The. Addressing these two kinds of errors comes up again and again in implementing speech and language processing systems. Reducing the overall error rate for an application thus involves two antagonistic efforts: \n• Increasing precision (minimizing false positives) • Increasing recall (minimizing false negatives) \nWe’ll come back to precision and recall with more precise definitions in Chapter 4. \n2.1.4 More Operators \nFigure 2.8 shows some aliases for common ranges, which can be used mainly to save typing. Besides the Kleene * and Kleene $^ +$ we can also use explicit numbers as counters, by enclosing them in curly brackets. The regular expression $/ { 3 } /$ means “exactly 3 occurrences of the previous character or expression”. So $/ { tt a } backslash . { 2 4 } z /$ will match $a$ followed by 24 dots followed by $z$ (but not $a$ followed by 23 or 25 dots followed by a z). \nA range of numbers can also be specified. So $/ { boldsymbol { mathrm { n } } , boldsymbol { mathrm { m } } } /$ specifies from $n$ to $m$ occurrences of the previous char or expression, and $/ { { boldsymbol { mathbf { n } } } , { boldsymbol { mathbf { mathit { 3 } } } } /$ means at least $n$ occurrences of the previous expression. REs for counting are summarized in Fig. 2.9. \nnewline \nFinally, certain special characters are referred to by special notation based on the backslash () (see Fig. 2.10). The most common of these are the newline character n and the tab character t. To refer to characters that are special themselves (like ., *, [, and ), precede them with a backslash, (i.e., $/ backslash . / , / backslash ^ { * } / , / backslash [ /$ , and $/ backslash backslash / )$ . \n\n2.1.5 A More Complex Example \nLet’s try out a more significant example of the power of REs. Suppose we want to build an application to help a user buy a computer on the Web. The user might want “any machine with at least $6 operatorname { G H z }$ and $5 0 0 mathrm { G B }$ of disk space for less than $$ 1000^{ prime }$ . To do this kind of retrieval, we first need to be able to look for expressions like $boldsymbol { mathscr { sigma } }$ $G H z$ or $5 0 0 G B$ or Mac or $$ 999.99$ . In the rest of this section we’ll work out some simple regular expressions for this task. \nFirst, let’s complete our regular expression for prices. Here’s a regular expression for a dollar sign followed by a string of digits: \nNote that the $$ 5$ character has a different function here than the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize that $$ 8$ here doesn’t mean end-of-line. (As a thought experiment, think about how regex parsers might figure out the function of $$ 8$ from the context.) \nNow we just need to deal with fractions of dollars. We’ll add a decimal point and two digits afterwards: \nThis pattern only allows $$ 199.99$ but not $$ 199$ . We need to make the cents optional and to make sure we’re at a word boundary: \nOne last catch! This pattern allows prices like $$ 199999.99$ which would be far too expensive! We need to limit the dollars: \nHow about disk space? We’ll need to allow for optional fractions again $( 5 . 5 G B )$ ; note the use of ? for making the final s optional, and the of $/ {  ! } boldsymbol { cdot } boldsymbol { ast } /$ to mean “zero or more spaces” since there might always be extra spaces lying around: \nModifying this regular expression so that it only matches more than $5 0 0 mathrm { G B }$ is left as an exercise for the reader.",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "More Operators",
        "subsubsection": "N/A"
    },
    {
        "content": "2.1.5 A More Complex Example \nLet’s try out a more significant example of the power of REs. Suppose we want to build an application to help a user buy a computer on the Web. The user might want “any machine with at least $6 operatorname { G H z }$ and $5 0 0 mathrm { G B }$ of disk space for less than $$ 1000^{ prime }$ . To do this kind of retrieval, we first need to be able to look for expressions like $boldsymbol { mathscr { sigma } }$ $G H z$ or $5 0 0 G B$ or Mac or $$ 999.99$ . In the rest of this section we’ll work out some simple regular expressions for this task. \nFirst, let’s complete our regular expression for prices. Here’s a regular expression for a dollar sign followed by a string of digits: \nNote that the $$ 5$ character has a different function here than the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize that $$ 8$ here doesn’t mean end-of-line. (As a thought experiment, think about how regex parsers might figure out the function of $$ 8$ from the context.) \nNow we just need to deal with fractions of dollars. We’ll add a decimal point and two digits afterwards: \nThis pattern only allows $$ 199.99$ but not $$ 199$ . We need to make the cents optional and to make sure we’re at a word boundary: \nOne last catch! This pattern allows prices like $$ 199999.99$ which would be far too expensive! We need to limit the dollars: \nHow about disk space? We’ll need to allow for optional fractions again $( 5 . 5 G B )$ ; note the use of ? for making the final s optional, and the of $/ {  ! } boldsymbol { cdot } boldsymbol { ast } /$ to mean “zero or more spaces” since there might always be extra spaces lying around: \nModifying this regular expression so that it only matches more than $5 0 0 mathrm { G B }$ is left as an exercise for the reader. \n2.1.6 Substitution, Capture Groups, and ELIZA \nsubstitution \nAn important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: \ns/colour/color/ \nIt is often useful to be able to refer to a particular subpart of the string matching the first pattern. For example, suppose we wanted to put angle brackets around all integers in a text, for example, changing the 35 boxes to the ${ < } 3 5 mathrm { > }$ boxes. We’d like a way to refer to the integer we’ve found so that we can easily add the brackets. To do this, we put parentheses ( and ) around the first pattern and use the number operator 1 in the second pattern to refer back. Here’s how it looks: \nThe parenthesis and number operators can also specify that a certain string or expression must occur twice in the text. For example, suppose we are looking for the pattern “the Xer they were, the Xer they will be”, where we want to constrain the two X’s to be the same string. We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator $setminus { 1 }$ , as follows: \n/the (.*)er they were, the 1er they will be/ \nregister \nHere the 1 will be replaced by whatever string matched the first item in parentheses. So this will match the bigger they were, the bigger they will be but not the bigger they were, the faster they will be. \nThis use of parentheses to store a pattern in memory is called a capture group. Every time a capture group is used (i.e., parentheses surround a pattern), the resulting match is stored in a numbered register. If you match two different sets of parentheses, 2 means whatever matched the second capture group. Thus \n/the (.*)er they (.*), the 1er we 2/ \nnon-capturing group \nwill match the faster they ran, the faster we ran but not the faster they ran, the faster we ate. Similarly, the third capture group is stored in 3, the fourth is 4, and so on. \nParentheses thus have a double function in regular expressions; they are used to group terms for specifying the order in which operators should apply, and they are used to capture something in a register. Occasionally we might want to use parentheses for grouping, but don’t want to capture the resulting pattern in a register. In that case we use a non-capturing group, which is specified by putting the commands ?: after the open paren, in the form (?: pattern ). \n/(?:some|a few) (people|cats) like some 1/ will match some cats like some cats but not some cats like some a few. \n\nSubstitutions and capture groups are very useful in implementing simple chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian psychologist by carrying on conversations like the following:",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "A More Complex Example",
        "subsubsection": "N/A"
    },
    {
        "content": "2.1.6 Substitution, Capture Groups, and ELIZA \nsubstitution \nAn important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: \ns/colour/color/ \nIt is often useful to be able to refer to a particular subpart of the string matching the first pattern. For example, suppose we wanted to put angle brackets around all integers in a text, for example, changing the 35 boxes to the ${ < } 3 5 mathrm { > }$ boxes. We’d like a way to refer to the integer we’ve found so that we can easily add the brackets. To do this, we put parentheses ( and ) around the first pattern and use the number operator 1 in the second pattern to refer back. Here’s how it looks: \nThe parenthesis and number operators can also specify that a certain string or expression must occur twice in the text. For example, suppose we are looking for the pattern “the Xer they were, the Xer they will be”, where we want to constrain the two X’s to be the same string. We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator $setminus { 1 }$ , as follows: \n/the (.*)er they were, the 1er they will be/ \nregister \nHere the 1 will be replaced by whatever string matched the first item in parentheses. So this will match the bigger they were, the bigger they will be but not the bigger they were, the faster they will be. \nThis use of parentheses to store a pattern in memory is called a capture group. Every time a capture group is used (i.e., parentheses surround a pattern), the resulting match is stored in a numbered register. If you match two different sets of parentheses, 2 means whatever matched the second capture group. Thus \n/the (.*)er they (.*), the 1er we 2/ \nnon-capturing group \nwill match the faster they ran, the faster we ran but not the faster they ran, the faster we ate. Similarly, the third capture group is stored in 3, the fourth is 4, and so on. \nParentheses thus have a double function in regular expressions; they are used to group terms for specifying the order in which operators should apply, and they are used to capture something in a register. Occasionally we might want to use parentheses for grouping, but don’t want to capture the resulting pattern in a register. In that case we use a non-capturing group, which is specified by putting the commands ?: after the open paren, in the form (?: pattern ). \n/(?:some|a few) (people|cats) like some 1/ will match some cats like some cats but not some cats like some a few. \n\nSubstitutions and capture groups are very useful in implementing simple chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian psychologist by carrying on conversations like the following: \nUser1: Men are all alike.   \n$mathrm { E L I Z A _ { 1 } }$ : IN WHAT WAY   \nUser2: They’re always bugging us about something or other. $mathrm { E L I Z A } _ { 2 }$ : CAN YOU THINK OF A SPECIFIC EXAMPLE User3: Well, my boyfriend made me come here.   \n$mathrm { E L I Z A } _ { 3 }$ : YOUR BOYFRIEND MADE YOU COME HERE User4: He says I’m depressed much of the time.   \n$mathrm { E L I Z A _ { 4 } }$ : I AM SORRY TO HEAR YOU ARE DEPRESSED \nELIZA works by having a series or cascade of regular expression substitutions each of which matches and changes some part of the input lines. Input lines are first uppercased. The first substitutions then change all instances of MY to YOUR, and $I ^ { prime } M$ to $Y O U A R E$ , and so on. The next set of substitutions matches and replaces other patterns in the input. Here are some examples: \ns/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE 1/ s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE 1/ s/.* all .*/IN WHAT WAY/ s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/ \nSince multiple substitutions can apply to a given input, substitutions are assigned a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we return to the details of the ELIZA architecture in Chapter 24. \nlookahead \n2.1.7 Lookahead Assertions \nzero-width \nFinally, there will be times when we need to predict the future: look ahead in the text to see if some pattern matches, but not advance the match cursor, so that we can then deal with the pattern if it occurs. \nThese lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator ( $ ? =$ pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance the cursor. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this: \n2.2 Words \nBefore we talk about processing words, we need to decide what counts as a word.   \ncorpus Let’s start by looking at one particular corpus (plural corpora), a computer-readable   \ncorpora collection of text or speech. For example the Brown corpus is a million-word collection of samples from 500 written English texts from different genres (newspaper, fiction, non-fiction, academic, etc.), assembled at Brown University in 1963–64 (Kucˇera and Francis, 1967). How many words are in the following Brown sentence? \nHe stepped out into the hall, was delighted to encounter a water brother.",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "Substitution, Capture Groups, and ELIZA",
        "subsubsection": "N/A"
    },
    {
        "content": "User1: Men are all alike.   \n$mathrm { E L I Z A _ { 1 } }$ : IN WHAT WAY   \nUser2: They’re always bugging us about something or other. $mathrm { E L I Z A } _ { 2 }$ : CAN YOU THINK OF A SPECIFIC EXAMPLE User3: Well, my boyfriend made me come here.   \n$mathrm { E L I Z A } _ { 3 }$ : YOUR BOYFRIEND MADE YOU COME HERE User4: He says I’m depressed much of the time.   \n$mathrm { E L I Z A _ { 4 } }$ : I AM SORRY TO HEAR YOU ARE DEPRESSED \nELIZA works by having a series or cascade of regular expression substitutions each of which matches and changes some part of the input lines. Input lines are first uppercased. The first substitutions then change all instances of MY to YOUR, and $I ^ { prime } M$ to $Y O U A R E$ , and so on. The next set of substitutions matches and replaces other patterns in the input. Here are some examples: \ns/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE 1/ s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE 1/ s/.* all .*/IN WHAT WAY/ s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/ \nSince multiple substitutions can apply to a given input, substitutions are assigned a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we return to the details of the ELIZA architecture in Chapter 24. \nlookahead \n2.1.7 Lookahead Assertions \nzero-width \nFinally, there will be times when we need to predict the future: look ahead in the text to see if some pattern matches, but not advance the match cursor, so that we can then deal with the pattern if it occurs. \nThese lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator ( $ ? =$ pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance the cursor. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this: \n2.2 Words \nBefore we talk about processing words, we need to decide what counts as a word.   \ncorpus Let’s start by looking at one particular corpus (plural corpora), a computer-readable   \ncorpora collection of text or speech. For example the Brown corpus is a million-word collection of samples from 500 written English texts from different genres (newspaper, fiction, non-fiction, academic, etc.), assembled at Brown University in 1963–64 (Kucˇera and Francis, 1967). How many words are in the following Brown sentence? \nHe stepped out into the hall, was delighted to encounter a water brother.",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Regular Expressions",
        "subsection": "Lookahead Assertions",
        "subsubsection": "N/A"
    },
    {
        "content": "User1: Men are all alike.   \n$mathrm { E L I Z A _ { 1 } }$ : IN WHAT WAY   \nUser2: They’re always bugging us about something or other. $mathrm { E L I Z A } _ { 2 }$ : CAN YOU THINK OF A SPECIFIC EXAMPLE User3: Well, my boyfriend made me come here.   \n$mathrm { E L I Z A } _ { 3 }$ : YOUR BOYFRIEND MADE YOU COME HERE User4: He says I’m depressed much of the time.   \n$mathrm { E L I Z A _ { 4 } }$ : I AM SORRY TO HEAR YOU ARE DEPRESSED \nELIZA works by having a series or cascade of regular expression substitutions each of which matches and changes some part of the input lines. Input lines are first uppercased. The first substitutions then change all instances of MY to YOUR, and $I ^ { prime } M$ to $Y O U A R E$ , and so on. The next set of substitutions matches and replaces other patterns in the input. Here are some examples: \ns/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE 1/ s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE 1/ s/.* all .*/IN WHAT WAY/ s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/ \nSince multiple substitutions can apply to a given input, substitutions are assigned a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we return to the details of the ELIZA architecture in Chapter 24. \nlookahead \n2.1.7 Lookahead Assertions \nzero-width \nFinally, there will be times when we need to predict the future: look ahead in the text to see if some pattern matches, but not advance the match cursor, so that we can then deal with the pattern if it occurs. \nThese lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator ( $ ? =$ pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance the cursor. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this: \n2.2 Words \nBefore we talk about processing words, we need to decide what counts as a word.   \ncorpus Let’s start by looking at one particular corpus (plural corpora), a computer-readable   \ncorpora collection of text or speech. For example the Brown corpus is a million-word collection of samples from 500 written English texts from different genres (newspaper, fiction, non-fiction, academic, etc.), assembled at Brown University in 1963–64 (Kucˇera and Francis, 1967). How many words are in the following Brown sentence? \nHe stepped out into the hall, was delighted to encounter a water brother. \nutterance \nThis sentence has 13 words if we don’t count punctuation marks as words, 15 if we count punctuation. Whether we treat period (“.”), comma (“,”), and so on as words depends on the task. Punctuation is critical for finding boundaries of things (commas, periods, colons) and for identifying some aspects of meaning (question marks, exclamation marks, quotation marks). For some tasks, like part-of-speech tagging or parsing or speech synthesis, we sometimes treat punctuation marks as if they were separate words. \nThe Switchboard corpus of American English telephone conversations between strangers was collected in the early 1990s; it contains 2430 conversations averaging 6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey et al., 1992). Such corpora of spoken language don’t have punctuation but do introduce other complications with regard to defining words. Let’s look at one utterance from Switchboard; an utterance is the spoken correlate of a sentence: \nI do uh main- mainly business data processing \ndisfluency fragment filled pause \nThis utterance has two kinds of disfluencies. The broken-off word main- is called a fragment. Words like uh and um are called fillers or filled pauses. Should we consider these to be words? Again, it depends on the application. If we are building a speech transcription system, we might want to eventually strip out the disfluencies. \nlemma \nBut we also sometimes keep disfluencies around. Disfluencies like uh or um are actually helpful in speech recognition in predicting the upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech recognition they are treated as regular words. Because people use different disfluencies they can also be a cue to speaker identification. In fact Clark and Fox Tree (2002) showed that uh and um have different meanings. What do you think they are? \nAre capitalized tokens like They and uncapitalized tokens like they the same word? These are lumped together in some tasks (speech recognition), while for partof-speech or named-entity tagging, capitalization is a useful feature and is retained. \nwordform \nHow about inflected forms like cats versus cat? These two words have the same lemma cat but are different wordforms. A lemma is a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense. The wordform is the full inflected or derived form of the word. For morphologically complex languages like Arabic, we often need to deal with lemmatization. For many tasks in English, however, wordforms are sufficient. \nword type \nword token \nHow many words are there in English? To answer this question we need to distinguish two ways of talking about words. Types are the number of distinct words in a corpus; if the set of words in the vocabulary is $V$ , the number of types is the vocabulary size $| V |$ . Tokens are the total number $N$ of running words. If we ignore punctuation, the following Brown sentence has 16 tokens and 14 types: \nHerdan’s Law Heaps’ Law \nThey picnicked by the pool, then lay back on the grass and looked at the stars. \nWhen we speak about the number of words in the language, we are generally referring to word types. \nFig. 2.11 shows the rough numbers of types and tokens computed from some popular English corpora. The larger the corpora we look at, the more word types we find, and in fact this relationship between the number of types $| V |$ and number of tokens $N$ is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978) after its discoverers (in linguistics and information retrieval respectively). It is shown in Eq. 2.1, where $k$ and $beta$ are positive constants, and $0 < beta < 1$ . \nThe value of $beta$ depends on the corpus size and the genre, but at least for the large corpora in Fig. 2.11, $beta$ ranges from .67 to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. \nAnother measure of the number of words in the language is the number of lemmas instead of wordform types. Dictionaries can help in giving lemma counts; dictionary entries or boldface forms are a very rough upper bound on the number of lemmas (since some lemmas have multiple boldface forms). The 1989 edition of the Oxford English Dictionary had 615,000 entries. \n2.3 Corpora \nWords don’t appear out of nowhere. Any particular piece of text that we study is produced by one or more specific speakers or writers, in a specific dialect of a specific language, at a specific time, in a specific place, for a specific function. \nAAE \nPerhaps the most important dimension of variation is the language. NLP algorithms are most useful when they apply across many languages. The world has 7097 languages at the time of this writing, according to the online Ethnologue catalog (Simons and Fennig, 2018). It is important to test algorithms on more than one language, and particularly on languages with different properties; by contrast there is an unfortunate current tendency for NLP algorithms to be developed or tested just on English (Bender, 2019). Even when algorithms are developed beyond English, they tend to be developed for the official languages of large industrialized nations (Chinese, Spanish, Japanese, German etc.), but we don’t want to limit tools to just these few languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by different social groups. Thus, for example, if we’re processing text that uses features of African American English (AAE) or African American Vernacular English (AAVE)—the variations of English used by millions of people in African American communities (King 2020)—we must use NLP tools that function with features of those varieties. Twitter posts might use features often used by speakers of African American English, such as constructions like iont (I don’t in Mainstream American English (MAE)), or talmbout corresponding to MAE talking about, both examples that influence word segmentation (Blodgett et al. 2016, Jones 2015). \nMAE \ncode switching \nIt’s also quite common for speakers or writers to use multiple languages in a single communicative act, a phenomenon called code switching. Code switching is enormously common across the world; here are examples showing Spanish and (transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens et al. 2017):",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Words",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The value of $beta$ depends on the corpus size and the genre, but at least for the large corpora in Fig. 2.11, $beta$ ranges from .67 to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. \nAnother measure of the number of words in the language is the number of lemmas instead of wordform types. Dictionaries can help in giving lemma counts; dictionary entries or boldface forms are a very rough upper bound on the number of lemmas (since some lemmas have multiple boldface forms). The 1989 edition of the Oxford English Dictionary had 615,000 entries. \n2.3 Corpora \nWords don’t appear out of nowhere. Any particular piece of text that we study is produced by one or more specific speakers or writers, in a specific dialect of a specific language, at a specific time, in a specific place, for a specific function. \nAAE \nPerhaps the most important dimension of variation is the language. NLP algorithms are most useful when they apply across many languages. The world has 7097 languages at the time of this writing, according to the online Ethnologue catalog (Simons and Fennig, 2018). It is important to test algorithms on more than one language, and particularly on languages with different properties; by contrast there is an unfortunate current tendency for NLP algorithms to be developed or tested just on English (Bender, 2019). Even when algorithms are developed beyond English, they tend to be developed for the official languages of large industrialized nations (Chinese, Spanish, Japanese, German etc.), but we don’t want to limit tools to just these few languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by different social groups. Thus, for example, if we’re processing text that uses features of African American English (AAE) or African American Vernacular English (AAVE)—the variations of English used by millions of people in African American communities (King 2020)—we must use NLP tools that function with features of those varieties. Twitter posts might use features often used by speakers of African American English, such as constructions like iont (I don’t in Mainstream American English (MAE)), or talmbout corresponding to MAE talking about, both examples that influence word segmentation (Blodgett et al. 2016, Jones 2015). \nMAE \ncode switching \nIt’s also quite common for speakers or writers to use multiple languages in a single communicative act, a phenomenon called code switching. Code switching is enormously common across the world; here are examples showing Spanish and (transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens et al. 2017): \n(2.2) Por primera vez veo a $@$ username actually being hateful! it was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) $jmath$   \n(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe [“he was and will remain a friend ... don’t worry ... but have faith”] \nAnother dimension of variation is the genre. The text that our algorithms must process might come from newswire, fiction or non-fiction books, scientific articles, Wikipedia, or religious texts. It might come from spoken genres like telephone conversations, business meetings, police body-worn cameras, medical interviews, or transcripts of television shows or movies. It might come from work situations like doctors’ notes, legal text, or parliamentary or congressional proceedings. \nText also reflects the demographic characteristics of the writer (or speaker): their age, gender, race, socioeconomic class can all influence the linguistic properties of the text we are processing. \nAnd finally, time matters too. Language changes over time, and for some languages we have good corpora of texts from different historical periods. \ndatasheet \nBecause language is so situated, when developing computational models for language processing from a corpus, it’s important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these details? The best way is for the corpus creator to build a datasheet (Gebru et al., 2020) or data statement (Bender and Friedman, 2018) for each corpus. A datasheet specifies properties of a dataset like: \nMotivation: Why was the corpus collected, by whom, and who funded it? \nSituation: When and in what situation was the text written/spoken? For example, was there a task? Was the language originally spoken conversation, edited text, social media communication, monologue vs. dialogue?   \nLanguage variety: What language (including dialect/region) was the corpus in?   \nSpeaker demographics: What was, e.g., age or gender of the authors of the text?   \nCollection process: How big is the data? If it is a subsample how was it sampled? Was the data collected with consent? How was the data pre-processed, and what metadata is available?   \nAnnotation process: What are the annotations, what are the demographics of the annotators, how were they trained, how was the data annotated?   \nDistribution: Are there copyright or other intellectual property restrictions? \n\n2.4 Text Normalization \nBefore almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: \n1. Tokenizing (segmenting) words   \n2. Normalizing word formats   \n3. Segmenting sentences \nIn the next sections we walk through each of these tasks.",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Corpora",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "(2.2) Por primera vez veo a $@$ username actually being hateful! it was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) $jmath$   \n(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe [“he was and will remain a friend ... don’t worry ... but have faith”] \nAnother dimension of variation is the genre. The text that our algorithms must process might come from newswire, fiction or non-fiction books, scientific articles, Wikipedia, or religious texts. It might come from spoken genres like telephone conversations, business meetings, police body-worn cameras, medical interviews, or transcripts of television shows or movies. It might come from work situations like doctors’ notes, legal text, or parliamentary or congressional proceedings. \nText also reflects the demographic characteristics of the writer (or speaker): their age, gender, race, socioeconomic class can all influence the linguistic properties of the text we are processing. \nAnd finally, time matters too. Language changes over time, and for some languages we have good corpora of texts from different historical periods. \ndatasheet \nBecause language is so situated, when developing computational models for language processing from a corpus, it’s important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these details? The best way is for the corpus creator to build a datasheet (Gebru et al., 2020) or data statement (Bender and Friedman, 2018) for each corpus. A datasheet specifies properties of a dataset like: \nMotivation: Why was the corpus collected, by whom, and who funded it? \nSituation: When and in what situation was the text written/spoken? For example, was there a task? Was the language originally spoken conversation, edited text, social media communication, monologue vs. dialogue?   \nLanguage variety: What language (including dialect/region) was the corpus in?   \nSpeaker demographics: What was, e.g., age or gender of the authors of the text?   \nCollection process: How big is the data? If it is a subsample how was it sampled? Was the data collected with consent? How was the data pre-processed, and what metadata is available?   \nAnnotation process: What are the annotations, what are the demographics of the annotators, how were they trained, how was the data annotated?   \nDistribution: Are there copyright or other intellectual property restrictions? \n\n2.4 Text Normalization \nBefore almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: \n1. Tokenizing (segmenting) words   \n2. Normalizing word formats   \n3. Segmenting sentences \nIn the next sections we walk through each of these tasks. \n2.4.1 Unix Tools for Crude Tokenization and Normalization \nLet’s begin with an easy, if somewhat naive version of word tokenization and normalization (and frequency computation) that can be accomplished for English solely in a single UNIX command-line, inspired by Church (1994). We’ll make use of some Unix commands: tr, used to systematically change particular characters in the input; sort, which sorts input lines in alphabetical order; and uniq, which collapses and counts adjacent identical lines. \nFor example let’s begin with the ‘complete words’ of Shakespeare in one file, sh.txt. We can use tr to tokenize the words by changing every sequence of nonalphabetic characters to a newline (’A-Za-z’ means alphabetic, the -c option complements to non-alphabet, and the -s option squeezes all sequences into a single character): \nThe output of this command will be: \nTHE   \nSONNETS   \nby   \nWilliam   \nShakespeare From   \nfairest   \ncreatures   \nWe   \n... \nNow that there is one word per line, we can sort the lines, and pass them to uniq -c which will collapse and count them: \ntr -sc ’A-Za-z’ ’n’ < sh.txt | sort | uniq -c with the following output: \n1945 A   \n72 AARON   \n19 ABBESS   \n25 Aaron   \n6 Abate   \n1 Abates   \n5 Abbess   \n6 Abbey   \n3 Abbot \nAlternatively, we can collapse all the upper case to lower case: tr -sc ’A-Za-z’ ’n’ $angle { bf Phi }$ sh.txt | tr A-Z a-z | sort | uniq -c whose output is \n14725 a 97 aaron 1 abaissiez 10 abandon 2 abandoned \n2 abase   \n1 abash   \n14 abate   \n3 abated   \n3 abatement \nNow we can sort again to find the frequent words. The $- mathtt { n }$ option to sort means to sort numerically rather than alphabetically, and the $- mathbf { r }$ option means to sort in reverse order (highest-to-lowest): \ntr -sc ’A-Za-z’ ’n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r \nThe results show that the most frequent words in Shakespeare, as in any other corpus, are the short function words like articles, pronouns, prepositions: \n27378 the   \n26084 and   \n22538 i   \n19771 to   \n17481 of   \n14725 a   \n13826 you   \n12489 my   \n11318 that   \n11112 in \nUnix tools of this sort can be very handy in building quick word count statistics for any corpus. \ntokenization \n2.4.2 Word Tokenization \nThe simple UNIX tools above were fine for getting rough word statistics but more sophisticated algorithms are generally necessary for tokenization, the task of segmenting running text into words. \nWhile the Unix command sequence just removed all the numbers and punctuation, for most NLP applications we’ll need to keep these in our tokenization. We often want to break off punctuation as a separate token; commas are a useful piece of information for parsers, periods help indicate sentence boundaries. But we’ll often want to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&T, and cap’n. Special characters and numbers will need to be kept in prices $( $ 45.55)$ and dates $( boldsymbol { mathfrak { O } } 1 / boldsymbol { mathfrak { O } } 2 / boldsymbol { mathfrak { O } } 6 )$ ; we don’t want to segment that price into separate tokens of $^ { 6 6 } 4 5 ^ { , , , }$ and “ $5 5 ^ { , , }$ . And there are URLs (http://www.stanford.edu), Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu). \nNumber expressions introduce other complications as well; while commas normally appear at word boundaries, commas are used inside numbers in English, every three digits: 555,500.50. Languages, and hence tokenization requirements, differ on this; many continental European languages like Spanish, French, and German, by contrast, use a comma to mark the decimal point, and spaces (or sometimes periods) where English puts commas, for example, 555 500,50. \nclitic \nA tokenizer can also be used to expand clitic contractions that are marked by apostrophes, for example, converting what’re to the two tokens what are, and we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only occur when it is attached to another word. Some such contractions occur in other alphabetic languages, including articles and pronouns in French (j’ai, l’homme).",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Text Normalization",
        "subsection": "Unix Tools for Crude Tokenization and Normalization",
        "subsubsection": "N/A"
    },
    {
        "content": "2 abase   \n1 abash   \n14 abate   \n3 abated   \n3 abatement \nNow we can sort again to find the frequent words. The $- mathtt { n }$ option to sort means to sort numerically rather than alphabetically, and the $- mathbf { r }$ option means to sort in reverse order (highest-to-lowest): \ntr -sc ’A-Za-z’ ’n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r \nThe results show that the most frequent words in Shakespeare, as in any other corpus, are the short function words like articles, pronouns, prepositions: \n27378 the   \n26084 and   \n22538 i   \n19771 to   \n17481 of   \n14725 a   \n13826 you   \n12489 my   \n11318 that   \n11112 in \nUnix tools of this sort can be very handy in building quick word count statistics for any corpus. \ntokenization \n2.4.2 Word Tokenization \nThe simple UNIX tools above were fine for getting rough word statistics but more sophisticated algorithms are generally necessary for tokenization, the task of segmenting running text into words. \nWhile the Unix command sequence just removed all the numbers and punctuation, for most NLP applications we’ll need to keep these in our tokenization. We often want to break off punctuation as a separate token; commas are a useful piece of information for parsers, periods help indicate sentence boundaries. But we’ll often want to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&T, and cap’n. Special characters and numbers will need to be kept in prices $( $ 45.55)$ and dates $( boldsymbol { mathfrak { O } } 1 / boldsymbol { mathfrak { O } } 2 / boldsymbol { mathfrak { O } } 6 )$ ; we don’t want to segment that price into separate tokens of $^ { 6 6 } 4 5 ^ { , , , }$ and “ $5 5 ^ { , , }$ . And there are URLs (http://www.stanford.edu), Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu). \nNumber expressions introduce other complications as well; while commas normally appear at word boundaries, commas are used inside numbers in English, every three digits: 555,500.50. Languages, and hence tokenization requirements, differ on this; many continental European languages like Spanish, French, and German, by contrast, use a comma to mark the decimal point, and spaces (or sometimes periods) where English puts commas, for example, 555 500,50. \nclitic \nA tokenizer can also be used to expand clitic contractions that are marked by apostrophes, for example, converting what’re to the two tokens what are, and we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only occur when it is attached to another word. Some such contractions occur in other alphabetic languages, including articles and pronouns in French (j’ai, l’homme). \n\nDepending on the application, tokenization algorithms may also tokenize multiword expressions like New York or rock ’n’ roll as a single token, which requires a multiword expression dictionary of some sort. Tokenization is thus intimately tied up with named entity recognition, the task of detecting names, dates, and organizations (Chapter 8). \nOne commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn’t becomes does plus $n ^ { prime } t$ ), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces ‘ ’ between tokens, although newlines is a more common output): \nInput: \"The San Francisco-based restaurant,\" they said, \"doesn’t charge $$ 10^ { \" }$ .   \nOutput: \" The San Francisco-based restaurant , \" they said , \" does n’t charge $ 10 \" . \nIn practice, since tokenization needs to be run before any other language processing, it needs to be very fast. The standard method for tokenization is therefore to use deterministic algorithms based on regular expressions compiled into very efficient finite state automata. For example, Fig. 2.12 shows an example of a basic regular expression that can be used to tokenize with the nltk.regexp tokenize function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009; http://www.nltk.org). \n$> > >$ text $mathbf { tau } = mathbf { tau }$ ’That U.S.A. poster-print costs $12.40..   \n$> > > { mathrm {  p a t t e r n ~ = ~ } } mathbf { r } ^ { prime } > mathbf { Omega } ^ { prime } left( ref { s i x } right)$ # set flag to allow verbose regexps $( [ mathtt { A } mathtt { - } mathtt { Z } ] setminus . ) +$ # abbreviations, e.g. U.S.A. | $mathbf { boldsymbol { w } } mathbf { + } left( - mathbf { boldsymbol { lambda } } mathbf { boldsymbol { w } } mathbf { + } right) cdots$ # words with optional internal hyphens $backslash  S ? setminus mathbf { d } + ( setminus . setminus mathbf { d } + ) ? %?$ # currency and percentages, e.g. $12.40, 82% | ... # ellipsis | [][.,;\"’?():-_‘] # these are separate tokens; includes ], [   \n$ggg > >$ nltk.regexp_tokenize(text, pattern)   \n[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’] \nCarefully designed deterministic algorithms can deal with the ambiguities that arise, such as the fact that the apostrophe needs to be tokenized differently when used as a genitive marker (as in the book’s cover), a quotative as in ‘The other class’, she said, or in clitics like they’re. \nWord tokenization is more complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential word-boundaries. In Chinese, for example, words are composed of characters (called hanzi in Chinese). Each character generally represents a single unit of meaning (called a morpheme) and is pronounceable as a single syllable. Words are about 2.4 characters long on average. \nBut deciding what counts as a word in Chinese is complex. For example, consider the following sentence: \n(2.4) 姚明进入总决赛 “Yao Ming reaches the finals”   \nAs Chen et al. (2017b) point out, this could be treated as 3 words (‘Chinese Tree  \nbank’ segmentation):   \n(2.5) 姚明 进入 总决赛 YaoMing reaches finals   \nor as 5 words (‘Peking University’ segmentation):   \n(2.6) 姚 明 进入 总 决赛 Yao Ming reaches overall finals   \nFinally, it is possible in Chinese simply to ignore words altogether and use characters   \nas the basic elements, treating the sentence as a series of 7 characters:   \n(2.7) 姚 明 进 入 总 决 赛 Yao Ming enter enter overall decision game   \nIn fact, for most Chinese NLP tasks it turns out to work better to take characters   \nrather than words as input, since characters are at a reasonable semantic level for   \nmost applications, and since most word standards, by contrast, result in a huge vo  \ncabulary with large numbers of very rare words (Li et al., 2019b). \nHowever, for Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese in the rare situations where word rather than character boundaries are required. The standard segmentation algorithms for these languages use neural sequence models trained via supervised machine learning on hand-segmented training sets; we’ll introduce sequence models in Chapter 8 and Chapter 9. \n2.4.3 Byte-Pair Encoding for Tokenization \nThere is a third option to tokenizing text. Instead of defining tokens as words (whether delimited by spaces or more complex algorithms), or as characters (as in Chinese), we can use our data to automatically tell us what the tokens should be. This is especially useful in dealing with unknown words, an important problem in language processing. As we will see in the next chapter, NLP algorithms often learn some facts about language from one corpus (a training corpus) and then use these facts to make decisions about a separate test corpus and its language. Thus if our training corpus contains, say the words low, new, newer, but not lower, then if the word lower appears in our test corpus, our system will not know what to do with it. \nsubwords \nTo deal with this unknown word problem, modern tokenizers often automatically induce sets of tokens that include tokens smaller than words, called subwords. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the morphemes -est or -er. (A morpheme is the smallest meaning-bearing unit of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.) In modern tokenization schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er. Every unseen word like lower can thus be represented by some sequence of known subword units, such as low and er, or even as a sequence of individual letters if necessary. \nMost tokenization schemes have two parts: a token learner, and a token segmenter. The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens. The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary. Three algorithms are widely used: byte-pair encoding (Sennrich et al., 2016), unigram language modeling (Kudo, 2018), and WordPiece (Schuster and Nakajima, 2012); there is also a SentencePiece library that includes implementations of the first two of the three (Kudo and Richardson, 2018).",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Text Normalization",
        "subsection": "Word Tokenization",
        "subsubsection": "N/A"
    },
    {
        "content": "But deciding what counts as a word in Chinese is complex. For example, consider the following sentence: \n(2.4) 姚明进入总决赛 “Yao Ming reaches the finals”   \nAs Chen et al. (2017b) point out, this could be treated as 3 words (‘Chinese Tree  \nbank’ segmentation):   \n(2.5) 姚明 进入 总决赛 YaoMing reaches finals   \nor as 5 words (‘Peking University’ segmentation):   \n(2.6) 姚 明 进入 总 决赛 Yao Ming reaches overall finals   \nFinally, it is possible in Chinese simply to ignore words altogether and use characters   \nas the basic elements, treating the sentence as a series of 7 characters:   \n(2.7) 姚 明 进 入 总 决 赛 Yao Ming enter enter overall decision game   \nIn fact, for most Chinese NLP tasks it turns out to work better to take characters   \nrather than words as input, since characters are at a reasonable semantic level for   \nmost applications, and since most word standards, by contrast, result in a huge vo  \ncabulary with large numbers of very rare words (Li et al., 2019b). \nHowever, for Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese in the rare situations where word rather than character boundaries are required. The standard segmentation algorithms for these languages use neural sequence models trained via supervised machine learning on hand-segmented training sets; we’ll introduce sequence models in Chapter 8 and Chapter 9. \n2.4.3 Byte-Pair Encoding for Tokenization \nThere is a third option to tokenizing text. Instead of defining tokens as words (whether delimited by spaces or more complex algorithms), or as characters (as in Chinese), we can use our data to automatically tell us what the tokens should be. This is especially useful in dealing with unknown words, an important problem in language processing. As we will see in the next chapter, NLP algorithms often learn some facts about language from one corpus (a training corpus) and then use these facts to make decisions about a separate test corpus and its language. Thus if our training corpus contains, say the words low, new, newer, but not lower, then if the word lower appears in our test corpus, our system will not know what to do with it. \nsubwords \nTo deal with this unknown word problem, modern tokenizers often automatically induce sets of tokens that include tokens smaller than words, called subwords. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the morphemes -est or -er. (A morpheme is the smallest meaning-bearing unit of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.) In modern tokenization schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er. Every unseen word like lower can thus be represented by some sequence of known subword units, such as low and er, or even as a sequence of individual letters if necessary. \nMost tokenization schemes have two parts: a token learner, and a token segmenter. The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens. The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary. Three algorithms are widely used: byte-pair encoding (Sennrich et al., 2016), unigram language modeling (Kudo, 2018), and WordPiece (Schuster and Nakajima, 2012); there is also a SentencePiece library that includes implementations of the first two of the three (Kudo and Richardson, 2018). \n\nIn this section we introduce the simplest of the three, the byte-pair encoding or BPE BPE algorithm (Sennrich et al., 2016); see Fig. 2.13. The BPE token learner begins with a vocabulary that is just the set of all individual characters. It then examines the training corpus, chooses the two symbols that are most frequently adjacent (say ‘A’, ‘B’), adds a new merged symbol ‘AB’ to the vocabulary, and replaces every adjacent $mathbf { nabla } ^ { , } mathbf { A } ^ { , } mathbf { nabla } ^ { , } mathbf { B } ^ { , }$ in the corpus with the new ‘AB’. It continues to count and merge, creating new longer and longer character strings, until $k$ merges have been done creating $k$ novel tokens; $k$ is thus a parameter of the algorithm. The resulting vocabulary consists of the original set of characters plus $k$ new symbols. \nThe algorithm is usually run inside words (not merging across word boundaries), so the input corpus is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a special end-of-word symbol , and its counts. Let’s see its operation on the following tiny input corpus of 18 word tokens with counts for each word (the word low appears 5 times, the word newer 6 times, and so on), which would have a starting vocabulary of 11 letters: \ncorpus vocabulary $begin{array} { r l } & { left| begin{array} { l l } { 5 } & { mathrm { ~ 1 ~ o ~ w ~ _ ~ } }  { 2 } & { mathrm { ~ 1 ~ o ~ w ~ e ~ s ~ t ~ _ ~ } }  { 5 } & { mathrm { ~ n ~ e ~ w ~ e ~ r ~ _ ~ } }  { 3 } & { mathrm { ~ w ~ i ~ d ~ e ~ r ~ _ ~ } } end{array} right| }  & { left| begin{array} { l l } { 2 } & { mathrm { ~ n ~ e ~ w ~ _ ~ } } end{array} right| } end{array}$ , d, e, i, l, n, o, r, s, t, w \nThe BPE algorithm first counts all pairs of adjacent symbols: the most frequent is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 occurrences1. We then merge these symbols, treating er as one symbol, and count again: \ncorpus vocabulary   \n5 $begin{array} { r } { begin{array} { l } { mathrm { ~ 1 ~ o ~ w ~ } _ { - } }  { mathrm { ~ 1 ~ o ~ w ~ e ~ s ~ t ~ } _ { - } }  { mathrm { ~ n ~ e ~ w ~ e ~ r ~ } _ { - } }  { mathrm { ~ w ~ i ~ d ~ e r ~ } _ { - } }  { mathrm { ~ n ~ e ~ w ~ } _ { - } } end{array} } end{array}$ , d, e, i, l, n, o, r, s, t, w, er   \n2   \n6   \n3   \n2 \nNow the most frequent pair is er , which we merge; our system has learned that there should be a token for word-final er, represented as er : \ncorpus \nvocabulary \n, d, e, i, l, n, o, r, s, t, w, er, er \n$begin{array} { r l } & { left{ begin{array} { l l } { 5 } & { mathrm { ~ verb ~ { ~ 1 ~ 0 ~ w ~ } ~ } - mathrm { ~ verb ~ { ~ 1 ~ 0 ~ w ~ } ~ } }  { 2 } & { mathrm { ~ verb ~ { ~ 1 ~ 0 ~ w ~ } ~ e ~ s ~ } mathrm { ~ verb ~ { ~ t ~ } ~ } - }  { 6 } & { mathrm { ~ verb ~ { ~ n ~ e ~ w ~ } ~ e r _ - } } end{array} right. }  & { left{ begin{array} { l l } { 3 } & { mathrm { ~ verb ~ { ~ w ~ i ~ } ~ d ~ } mathrm { ~ e r _ - } }  { 2 } & { mathrm { ~ verb ~ { ~ n ~ e ~ w ~ } ~ } - } end{array} right. } end{array}$ \nNext n e (total count of 8) get merged to ne: \ncorpus $begin{array} { r l } & { begin{array} { r l } { 5 } & { mathrm { ~ tt ~ l ~ o ~ w ~ tt ~ - ~ } }  { 2 } & { mathrm { ~ tt ~ l ~ o ~ w ~ e ~ s ~ t ~ tt ~ - ~ } }  { 6 } & { mathrm { ~ tt ~ n e ~ w ~ e r _ ~ } }  { 3 } & { mathrm { ~ tt ~ w ~ i ~ d ~ e r _ ~ } }  { 2 } & { mathrm { ~ tt ~ n e ~ w ~ tt ~ - ~ } } end{array} } end{array}$ \nvocabulary \n, d, e, i, l, n, o, r, s, t, w, er, er , ne \nIf we continue, the next merges are: \nMerge Current Vocabulary \n(ne, w) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new (l, o) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo (lo, w) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low (new, er ) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer (low, ) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer , low function BYTE-PAIR ENCODING(strings $C$ , number of merges $k$ ) returns vocab $V$ \n\n$V $ all unique characters in $C$ # initial set of tokens is characters for $i = 1$ to $k$ do # merge tokens til $k$ times $t _ { L }$ , $t _ { R } gets mathbf { M o s t }$ frequent pair of adjacent tokens in $C$ $begin{array} { c } { { t _ { N E W }  t _ { L } + t _ { R } } }  { { V  V + t _ { N E W } } } end{array}$ # update the vocabulary # make new token by concatenating Replace each occurrence of $t _ { L } , t _ { R }$ in $C$ with $t _ { N E W }$ # and update the corpus return $V$ \nFigure 2.13 The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens. Figure adapted from Bostrom and Durrett (2020). \nOnce we’ve learned our vocabulary, the token parser is used to tokenize a test sentence. The token parser just runs on the test data the merges we have learned from the training data, greedily, in the order we learned them. (Thus the frequencies in the test data don’t play a role, just the frequencies in the training data). So first we segment each test sentence word into characters. Then we apply the first rule: replace every instance of e r in the test corpus with er, and then the second rule: replace every instance of er in the test corpus with er , and so on. By the end, if the test corpus contained the word n e w e r , it would be tokenized as a full word. But a new (unknown) word like l o w e r would be merged into the two tokens low er . \nOf course in real algorithms BPE is run with many thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts. \n2.4.4 Word Normalization, Lemmatization and Stemming \nnormalization \nWord normalization is the task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in the normalization process. For information retrieval or information extraction about the US, we might want to see information from documents whether they mention the US or the USA.",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Text Normalization",
        "subsection": "Byte-Pair Encoding for Tokenization",
        "subsubsection": "N/A"
    },
    {
        "content": "corpus $begin{array} { r l } & { begin{array} { r l } { 5 } & { mathrm { ~ tt ~ l ~ o ~ w ~ tt ~ - ~ } }  { 2 } & { mathrm { ~ tt ~ l ~ o ~ w ~ e ~ s ~ t ~ tt ~ - ~ } }  { 6 } & { mathrm { ~ tt ~ n e ~ w ~ e r _ ~ } }  { 3 } & { mathrm { ~ tt ~ w ~ i ~ d ~ e r _ ~ } }  { 2 } & { mathrm { ~ tt ~ n e ~ w ~ tt ~ - ~ } } end{array} } end{array}$ \nvocabulary \n, d, e, i, l, n, o, r, s, t, w, er, er , ne \nIf we continue, the next merges are: \nMerge Current Vocabulary \n(ne, w) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new (l, o) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo (lo, w) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low (new, er ) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer (low, ) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer , low function BYTE-PAIR ENCODING(strings $C$ , number of merges $k$ ) returns vocab $V$ \n\n$V $ all unique characters in $C$ # initial set of tokens is characters for $i = 1$ to $k$ do # merge tokens til $k$ times $t _ { L }$ , $t _ { R } gets mathbf { M o s t }$ frequent pair of adjacent tokens in $C$ $begin{array} { c } { { t _ { N E W }  t _ { L } + t _ { R } } }  { { V  V + t _ { N E W } } } end{array}$ # update the vocabulary # make new token by concatenating Replace each occurrence of $t _ { L } , t _ { R }$ in $C$ with $t _ { N E W }$ # and update the corpus return $V$ \nFigure 2.13 The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens. Figure adapted from Bostrom and Durrett (2020). \nOnce we’ve learned our vocabulary, the token parser is used to tokenize a test sentence. The token parser just runs on the test data the merges we have learned from the training data, greedily, in the order we learned them. (Thus the frequencies in the test data don’t play a role, just the frequencies in the training data). So first we segment each test sentence word into characters. Then we apply the first rule: replace every instance of e r in the test corpus with er, and then the second rule: replace every instance of er in the test corpus with er , and so on. By the end, if the test corpus contained the word n e w e r , it would be tokenized as a full word. But a new (unknown) word like l o w e r would be merged into the two tokens low er . \nOf course in real algorithms BPE is run with many thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts. \n2.4.4 Word Normalization, Lemmatization and Stemming \nnormalization \nWord normalization is the task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in the normalization process. For information retrieval or information extraction about the US, we might want to see information from documents whether they mention the US or the USA. \ncase folding \nmorpheme stem affix \nCase folding is another kind of normalization. Mapping everything to lower case means that Woodchuck and woodchuck are represented identically, which is very helpful for generalization in many tasks, such as information retrieval or speech recognition. For sentiment analysis and other text classification tasks, information extraction, and machine translation, by contrast, case can be quite helpful and case folding is generally not done. This is because maintaining the difference between, for example, US the country and us the pronoun can outweigh the advantage in generalization that case folding would have provided for other words. \nFor many natural language processing situations we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages that mention woodchuck with no $s$ . This is especially common in morphologically complex languages like Russian, where for example the word Moscow has different endings in the phrases Moscow, of Moscow, to Moscow, and so on. \nLemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story. \nHow is lemmatization done? The most sophisticated methods for lemmatization involve complete morphological parsing of the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes. Two broad classes of morphemes can be distinguished: stems—the central morpheme of the word, supplying the main meaning—and affixes—adding “additional” meanings of various kinds. So, for example, the word fox consists of one morpheme (the morpheme fox) and the word cats consists of two: the morpheme cat and the morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and $s$ , or parses a Spanish word like amaren (‘if in the future they would love’) into the morpheme amar ‘to love’, and the morphological features 3PL and future subjunctive. \nstemming Porter stemmer \nThe Porter Stemmer \nLemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off word-final affixes. This naive version of morphological analysis is called stemming. One of the most widely used stemming algorithms is the Porter (1980). The Porter stemmer applied to the following paragraph: \nThis was not the map we found in Billy Bones’s chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes. \nproduces the following stemmed output: \nThi wa not the map we found in Billi Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red cross and the written note \ncascade \nThe algorithm is based on series of rewrite rules run in series, as a cascade, in which the output of each pass is fed as input to the next pass; here is a sampling of \nthe rules: \nDetailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.) can be found on Martin Porter’s homepage; see also the original paper (Porter, 1980). \nSimple stemmers can be useful in cases where we need to collapse across different variants of the same lemma. Nonetheless, they do tend to commit errors of both over- and under-generalizing, as shown in the table below (Krovetz, 1993): \n2.4.5 Sentence Segmentation \nsentence segmentation \nSentence segmentation is another important step in text processing. The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points. Question marks and exclamation points are relatively unambiguous markers of sentence boundaries. Periods, on the other hand, are more ambiguous. The period character “.” is ambiguous between a sentence boundary marker and a marker of abbreviations like $M r$ or Inc. The previous sentence that you just read showed an even more complex case of this ambiguity, in which the final period of Inc. marked both an abbreviation and the sentence boundary marker. For this reason, sentence tokenization and word tokenization may be addressed jointly. \nIn general, sentence tokenization methods work by first deciding (based on rules or machine learning) whether a period is part of the word or is a sentence-boundary marker. An abbreviation dictionary can help determine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or machinelearned (Kiss and Strunk, 2006), as can the final sentence splitter. In the Stanford CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based, a deterministic consequence of tokenization; a sentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped with other characters into a token (such as for an abbreviation or number), optionally followed by additional final quotes or brackets. \n2.5 Minimum Edit Distance \nMuch of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous string—let’s say graffe–and we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity:",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Text Normalization",
        "subsection": "Word Normalization, Lemmatization and Stemming",
        "subsubsection": "N/A"
    },
    {
        "content": "the rules: \nDetailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.) can be found on Martin Porter’s homepage; see also the original paper (Porter, 1980). \nSimple stemmers can be useful in cases where we need to collapse across different variants of the same lemma. Nonetheless, they do tend to commit errors of both over- and under-generalizing, as shown in the table below (Krovetz, 1993): \n2.4.5 Sentence Segmentation \nsentence segmentation \nSentence segmentation is another important step in text processing. The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points. Question marks and exclamation points are relatively unambiguous markers of sentence boundaries. Periods, on the other hand, are more ambiguous. The period character “.” is ambiguous between a sentence boundary marker and a marker of abbreviations like $M r$ or Inc. The previous sentence that you just read showed an even more complex case of this ambiguity, in which the final period of Inc. marked both an abbreviation and the sentence boundary marker. For this reason, sentence tokenization and word tokenization may be addressed jointly. \nIn general, sentence tokenization methods work by first deciding (based on rules or machine learning) whether a period is part of the word or is a sentence-boundary marker. An abbreviation dictionary can help determine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or machinelearned (Kiss and Strunk, 2006), as can the final sentence splitter. In the Stanford CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based, a deterministic consequence of tokenization; a sentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped with other characters into a token (such as for an abbreviation or number), optionally followed by additional final quotes or brackets. \n2.5 Minimum Edit Distance \nMuch of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous string—let’s say graffe–and we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity:",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Text Normalization",
        "subsection": "Sentence Segmentation",
        "subsubsection": "N/A"
    },
    {
        "content": "Stanford President Marc Tessier-Lavigne Stanford University President Marc Tessier-Lavigne \nminimum edit distance \nAgain, the fact that these two strings are very similar (differing by only one word) seems like useful evidence for deciding that they might be coreferent. \nEdit distance gives us a way to quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. \nThe gap between intention and execution, for example, is 5 (delete an i, substitute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see this by looking at the most important visualization for string distances, an alignment between the two strings, shown in Fig. 2.14. Given two sequences, an alignment is a correspondence between substrings of the two sequences. Thus, we say I aligns with the empty string, N with E, and so on. Beneath the aligned strings is another representation; a series of symbols expressing an operation list for converting the top string into the bottom string: d for deletion, s for substitution, i for insertion. \nWe can also assign a particular cost or weight to each of these operations. The Levenshtein distance between two sequences is the simplest weighting factor in which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume that the substitution of a letter for itself, for example, t for t, has zero cost. The Levenshtein distance between intention and execution is 5. Levenshtein also proposed an alternative version of his metric in which each insertion or deletion has a cost of 1 and substitutions are not allowed. (This is equivalent to allowing substitution, but giving each substitution a cost of 2 since any substitution can be represented by one insertion and one deletion). Using this version, the Levenshtein distance between intention and execution is 8. \n2.5.1 The Minimum Edit Distance Algorithm \nHow do we find the minimum edit distance? We can think of this as a search task, in which we are searching for the shortest path—a sequence of edits—from one string to another. \nThe space of all possible edits is enormous, so we can’t search naively. However, lots of distinct edit paths will end up in the same state (string), so rather than recomputing all those paths, we could just remember the shortest path to a state each time we saw it. We can do this by using dynamic programming. Dynamic programming is the name for a class of algorithms, first introduced by Bellman (1957), that apply a table-driven method to solve problems by combining solutions to sub-problems. Some of the most commonly used algorithms in natural language processing make use of dynamic programming, such as the Viterbi algorithm (Chapter 8) and the CKY algorithm for parsing (Chapter 13). \n\nThe intuition of a dynamic programming problem is that a large problem can be solved by properly combining the solutions to various sub-problems. Consider the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16. \nImagine some string (perhaps it is exention) that is in this optimal path (whatever it is). The intuition of dynamic programming is that if exention is in the optimal operation list, then the optimal sequence must also include the optimal path from intention to exention. Why? If there were a shorter path from intention to exention, then we could use it instead, resulting in a shorter overall path, and the optimal sequence wouldn’t be optimal, thus leading to a contradiction. \nThe minimum edit distance algorithm algorithm was named by Wagner and Fischer (1974) but independently discovered by many people (see the Historical Notes section of Chapter 8). \nLet’s first define the minimum edit distance between two strings. Given two strings, the source string $X$ of length $n$ , and target string $Y$ of length $m$ , we’ll define $D [ i , j ]$ as the edit distance between $X [ 1 . . i ]$ and $Y [ 1 . . j ]$ , i.e., the first $i$ characters of $X$ and the first $j$ characters of $Y$ . The edit distance between $X$ and $Y$ is thus $D [ n , m ]$ . \nWe’ll use dynamic programming to compute $D [ n , m ]$ bottom up, combining solutions to subproblems. In the base case, with a source substring of length $i$ but an empty target string, going from $i$ characters to 0 requires $i$ deletes. With a target substring of length $j$ but an empty source going from 0 characters to $j$ characters requires $j$ inserts. Having computed $D [ i , j ]$ for small $i , j$ we then compute larger $D [ i , j ]$ based on previously computed smaller values. The value of $D [ i , j ]$ is computed by taking the minimum of the three possible paths through the matrix which \narrive there: \nIf we assume the version of Levenshtein distance in which the insertions and deletions each have a cost of $1  ( mathrm { i n s - c o s t ( cdot ) } = mathrm { d e l - c o s t ( cdot ) } = 1 )$ , and substitutions have a cost of 2 (except substitution of identical letters have zero cost), the computation for $D [ i , j ]$ becomes: \nThe algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying the algorithm to the distance between intention and execution with the version of Levenshtein in Eq. 2.8. \nFigure 2.17 The minimum edit distance algorithm, an example of the class of dynamic programming algorithms. The various costs can either be fixed (e.g., $forall x , operatorname { i n s - c o s t } ( x ) = 1 )$ or can be specific to the letter (to model the fact that some letters are more likely to be inserted than others). We assume that there is no cost for substituting a letter for itself (i.e., sub-cost $( x , x ) = 0 quad$ ). \nAlignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is \nbacktrace \nused to compute the word error rate (Chapter 26). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \nTo extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path through the edit distance matrix. Figure 2.19 shows this path with the boldfaced cell. Each boldfaced cell represents an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion. \nFigure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a backtrace, we start from the last cell (at the final row and column), and follow the pointers back through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment. \nWhile we worked our example with simple Levenshtein distance, the algorithm in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for example, substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi algorithm is a probabilistic extension of minimum edit distance. Instead of computing the “minimum edit distance” between two strings, Viterbi computes the “maximum probability alignment” of one string with another. We’ll discuss this more in Chapter 8. \n2.6 Summary \nThis chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Minimum Edit Distance",
        "subsection": "The Minimum Edit Distance Algorithm",
        "subsubsection": "N/A"
    },
    {
        "content": "backtrace \nused to compute the word error rate (Chapter 26). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \nTo extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path through the edit distance matrix. Figure 2.19 shows this path with the boldfaced cell. Each boldfaced cell represents an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion. \nFigure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a backtrace, we start from the last cell (at the final row and column), and follow the pointers back through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment. \nWhile we worked our example with simple Levenshtein distance, the algorithm in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for example, substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi algorithm is a probabilistic extension of minimum edit distance. Instead of computing the “minimum edit distance” between two strings, Viterbi computes the “maximum probability alignment” of one string with another. We’ll discuss this more in Chapter 8. \n2.6 Summary \nThis chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "backtrace \nused to compute the word error rate (Chapter 26). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \nTo extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path through the edit distance matrix. Figure 2.19 shows this path with the boldfaced cell. Each boldfaced cell represents an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion. \nFigure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a backtrace, we start from the last cell (at the final row and column), and follow the pointers back through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment. \nWhile we worked our example with simple Levenshtein distance, the algorithm in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for example, substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi algorithm is a probabilistic extension of minimum edit distance. Instead of computing the “minimum edit distance” between two strings, Viterbi computes the “maximum probability alignment” of one string with another. We’ll discuss this more in Chapter 8. \n2.6 Summary \nThis chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "backtrace \nused to compute the word error rate (Chapter 26). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \nTo extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path through the edit distance matrix. Figure 2.19 shows this path with the boldfaced cell. Each boldfaced cell represents an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion. \nFigure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a backtrace, we start from the last cell (at the final row and column), and follow the pointers back through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment. \nWhile we worked our example with simple Levenshtein distance, the algorithm in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for example, substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi algorithm is a probabilistic extension of minimum edit distance. Instead of computing the “minimum edit distance” between two strings, Viterbi computes the “maximum probability alignment” of one string with another. We’ll discuss this more in Chapter 8. \n2.6 Summary \nThis chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including \nFigure 2.19 When entering a value in each cell, we mark which of the three neighboring cells we came from with up to three arrows. After the table is full we compute an alignment (minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and following the arrows back. The sequence of bold cells represents one possible minimum cost alignment between the two strings. Diagram design after Gusfield (1997). \nword segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here’s a summary of the main points we covered about these ideas: \n• The regular expression language is a powerful tool for pattern-matching.   \n• Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters $(  \" , + ,$ , and ${ mathtt { n } , mathtt { m } } ,$ ), anchors $( hat { bf Phi } ,  S )$ and precedence operators ((,)).   \n• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.   \n• The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.   \n• The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings. \nBibliographical and Historical Notes \nKleene 1951; 1956 first defined regular expressions and the finite automaton, based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for text searching (Thompson, 1968). His editor ed included a command “g/regular expression/p”, or Global Regular Expression Print, which later became the Unix grep utility. \nText normalization algorithms have been applied since the beginning of the field. One of the earliest widely used stemmers was Lovins (1968). Stemming was also applied early to the digital humanities, by Packard (1973), who built an affix-stripping morphological parser for Ancient Greek. Currently a wide variety of code for tokenization and normalization is available, such as the Stanford Tokenizer (http://nlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for Twitter (O’Connor et al., 2010), or for sentiment (http: //sentiment.christopherpotts.net/tokenizing.html). See Palmer (2012) for a survey of text preprocessing. NLTK is an essential tool that offers both useful \nPython libraries (http://www.nltk.org) and textbook descriptions (Bird et al., 2009) of many algorithms including text normalization and corpus interfaces. \nFor more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps (1978), Egghe (2007) and Baayen (2001); Yasseri et al. (2012) discuss the relationship with other measures of linguistic complexity. For more on edit distance, see the excellent Gusfield (1997). Our example measuring the edit distance from ‘intention’ to ‘execution’ was adapted from Kruskal (1983). There are various publicly available packages to compute edit distance, including Unix diff and the NIST sclite program (NIST, 2005). \nIn his autobiography Bellman (1984) explains how he originally came up with the term dynamic programming: \n“...The 1950s were not good years for mathematical research. [the] Secretary of Defense ...had a pathological fear and hatred of the word, research... I decided therefore to use the word, “programming”. I wanted to get across the idea that this was dynamic, this was multistage... I thought, let’s ... take a word that has an absolutely precise meaning, namely dynamic... it’s impossible to use the word, dynamic, in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to.” \nExercises \n2.1 Write regular expressions for the following languages. \n1. the set of all alphabetic strings; 2. the set of all lower case alphabetic strings ending in a $b$ ; 3. the set of all strings from the alphabet $^ { a , b }$ such that each $a$ is immediately preceded by and immediately followed by a $b$ ; \n2.2 Write regular expressions for the following languages. By “word”, we mean an alphabetic string separated from other words by whitespace, any relevant punctuation, line breaks, and so forth. \n1. the set of all strings with two consecutive repeated words (e.g., “Humbert Humbert” and “the the” but not “the bug” or “the big bug”);   \n2. all strings that start at the beginning of the line with an integer and that end at the end of the line with a word;   \n3. all strings that have both the word grotto and the word raven in them (but not, e.g., words like grottos that merely contain the word grotto);   \n4. write a pattern that places the first word of an English sentence in a register. Deal with punctuation.   \n2.3 Implement an ELIZA-like program, using substitutions such as those described on page 11. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.   \n2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of “leda” to “deal”. Show your work (using the edit distance grid).   \n2.5 Figure out whether drive is closer to brief or to divers and what the edit distance is to each. You may use any version of distance that you like.   \n2.6 Now implement a minimum edit distance algorithm and use your hand-computed results to check your code.   \n2.7 Augment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace. \n\n3 N-gram Language Models \n“You are uniformly charming!” cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for. Random sentence generated from a Jane Austen trigram model \nPredicting is difficult—especially about the future, as the old quip goes. But how about predicting something that seems much easier, like the next few words someone is going to say? What word, for example, is likely to follow \nPlease turn your homework \nHopefully, most of you concluded that a very likely word is in, or possibly over, but probably not refrigerator or the. In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word. The same models will also serve to assign a probability to an entire sentence. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text: \nall of a sudden I notice three guys standing on the sidewalk than does this same set of words in a different order: \non guys all I of notice sidewalk three a sudden standing the \nWhy would you want to predict upcoming words, or assign probabilities to sentences? Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition. For a speech recognizer to realize that you said I will be back soonish and not I will be bassoon dish, it helps to know that back soonish is a much more probable sequence than bassoon dish. For writing tools like spelling correction or grammatical error correction, we need to find and correct errors in writing like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are will be much more probable than Their are, and has improved than has improve, allowing us to help users by detecting and correcting these errors. \nAssigning probabilities to sequences of words is also essential in machine translation. Suppose we are translating a Chinese source sentence: \n他 向 记者 介绍了 主要 内容He to reporters introduced main content\nAs part of the process we might have built the following set of potential rough English translations: \nhe introduced reporters to the main contents of the statement he briefed to reporters the main contents of the statement he briefed reporters on the main contents of the statement",
        "chapter": "Regular Expressions, Text Normalization, Edit Distance",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A probabilistic model of word sequences could suggest that briefed reporters on is a more probable English phrase than briefed to reporters (which has an awkward to after briefed) or introduced reporters to (which uses a verb that is less fluent English in this context), allowing us to correctly select the boldfaced sentence above. \nAAC \nProbabilities are also important for augmentative and alternative communication systems (Trnka et al. 2007, Kane et al. 2017). People often use such AAC devices if they are physically unable to speak or sign but can instead use eye gaze or other specific movements to select words from a menu to be spoken by the system. Word prediction can be used to suggest likely words for the menu. \nlanguage model LM n-gram \nModels that assign probabilities to sequences of words are called language models or LMs. In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram. An n-gram is a sequence of $n$ words: a 2-gram (which we’ll call bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”. We’ll see how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences. In a bit of terminological ambiguity, we usually drop the word “model”, and use the term n-gram (and bigram, etc.) to mean either the word sequence itself or the predictive model that assigns it a probability. While n-gram models are much simpler than state-of-the art neural language models based on the RNNs and transformers we will introduce in Chapter 9, they are an important foundational tool for understanding the fundamental concepts of language modeling. \n3.1 N-Grams \nLet’s begin with the task of computing $P ( w | h )$ , the probability of a word $w$ given some history $h$ . Suppose the history $h$ is “its water is so transparent that” and we want to know the probability that the next word is the: \nOne way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we see its water is so transparent that, and count the number of times this is followed by the. This would be answering the question “Out of the times we saw the history $h$ , how many times was it followed by the word $w ^ { prime prime }$ , as follows: \nWith a large enough corpus, such as the web, we can compute these counts and estimate the probability from Eq. 3.2. You should pause now, go to the web, and compute this estimate for yourself. \nWhile this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web isn’t big enough to give us good estimates in most cases. This is because language is creative; new sentences are created all the time, and we won’t always be able to count entire sentences. Even simple extensions of the example sentence may have counts of zero on the web (such as “Walden Pond’s water is so transparent that the”; well, used to have counts of zero). \n\nSimilarly, if we wanted to know the joint probability of an entire sequence of words like its water is so transparent, we could do it by asking “out of all possible sequences of five words, how many of them are its water is so transparent?” We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences. That seems rather a lot to estimate! \nFor this reason, we’ll need to introduce more clever ways of estimating the probability of a word $w$ given a history $h$ , or the probability of an entire word sequence $W$ . Let’s start with a little formalizing of notation. To represent the probability of a particular random variable $X _ { i }$ taking on the value “the”, or $P ( X _ { i } = ^ { cdots } { mathrm { t h e } } ^ { cdots } )$ , we will use the simplification $P ( t h e )$ . We’ll represent a sequence of $N$ words either as $w _ { 1 } ldots w _ { n }$ or $w _ { 1 : n }$ (so the expression $w _ { 1 : n - 1 }$ means the string $w _ { 1 } , w _ { 2 } , . . . , w _ { n - 1 } )$ . For the joint probability of each word in a sequence having a particular value $P ( X _ { 1 } = w _ { 1 } , X _ { 2 } =$ $w _ { 2 } , X _ { 3 } = w _ { 3 } , . . . , X _ { n } = w _ { n } )$ we’ll use $P ( w _ { 1 } , w _ { 2 } , . . . , w _ { n } )$ . \nNow how can we compute probabilities of entire sequences like $P ( w _ { 1 } , w _ { 2 } , . . . , w _ { n } ) ?$ One thing we can do is decompose this probability using the chain rule of probability: \nApplying the chain rule to words, we get \nThe chain rule shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. Equation 3.4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities. But using the chain rule doesn’t really seem to help us! We don’t know any way to compute the exact probability of a word given a long sequence of preceding words, $P ( w _ { n } | w _ { 1 : n - 1 } )$ . As we said above, we can’t just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before! \nThe intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words. \nbigram \nThe bigram model, for example, approximates the probability of a word given all the previous words $P ( w _ { n } | w _ { 1 : n - 1 } )$ by using only the conditional probability of the preceding word $P ( w _ { n } | w _ { n - 1 } )$ . In other words, instead of computing the probability \nwe approximate it with the probability \nWhen we use a bigram model to predict the conditional probability of the next word, we are thus making the following approximation: \nMarkov \nThe assumption that the probability of a word depends only on the previous word is called a Markov assumption. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the $mathbf { n }$ -gram (which looks $n - 1$ words into the past). \nn-gram \nLet’s see a general equation for this n-gram approximation to the conditional probability of the next word in a sequence. We’ll use $N$ here to mean the $mathfrak { n }$ -gram size, so $N = 2$ means bigrams and $N = 3$ means trigrams. Then we approximate the probability of a word given its entire context as follows: \nGiven the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4: \nHow do we estimate these bigram or $mathfrak { n }$ -gram probabilities? An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE. We get the MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.1 \nFor example, to compute a particular bigram probability of a word $w _ { n }$ given a previous word $w _ { n - 1 }$ , we’ll compute the count of the bigram $C ( w _ { n - 1 } w _ { n } )$ and normalize by the sum of all the bigrams that share the same first word $w _ { n - 1 }$ : \nWe can simplify this equation, since the sum of all bigram counts that start with a given word $w _ { n - 1 }$ must be equal to the unigram count for that word $w _ { n - 1 }$ (the reader should take a moment to be convinced of this): \nLet’s work through an example using a mini-corpus of three sentences. We’ll first need to augment each sentence with a special symbol ${ < s > }$ at the beginning of the sentence, to give us the bigram context of the first word. We’ll also need a special end-symbol. $< / { mathsf { s } } { geqslant } ^ { 2 }$ \nHere are the calculations for some of the bigram probabilities from this corpus \nFor the general case of MLE $mathfrak { n }$ -gram parameter estimation: \nEquation 3.12 (like Eq. 3.11) estimates the $mathfrak { n }$ -gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix. This ratio is called a relative frequency. We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of the training set $T$ given the model $M$ (i.e., $P ( T | M ) )$ . For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus. What is the probability that a random word selected from some other text of, say, a million words will be the word Chinese? The MLE of its probability is 10400000 or .0004. Now .0004 is not the best possible estimate of the probability of Chinese occurring in all situations; it might turn out that in some other corpus or context Chinese is a very unlikely word. But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus. We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3.5. \nLet’s move on to some examples from a slightly larger corpus than our 14-word example above. We’ll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some textnormalized sample user queries (a sample of 9332 sentences is on the website): \ncan you tell me about any good cantonese restaurants close by mid priced thai food is what i’m looking for tell me about chez panisse can you give me a listing of the kinds of food that are available i’m looking for a good place to eat breakfast when is caffe venezia open during the day \nFigure 3.1 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project. Note that the majority of the values are zero. In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse. \nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of unigram probabilities): \nHere are a few other useful probabilities: \nNow we can compute the probability of sentences like I want English food or I want Chinese food by simply multiplying the appropriate bigram probabilities together, as follows: \nWe leave it as Exercise 3.2 to compute the probability of $i$ want chinese food. \nWhat kinds of linguistic phenomena are captured in these bigram statistics? Some of the bigram probabilities above encode some facts that we think of as strictly syntactic in nature, like the fact that what comes after eat is usually a noun or an adjective, or that what comes after to is usually a verb. Others might be a fact about the personal assistant task, like the high probability of sentences beginning with the words I. And some might even be cultural rather than linguistic, like the higher probability that people are looking for Chinese versus English food. \nSome practical issues: Although for pedagogical purposes we have only described bigram models, in practice it’s more common to use trigram models, which condition on the previous two words rather than the previous word, or 4-gram or even 5-gram models, when there is sufficient training data. Note that for these larger ngrams, we’ll need to assume extra contexts to the left and right of the sentence end. For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., $P ( mathbb { I } | < { mathsf { s } } > < { mathsf { s } } > )$ . \nWe always represent and compute language model probabilities in log format \nlog probabilities \nas log probabilities. Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes. Multiplying enough n-grams together would result in numerical underflow. By using log probabilities instead of raw probabilities, we get numbers that are not as small. Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them. The result of doing all computation and storage in log space is that we only need to convert back into probabilities if we need to report them at the end; then we can just take the exp of the logprob: \n3.2 Evaluating Language Models \nextrinsic evaluation \nThe best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in a component is really going to help the task at hand. Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription. \nintrinsic evaluation \nUnfortunately, running big NLP systems end-to-end is often very expensive. Instead, it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that measures the quality of a model independent of any application. \ntraining set \ntest set \nFor an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus. We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus. \nSo if we are given a corpus of text and want to compare two different n-gram models, we divide the data into training and test sets, train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. \nBut what does it mean to “fit the test set”? The answer is simple: whichever model assigns a higher probability to the test set—meaning it more accurately predicts the test set—is a better model. Given two probabilistic models, the better model is the one that has a tighter fit to the test data or that better predicts the details of the test data, and hence will assign a higher probability to the test data. \nSince our evaluation metric is based on test set probability, it’s important not to let the test sentences into the training set. Suppose we are trying to compute the probability of a particular “test” sentence. If our test sentence is part of the training corpus, we will mistakenly assign it an artificially high probability when it occurs in the test set. We call this situation training on the test set. Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric we introduce below. \ndevelopment test \nSometimes we use a particular test set so often that we implicitly tune to its characteristics. We then need a fresh test set that is truly unseen. In such cases, we call the initial test set the development test set or, devset. How do we divide our data into training, development, and test sets? We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible. At the minimum, we would want to pick the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models. In practice, we often just divide our data into $80 %$ training, $10 %$ development, and $10 %$ test. Given a large corpus that we want to divide into training and test, test data can either be taken from some continuous sequence of text inside the corpus, or we can remove smaller “stripes” of text from randomly selected parts of our corpus and combine them into a test set.",
        "chapter": "N-gram Language Models",
        "section": "N-Grams",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "3.2.1 Perplexity \nIn practice we don’t use raw probability as our metric for evaluating language models, but a variant called perplexity. The perplexity (sometimes called $P P$ for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words. For a test set $W = w _ { 1 } w _ { 2 } dots w _ { N } ,$ ,: \nWe can use the chain rule to expand the probability of $W$ : \nThus, if we are computing the perplexity of $W$ with a bigram language model, we get: \nNote that because of the inverse in Eq. 3.15, the higher the conditional probability of the word sequence, the lower the perplexity. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers $< s >$ and $< / { mathsf { s } } >$ in the probability computation. We also need to include the end-of-sentence marker $< / { mathsf { s } } >$ (but not the beginning-of-sentence marker $< mathsf { s } >$ ) in the total count of word tokens $N$ . \nThere is another way to think about perplexity: as the weighted average branching factor of a language. The branching factor of a language is the number of possible next words that can follow any word. Consider the task of recognizing the digits in English (zero, one, two,..., nine), given that (both in some training set and in some test set) each of the 10 digits occurs with equal probability $textstyle P = { frac { 1 } { 1 0 } }$ . The perplexity of this mini-language is in fact 10. To see that, imagine a test string of digits of length $N$ , and assume that in the training set all the digits occurred with equal probability. By Eq. 3.15, the perplexity will be \nBut suppose that the number zero is really frequent and occurs far more often than other numbers. Let’s say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: $mathrm { ~ 0 ~ 0 ~ }$ $mathrm { ~ 0 ~ 0 ~ 0 ~ 3 ~ 0 ~ 0 ~ 0 ~ 0 ~ }$ . We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller. We leave this exact calculation as exercise 12. \nWe see in Section 3.8 that perplexity is also closely related to the informationtheoretic notion of entropy. \nFinally, let’s look at an example of how perplexity can be used to compare different n-gram models. We trained unigram, bigram, and trigram grammars on 38 million words (including start-of-sentence tokens) from the Wall Street Journal, using a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words with Eq. 3.16. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. \nAs we see above, the more information the $mathfrak { n }$ -gram gives us about the word sequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model). \nNote that in computing perplexities, the n-gram model $P$ must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies. \nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model’s improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model. \n3.3 Sampling sentences from a language model \nsampling \nOne important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points according to their likelihood. Thus sampling from a language model—which represents a distribution over sentences—means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability.",
        "chapter": "N-gram Language Models",
        "section": "Evaluating Language Models ",
        "subsection": "Perplexity",
        "subsubsection": "N/A"
    },
    {
        "content": "But suppose that the number zero is really frequent and occurs far more often than other numbers. Let’s say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: $mathrm { ~ 0 ~ 0 ~ }$ $mathrm { ~ 0 ~ 0 ~ 0 ~ 3 ~ 0 ~ 0 ~ 0 ~ 0 ~ }$ . We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller. We leave this exact calculation as exercise 12. \nWe see in Section 3.8 that perplexity is also closely related to the informationtheoretic notion of entropy. \nFinally, let’s look at an example of how perplexity can be used to compare different n-gram models. We trained unigram, bigram, and trigram grammars on 38 million words (including start-of-sentence tokens) from the Wall Street Journal, using a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words with Eq. 3.16. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. \nAs we see above, the more information the $mathfrak { n }$ -gram gives us about the word sequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model). \nNote that in computing perplexities, the n-gram model $P$ must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies. \nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model’s improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model. \n3.3 Sampling sentences from a language model \nsampling \nOne important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points according to their likelihood. Thus sampling from a language model—which represents a distribution over sentences—means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. \n\nThis technique of visualizing a language model by sampling was first suggested very early on by Shannon (1951) and Miller and Selfridge (1950) It’s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM computed from the text of this book. We choose a random value between 0 and 1, find that point on the probability line, and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence-final token $< / mathsf { s } >$ . \nWe can use the same technique to generate bigrams by first generating a random bigram that starts with $< s >$ (according to its bigram probability). Let’s say the second word of that bigram is $w$ . We next choose a random bigram starting with $w$ (again, drawn according to its bigram probability), and so on. \n3.4 Generalization and Zeros \nThe n-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that n-grams do a better and better job of modeling the training corpus as we increase the value of $N$ . \nWe can use the sampling method from the prior section to visualize both of these facts! To give an intuition for the increasing power of higher-order n-grams, Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4- gram models trained on Shakespeare’s works. \nThe longer the context on which we train the model, the more coherent the sentences. In the unigram sentences, there is no coherent relation between words or any sentence-final punctuation. The bigram sentences have some local word-to-word coherence (especially if we consider that punctuation counts as a word). The trigram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a careful investigation of the 4-gram sentences shows that they look a little too much like Shakespeare. The words It cannot be but so are directly from King John. This is because, not to put the knock on Shakespeare, his oeuvre is not very large as corpora",
        "chapter": "N-gram Language Models",
        "section": "Sampling sentences from a language model",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "It is equally important to get training data in the appropriate dialect or variety, especially when processing social media posts or spoken transcripts. For example some tweets will use features of African American Language (AAL)— the name for the many variations of language used in African American communities (King, 2020). Such features include words like finna—an auxiliary verb that marks immediate future tense —that don’t occur in other varieties, or spellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017): \n(3.18) Bored af den my phone finna die!!! \nwhile tweets from varieties like Nigerian English have markedly different vocabulary and n-gram patterns from American English (Jurgens et al., 2017): \n(3.19) $@$ username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter \nMatching genres and dialects is still not sufficient. Our models may still be subject to the problem of sparsity. For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability. But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it. That is, we’ll have many cases of putative “zero probability n-grams” that should really have some non-zero probability. Consider the words that follow the bigram denied the in the WSJ Treebank3 corpus, together with their counts: \ndenied the allegations: 5   \ndenied the speculation: 2   \ndenied the rumors: 1   \ndenied the report: 1 \nBut suppose our test set has phrases like: \ndenied the offer denied the loan \nOur model will incorrectly estimate that the $P$ (offer denied the) is 0! \nzeros \nThese zeros—things that don’t ever occur in the training set but do occur in the test set—are a problem for two reasons. First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data. \nSecond, if the probability of any word in the test set is 0, the entire probability of the test set is 0. By definition, perplexity is based on the inverse probability of the test set. Thus if some words have zero probability, we can’t compute perplexity at all, since we can’t divide by 0! \n3.4.1 Unknown Words \nThe previous section discussed the problem of words whose bigram probability is zero. But what about words we simply have never seen before? \nSometimes we have a language task in which this can’t happen because we know all the words that can occur. In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words. This is a reasonable assumption in some domains, such as speech recognition or machine translation, where we have a pronunciation dictionary or a phrase table that are fixed in advance, and so the language model can only use the words in that dictionary or phrase table. \nIn other cases we have to deal with words we haven’t seen before, which we’ll call unknown words, or out of vocabulary (OOV) words. The percentage of OOV words that appear in the test set is called the OOV rate. An open vocabulary system is one in which we model these potential unknown words in the test set by adding a pseudo-word called ${ tt < U N K > }$ . \nThere are two common ways to train the probabilities of the unknown word model ${ tt < U N K > }$ . The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: \n1. Choose a vocabulary (word list) that is fixed in advance.   \n2. Convert in the training set any word that is not in this set (any OOV word) to the unknown word token ${ tt < U N K > }$ in a text normalization step.   \n3. Estimate the probabilities for ${ tt < U N K > }$ from its counts just like any other regular word in the training set. \nThe second alternative, in situations where we don’t have a prior vocabulary in advance, is to create such a vocabulary implicitly, replacing words in the training data by ${ tt < U N K > }$ based on their frequency. For example we can replace by ${ tt < U N K > }$ all words that occur fewer than $n$ times in the training set, where $n$ is some small number, or equivalently select a vocabulary size V in advance (say 50,000) and choose the top V words by frequency and replace the rest by UNK. In either case we then proceed to train the language model as before, treating ${ tt < U N K > }$ like a regular word. \nThe exact choice of ${ tt < U N K > }$ model does have an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. For this reason, perplexities should only be compared across language models with the same vocabularies (Buck et al., 2014). \n3.5 Smoothing \nsmoothing discounting \nWhat do we do with words that are in our vocabulary (they are not unknown words) but appear in a test set in an unseen context (for example they appear after a word they never appeared after in training)? To keep a language model from assigning zero probability to these unseen events, we’ll have to shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen. This modification is called smoothing or discounting. In this section and the following ones we’ll introduce a variety of ways to do smoothing: Laplace (add-one) smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing. \n3.5.1 Laplace Smoothing \nLaplace smoothing \nThe simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4). \nLet’s start with the application of Laplace smoothing to unigram probabilities. Recall that the unsmoothed maximum likelihood estimate of the unigram probability of the word $w _ { i }$ is its count $c _ { i }$ normalized by the total number of word tokens $N$ :",
        "chapter": "N-gram Language Models",
        "section": "Generalization and Zeros",
        "subsection": "Unknown Words",
        "subsubsection": "N/A"
    },
    {
        "content": "In other cases we have to deal with words we haven’t seen before, which we’ll call unknown words, or out of vocabulary (OOV) words. The percentage of OOV words that appear in the test set is called the OOV rate. An open vocabulary system is one in which we model these potential unknown words in the test set by adding a pseudo-word called ${ tt < U N K > }$ . \nThere are two common ways to train the probabilities of the unknown word model ${ tt < U N K > }$ . The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: \n1. Choose a vocabulary (word list) that is fixed in advance.   \n2. Convert in the training set any word that is not in this set (any OOV word) to the unknown word token ${ tt < U N K > }$ in a text normalization step.   \n3. Estimate the probabilities for ${ tt < U N K > }$ from its counts just like any other regular word in the training set. \nThe second alternative, in situations where we don’t have a prior vocabulary in advance, is to create such a vocabulary implicitly, replacing words in the training data by ${ tt < U N K > }$ based on their frequency. For example we can replace by ${ tt < U N K > }$ all words that occur fewer than $n$ times in the training set, where $n$ is some small number, or equivalently select a vocabulary size V in advance (say 50,000) and choose the top V words by frequency and replace the rest by UNK. In either case we then proceed to train the language model as before, treating ${ tt < U N K > }$ like a regular word. \nThe exact choice of ${ tt < U N K > }$ model does have an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. For this reason, perplexities should only be compared across language models with the same vocabularies (Buck et al., 2014). \n3.5 Smoothing \nsmoothing discounting \nWhat do we do with words that are in our vocabulary (they are not unknown words) but appear in a test set in an unseen context (for example they appear after a word they never appeared after in training)? To keep a language model from assigning zero probability to these unseen events, we’ll have to shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen. This modification is called smoothing or discounting. In this section and the following ones we’ll introduce a variety of ways to do smoothing: Laplace (add-one) smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing. \n3.5.1 Laplace Smoothing \nLaplace smoothing \nThe simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4). \nLet’s start with the application of Laplace smoothing to unigram probabilities. Recall that the unsmoothed maximum likelihood estimate of the unigram probability of the word $w _ { i }$ is its count $c _ { i }$ normalized by the total number of word tokens $N$ : \n\nadd-one \nLaplace smoothing merely adds one to each count (hence its alternate name addone smoothing). Since there are $V$ words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra $V$ observations. (What happens to our $P$ values if we don’t increase the denominator?) \nInstead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count $c ^ { * }$ . This adjusted count is easier to compare directly with the MLE counts and can be turned into a probability like an MLE count by normalizing by $N$ . To define this count, since we are only changing the numerator in addition to adding 1 we’ll also need to multiply by a normalization factor NN+V : \nWe can now turn $c _ { i } ^ { * }$ into a probability $P _ { i } ^ { * }$ by normalizing by $N$ . \nA related way to view smoothing is as discounting (lowering) some non-zero counts in order to get the probability mass that will be assigned to the zero counts. Thus, instead of referring to the discounted counts $c ^ { * }$ , we might describe a smoothing algorithm in terms of a relative discount $d _ { c }$ , the ratio of the discounted counts to the original counts: \nNow that we have the intuition for the unigram case, let’s smooth our Berkeley Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the bigrams in Fig. 3.1. \nFigure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2. Recall that normal bigram probabilities are computed by normalizing each row of counts by the unigram count: \nFor add-one smoothed bigram counts, we need to augment the unigram count by the number of total word types in the vocabulary $V$ : \nThus, each of the unigram counts given in the previous section will need to be augmented by $V = 1 4 4 6$ . The result is the smoothed bigram probabilities in Fig. 3.7. \nIt is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts. These adjusted counts can be computed by Eq. 3.24. Figure 3.8 shows the reconstructed counts. \nNote that add-one smoothing has made a very big change to the counts. $C ( w a n t t o )$ changed from 609 to 238! We can see this in probability space as well: $P ( t o | w a n t )$ decreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at the discount $d$ (the ratio between new and old counts) shows us how strikingly the counts for each prefix word have been reduced; the discount for the bigram want to is .39, while the discount for Chinese food is .10, a factor of 10! \nThe sharp change in counts and probabilities occurs because too much probability mass is moved to all the zeros. \n3.5.2 Add-k smoothing \nadd-k \nOne alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a fractional count $k$ (.5? .05? .01?). This algorithm is therefore called add- $mathbf { nabla } cdot mathbf { k }$ smoothing. \nbackoff interpolation \n\nAdd- $mathbf { nabla } cdot mathbf { k }$ smoothing requires that we have a method for choosing $k$ ; this can be done, for example, by optimizing on a devset. Although add- $mathbf { nabla } cdot mathbf { k }$ is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts (Gale and Church, 1994). \n3.5.3 Backoff and Interpolation \nThe discounting we have been discussing so far can help solve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ but we have no examples of a particular trigram $w _ { n - 2 } w _ { n - 1 } w _ { n }$ , we can instead estimate its probability by using the bigram probability $P ( w _ { n } | w _ { n - 1 } )$ . Similarly, if we don’t have counts to compute $P ( w _ { n } | w _ { n - 1 } )$ , we can look to the unigram $P ( w _ { n } )$ . \nIn other words, sometimes using less context is a good thing, helping to generalize more for contexts that the model hasn’t learned much about. There are two ways to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram. By contrast, in interpolation, we always mix the probability estimates from all the n-gram estimators, weighting and combining the trigram, bigram, and unigram counts. \nIn simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $lambda$ : \nThe $lambda mathrm { s }$ must sum to 1, making Eq. 3.26 equivalent to a weighted average: \nIn a slightly more sophisticated version of linear interpolation, each $lambda$ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the $lambda mathrm { s }$ for those trigrams higher and thus give that trigram more weight in the interpolation. Equation 3.28 shows the equation for interpolation with context-conditioned weights:",
        "chapter": "N-gram Language Models",
        "section": "Smoothing",
        "subsection": "Laplace Smoothing",
        "subsubsection": "N/A"
    },
    {
        "content": "3.5.2 Add-k smoothing \nadd-k \nOne alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a fractional count $k$ (.5? .05? .01?). This algorithm is therefore called add- $mathbf { nabla } cdot mathbf { k }$ smoothing. \nbackoff interpolation \n\nAdd- $mathbf { nabla } cdot mathbf { k }$ smoothing requires that we have a method for choosing $k$ ; this can be done, for example, by optimizing on a devset. Although add- $mathbf { nabla } cdot mathbf { k }$ is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts (Gale and Church, 1994). \n3.5.3 Backoff and Interpolation \nThe discounting we have been discussing so far can help solve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ but we have no examples of a particular trigram $w _ { n - 2 } w _ { n - 1 } w _ { n }$ , we can instead estimate its probability by using the bigram probability $P ( w _ { n } | w _ { n - 1 } )$ . Similarly, if we don’t have counts to compute $P ( w _ { n } | w _ { n - 1 } )$ , we can look to the unigram $P ( w _ { n } )$ . \nIn other words, sometimes using less context is a good thing, helping to generalize more for contexts that the model hasn’t learned much about. There are two ways to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram. By contrast, in interpolation, we always mix the probability estimates from all the n-gram estimators, weighting and combining the trigram, bigram, and unigram counts. \nIn simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $lambda$ : \nThe $lambda mathrm { s }$ must sum to 1, making Eq. 3.26 equivalent to a weighted average: \nIn a slightly more sophisticated version of linear interpolation, each $lambda$ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the $lambda mathrm { s }$ for those trigrams higher and thus give that trigram more weight in the interpolation. Equation 3.28 shows the equation for interpolation with context-conditioned weights:",
        "chapter": "N-gram Language Models",
        "section": "Smoothing",
        "subsection": "Add-k smoothing",
        "subsubsection": "N/A"
    },
    {
        "content": "3.5.2 Add-k smoothing \nadd-k \nOne alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a fractional count $k$ (.5? .05? .01?). This algorithm is therefore called add- $mathbf { nabla } cdot mathbf { k }$ smoothing. \nbackoff interpolation \n\nAdd- $mathbf { nabla } cdot mathbf { k }$ smoothing requires that we have a method for choosing $k$ ; this can be done, for example, by optimizing on a devset. Although add- $mathbf { nabla } cdot mathbf { k }$ is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts (Gale and Church, 1994). \n3.5.3 Backoff and Interpolation \nThe discounting we have been discussing so far can help solve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ but we have no examples of a particular trigram $w _ { n - 2 } w _ { n - 1 } w _ { n }$ , we can instead estimate its probability by using the bigram probability $P ( w _ { n } | w _ { n - 1 } )$ . Similarly, if we don’t have counts to compute $P ( w _ { n } | w _ { n - 1 } )$ , we can look to the unigram $P ( w _ { n } )$ . \nIn other words, sometimes using less context is a good thing, helping to generalize more for contexts that the model hasn’t learned much about. There are two ways to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram. By contrast, in interpolation, we always mix the probability estimates from all the n-gram estimators, weighting and combining the trigram, bigram, and unigram counts. \nIn simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $lambda$ : \nThe $lambda mathrm { s }$ must sum to 1, making Eq. 3.26 equivalent to a weighted average: \nIn a slightly more sophisticated version of linear interpolation, each $lambda$ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the $lambda mathrm { s }$ for those trigrams higher and thus give that trigram more weight in the interpolation. Equation 3.28 shows the equation for interpolation with context-conditioned weights: \n\nheld-out \nHow are these $lambda$ values set? Both the simple interpolation and conditional interpolation $lambda mathrm { s }$ are learned from a held-out corpus. A held-out corpus is an additional training corpus, so-called because we hold it out from the training data, that we use to set hyperparameters like these $lambda$ values. We do so by choosing the $lambda$ values that maximize the likelihood of the held-out corpus. That is, we fix the n-gram probabilities and then search for the $lambda$ values that—when plugged into Eq. 3.26—give us the highest probability of the held-out set. There are various ways to find this optimal set of $lambda mathrm { s }$ . One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal λ s (Jelinek and Mercer, 1980). \ndiscount \nIn a backoff n-gram model, if the n-gram we need has zero counts, we approximate it by backing off to the (N-1)-gram. We continue backing off until we reach a history that has some counts. \nIn order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t discounted and we just used the undiscounted MLE probability, then as soon as we replaced an n-gram which has zero probability with a lower-order n-gram, we would be adding probability mass, and the total probability assigned to all possible strings by the language model would be greater than 1! In addition to this explicit discount factor, we’ll need a function $alpha$ to distribute this probability mass to the lower order n-grams. \nKatz backoff \nThis kind of backoff with discounting is also called Katz backoff. In Katz backoff we rely on a discounted probability $P ^ { * }$ if we’ve seen this n-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probability for the shorter-history (N-1)-gram. The probability for a backoff $mathfrak { n }$ -gram $P _ { mathrm { B O } }$ is thus computed as follows: \nGood-Turing \nKatz backoff is often combined with a smoothing method called Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the $P ^ { * }$ and $alpha$ values. \n3.6 Kneser-Ney Smoothing \nKneser-Ney \nOne of the most commonly used and best performing n-gram smoothing methods is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Goodman 1998). \nKneser-Ney has its roots in a method called absolute discounting. Recall that discounting of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen n-grams.",
        "chapter": "N-gram Language Models",
        "section": "Smoothing",
        "subsection": "Backoff and Interpolation",
        "subsubsection": "N/A"
    },
    {
        "content": "held-out \nHow are these $lambda$ values set? Both the simple interpolation and conditional interpolation $lambda mathrm { s }$ are learned from a held-out corpus. A held-out corpus is an additional training corpus, so-called because we hold it out from the training data, that we use to set hyperparameters like these $lambda$ values. We do so by choosing the $lambda$ values that maximize the likelihood of the held-out corpus. That is, we fix the n-gram probabilities and then search for the $lambda$ values that—when plugged into Eq. 3.26—give us the highest probability of the held-out set. There are various ways to find this optimal set of $lambda mathrm { s }$ . One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal λ s (Jelinek and Mercer, 1980). \ndiscount \nIn a backoff n-gram model, if the n-gram we need has zero counts, we approximate it by backing off to the (N-1)-gram. We continue backing off until we reach a history that has some counts. \nIn order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t discounted and we just used the undiscounted MLE probability, then as soon as we replaced an n-gram which has zero probability with a lower-order n-gram, we would be adding probability mass, and the total probability assigned to all possible strings by the language model would be greater than 1! In addition to this explicit discount factor, we’ll need a function $alpha$ to distribute this probability mass to the lower order n-grams. \nKatz backoff \nThis kind of backoff with discounting is also called Katz backoff. In Katz backoff we rely on a discounted probability $P ^ { * }$ if we’ve seen this n-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probability for the shorter-history (N-1)-gram. The probability for a backoff $mathfrak { n }$ -gram $P _ { mathrm { B O } }$ is thus computed as follows: \nGood-Turing \nKatz backoff is often combined with a smoothing method called Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the $P ^ { * }$ and $alpha$ values. \n3.6 Kneser-Ney Smoothing \nKneser-Ney \nOne of the most commonly used and best performing n-gram smoothing methods is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Goodman 1998). \nKneser-Ney has its roots in a method called absolute discounting. Recall that discounting of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen n-grams. \nTo see this, we can use a clever idea from Church and Gale (1991). Consider an n-gram that has count 4. We need to discount this count by some amount. But how much should we discount it? Church and Gale’s clever idea was to look at a held-out corpus and just see what the count is for all those bigrams that had count 4 in the training set. They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words. On average, a bigram that occurred 4 times in the first 22 million words occurred 3.23 times in the next 22 million words. Fig. 3.9 from Church and Gale (1991) shows these counts for bigrams with $c$ from 0 to 9. \nNotice in Fig. 3.9 that except for the held-out counts for 0 and 1, all the other bigram counts in the held-out set could be estimated pretty well by just subtracting 0.75 from the count in the training set! Absolute discounting formalizes this intuition by subtracting a fixed (absolute) discount $d$ from each count. The intuition is that since we have good estimates already for the very high counts, a small discount $d$ won’t affect them much. It will mainly modify the smaller counts, for which we don’t necessarily trust the estimate anyway, and Fig. 3.9 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9. The equation for interpolated absolute discounting applied to bigrams: \nThe first term is the discounted bigram, and the second term is the unigram with an interpolation weight $lambda$ . We could just set all the $d$ values to .75, or we could keep a separate discount value of 0.5 for the bigrams with counts of 1. \nKneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution. Consider the job of predicting the next word in this sentence, assuming we are interpolating a bigram and a unigram model. \nI can’t see without my reading \nThe word glasses seems much more likely to follow here than, say, the word Kong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is more common, since Hong Kong is a very frequent word. A standard unigram model will assign Kong a higher probability than glasses. We would like to capture the intuition that although Kong is frequent, it is mainly only frequent in the phrase Hong Kong, that is, after the word Hong. The word glasses has a much wider distribution. \n\nIn other words, instead of $P ( w )$ , which answers the question “How likely is $w ? ^ { dag }$ , we’d like to create a unigram model that we might call PCONTINUATION, which answers the question “How likely is $w$ to appear as a novel continuation?”. How can we estimate this probability of seeing the word $w$ as a novel continuation, in a new unseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION on the number of different contexts word $w$ has appeared in, that is, the number of bigram types it completes. Every bigram type was a novel continuation the first time it was seen. We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well. The number of times a word $w$ appears as a novel continuation can be expressed as: \nTo turn this count into a probability, we normalize by the total number of word bigram types. In summary: \nAn equivalent formulation based on a different metaphor is to use the number of word types seen to precede $w$ (Eq. 3.31 repeated): \nnormalized by the number of words preceding all words, as follows: \nA frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability. \nThe final equation for Interpolated Kneser-Ney smoothing for bigrams is then: \nThe $lambda$ is a normalizing constant that is used to distribute the probability mass we’ve discounted.: \nThe first term, $frac { d } { sum _ { nu } C ( w _ { i - 1 } nu ) }$ is the normalized discount. The second term, $left| left{ w : C ( w _ { i - 1 } w ) > 0 right} right|$ , is the number of word types that can follow $w _ { i - 1 }$ or, equivalently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount. \nThe general recursive formulation is as follows: \nwhere the definition of the count $c _ { K N }$ depends on whether we are counting the highest-order n-gram being interpolated (for example trigram if we are interpolating trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram): \nThe continuation count is the number of unique single word contexts for $mathbf { nabla } cdot mathbf { mu }$ . \nAt the termination of the recursion, unigrams are interpolated with the uniform distribution, where the parameter $epsilon$ is the empty string: \nIf we want to include an unknown word ${ tt < U N K > }$ , it’s just included as a regular vocabulary entry with count zero, and hence its probability will be a lambda-weighted uniform distribution λV( . \nThe best performing version of Kneser-Ney smoothing is called modified KneserNey smoothing, and is due to Chen and Goodman (1998). Rather than use a single fixed discount $d$ , modified Kneser-Ney uses three different discounts $d _ { 1 } , d _ { 2 }$ , and $d _ { 3 + }$ for $mathfrak { n }$ -grams with counts of 1, 2 and three or more, respectively. See Chen and Goodman (1998, p. 19) or Heafield et al. (2013) for the details. \n3.7 Huge Language Models and Stupid Backoff \nBy using text from the web or other enormous collections, it is possible to build extremely large language models. The Web 1 Trillion 5-gram corpus released by Google includes various large sets of n-grams, including 1-grams through 5-grams from all the five-word sequences that appear in at least 40 distinct books from 1,024,908,267,229 words of text from publicly accessible Web pages in English (Franz and Brants, 2006). Google has also released Google Books Ngrams corpora with n-grams drawn from their book collections, including another 800 billion tokens of n-grams from Chinese, English, French, German, Hebrew, Italian, Russian, and Spanish (Lin et al., 2012a). Smaller but more carefully curated n-gram corpora for English include the million most frequent n-grams drawn from the COCA (Corpus of Contemporary American English) 1 billion word corpus of American English (Davies, 2020). COCA is a balanced corpus, meaning that it has roughly equal numbers of words from different genres: web, newspapers, spoken conversation transcripts, fiction, and so on, drawn from the period 1990-2019, and has the context of each n-gram as well as labels for genre and provenance). \nSome example 4-grams from the Google Web corpus:",
        "chapter": "N-gram Language Models",
        "section": "Kneser-Ney Smoothing",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The general recursive formulation is as follows: \nwhere the definition of the count $c _ { K N }$ depends on whether we are counting the highest-order n-gram being interpolated (for example trigram if we are interpolating trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram): \nThe continuation count is the number of unique single word contexts for $mathbf { nabla } cdot mathbf { mu }$ . \nAt the termination of the recursion, unigrams are interpolated with the uniform distribution, where the parameter $epsilon$ is the empty string: \nIf we want to include an unknown word ${ tt < U N K > }$ , it’s just included as a regular vocabulary entry with count zero, and hence its probability will be a lambda-weighted uniform distribution λV( . \nThe best performing version of Kneser-Ney smoothing is called modified KneserNey smoothing, and is due to Chen and Goodman (1998). Rather than use a single fixed discount $d$ , modified Kneser-Ney uses three different discounts $d _ { 1 } , d _ { 2 }$ , and $d _ { 3 + }$ for $mathfrak { n }$ -grams with counts of 1, 2 and three or more, respectively. See Chen and Goodman (1998, p. 19) or Heafield et al. (2013) for the details. \n3.7 Huge Language Models and Stupid Backoff \nBy using text from the web or other enormous collections, it is possible to build extremely large language models. The Web 1 Trillion 5-gram corpus released by Google includes various large sets of n-grams, including 1-grams through 5-grams from all the five-word sequences that appear in at least 40 distinct books from 1,024,908,267,229 words of text from publicly accessible Web pages in English (Franz and Brants, 2006). Google has also released Google Books Ngrams corpora with n-grams drawn from their book collections, including another 800 billion tokens of n-grams from Chinese, English, French, German, Hebrew, Italian, Russian, and Spanish (Lin et al., 2012a). Smaller but more carefully curated n-gram corpora for English include the million most frequent n-grams drawn from the COCA (Corpus of Contemporary American English) 1 billion word corpus of American English (Davies, 2020). COCA is a balanced corpus, meaning that it has roughly equal numbers of words from different genres: web, newspapers, spoken conversation transcripts, fiction, and so on, drawn from the period 1990-2019, and has the context of each n-gram as well as labels for genre and provenance). \nSome example 4-grams from the Google Web corpus: \nBloom filters \nEfficiency considerations are important when building language models that use such large sets of n-grams. Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries. \nAn n-gram language model can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams (Stolcke, 1998). Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al. 2007). Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al. 2013) use sorted arrays, efficiently combine probabilities and backoffs in a single value, and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus. \nstupid backoff \nAlthough with these toolkits it is possible to build web-scale language models using full Kneser-Ney smoothing, Brants et al. (2007) show that with very large language models a much simpler algorithm may be sufficient. The algorithm is called stupid backoff. Stupid backoff gives up the idea of trying to make the language model a true probability distribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algorithm does not produce a probability distribution, so we’ll follow Brants et al. (2007) in referring to it as $s$ : \nThe backoff terminates in the unigram, which has probability $begin{array} { r } { S ( w ) = frac { c o u n t ( w ) } { N } } end{array}$ . Brants et al. (2007) find that a value of 0.4 worked well for $lambda$ . \n3.8 Advanced: Perplexity’s Relation to Entropy \nEntropy \nWe introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on a test set. A better n-gram model is one that assigns a higher probability to the test data, and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example?) and its relationship to entropy. Entropy is a measure of information. Given a random variable $X$ ranging over whatever we are predicting (words, letters, parts of speech, the set of which we’ll call $chi$ ) and with a particular probability function, call it $p ( x )$ , the entropy of the random variable $X$ is:",
        "chapter": "N-gram Language Models",
        "section": "Huge Language Models and Stupid Backoff",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Bloom filters \nEfficiency considerations are important when building language models that use such large sets of n-grams. Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries. \nAn n-gram language model can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams (Stolcke, 1998). Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al. 2007). Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al. 2013) use sorted arrays, efficiently combine probabilities and backoffs in a single value, and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus. \nstupid backoff \nAlthough with these toolkits it is possible to build web-scale language models using full Kneser-Ney smoothing, Brants et al. (2007) show that with very large language models a much simpler algorithm may be sufficient. The algorithm is called stupid backoff. Stupid backoff gives up the idea of trying to make the language model a true probability distribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algorithm does not produce a probability distribution, so we’ll follow Brants et al. (2007) in referring to it as $s$ : \nThe backoff terminates in the unigram, which has probability $begin{array} { r } { S ( w ) = frac { c o u n t ( w ) } { N } } end{array}$ . Brants et al. (2007) find that a value of 0.4 worked well for $lambda$ . \n3.8 Advanced: Perplexity’s Relation to Entropy \nEntropy \nWe introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on a test set. A better n-gram model is one that assigns a higher probability to the test data, and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example?) and its relationship to entropy. Entropy is a measure of information. Given a random variable $X$ ranging over whatever we are predicting (words, letters, parts of speech, the set of which we’ll call $chi$ ) and with a particular probability function, call it $p ( x )$ , the entropy of the random variable $X$ is: \n\nThe log can, in principle, be computed in any base. If we use log base 2, the resulting value of entropy will be measured in bits. \nOne intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme. \nConsider an example from the standard information theory textbook Cover and Thomas (1991). Imagine that we want to place a bet on a horse race but it is too far to go all the way to Yonkers Racetrack, so we’d like to send a short message to the bookie to tell him which of the eight horses to bet on. One way to encode this message is just to use the binary representation of the horse’s number as the code; thus, horse 1 would be 001, horse 2 010, horse $3 updownarrow 1$ , and so on, with horse 8 coded as $boldsymbol { mathfrak { O } } boldsymbol { mathfrak { O } } boldsymbol { mathfrak { O } }$ . If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race. \nCan we do better? Suppose that the spread is the actual distribution of the bets placed and that we represent it as the prior probability of each horse as follows: \nThe entropy of the random variable $X$ that ranges over horses gives us a lower bound on the number of bits and is \nA code that averages 2 bits per race can be built with short encodings for more probable horses, and longer encodings for less probable horses. For example, we could encode the most likely horse with the code 0, and the remaining horses as 10, then 110, 1110, 111100, 111101, 111110, and 111111. \nWhat if the horses are equally likely? We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3. Is the entropy the same? In this case each horse would have a probability of $frac { 1 } { 8 }$ . The entropy of the choice of horses is then \nUntil now we have been computing the entropy of a single variable. But most of what we will use entropy for involves sequences. For a grammar, for example, we will be computing the entropy of some sequence of words $W = { w _ { 1 } , w _ { 2 } , . . . , w _ { n } }$ . One way to do this is to have a variable that ranges over sequences of words. For example we can compute the entropy of a random variable that ranges over all finite sequences of words of length $n$ in some language $L$ as follows: \n\nentropy rate \nWe could define the entropy rate (we could also think of this as the per-word entropy) as the entropy of this sequence divided by the number of words: \nBut to measure the true entropy of a language, we need to consider sequences of infinite length. If we think of a language as a stochastic process $L$ that produces a sequence of words, and allow $W$ to represent the sequence of words $w _ { 1 } , ldots , w _ { n }$ , then $L$ ’s entropy rate $H ( L )$ is defined as \nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas 1991) states that if the language is regular in certain ways (to be exact, if it is both stationary and ergodic), \nThat is, we can take a single sequence that is long enough instead of summing over all possible sequences. The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities. \nStationary \nA stochastic process is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index. In other words, the probability distribution for words at time $t$ is the same as the probability distribution at time $t + 1$ . Markov models, and hence n-grams, are stationary. For example, in a bigram, $P _ { i }$ is dependent only on $P _ { i - 1 }$ . So if we shift our time index by $x _ { i }$ , $P _ { i + x }$ is still dependent on $P _ { i + x - 1 }$ . But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language. \nTo summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability. \ncross-entropy \nNow we are ready to introduce cross-entropy. The cross-entropy is useful when we don’t know the actual probability distribution $p$ that generated some data. It allows us to use some $m$ , which is a model of $p$ (i.e., an approximation to $p$ ). The cross-entropy of $m$ on $p$ is defined by \nThat is, we draw sequences according to the probability distribution $p$ , but sum the log of their probabilities according to $m$ . \nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process: \nThis means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences. \nWhat makes the cross-entropy useful is that the cross-entropy $H ( p , m )$ is an upper bound on the entropy $H ( p )$ . For any model $m$ : \nThis means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$ . The more accurate $m$ is, the closer the cross-entropy $H ( p , m )$ will be to the true entropy $H ( p )$ . Thus, the difference between $H ( p , m )$ and $H ( p )$ is a measure of how accurate a model is. Between two models $m _ { 1 }$ and $m _ { 2 }$ , the more accurate model will be the one with the lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.) \nWe are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq. 3.49. Cross-entropy is defined in the limit as the length of the observed word sequence goes to infinity. We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length. This approximation to the cross-entropy of a model $M = P ( w _ { i } | w _ { i - N + 1 : i - 1 } )$ on a sequence of words $W$ is \nperplexity \nThe perplexity of a model $P$ on a sequence of words $W$ is now formally defined as 2 raised to the power of this cross-entropy: \n3.9 Summary \nThis chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing. \n• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.",
        "chapter": "N-gram Language Models",
        "section": "Advanced: Perplexity's Relation to Entropy",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "That is, we draw sequences according to the probability distribution $p$ , but sum the log of their probabilities according to $m$ . \nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process: \nThis means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences. \nWhat makes the cross-entropy useful is that the cross-entropy $H ( p , m )$ is an upper bound on the entropy $H ( p )$ . For any model $m$ : \nThis means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$ . The more accurate $m$ is, the closer the cross-entropy $H ( p , m )$ will be to the true entropy $H ( p )$ . Thus, the difference between $H ( p , m )$ and $H ( p )$ is a measure of how accurate a model is. Between two models $m _ { 1 }$ and $m _ { 2 }$ , the more accurate model will be the one with the lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.) \nWe are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq. 3.49. Cross-entropy is defined in the limit as the length of the observed word sequence goes to infinity. We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length. This approximation to the cross-entropy of a model $M = P ( w _ { i } | w _ { i - N + 1 : i - 1 } )$ on a sequence of words $W$ is \nperplexity \nThe perplexity of a model $P$ on a sequence of words $W$ is now formally defined as 2 raised to the power of this cross-entropy: \n3.9 Summary \nThis chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing. \n• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.",
        "chapter": "N-gram Language Models",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "That is, we draw sequences according to the probability distribution $p$ , but sum the log of their probabilities according to $m$ . \nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process: \nThis means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences. \nWhat makes the cross-entropy useful is that the cross-entropy $H ( p , m )$ is an upper bound on the entropy $H ( p )$ . For any model $m$ : \nThis means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$ . The more accurate $m$ is, the closer the cross-entropy $H ( p , m )$ will be to the true entropy $H ( p )$ . Thus, the difference between $H ( p , m )$ and $H ( p )$ is a measure of how accurate a model is. Between two models $m _ { 1 }$ and $m _ { 2 }$ , the more accurate model will be the one with the lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.) \nWe are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq. 3.49. Cross-entropy is defined in the limit as the length of the observed word sequence goes to infinity. We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length. This approximation to the cross-entropy of a model $M = P ( w _ { i } | w _ { i - N + 1 : i - 1 } )$ on a sequence of words $W$ is \nperplexity \nThe perplexity of a model $P$ on a sequence of words $W$ is now formally defined as 2 raised to the power of this cross-entropy: \n3.9 Summary \nThis chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing. \n• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.",
        "chapter": "N-gram Language Models",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "That is, we draw sequences according to the probability distribution $p$ , but sum the log of their probabilities according to $m$ . \nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process: \nThis means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences. \nWhat makes the cross-entropy useful is that the cross-entropy $H ( p , m )$ is an upper bound on the entropy $H ( p )$ . For any model $m$ : \nThis means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$ . The more accurate $m$ is, the closer the cross-entropy $H ( p , m )$ will be to the true entropy $H ( p )$ . Thus, the difference between $H ( p , m )$ and $H ( p )$ is a measure of how accurate a model is. Between two models $m _ { 1 }$ and $m _ { 2 }$ , the more accurate model will be the one with the lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.) \nWe are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq. 3.49. Cross-entropy is defined in the limit as the length of the observed word sequence goes to infinity. We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length. This approximation to the cross-entropy of a model $M = P ( w _ { i } | w _ { i - N + 1 : i - 1 } )$ on a sequence of words $W$ is \nperplexity \nThe perplexity of a model $P$ on a sequence of words $W$ is now formally defined as 2 raised to the power of this cross-entropy: \n3.9 Summary \nThis chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing. \n• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. \n• n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).   \n• n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.   \n• The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.   \n• Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.   \n• Both backoff and interpolation require discounting to create a probability distribution.   \n• Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability. \nBibliographical and Historical Notes \nThe underlying mathematics of the n-gram was first proposed by Markov (1913), who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a consonant. Markov classified 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters. Shannon (1948) applied n-grams to compute approximations to English word sequences. Based on Shannon’s work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s. In a series of extremely influential papers starting with Chomsky (1956) and including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued that “finite-state Markov processes”, while a possibly useful engineering heuristic, were incapable of being a complete cognitive model of human grammatical knowledge. These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades. \nThe resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues. Independently these two labs successfully used n-grams in their speech recognition systems (Baker 1975b, Jelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). \nAdd-one smoothing derives from Laplace’s 1812 law of succession and was first applied as an engineering solution to the zero frequency problem by Jeffreys (1948) based on an earlier Add-K suggestion by Johnson (1932). Problems with the addone algorithm are summarized in Gale and Church (1994). \nA wide variety of different language modeling and smoothing techniques were proposed in the 80s and 90s, including Good-Turing discounting—first applied to the n-gram smoothing at IBM by Katz (Na´das 1984, Church and Gale 1991)— Witten-Bell discounting (Witten and Bell, 1991), and varieties of class-based ngram models that used information about word classes. \nStarting in the late 1990s, Chen and Goodman performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters (Chen and Goodman \n1999, Goodman 2006, inter alia). They showed the advantages of Modified Interpolated Kneser-Ney, which became the standard baseline for n-gram language modeling, especially because they showed that caches and class-based models provided only minor additional improvement. These papers are recommended for any reader with further interest in n-gram language modeling. SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al. 2013) are publicly available toolkits for building n-gram language models. \nModern language modeling is more commonly done with neural network language models, which solve the major problems with n-grams: the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We’ll introduce both feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7, and recurrent language models (Mikolov, 2012) in Chapter 9. \nExercises \n3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 33. \n3.2 Calculate the probability of the sentence i want chinese food. Give two probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on page 35, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P ( mathbf { i } | < mathbf { s } > ) = 0 . 1 9$ and $P ( < / { bf s } > | mathrm { f o o d } ) =$ 0.40. \n3.3 Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why. \n3.4 We are given the following corpus, modified from the one in the chapter: \n$< s >  I$ am Sam $< / mathsf { s } >$   \n$< mathsf { s } >$ Sam I am $< / mathsf { s } >$   \n$< s > texttt { I }$ am Sam $< / mathsf { s } >$   \n$< s > texttt { I }$ do not like green eggs and Sam $< / mathsf { s } >$ \nUsing a bigram language model with add-one smoothing, what is $operatorname { P } ( operatorname { S a m } mid$ am)? Include $< mathsf { s } >$ and $< / mathsf { s } >$ in your counts just like any other token. \n3.5 Suppose we didn’t use the end-symbol $< / mathsf { s } >$ . Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $< / mathsf { s } >$ : \n$< mathsf { s } >$ a b $< mathsf { s } >$ b b $< mathsf { s } >$ b a $< mathsf { s } >$ a a \nDemonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet ${ mathbf { a } , mathbf { b } }$ is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet ${ mathrm { a } , mathrm { b } }$ is also 1.0. \n3.6 Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $mathrm { Delta V }$ word types. Express a formula for estimating $mathrm { P } ( mathrm { w } 3 | mathrm { w } 1 , mathrm { w } 2 )$ , where w3 is a word which follows the bigram (w1,w2), in terms of various n-gram counts and V. Use the notation $mathbf { c } ( mathrm { w } 1 , mathrm { w } 2 , mathrm { w } 3 )$ to denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and so on for bigrams and unigrams. \n3.7 We are given the following corpus, modified from the one in the chapter: \n$< s > texttt { I }$ am Sam $< / mathsf { s } >$   \n$< s >$ Sam I am $< / mathsf { s } >$   \n$< s > texttt { I }$ am Sam $< / { mathsf { s } } >$   \n$< s > texttt { I }$ do not like green eggs and Sam $< / mathsf { s } >$ \nIf we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $begin{array} { r } { lambda _ { 1 } = frac { 1 } { 2 } } end{array}$ and $lambda _ { 2 } =$ $frac { 1 } { 2 }$ , what is $mathrm { P } ( mathrm { S a m } | mathrm { a m } ) ^ { cdot }$ ? Include $< mathsf { s } >$ and $< / { mathsf { s } } >$ in your counts just like any other token. \n3.8 Write a program to compute unsmoothed unigrams and bigrams. \n3.9 Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams? \n3.10 Add an option to your program to generate random sentences. \n3.11 Add an option to your program to compute the perplexity of a test set. \n3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity? \n4 NClaisvsei cBatiyoens and Sentiment \nsentiment analysis \nClassification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input. The potential challenges of this task are highlighted by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into: \n(a) those that belong to the Emperor, (b) embalmed ones, (c) those that are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray dogs, (h) those that are included in this classification, (i) those that tremble as if they were mad, (j) innumerable ones, (k) those drawn with a very fine camel’s hair brush, (l) others, (m) those that have just broken a flower vase, (n) those that resemble flies from a distance. \nMany language processing tasks involve classification, although luckily our classes are much easier to define than those of Borges. In this chapter we introduce the naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document. \nWe focus on one common text categorization task, sentiment analysis, the extraction of sentiment, the positive or negative orientation that a writer expresses toward some object. A review of a movie, book, or product on the web expresses the author’s sentiment toward the product, while an editorial or political text expresses sentiment toward a candidate or political action. Extracting consumer or public sentiment is thus relevant for fields from marketing to politics. \nThe simplest version of sentiment analysis is a binary classification task, and the words of the review provide excellent cues. Consider, for example, the following phrases extracted from positive and negative reviews of movies and restaurants. Words like great, richly, awesome, and pathetic, and awful and ridiculously are very informative cues: \nspam detection \n$+$ ...zany characters and richly applied satire, and some great plot twists − It was pathetic. The worst part about it was the boxing scenes... $+$ ...awesome caramel sauce and sweet toasty almonds. I love this place! ...awful pizza and ridiculously overpriced... \nSpam detection is another important commercial application, the binary classification task of assigning an email to one of the two classes spam or not-spam. Many lexical and other features can be used to perform this classification. For example you might quite reasonably be suspicious of an email containing phrases like “online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”. \nAnother thing we might want to know about a text is the language it’s written in. Texts on social media, for example, can be in any number of languages and we’ll need to apply different processing. The task of language id is thus the first step in most language processing pipelines. Related text classification tasks like authorship attribution— determining a text’s author— are also relevant to the digital humanities, social sciences, and forensic linguistics.",
        "chapter": "N-gram Language Models",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961. \nClassification is essential for tasks below the level of the document as well. We’ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 8) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. \nThe goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes. One method for classifying text is to use handwritten rules. There are many areas of language processing where handwritten rule-based classifiers constitute a state-ofthe-art system, or at least part of it. \nRules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification in language processing are instead done via supervised machine learning, and this will be the subject of the remainder of this chapter. In supervised learning, we have a data set of input observations, each associated with some correct output (a ‘supervision signal’). The goal of the algorithm is to learn how to map from a new observation to a correct output. \nFormally, the task of supervised classification is to take an input $x$ and a fixed set of output classes $Y = y _ { 1 } , y _ { 2 } , . . . , y _ { M }$ and return a predicted class $y in Y$ . For text classification, we’ll sometimes talk about $c$ (for “class”) instead of $y$ as our output variable, and $d$ (for “document”) instead of $x$ as our input variable. In the supervised situation we have a training set of $N$ documents that have each been hand-labeled with a class: ${ ( d _ { 1 } , c _ { 1 } ) , . . . . , ( d _ { N } , c _ { N } ) }$ . Our goal is to learn a classifier that is capable of mapping from a new document $d$ to its correct class $c in C$ . A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems. \nMany kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. \n4.1 Naive Bayes Classifiers \nnaive Bayes classifier \nIn this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about \nhow the features interact. \nbag-of-words \nThe intuition of the classifier is shown in Fig. 4.1. We represent a text document as if it were a bag-of-words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the figure, instead of representing the word order in all the phrases like “I love this movie” and “I would recommend it”, we simply note that the word $I$ occurred 5 times in the entire excerpt, the word $i t 6$ times, the words love, recommend, and movie once, and so on. \nit 6   \nI 5   \nI love this movie! It's sweet, 1 the 4   \nbut with satirical humor. The fairy always loveto it to 3   \ndialogue is great and the it whimsical it I and 3   \nIatdmveantaugres tcoenbeswahriemfsuinc.a.l. fraienndd seendialoargeuaenyone yet seen 21   \nand romantic while laughing adventure recommend would 1   \nfat rtyhetacleongvenrtei.oInswofutldhe recommend it to just about tainmyeosn,ea.nI'dvIe'mseaelnwiatysehvaerpaply sweet of satirical several the hito I but to romovainetic Iit again it the humor seen would twimhiemssical sweet saadtvireicntalure 1 1 1   \nto see it again whenever I to scenes I the manages genre 1   \nsheaeven iat fyreiet!nd who hasn't fun whenever abouhtave while the timesand fhauirmyor 1   \nwith great 1   \n· · \nNaive Bayes is a probabilistic classifier, meaning that for a document $d$ , out of all classes $c in C$ the classifier returns the class $hat { c }$ which has the maximum posterior probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our estimate of the correct class”. \nBayesian inference \nThis idea of Bayesian inference has been known since the work of Bayes (1763), and was first applied to text classification by Mosteller and Wallace (1964). The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into other probabilities that have some useful properties. Bayes’ rule is presented in Eq. 4.2; it gives us a way to break down any conditional probability $P ( x | y )$ into three other probabilities: \nWe can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3: \nWe can conveniently simplify Eq. 4.3 by dropping the denominator $P ( d )$ . This is possible because we will be computing $frac { P ( d | c ) P ( c ) } { P ( d ) }$ for each possible class. But $P ( d )$ doesn’t change for each class; we are always asking about the most likely class for the same document $d$ , which must have the same probability $P ( d )$ . Thus, we can choose the class that maximizes this simpler formula: \nWe call Naive Bayes a generative model because we can read Eq. 4.4 as stating a kind of implicit assumption about how a document is generated: first a class is sampled from $P ( c )$ , and then the words are generated by sampling from $P ( d | c )$ . (In fact we could imagine generating artificial documents, or at least their word counts, by following this process). We’ll say more about this intuition of generative models in Chapter 5. \nTo return to classification: we compute the most probable class $hat { c }$ given some document $d$ by choosing the class which has the highest product of two probabilities: the prior probability of the class $P ( c )$ and the likelihood of the document $P ( d | c )$ : \nWithout loss of generalization, we can represent a document $d$ as a set of features $f _ { 1 } , f _ { 2 } , . . . , f _ { n }$ : \nUnfortunately, Eq. 4.6 is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions. \nThe first is the bag of words assumption discussed intuitively above: we assume position doesn’t matter, and that the word “love” has the same effect on classification whether it occurs as the 1st, 20th, or last word in the document. Thus we assume that the features $f _ { 1 } , f _ { 2 } , . . . , f _ { n }$ only encode word identity and not position. \nThe second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities $P ( f _ { i } | c )$ are independent given the class $c$ and hence can be ‘naively’ multiplied as follows: \nThe final equation for the class chosen by a naive Bayes classifier is thus: \nTo apply the naive Bayes classifier to text, we need to consider word positions, by simply walking an index through every word position in the document: \nNaive Bayes calculations, like calculations for language modeling, are done in log space, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead expressed as \nlinear classifiers \nBy considering features in log space, Eq. 4.10 computes the predicted class as a linear function of input features. Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression— are called linear classifiers. \n4.2 Training the Naive Bayes Classifier \nHow can we learn the probabilities $P ( c )$ and $P ( f _ { i } | c ) ?$ Let’s first consider the maximum likelihood estimate. We’ll simply use the frequencies in the data. For the class prior $P ( c )$ we ask what percentage of the documents in our training set are in each class $c$ . Let $N _ { c }$ be the number of documents in our training data with class $c$ and $N _ { d o c }$ be the total number of documents. Then: \nTo learn the probability $P (  f _ { i } | c )$ , we’ll assume a feature is just the existence of a word in the document’s bag of words, and so we’ll want $P ( w _ { i } | boldsymbol { c } )$ , which we compute as the fraction of times the word $w _ { i }$ appears among all words in all documents of topic $c$ . We first concatenate all documents with category $c$ into one big “category $c ^ { prime prime }$ text. Then we use the frequency of $w _ { i }$ in this concatenated document to give a maximum likelihood estimate of the probability: \nHere the vocabulary $mathrm { Delta V }$ consists of the union of all the word types in all classes, not just the words in one class $c$ . \nThere is a problem, however, with maximum likelihood training. Imagine we are trying to estimate the likelihood of the word “fantastic” given class positive, but suppose there are no training documents that both contain the word “fantastic” and are classified as positive. Perhaps the word “fantastic” happens to occur (sarcastically?) in the class negative. In such a case the probability for this feature will be zero: \nBut since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence! \nThe simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Naive Bayes Classifiers",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Naive Bayes calculations, like calculations for language modeling, are done in log space, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead expressed as \nlinear classifiers \nBy considering features in log space, Eq. 4.10 computes the predicted class as a linear function of input features. Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression— are called linear classifiers. \n4.2 Training the Naive Bayes Classifier \nHow can we learn the probabilities $P ( c )$ and $P ( f _ { i } | c ) ?$ Let’s first consider the maximum likelihood estimate. We’ll simply use the frequencies in the data. For the class prior $P ( c )$ we ask what percentage of the documents in our training set are in each class $c$ . Let $N _ { c }$ be the number of documents in our training data with class $c$ and $N _ { d o c }$ be the total number of documents. Then: \nTo learn the probability $P (  f _ { i } | c )$ , we’ll assume a feature is just the existence of a word in the document’s bag of words, and so we’ll want $P ( w _ { i } | boldsymbol { c } )$ , which we compute as the fraction of times the word $w _ { i }$ appears among all words in all documents of topic $c$ . We first concatenate all documents with category $c$ into one big “category $c ^ { prime prime }$ text. Then we use the frequency of $w _ { i }$ in this concatenated document to give a maximum likelihood estimate of the probability: \nHere the vocabulary $mathrm { Delta V }$ consists of the union of all the word types in all classes, not just the words in one class $c$ . \nThere is a problem, however, with maximum likelihood training. Imagine we are trying to estimate the likelihood of the word “fantastic” given class positive, but suppose there are no training documents that both contain the word “fantastic” and are classified as positive. Perhaps the word “fantastic” happens to occur (sarcastically?) in the class negative. In such a case the probability for this feature will be zero: \nBut since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence! \nThe simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing \nunknown word \nalgorithms in language modeling, it is commonly used in naive Bayes text categorization: \nstop words \nNote once again that it is crucial that the vocabulary $mathrm { v }$ consists of the union of all the word types in all classes, not just the words in one class $c$ (try to convince yourself why this must be true; see the exercise at the end of the chapter). \nWhat do we do about words that occur in our test data but are not in our vocabulary at all because they did not occur in any training document in any class? The solution for such unknown words is to ignore them—remove them from the test document and not include any probability for them at all. \nFinally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and $a$ . This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10–100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop word lists available online. Then each instance of these stop words is simply removed from both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn’t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list. \nFig. 4.2 shows the final algorithm. \n4.3 Worked example \nLet’s walk through an example of training and testing naive Bayes with add-one smoothing. We’ll use a sentiment analysis domain with the two classes positive $( + )$ and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. \nThe prior $P ( c )$ for the two classes is computed via Eq. 4.11 as $frac { N _ { c } } { N _ { d o c } }$ \nThe word with doesn’t occur in the training set, so we drop it completely (as mentioned above, we don’t use unknown word models for naive Bayes). The likelihoods from the training set for the remaining three words “predictable”, “no”, and “fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder of the words in the training set is left as an exercise for the reader): \nFor the test sentence ${ boldsymbol { mathrm { S } } } =$ “predictable with no fun”, after removing the word ‘with’, the chosen class, via Eq. 4.9, is therefore computed as follows: \nThe model thus predicts the class negative for the test sentence. \n4.4 Optimizing for Sentiment Analysis \nWhile standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. \nFirst, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. Thus it often improves performance to clip the word counts in each document at 1 (see the end of the chapter for pointers to these results). This variant is called binary",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Training the Naive Bayes Classifier",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "4.3 Worked example \nLet’s walk through an example of training and testing naive Bayes with add-one smoothing. We’ll use a sentiment analysis domain with the two classes positive $( + )$ and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. \nThe prior $P ( c )$ for the two classes is computed via Eq. 4.11 as $frac { N _ { c } } { N _ { d o c } }$ \nThe word with doesn’t occur in the training set, so we drop it completely (as mentioned above, we don’t use unknown word models for naive Bayes). The likelihoods from the training set for the remaining three words “predictable”, “no”, and “fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder of the words in the training set is left as an exercise for the reader): \nFor the test sentence ${ boldsymbol { mathrm { S } } } =$ “predictable with no fun”, after removing the word ‘with’, the chosen class, via Eq. 4.9, is therefore computed as follows: \nThe model thus predicts the class negative for the test sentence. \n4.4 Optimizing for Sentiment Analysis \nWhile standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. \nFirst, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. Thus it often improves performance to clip the word counts in each document at 1 (see the end of the chapter for pointers to these results). This variant is called binary",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Worked example",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "4.3 Worked example \nLet’s walk through an example of training and testing naive Bayes with add-one smoothing. We’ll use a sentiment analysis domain with the two classes positive $( + )$ and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. \nThe prior $P ( c )$ for the two classes is computed via Eq. 4.11 as $frac { N _ { c } } { N _ { d o c } }$ \nThe word with doesn’t occur in the training set, so we drop it completely (as mentioned above, we don’t use unknown word models for naive Bayes). The likelihoods from the training set for the remaining three words “predictable”, “no”, and “fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder of the words in the training set is left as an exercise for the reader): \nFor the test sentence ${ boldsymbol { mathrm { S } } } =$ “predictable with no fun”, after removing the word ‘with’, the chosen class, via Eq. 4.9, is therefore computed as follows: \nThe model thus predicts the class negative for the test sentence. \n4.4 Optimizing for Sentiment Analysis \nWhile standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. \nFirst, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. Thus it often improves performance to clip the word counts in each document at 1 (see the end of the chapter for pointers to these results). This variant is called binary \nbinary NB \nmultinomial naive Bayes or binary NB. The variant uses the same Eq. 4.10 except that for each document we remove all duplicate words before concatenating them into the single big document. Fig. 4.3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right. The example is worked without add-1 smoothing to make the differences clearer. Note that the results counts need not be 1; the word great has a count of 2 even for Binary NB, because it appears in multiple documents. \nA second important addition commonly made when doing text classification for sentiment is to deal with negation. Consider the difference between I really like this movie (positive) and I didn’t like this movie (negative). The negation expressed by didn’t completely alters the inferences we draw from the predicate like. Similarly, negation can modify a negative word to produce a positive review (don’t dismiss this film, doesn’t let us get bored). \nA very simple baseline that is commonly used in sentiment analysis to deal with negation is the following: during text normalization, prepend the prefix NOT to every word after a token of logical negation (n’t, not, no, never) until the next punctuation mark. Thus the phrase \ndidn’t like this movie , but I \nbecomes \ndidn’t NOT_like NOT_this NOT_movie , but I \nNewly formed ‘words’ like NOT like, NOT recommend will thus occur more often in negative document and act as cues for negative sentiment, while words like NOT bored, NOT dismiss will acquire positive associations. We will return in Chapter 16 to the use of parsing to deal more accurately with the scope relationship between these negation words and the predicates they modify, but this simple baseline works quite well in practice. \nFinally, in some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive \nGeneral Inquirer LIWC \nand negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). \nFor example the MPQA subjectivity lexicon has 6885 words, 2718 positive and 4912 negative, each marked for whether it is strongly or weakly biased. Some samples of positive and negative words from the MPQA lexicon include: \n+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate \nA common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called ‘this word occurs in the positive lexicon’, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature ‘this word occurs in the negative lexicon’ of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won’t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better. \nWe’ll return to this use of lexicons in Chapter 20, showing how these lexicons can be learned automatically, and how they can be applied to many other tasks beyond sentiment classification. \n4.5 Naive Bayes for other text classification tasks \nspam detection \nIn the previous section we pointed out that naive Bayes doesn’t require that our classifier use all the words in the training data as features. In fact features in naive Bayes can express any property of the input text we want. \nConsider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)—one of the first applications of naive Bayes to text classification (Sahami et al., 1998). \nA common solution here, rather than using all the words as individual features, is to predefine likely sets of words or phrases as features, combined with features that are not purely linguistic. For example the open-source SpamAssassin tool1 predefines features like the phrase “one hundred percent guaranteed”, or the feature mentions millions of dollars, which is a regular expression that matches suspiciously large sums of money. But it also includes features like HTML has a low ratio of text to image area, that aren’t purely linguistic and might require some sophisticated computation, or totally non-linguistic features about, say, the path that the email took to arrive. More sample SpamAssassin features: \n• Email subject line is all capital letters • Contains phrases of urgency like “urgent reply” • Email subject line contains “online pharmaceutical” • HTML has unbalanced “head” tags • Claims you can be removed from the list \nFor other tasks, like language id—determining what language a given piece of text is written in—the most effective naive Bayes features are not words at all, but character n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie $z '$ , ‘thei’), or, even simpler byte n-grams, where instead of using the multibyte Unicode character representations called codepoints, we just pretend everything is a string of raw bytes. Because spaces count as a byte, byte n-grams can model statistics about the beginning or ending of words. A widely used naive Bayes system, langid.py (Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature selection to winnow down to the most informative 7000 final features.",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Optimizing for Sentiment Analysis",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "General Inquirer LIWC \nand negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). \nFor example the MPQA subjectivity lexicon has 6885 words, 2718 positive and 4912 negative, each marked for whether it is strongly or weakly biased. Some samples of positive and negative words from the MPQA lexicon include: \n+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate \nA common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called ‘this word occurs in the positive lexicon’, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature ‘this word occurs in the negative lexicon’ of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won’t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better. \nWe’ll return to this use of lexicons in Chapter 20, showing how these lexicons can be learned automatically, and how they can be applied to many other tasks beyond sentiment classification. \n4.5 Naive Bayes for other text classification tasks \nspam detection \nIn the previous section we pointed out that naive Bayes doesn’t require that our classifier use all the words in the training data as features. In fact features in naive Bayes can express any property of the input text we want. \nConsider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)—one of the first applications of naive Bayes to text classification (Sahami et al., 1998). \nA common solution here, rather than using all the words as individual features, is to predefine likely sets of words or phrases as features, combined with features that are not purely linguistic. For example the open-source SpamAssassin tool1 predefines features like the phrase “one hundred percent guaranteed”, or the feature mentions millions of dollars, which is a regular expression that matches suspiciously large sums of money. But it also includes features like HTML has a low ratio of text to image area, that aren’t purely linguistic and might require some sophisticated computation, or totally non-linguistic features about, say, the path that the email took to arrive. More sample SpamAssassin features: \n• Email subject line is all capital letters • Contains phrases of urgency like “urgent reply” • Email subject line contains “online pharmaceutical” • HTML has unbalanced “head” tags • Claims you can be removed from the list \nFor other tasks, like language id—determining what language a given piece of text is written in—the most effective naive Bayes features are not words at all, but character n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie $z '$ , ‘thei’), or, even simpler byte n-grams, where instead of using the multibyte Unicode character representations called codepoints, we just pretend everything is a string of raw bytes. Because spaces count as a byte, byte n-grams can model statistics about the beginning or ending of words. A widely used naive Bayes system, langid.py (Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature selection to winnow down to the most informative 7000 final features. \n\nLanguage ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geotagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017). \n4.6 Naive Bayes as a Language Model \nAs we saw in the previous section, naive Bayes classifiers can use any sort of feature: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in the previous section, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similarity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. \nSince the likelihood features from the naive Bayes model assign a probability to each word $P ( w o r d | c )$ , the model also assigns a probability to each sentence: \nThus consider a naive Bayes model with the classes positive $( + )$ and negative (-) and the following model parameters: \nEach of the two columns above instantiates a language model that can assign a probability to the sentence “I love this fun film”: \nP(“I love this fun film” $| + rrangle$ ) = 0.1 0.1 0.01 0.05 0.1 = 0.0000005 $P ( { mathrm { ' I ~ l o v e ~ t h i s ~ f u n ~ f i l m \" } } | - ) ~ = ~ 0 . 2 times 0 . 0 0 1 times 0 . 0 1 times 0 . 0 0 5 times 0 . 1 = . 0 0 0 0 0 0 0 1 0$",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Naive Bayes for other text classification tasks",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geotagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017). \n4.6 Naive Bayes as a Language Model \nAs we saw in the previous section, naive Bayes classifiers can use any sort of feature: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in the previous section, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similarity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. \nSince the likelihood features from the naive Bayes model assign a probability to each word $P ( w o r d | c )$ , the model also assigns a probability to each sentence: \nThus consider a naive Bayes model with the classes positive $( + )$ and negative (-) and the following model parameters: \nEach of the two columns above instantiates a language model that can assign a probability to the sentence “I love this fun film”: \nP(“I love this fun film” $| + rrangle$ ) = 0.1 0.1 0.01 0.05 0.1 = 0.0000005 $P ( { mathrm { ' I ~ l o v e ~ t h i s ~ f u n ~ f i l m \" } } | - ) ~ = ~ 0 . 2 times 0 . 0 0 1 times 0 . 0 1 times 0 . 0 0 5 times 0 . 1 = . 0 0 0 0 0 0 0 1 0$ \nAs it happens, the positive model assigns a higher probability to the sentence: $P ( s | p o s ) > P ( s | n e g )$ . Note that this is just the likelihood part of the naive Bayes model; once we multiply in the prior a full naive Bayes model might well make a different classification decision. \n4.7 Evaluation: Precision, Recall, $mathrm { F }$ -measure \ngold labels \nconfusion matrix \nTo introduce the methods for evaluating text classification, let’s first consider some simple binary detection tasks. For example, in spam detection, our goal is to label every text as being in the spam category (“positive”) or not in the spam category (“negative”). For each item (email document) we therefore need to know whether our system called it spam or not. We also need to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to match. We will refer to these human labels as the gold labels. \nOr imagine you’re the CEO of the Delicious Pie Company and you need to know what people are saying about your pies on social media, so you build a system that detects tweets concerning Delicious Pie. Here the positive class is tweets about Delicious Pie and the negative class is all other tweets. \nIn both cases, we need a metric for knowing how well our spam detector (or pie-tweet-detector) is doing. To evaluate any system for detecting things, we start by building a confusion matrix like the one shown in Fig. 4.4. A confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using two dimensions (system output and gold labels), and each cell labeling a set of possible outcomes. In the spam detection case, for example, true positives are documents that are indeed spam (indicated by human-created gold labels) that our system correctly said were spam. False negatives are documents that are indeed spam but our system incorrectly labeled as non-spam. \nTo the bottom right of the table is the equation for accuracy, which asks what percentage of all the observations (for the spam or pie examples that means all emails or tweets) our system labeled correctly. Although accuracy might seem a natural metric, we generally don’t use it for text classification tasks. That’s because accuracy doesn’t work well when the classes are unbalanced (as indeed they are with spam, which is a large majority of email, or with tweets, which are mainly not about pie). \nTo make this more explicit, imagine that we looked at a million tweets, and let’s say that only 100 of them are discussing their love (or hatred) for our pie, while the other 999,900 are tweets about something completely unrelated. Imagine a simple classifier that stupidly classified every tweet as “not about pie”. This classifier would have 999,900 true negatives and only 100 false negatives for an accuracy of 999,900/1,000,000 or $9 9 . 9 9 %$ ! What an amazing accuracy level! Surely we should be happy with this classifier? But of course this fabulous ‘no pie’ classifier would be completely useless, since it wouldn’t find a single one of the customer comments we are looking for. In other words, accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely balanced in frequency, which is a very common situation in the world.",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Naive Bayes as a Language Model",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Harmonic mean is used because it is a conservative metric; the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is. Thus it weighs the lower of the two numbers more heavily. \n4.7.1 Evaluating with more than two classes \nUp to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. \nmacroaveraging microaveraging \nBut we’ll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for example, that the system mistakenly labeled one spam document as urgent, and we have shown how to compute a distinct precision and recall value for each class. In order to derive a single metric that tells us how well the system is doing, we can combine these values in two ways. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and recall from that table. Fig. 4.6 shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision. \nAs the figure shows, a microaverage is dominated by the more frequent class (in this case spam), since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. \n4.8 Test sets and Cross-validation \ndevelopment test set devset \nThe training and testing procedure for text classification follows what we saw with language modeling (Section 3.2): we use the training set to train the model, then use the development test set (also called a devset) to perhaps tune some parameters,",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Evaluation: Precision, Recall, F-measure",
        "subsection": "Evaluating with more than two classes",
        "subsubsection": "N/A"
    },
    {
        "content": "Harmonic mean is used because it is a conservative metric; the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is. Thus it weighs the lower of the two numbers more heavily. \n4.7.1 Evaluating with more than two classes \nUp to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. \nmacroaveraging microaveraging \nBut we’ll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for example, that the system mistakenly labeled one spam document as urgent, and we have shown how to compute a distinct precision and recall value for each class. In order to derive a single metric that tells us how well the system is doing, we can combine these values in two ways. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and recall from that table. Fig. 4.6 shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision. \nAs the figure shows, a microaverage is dominated by the more frequent class (in this case spam), since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. \n4.8 Test sets and Cross-validation \ndevelopment test set devset \nThe training and testing procedure for text classification follows what we saw with language modeling (Section 3.2): we use the training set to train the model, then use the development test set (also called a devset) to perhaps tune some parameters, \ncross-validation \nand in general decide what the best model is. Once we come up with what we think is the best model, we run it on the (hitherto unseen) test set to report its performance. \nWhile the use of a devset avoids overfitting the test set, having a fixed training set, devset, and test set creates another problem: in order to save lots of data for training, the test set (or devset) might not be large enough to be representative. Wouldn’t it be better if we could somehow use all our data for training and still use all our data for test? We can do this by cross-validation. \nfolds \n10-fold cross-validation \nIn cross-validation, we choose a number $k$ , and partition our data into $k$ disjoint subsets called folds. Now we choose one of those $k$ folds as a test set, train our classifier on the remaining $k - 1$ folds, and then compute the error rate on the test set. Then we repeat with another fold as the test set, again training on the other $k - 1$ folds. We do this sampling process $k$ times and average the test set error rate from these $k$ runs to get an average error rate. If we choose $k = 1 0$ , we would train 10 different models (each on $90 %$ of our data), test the model 10 times, and average these 10 values. This is called 10-fold cross-validation. \nThe only problem with cross-validation is that because all the data is used for testing, we need the whole corpus to be blind; we can’t examine any of the data to suggest possible features and in general see what’s going on, because we’d be peeking at the test set, and such cheating would cause us to overestimate the performance of our system. However, looking at the corpus to understand what’s going on is important in designing NLP systems! What to do? For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set, as shown in Fig. 4.7. \n4.9 Statistical Significance Testing \nIn building systems we often need to compare the performance of two systems. How can we know if the new system we just built is better than our old one? Or better than some other system described in the literature? This is the domain of statistical hypothesis testing, and in this section we introduce tests for statistical significance for NLP classifiers, drawing especially on the work of Dror et al. (2020) and BergKirkpatrick et al. (2012). \nSuppose we’re comparing the performance of classifiers $A$ and $B$ on a metric $M$ such as $mathrm { F } _ { 1 }$ , or accuracy. Perhaps we want to know if our logistic regression sentiment classifier $A$ (Chapter 5) gets a higher $mathrm { F } _ { 1 }$ score than our naive Bayes sentiment classifier $B$ on a particular test set $x$ . Let’s call $M ( A , x )$ the score that system $A$ gets on test set $x$ , and $delta ( x )$ the performance difference between $A$ and $B$ on $x$ :",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Test sets and Cross-validation",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "statistically significant \n(unlikely to have such a large $delta$ if $A$ is in fact not better than $B$ ). But if $delta ( x )$ is very small, it might be less surprising to us even if $H _ { 0 }$ were true and $A$ is not really better than $B$ , and so the p-value would be higher. \nA very small p-value means that the difference we observed is very unlikely under the null hypothesis, and we can reject the null hypothesis. What counts as very small? It is common to use values like .05 or .01 as the thresholds. A value of .01 means that if the $mathsf { p }$ -value (the probability of observing the δ we saw assuming $H _ { 0 }$ is true) is less than .01, we reject the null hypothesis and assume that $A$ is indeed better than $B$ . We say that a result (e.g., $^ { 6 6 } A$ is better than $B ^ { prime prime }$ ) is statistically significant if the $delta$ we saw has a probability that is below the threshold and we therefore reject this null hypothesis. \nHow do we compute this probability we need for the p-value? In NLP we generally don’t use simple parametric tests like t-tests or ANOVAs that you might be familiar with. Parametric tests make assumptions about the distributions of the test statistic (such as normality) that don’t generally hold in our cases. So in NLP we usually use non-parametric tests based on sampling: we artificially create many versions of the experimental setup. For example, if we had lots of different test sets $x ^ { prime }$ we could just measure all the $delta ( x ^ { prime } )$ for all the $x ^ { prime }$ . That gives us a distribution. Now we set a threshold (like .01) and if we see in this distribution that $9 9 %$ or more of those deltas are smaller than the delta we observed, i.e., that p-value $( x )$ —the probability of seeing a $delta ( x )$ as big as the one we saw—is less than .01, then we can reject the null hypothesis and agree that $delta ( x )$ was a sufficiently surprising difference and $A$ is really a better algorithm than $B$ . \napproximate randomization \npaired \nThere are two common non-parametric tests used in NLP: approximate randomization (Noreen, 1989) and the bootstrap test. We will describe bootstrap below, showing the paired version of the test, which again is most common in NLP. Paired tests are those in which we compare two sets of observations that are aligned: each observation in one set can be paired with an observation in another. This happens naturally when we are comparing the performance of two systems on the same test set; we can pair the performance of system $A$ on an individual observation $x _ { i }$ with the performance of system $B$ on the same $x _ { i }$ . \nbootstrap test \nbootstrapping \n4.9.1 The Paired Bootstrap Test \nThe bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word bootstrapping refers to repeatedly drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sample. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population. \nConsider a tiny text classification example with a test set $x$ of 10 documents. The first row of Fig. 4.8 shows the results of two classifiers (A and B) on this test set, with each document labeled by one of the four possibilities: (A and B both right, both wrong, A right and B wrong, A wrong and B right); a slash through a letter $mathbf { beta } ( mathbf { B } )$ means that that classifier got the answer wrong. On the first document both A and B get the correct class (AB), while on the second document A got it right but B got it wrong (A\u0013B). If we assume for simplicity that our metric is accuracy, A has an accuracy of .70 and B of .50, so $delta ( x )$ is .20. \nNow we create a large number $b$ (perhaps $1 0 ^ { 5 }$ ) of virtual test sets $x ^ { ( i ) }$ , each of size $n = 1 0$ . Fig. 4.8 shows a couple examples. To create each virtual test set $x ^ { ( i ) }$ , we repeatedly ${ mathrm { ' } n = 1 0 }$ times) select a cell from row $x$ with replacement. For example, to create the first cell of the first virtual test set $x ^ { ( 1 ) }$ , if we happened to randomly select the second cell of the $x$ row; we would copy the value ${ bf A , omega ^ { prime } }$ into our new cell, and move on to create the second cell of $x ^ { ( 1 ) }$ , each time sampling (randomly choosing) from the original $x$ with replacement. \n\nFigure 4.8 The paired bootstrap test: Examples of $b$ pseudo test sets $widehat { x ^ { ( i ) } }$ being created from an initial true test set $x$ . Each pseudo test set is created by sampling $n = 1 0$ times with replacement; thus an individual sample is a single cell, a document with its gold label and the correct or incorrect performance of classifiers A and B. Of course real test sets don’t have only 10 examples, and $b$ needs to be large as well. \nNow that we have the $b$ test sets, providing a sampling distribution, we can do statistics on how often $A$ has an accidental advantage. There are various ways to compute this advantage; here we follow the version laid out in Berg-Kirkpatrick et al. (2012). Assuming $H _ { 0 }$ (A isn’t better than $B$ ), we would expect that $delta ( X )$ , estimated over many test sets, would be zero; a much higher value would be surprising, since $H _ { 0 }$ specifically assumes $A$ isn’t better than $B$ . To measure exactly how surprising our observed $delta ( x )$ is, we would in other circumstances compute the p-value by counting over many test sets how often $delta ( boldsymbol { x } ^ { ( i ) } )$ exceeds the expected zero value by $delta ( x )$ or more: \n(We use the notation $mathbb { 1 } ( x )$ to mean $^ { ast } 1$ if $x$ is true, and 0 otherwise”.) However, although it’s generally true that the expected value of $delta ( X )$ over many test sets, (again assuming $A$ isn’t better than $B$ ) is 0, this isn’t true for the bootstrapped test sets we created. That’s because we didn’t draw these samples from a distribution with 0 mean; we happened to create them from the original test set $x$ , which happens to be biased (by .20) in favor of $A$ . So to measure how surprising is our observed $delta ( x )$ , we actually compute the $mathsf { p }$ -value by counting over many test sets how often $delta ( boldsymbol { x } ^ { ( i ) } )$ exceeds the expected value of $delta ( x )$ by $delta ( x )$ or more: \nSo if for example we have 10,000 test sets $x ^ { ( i ) }$ and a threshold of .01, and in only 47 of the test sets do we find that $delta ( x ^ { ( i ) } ) geq 2 delta ( x )$ , the resulting p-value of .0047 is smaller than .01, indicating $delta ( x )$ is indeed sufficiently surprising, and we can reject the null hypothesis and conclude $A$ is better than $B$ . \nfunction BOOTSTRAP(test set $x ,$ , num of samples $b$ ) returns $p$ -value(x)   \nCalculate $delta ( x )$ # how much better does algorithm A do than B on $x$   \n$s = 0$   \nfor $i = 1$ to $b$ do for $j = 1$ to $n$ do # Draw a bootstrap sample $x ^ { ( i ) }$ of size n Select a member of $x$ at random and add it to $x ^ { ( i ) }$ Calculate $delta ( boldsymbol { x } ^ { ( i ) } )$ # how much better does algorithm A do than B on $x ^ { ( i ) }$ $s gets s + 1$ if $delta ( x ^ { ( i ) } ) geq 2 delta ( x )$   \np-value $begin{array} { r } { left( x right) approx frac { s } { b } } end{array}$ # on what $%$ of the b samples did algorithm A beat expectations?   \nreturn p-value(x) # if very few did, our observed $delta$ is probably not accidental \nThe full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set $x$ , a number of samples $b$ , and counts the percentage of the $b$ bootstrap test sets in which $delta ( x ^ { * ( i ) } ) > 2 delta ( x )$ . This percentage then acts as a one-sided empirical p-value \n4.10 Avoiding Harms in Classification \nrepresentational harms \nIt is important to avoid harms that may result from classifiers, harms that exist both for naive Bayes classifiers and for the other classification algorithms we introduce in later chapters. \nOne class of harms is representational harms (Crawford 2017, Blodgett et al. 2020), harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. For example Kiritchenko and Mohammad (2018) examined the performance of 200 sentiment analysis systems on pairs of sentences that were identical except for containing either a common African American first name (like Shaniqua) or a common European American first name (like Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 6. They found that most systems assigned lower sentiment and more negative emotion to sentences with African American names, reflecting and perpetuating stereotypes that associate African Americans with negative emotions (Popp et al., 2003). \ntoxicity detection \nIn other tasks classifiers may lead to both representational harms and other harms, such as censorship. For example the important text classification task of toxicity detection is the task of detecting hate speech, abuse, harassment, or other kinds of toxic language. While the goal of such classifiers is to help reduce societal harm, toxicity classifiers can themselves cause harms. For example, researchers have shown that some widely used toxicity classifiers incorrectly flag as being toxic sentences that are non-toxic but simply mention minority identities like women (Park et al., 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018), or simply use linguistic features characteristic of varieties like AfricanAmerican Vernacular English (Sap et al. 2019, Davidson et al. 2019). Such false positive errors, if employed by toxicity detection systems without human oversight, could lead to the censoring of discourse by or about these groups. \nThese model problems can be caused by biases or other problems in the training data; in general, machine learning systems replicate and even amplify the biases in their training data. But these problems can also be caused by the labels (for example due to biases in the human labelers), by the resources used (like lexicons, or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize). While the mitigation of these biases (for example by carefully considering the training data sources) is an important area of research, we currently don’t have general solutions. For this reason it’s important, when introducing any NLP model, to study these these kinds of factors and make them clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for each version of a model. A model card documents a machine learning model with information like:",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Statistical Significance Testing",
        "subsection": "The Paired Bootstrap Test",
        "subsubsection": "N/A"
    },
    {
        "content": "function BOOTSTRAP(test set $x ,$ , num of samples $b$ ) returns $p$ -value(x)   \nCalculate $delta ( x )$ # how much better does algorithm A do than B on $x$   \n$s = 0$   \nfor $i = 1$ to $b$ do for $j = 1$ to $n$ do # Draw a bootstrap sample $x ^ { ( i ) }$ of size n Select a member of $x$ at random and add it to $x ^ { ( i ) }$ Calculate $delta ( boldsymbol { x } ^ { ( i ) } )$ # how much better does algorithm A do than B on $x ^ { ( i ) }$ $s gets s + 1$ if $delta ( x ^ { ( i ) } ) geq 2 delta ( x )$   \np-value $begin{array} { r } { left( x right) approx frac { s } { b } } end{array}$ # on what $%$ of the b samples did algorithm A beat expectations?   \nreturn p-value(x) # if very few did, our observed $delta$ is probably not accidental \nThe full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set $x$ , a number of samples $b$ , and counts the percentage of the $b$ bootstrap test sets in which $delta ( x ^ { * ( i ) } ) > 2 delta ( x )$ . This percentage then acts as a one-sided empirical p-value \n4.10 Avoiding Harms in Classification \nrepresentational harms \nIt is important to avoid harms that may result from classifiers, harms that exist both for naive Bayes classifiers and for the other classification algorithms we introduce in later chapters. \nOne class of harms is representational harms (Crawford 2017, Blodgett et al. 2020), harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. For example Kiritchenko and Mohammad (2018) examined the performance of 200 sentiment analysis systems on pairs of sentences that were identical except for containing either a common African American first name (like Shaniqua) or a common European American first name (like Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 6. They found that most systems assigned lower sentiment and more negative emotion to sentences with African American names, reflecting and perpetuating stereotypes that associate African Americans with negative emotions (Popp et al., 2003). \ntoxicity detection \nIn other tasks classifiers may lead to both representational harms and other harms, such as censorship. For example the important text classification task of toxicity detection is the task of detecting hate speech, abuse, harassment, or other kinds of toxic language. While the goal of such classifiers is to help reduce societal harm, toxicity classifiers can themselves cause harms. For example, researchers have shown that some widely used toxicity classifiers incorrectly flag as being toxic sentences that are non-toxic but simply mention minority identities like women (Park et al., 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018), or simply use linguistic features characteristic of varieties like AfricanAmerican Vernacular English (Sap et al. 2019, Davidson et al. 2019). Such false positive errors, if employed by toxicity detection systems without human oversight, could lead to the censoring of discourse by or about these groups. \nThese model problems can be caused by biases or other problems in the training data; in general, machine learning systems replicate and even amplify the biases in their training data. But these problems can also be caused by the labels (for example due to biases in the human labelers), by the resources used (like lexicons, or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize). While the mitigation of these biases (for example by carefully considering the training data sources) is an important area of research, we currently don’t have general solutions. For this reason it’s important, when introducing any NLP model, to study these these kinds of factors and make them clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for each version of a model. A model card documents a machine learning model with information like: \n\nmodel card \n• training algorithms and parameters   \n• training data sources, motivation, and preprocessing   \n• evaluation data sources, motivation, and preprocessing   \n• intended use and users   \n• model performance across different demographic or other groups and environmental situations \n4.11 Summary \nThis chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. \n• Many language processing tasks can be viewed as tasks of classification.   \n• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.   \n• Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.   \n• Naive Bayes is a generative model that makes the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class)   \n• Naive Bayes with binarized features seems to work better for many text classification tasks.   \n• Classifiers are evaluated based on precision and recall.   \n• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.   \n• Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.   \n• Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card. \nBibliographical and Historical Notes \nMultinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add-δ smoothing and information-based feature selection.",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Avoiding Harms in Classification",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "model card \n• training algorithms and parameters   \n• training data sources, motivation, and preprocessing   \n• evaluation data sources, motivation, and preprocessing   \n• intended use and users   \n• model performance across different demographic or other groups and environmental situations \n4.11 Summary \nThis chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. \n• Many language processing tasks can be viewed as tasks of classification.   \n• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.   \n• Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.   \n• Naive Bayes is a generative model that makes the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class)   \n• Naive Bayes with binarized features seems to work better for many text classification tasks.   \n• Classifiers are evaluated based on precision and recall.   \n• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.   \n• Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.   \n• Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card. \nBibliographical and Historical Notes \nMultinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add-δ smoothing and information-based feature selection.",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "model card \n• training algorithms and parameters   \n• training data sources, motivation, and preprocessing   \n• evaluation data sources, motivation, and preprocessing   \n• intended use and users   \n• model performance across different demographic or other groups and environmental situations \n4.11 Summary \nThis chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. \n• Many language processing tasks can be viewed as tasks of classification.   \n• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.   \n• Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.   \n• Naive Bayes is a generative model that makes the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class)   \n• Naive Bayes with binarized features seems to work better for many text classification tasks.   \n• Classifiers are evaluated based on precision and recall.   \n• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.   \n• Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.   \n• Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card. \nBibliographical and Historical Notes \nMultinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add-δ smoothing and information-based feature selection.",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "model card \n• training algorithms and parameters   \n• training data sources, motivation, and preprocessing   \n• evaluation data sources, motivation, and preprocessing   \n• intended use and users   \n• model performance across different demographic or other groups and environmental situations \n4.11 Summary \nThis chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. \n• Many language processing tasks can be viewed as tasks of classification.   \n• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.   \n• Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.   \n• Naive Bayes is a generative model that makes the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class)   \n• Naive Bayes with binarized features seems to work better for many text classification tasks.   \n• Classifiers are evaluated based on precision and recall.   \n• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.   \n• Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.   \n• Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card. \nBibliographical and Historical Notes \nMultinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add-δ smoothing and information-based feature selection. \nThe conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron’s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 1787–1788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). \nMetsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also use a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates $P ( w | c )$ as the fraction of documents that contain a term, and includes a probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text tasks. \nThere are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles. \nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classification; classification in general is covered in machine learning textbooks (Hastie et al. 2001, Witten and Frank 2005, Bishop 2006, Murphy 2012). \ninformation gain \nNon-parametric methods for computing statistical significance were used first in NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work has focused on issues including multiple test sets and multiple metrics (Søgaard et al. 2014, Dror et al. 2017). \nFeature selection is a method of removing features that are unlikely to generalize well. Features are generally ranked by how informative they are about the classification decision. A very common metric, information gain, tells us how many bits of information the presence of the word gives us for guessing the class. Other feature selection metrics include $chi ^ { 2 }$ , pointwise mutual information, and GINI index; see Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an introduction to feature selection. \nExercises \n4.1 Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class. \nWhat class will Naive bayes assign to the sentence “I always like foreign films.”? \n4.2 Given the following short movie reviews, each labeled with a genre, either comedy or action: \n1. fun, couple, love, love comedy   \n2. fast, furious, shoot action   \n3. couple, fly, fast, fun, fun comedy   \n4. furious, shoot, shoot, fun action   \n5. fly, fast, shoot, love action \nand a new document D: \nfast, couple, shoot, fly compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods. \n4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted. \nUse both naive Bayes models to assign a class (pos or neg) to this sentence: \nA good, good plot and great characters, but poor acting. \nRecall from page 62 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don’t throw out words that appear in some classes but not others; that’s what add-one smoothing is for.) Do the two models agree or disagree? \n5 Logistic Regression \nlogistic regression \n“And how do you know that these fine begonias are not of equal importance?” Hercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles \nDetective stories are as littered with clues as texts are with words. Yet for the poor reader it can be challenging to know how to weigh the author’s clues in order to make the crucial classification task: deciding whodunnit. \nIn this chapter we introduce an algorithm that is admirably suited for discovering the link between features or cues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. \nLogistic regression can be used to classify an observation into one of two classes (like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes. Because the mathematics for the two-class case is simpler, we’ll describe this special case of logistic regression first in the next few sections, and then briefly summarize the use of multinomial logistic regression for more than two classes in Section 5.3. \nWe’ll introduce the mathematics of logistic regression in the next few sections. But let’s begin with some high-level issues. \nGenerative and Discriminative Classifiers: The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier. \nThese are two very different frameworks for how to build a machine learning model. Consider a visual metaphor: imagine we’re trying to distinguish dog images from cat images. A generative model would have the goal of understanding what dogs look like and what cats look like. You might literally ask such a model to ‘generate’, i.e., draw, a dog. Given a test image, the system then asks whether it’s the cat model or the dog model that better fits (is less surprised by) the image, and chooses that as its label. \n\nA discriminative model, by contrast, is only trying to learn to distinguish the classes (perhaps without learning much about them). So maybe all the dogs in the training data are wearing collars and the cats aren’t. If that one feature neatly separates the classes, the model is satisfied. If you ask such a model what it knows about cats all it can say is that they don’t wear collars.",
        "chapter": "Naive Bayes and Sentiment Classification",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "More formally, recall that the naive Bayes assigns a class $c$ to a document $d$ not by directly computing $P ( c | d )$ but by computing a likelihood and a prior \nA generative model like naive Bayes makes use of this likelihood term, which expresses how to generate the features of a document $i f$ we knew it was of class $c$ . \nBy contrast a discriminative model in this text categorization scenario attempts to directly compute $P ( c | d )$ . Perhaps it will learn to assign a high weight to document features that directly improve its ability to discriminate between possible classes, even if it couldn’t generate an example of one of the classes. \nComponents of a probabilistic machine learning classifier: Like naive Bayes, logistic regression is a probabilistic classifier that makes use of supervised machine learning. Machine learning classifiers require a training corpus of $m$ input/output pairs $( bar { x ^ { ( i ) } } , y ^ { ( i ) } )$ . (We’ll use superscripts in parentheses to refer to individual instances in the training set—for sentiment classification each instance might be an individual document to be classified.) A machine learning system for classification then has four components: \n1. A feature representation of the input. For each input observation $x ^ { ( i ) }$ , this will be a vector of features $left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } right]$ . We will generally refer to feature $i$ for input $x ^ { ( j ) }$ as $x _ { i } ^ { ( j ) }$ , sometimes simplified as $x _ { i }$ , but we will also see the notation $f _ { i } , f _ { i } ( x )$ , or, for multiclass classification, $f _ { i } ( c , x )$ .   \n2. A classification function that computes $hat { y }$ , the estimated class, via $p ( y | x )$ . In the next section we will introduce the sigmoid and softmax tools for classification.   \n3. An objective function for learning, usually involving minimizing error on training examples. We will introduce the cross-entropy loss function.   \n4. An algorithm for optimizing the objective function. We introduce the stochastic gradient descent algorithm. \nLogistic regression has two phases: \ntraining: we train the system (specifically the weights $w$ and $b$ ) using stochastic gradient descent and the cross-entropy loss.   \ntest: Given a test example $x$ we compute $p ( y | x )$ and return the higher probability label $y = 1$ or $y = 0$ . \n5.1 The sigmoid function \nThe goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. Here we introduce the sigmoid classifier that will help us make this decision. \nConsider a single input observation $x$ , which we will represent by a vector of features $left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } right]$ (we’ll show sample features in the next subsection). The classifier output $y$ can be 1 (meaning the observation is a member of the class) or 0 (the observation is not a member of the class). We want to know the probability $P ( y = 1 | x )$ that this observation is a member of the class. So perhaps the decision is “positive sentiment” versus “negative sentiment”, the features represent counts of words in a document, $P ( y = 1 | x )$ is the probability that the document has positive sentiment, and $P ( y = 0 | x )$ is the probability that the document has negative sentiment. \n\nLogistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight $w _ { i }$ is a real number, and is associated with one of the input features $x _ { i }$ . The weight $w _ { i }$ represents how important that input feature is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number that’s added to the weighted inputs. \nbias term intercept \nTo make a decision on a test instance— after we’ve learned the weights in training— the classifier first multiplies each $x _ { i }$ by its weight $w _ { i }$ , sums up the weighted features, and adds the bias term $b$ . The resulting single number $z$ expresses the weighted sum of the evidence for the class. \ndot product \nIn the rest of the book we’ll represent such sums using the dot product notation from linear algebra. The dot product of two vectors a and $mathbf { b }$ , written as $mathbf { a } cdot mathbf { b }$ is the sum of the products of the corresponding elements of each vector. (Notice that we represent vectors using the boldface notation b). Thus the following is an equivalent formation to Eq. 5.2: \nBut note that nothing in Eq. 5.3 forces $z$ to be a legal probability, that is, to lie between 0 and 1. In fact, since weights are real-valued, the output might even be negative; z ranges from $- infty$ to $infty$ . \nsigmoid \nTo create a probability, we’ll pass $z$ through the sigmoid function, $sigma ( z )$ . The sigmoid function (named because it looks like an $s$ ) is also called the logistic function, and gives logistic regression its name. The sigmoid has the following equation, shown graphically in Fig. 5.1: \n(For the rest of the book, we’ll use the notation $exp ( x )$ to mean $e ^ { x }$ .) The sigmoid has a number of advantages; it takes a real-valued number and maps it into the range \n[0, 1], which is just what we want for a probability. Because it is nearly linear around 0 but flattens toward the ends, it tends to squash outlier values toward 0 or 1. And it’s differentiable, which as we’ll see in Section 5.10 will be handy for learning. \nWe’re almost there. If we apply the sigmoid to the sum of the weighted features, we get a number between 0 and 1. To make it a probability, we just need to make sure that the two cases, $p ( y = 1 )$ and $p ( y = 0 )$ , sum to 1. We can do this as follows: \nThe sigmoid function has the property \nso we could also have expressed $P ( y = 0 )$ as $pmb { sigma } ( - ( mathbf { w } cdot mathbf { x } + b ) )$ . \n5.2 Classification with Logistic Regression \nThe sigmoid function from the prior section thus gives us a way to take an instance $x$ and compute the probability $P ( y = 1 | x )$ . \nHow do we make a decision about which class to apply to a test instance $x ?$ For a given $x$ , we say yes if the probability $P ( y = 1 | x )$ is more than .5, and no otherwise. We call .5 the decision boundary: \ndecision boundary \nLet’s have some examples of applying logistic regression as a classifier for language tasks. \n5.2.1 Sentiment Classification \nSuppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class $+$ or to a review document doc. We’ll represent each input observation by the 6 features $x _ { 1 } ldots x _ { 6 }$ of the input shown in the following table; Fig. 5.2 shows the features in a sample mini",
        "chapter": "Logistic Regression",
        "section": "The sigmoid function",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "[0, 1], which is just what we want for a probability. Because it is nearly linear around 0 but flattens toward the ends, it tends to squash outlier values toward 0 or 1. And it’s differentiable, which as we’ll see in Section 5.10 will be handy for learning. \nWe’re almost there. If we apply the sigmoid to the sum of the weighted features, we get a number between 0 and 1. To make it a probability, we just need to make sure that the two cases, $p ( y = 1 )$ and $p ( y = 0 )$ , sum to 1. We can do this as follows: \nThe sigmoid function has the property \nso we could also have expressed $P ( y = 0 )$ as $pmb { sigma } ( - ( mathbf { w } cdot mathbf { x } + b ) )$ . \n5.2 Classification with Logistic Regression \nThe sigmoid function from the prior section thus gives us a way to take an instance $x$ and compute the probability $P ( y = 1 | x )$ . \nHow do we make a decision about which class to apply to a test instance $x ?$ For a given $x$ , we say yes if the probability $P ( y = 1 | x )$ is more than .5, and no otherwise. We call .5 the decision boundary: \ndecision boundary \nLet’s have some examples of applying logistic regression as a classifier for language tasks. \n5.2.1 Sentiment Classification \nSuppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class $+$ or to a review document doc. We’ll represent each input observation by the 6 features $x _ { 1 } ldots x _ { 6 }$ of the input shown in the following table; Fig. 5.2 shows the features in a sample mini \nLet’s assume for the moment that we’ve already learned a real-valued weight for \nIt's hokey . There are virtually no surprises , and the writing is second-rate .   \nSo why was it so enjoyable  ? For one thing , the cast is   \ngreat . Another nice touch is the music . I was overcome with the urge to get off   \nthe couch and start dancing .  It sucked me in , and it'll do the same to $textcircled { scriptsize { 1 0 9 } }$ 二 i x1=3 x =0 x6=4.19 \neach of these features, and that the 6 weights corresponding to the 6 features are [ $2 . 5 , - 5 . 0 , - 1 . 2 , 0 . 5 , 2 . 0 , 0 . 7 ]$ , while $b = 0 . 1$ . (We’ll discuss in the next section how the weights are learned.) The weight $w _ { 1 }$ , for example indicates how important a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to a positive sentiment decision, while $w _ { 2 }$ tells us the importance of negative lexicon words. Note that $w _ { 1 } = 2 . 5$ is positive, while $w _ { 2 } = - 5 . 0$ , meaning that negative words are negatively associated with a positive sentiment decision, and are about twice as important as positive words. \nGiven these 6 features and the input review $x$ , $P ( + | x )$ and $P ( - | x )$ can be computed using Eq. 5.5: \n5.2.2 Other classification tasks and features \nLogistic regression is commonly applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes EOS (end-of-sentence) and not-EOS. We might use features like $x _ { 1 }$ below expressing that the current word is lower case (perhaps with a positive weight), or that the current word is in our abbreviations dictionary (“Prof.”) (perhaps with a negative weight). A feature can also express a quite complex combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is $S t .$ . and the previous word is capitalized, then the period is likely part of a shortening of the word street.",
        "chapter": "Logistic Regression",
        "section": "Classification with Logistic Regression",
        "subsection": "Sentiment Classification",
        "subsubsection": "N/A"
    },
    {
        "content": "Let’s assume for the moment that we’ve already learned a real-valued weight for \nIt's hokey . There are virtually no surprises , and the writing is second-rate .   \nSo why was it so enjoyable  ? For one thing , the cast is   \ngreat . Another nice touch is the music . I was overcome with the urge to get off   \nthe couch and start dancing .  It sucked me in , and it'll do the same to $textcircled { scriptsize { 1 0 9 } }$ 二 i x1=3 x =0 x6=4.19 \neach of these features, and that the 6 weights corresponding to the 6 features are [ $2 . 5 , - 5 . 0 , - 1 . 2 , 0 . 5 , 2 . 0 , 0 . 7 ]$ , while $b = 0 . 1$ . (We’ll discuss in the next section how the weights are learned.) The weight $w _ { 1 }$ , for example indicates how important a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to a positive sentiment decision, while $w _ { 2 }$ tells us the importance of negative lexicon words. Note that $w _ { 1 } = 2 . 5$ is positive, while $w _ { 2 } = - 5 . 0$ , meaning that negative words are negatively associated with a positive sentiment decision, and are about twice as important as positive words. \nGiven these 6 features and the input review $x$ , $P ( + | x )$ and $P ( - | x )$ can be computed using Eq. 5.5: \n5.2.2 Other classification tasks and features \nLogistic regression is commonly applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes EOS (end-of-sentence) and not-EOS. We might use features like $x _ { 1 }$ below expressing that the current word is lower case (perhaps with a positive weight), or that the current word is in our abbreviations dictionary (“Prof.”) (perhaps with a negative weight). A feature can also express a quite complex combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is $S t .$ . and the previous word is capitalized, then the period is likely part of a shortening of the word street. \n\nfeature templates \nDesigning features: Features are generally designed by examining the training set with an eye to linguistic intuitions and the linguistic literature on the domain. A careful error analysis on the training set or devset of an early version of a system often provides insights into features. \nFor some tasks it is especially helpful to build complex features that are combinations of more primitive features. We saw such a feature for period disambiguation above, where a period on the word $S t$ . was less likely to be the end of the sentence if the previous word was capitalized. For logistic regression and naive Bayes these combination features or feature interactions have to be designed by hand. \nFor many tasks (especially when feature values can reference specific words) we’ll need large numbers of features. Often these are created automatically via feature templates, abstract specifications of features. For example a bigram template for period disambiguation might create a feature for every pair of words that occurs before a period in the training set. Thus the feature space is sparse, since we only have to create a feature if that n-gram exists in that position in the training set. The feature is generally created as a hash from the string descriptions. A user description of a feature as, “bigram(American breakfast)” is hashed into a unique integer $i$ that becomes the feature number $f _ { i }$ . \nstandardize z-score \nIn order to avoid the extensive human effort of feature design, recent research in NLP has focused on representation learning: ways to learn features automatically in an unsupervised way from the input. We’ll introduce methods for representation learning in Chapter 6 and Chapter 7. \nScaling input features: When different input features have extremely different ranges of values, it’s common to rescale them so they have comparable ranges. We standardize input values by centering them to result in a zero mean and a standard deviation of one (this transformation is sometimes called the $mathbf { z }$ -score). That is, if $mu _ { i }$ is the mean of the values of feature $mathbf { x } _ { i }$ across the $m$ observations in the input dataset, and $sigma _ { i }$ is the standard deviation of the values of features $mathbf { x } _ { i }$ across the input dataset, we can replace each feature $mathbf { x } _ { i }$ by a new feature $mathbf { x } _ { i } ^ { prime }$ computed as follows: \nnormalize \nAlternatively, we can normalize the input features values to lie between 0 and 1: \nHaving input data with comparable range is is useful when comparing values across features. Data scaling is especially important in large neural networks, since it helps speed up gradient descent. \n5.2.3 Processing many examples at once \nWe’ve shown the equations for logistic regression for a single example. But in practice we’ll of course want to process an entire test set with many examples. Let’s suppose we have a test set consisting of $m$ test examples each of which we’d like to classify. We’ll continue to use the notation from page 79, in which a superscript value in parentheses refers to the example index in some set of data (either for training or for test). So in this case each test example $x ^ { ( i ) }$ has a feature vector $mathbf { boldsymbol { x } } ^ { ( i ) }$ , $1 < i < m$ . (As usual, we’ll represent vectors and matrices in bold.) \nOne way to compute each output value $hat { y } ^ { ( i ) }$ is just to have a for-loop, and compute each test example one at a time: \nFor the first 3 test examples, then, we would be separately computing the predicted $hat { y } ^ { ( i ) }$ as follows: \nBut it turns out that we can slightly modify our original equation Eq. 5.5 to do this much more efficiently. We’ll use matrix arithmetic to assign a class to all the examples with one matrix operation! \nFirst, we’ll pack all the input feature vectors for each input $x$ into a single input matrix $pmb { times }$ , where each row $i$ is a row vector consisting of the feature vector for input example $x ^ { ( i ) }$ (i.e., the vector $mathbf { x } ^ { ( i ) }$ ). Assuming each example has $f$ features and weights, $pmb { times }$ will therefore be a matrix of shape $[ m times f ]$ , as follows: \nNow if we introduce $mathbf { b }$ as a vector of length $m$ which consists of the scalar bias term $b$ repeated $m$ times, $mathbf { b } = [ b , b , . . . , b ]$ , and $pmb { hat { y } } = [ hat { y } ^ { ( 1 ) } , hat { y } ^ { ( 2 ) } . . . , hat { y } ^ { ( m ) } ]$ as the vector of outputs (one scalar $hat { y } ^ { ( i ) }$ for each input $x ^ { ( i ) }$ and its feature vector $mathbf { x } ^ { ( bar { i } ) }$ ), and represent the weight vector $pmb { w }$ as a column vector, we can compute all the outputs with a single matrix multiplication and one addition: \nYou should convince yourself that Eq. 5.12 computes the same thing as our for-loop in Eq. 5.10. For example $hat { y } ^ { ( 1 ) }$ , the first entry of the output vector $pmb {  y }$ , will correctly be:",
        "chapter": "Logistic Regression",
        "section": "Classification with Logistic Regression",
        "subsection": "Other classification tasks and features",
        "subsubsection": "N/A"
    },
    {
        "content": "Having input data with comparable range is is useful when comparing values across features. Data scaling is especially important in large neural networks, since it helps speed up gradient descent. \n5.2.3 Processing many examples at once \nWe’ve shown the equations for logistic regression for a single example. But in practice we’ll of course want to process an entire test set with many examples. Let’s suppose we have a test set consisting of $m$ test examples each of which we’d like to classify. We’ll continue to use the notation from page 79, in which a superscript value in parentheses refers to the example index in some set of data (either for training or for test). So in this case each test example $x ^ { ( i ) }$ has a feature vector $mathbf { boldsymbol { x } } ^ { ( i ) }$ , $1 < i < m$ . (As usual, we’ll represent vectors and matrices in bold.) \nOne way to compute each output value $hat { y } ^ { ( i ) }$ is just to have a for-loop, and compute each test example one at a time: \nFor the first 3 test examples, then, we would be separately computing the predicted $hat { y } ^ { ( i ) }$ as follows: \nBut it turns out that we can slightly modify our original equation Eq. 5.5 to do this much more efficiently. We’ll use matrix arithmetic to assign a class to all the examples with one matrix operation! \nFirst, we’ll pack all the input feature vectors for each input $x$ into a single input matrix $pmb { times }$ , where each row $i$ is a row vector consisting of the feature vector for input example $x ^ { ( i ) }$ (i.e., the vector $mathbf { x } ^ { ( i ) }$ ). Assuming each example has $f$ features and weights, $pmb { times }$ will therefore be a matrix of shape $[ m times f ]$ , as follows: \nNow if we introduce $mathbf { b }$ as a vector of length $m$ which consists of the scalar bias term $b$ repeated $m$ times, $mathbf { b } = [ b , b , . . . , b ]$ , and $pmb { hat { y } } = [ hat { y } ^ { ( 1 ) } , hat { y } ^ { ( 2 ) } . . . , hat { y } ^ { ( m ) } ]$ as the vector of outputs (one scalar $hat { y } ^ { ( i ) }$ for each input $x ^ { ( i ) }$ and its feature vector $mathbf { x } ^ { ( bar { i } ) }$ ), and represent the weight vector $pmb { w }$ as a column vector, we can compute all the outputs with a single matrix multiplication and one addition: \nYou should convince yourself that Eq. 5.12 computes the same thing as our for-loop in Eq. 5.10. For example $hat { y } ^ { ( 1 ) }$ , the first entry of the output vector $pmb {  y }$ , will correctly be: \nNote that we had to reorder $pmb { x }$ and $pmb { w }$ from the order they appeared in in Eq. 5.5 to make the multiplications come out properly. Here is Eq. 5.12 again with the shapes shown: \nModern compilers and compute hardware can compute this matrix operation very efficiently, making the computation much faster, which becomes important when training or testing on very large datasets. \n5.2.4 Choosing a classifier \nLogistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature $f _ { 1 }$ twice. Naive Bayes will treat both copies of $f _ { 1 }$ as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features $f _ { 1 }$ and $f _ { 2 }$ are perfectly correlated, regression will simply assign part of the weight to $w _ { 1 }$ and part to $w _ { 2 }$ . Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. \nDespite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets $mathrm { N g }$ and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there’s no optimization step). So it’s still a reasonable approach to use in some situations. \n5.3 Multinomial logistic regression \nmultinomial logistic regression \nSometimes we need more than two classes. Perhaps we might want to do 3-way sentiment classification (positive, negative, or neutral). Or we could be assigning some of the labels we will introduce in Chapter 8, like the part of speech of a word (choosing from 10, 30, or even 50 different parts of speech), or the named entity type of a phrase (choosing from tags like person, location, organization). \nIn such cases we use multinomial logistic regression, also called softmax regression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a class $k$ from a set of $K$ classes, under the stipulation that only one of these classes is the correct one (sometimes called hard classification; an observation can not be in multiple classes). Let’s use the following representation: the output $pmb {  y }$ for each input $pmb { x }$ will be a vector of length $K$ . If class $c$ is the correct class, we’ll set $mathbf { y } _ { c } = 1$ , and set all the other elements of $pmb {  y }$ to be 0, i.e., $mathbf { y } _ { c } = 1$ and $mathbf { y } _ { j } = 0 forall j neq c$ . A vector like this $pmb {  y }$ , with one value $_ { = 1 }$ and the rest 0, is called a one-hot vector. The job of the classifier is produce an estimate vector $hat { pmb y }$ . For each class $k$ , the value $hat { mathbf { y } } _ { k }$ will be the classifier’s estimate of the probability $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ .",
        "chapter": "Logistic Regression",
        "section": "Classification with Logistic Regression",
        "subsection": "Processing many examples at once",
        "subsubsection": "N/A"
    },
    {
        "content": "Note that we had to reorder $pmb { x }$ and $pmb { w }$ from the order they appeared in in Eq. 5.5 to make the multiplications come out properly. Here is Eq. 5.12 again with the shapes shown: \nModern compilers and compute hardware can compute this matrix operation very efficiently, making the computation much faster, which becomes important when training or testing on very large datasets. \n5.2.4 Choosing a classifier \nLogistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature $f _ { 1 }$ twice. Naive Bayes will treat both copies of $f _ { 1 }$ as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features $f _ { 1 }$ and $f _ { 2 }$ are perfectly correlated, regression will simply assign part of the weight to $w _ { 1 }$ and part to $w _ { 2 }$ . Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. \nDespite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets $mathrm { N g }$ and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there’s no optimization step). So it’s still a reasonable approach to use in some situations. \n5.3 Multinomial logistic regression \nmultinomial logistic regression \nSometimes we need more than two classes. Perhaps we might want to do 3-way sentiment classification (positive, negative, or neutral). Or we could be assigning some of the labels we will introduce in Chapter 8, like the part of speech of a word (choosing from 10, 30, or even 50 different parts of speech), or the named entity type of a phrase (choosing from tags like person, location, organization). \nIn such cases we use multinomial logistic regression, also called softmax regression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a class $k$ from a set of $K$ classes, under the stipulation that only one of these classes is the correct one (sometimes called hard classification; an observation can not be in multiple classes). Let’s use the following representation: the output $pmb {  y }$ for each input $pmb { x }$ will be a vector of length $K$ . If class $c$ is the correct class, we’ll set $mathbf { y } _ { c } = 1$ , and set all the other elements of $pmb {  y }$ to be 0, i.e., $mathbf { y } _ { c } = 1$ and $mathbf { y } _ { j } = 0 forall j neq c$ . A vector like this $pmb {  y }$ , with one value $_ { = 1 }$ and the rest 0, is called a one-hot vector. The job of the classifier is produce an estimate vector $hat { pmb y }$ . For each class $k$ , the value $hat { mathbf { y } } _ { k }$ will be the classifier’s estimate of the probability $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ .",
        "chapter": "Logistic Regression",
        "section": "Classification with Logistic Regression",
        "subsection": "Choosing a classifier",
        "subsubsection": "N/A"
    },
    {
        "content": "Note that we had to reorder $pmb { x }$ and $pmb { w }$ from the order they appeared in in Eq. 5.5 to make the multiplications come out properly. Here is Eq. 5.12 again with the shapes shown: \nModern compilers and compute hardware can compute this matrix operation very efficiently, making the computation much faster, which becomes important when training or testing on very large datasets. \n5.2.4 Choosing a classifier \nLogistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature $f _ { 1 }$ twice. Naive Bayes will treat both copies of $f _ { 1 }$ as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features $f _ { 1 }$ and $f _ { 2 }$ are perfectly correlated, regression will simply assign part of the weight to $w _ { 1 }$ and part to $w _ { 2 }$ . Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. \nDespite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets $mathrm { N g }$ and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there’s no optimization step). So it’s still a reasonable approach to use in some situations. \n5.3 Multinomial logistic regression \nmultinomial logistic regression \nSometimes we need more than two classes. Perhaps we might want to do 3-way sentiment classification (positive, negative, or neutral). Or we could be assigning some of the labels we will introduce in Chapter 8, like the part of speech of a word (choosing from 10, 30, or even 50 different parts of speech), or the named entity type of a phrase (choosing from tags like person, location, organization). \nIn such cases we use multinomial logistic regression, also called softmax regression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a class $k$ from a set of $K$ classes, under the stipulation that only one of these classes is the correct one (sometimes called hard classification; an observation can not be in multiple classes). Let’s use the following representation: the output $pmb {  y }$ for each input $pmb { x }$ will be a vector of length $K$ . If class $c$ is the correct class, we’ll set $mathbf { y } _ { c } = 1$ , and set all the other elements of $pmb {  y }$ to be 0, i.e., $mathbf { y } _ { c } = 1$ and $mathbf { y } _ { j } = 0 forall j neq c$ . A vector like this $pmb {  y }$ , with one value $_ { = 1 }$ and the rest 0, is called a one-hot vector. The job of the classifier is produce an estimate vector $hat { pmb y }$ . For each class $k$ , the value $hat { mathbf { y } } _ { k }$ will be the classifier’s estimate of the probability $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ . \n5.3.1 Softmax \nsoftmax \nThe multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ . The softmax function takes a vector ${ pmb z } = [ { pmb z } _ { 1 } , { pmb z } _ { 2 } , . . . , { pmb z } _ { K } ]$ of $K$ arbitrary values and maps them to a probability distribution, with each value in the range (0,1), and all the values summing to 1. Like the sigmoid, it is an exponential function. \nFor a vector $pmb { z }$ of dimensionality $K$ , the softmax is defined as: \nThe softmax of an input vector ${ pmb z } = [ { pmb z } _ { 1 } , { pmb z } _ { 2 } , . . . , { pmb z } _ { K } ]$ is thus a vector itself: \nThe denominator $begin{array} { r } { sum _ { i = 1 } ^ { K } exp left( pmb { z } _ { i } right) } end{array}$ is used to normalize all the values into probabilities. Thus for example given a vector: \nthe resulting (rounded) softmax $( pmb { z } )$ is \nLike the sigmoid, the softmax has the property of squashing values toward 0 or 1. Thus if one of the inputs is larger than the others, it will tend to push its probability toward 1, and suppress the probabilities of the smaller inputs. \n5.3.2 Applying softmax in logistic regression \nWhen we apply softmax for logistic regression, the input will (just as for the sigmoid) be the dot product between a weight vector $pmb { w }$ and an input vector $pmb { x }$ (plus a bias). But now we’ll need separate weight vectors $pmb { w } _ { k }$ and bias $b _ { k }$ for each of the $K$ classes. The probability of each of our output classes $hat { y } _ { k }$ can thus be computed as: \nThe form of Eq. 5.17 makes it seem that we would compute each output separately. Instead, it’s more common to set up the equation for more efficient computation by modern vector processing hardware. We’ll do this by representing the set of $K$ weight vectors as a weight matrix $W$ and a bias vector b. Each row $k$ of $boldsymbol { mathsf { w } }$ corresponds to the vector of weights $w _ { k }$ . $boldsymbol { mathsf { w } }$ thus has shape $[ K times f ]$ , for $K$ the number of output classes and $f$ the number of input features. The bias vector $mathbf { b }$ has one value for each of the $K$ output classes. If we represent the weights in this way, we can compute $hat { pmb y }$ , the vector of output probabilities for each of the $K$ classes, by a single elegant equation:",
        "chapter": "Logistic Regression",
        "section": "Multinomial logistic regression",
        "subsection": "Softmax",
        "subsubsection": "N/A"
    },
    {
        "content": "5.3.1 Softmax \nsoftmax \nThe multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ . The softmax function takes a vector ${ pmb z } = [ { pmb z } _ { 1 } , { pmb z } _ { 2 } , . . . , { pmb z } _ { K } ]$ of $K$ arbitrary values and maps them to a probability distribution, with each value in the range (0,1), and all the values summing to 1. Like the sigmoid, it is an exponential function. \nFor a vector $pmb { z }$ of dimensionality $K$ , the softmax is defined as: \nThe softmax of an input vector ${ pmb z } = [ { pmb z } _ { 1 } , { pmb z } _ { 2 } , . . . , { pmb z } _ { K } ]$ is thus a vector itself: \nThe denominator $begin{array} { r } { sum _ { i = 1 } ^ { K } exp left( pmb { z } _ { i } right) } end{array}$ is used to normalize all the values into probabilities. Thus for example given a vector: \nthe resulting (rounded) softmax $( pmb { z } )$ is \nLike the sigmoid, the softmax has the property of squashing values toward 0 or 1. Thus if one of the inputs is larger than the others, it will tend to push its probability toward 1, and suppress the probabilities of the smaller inputs. \n5.3.2 Applying softmax in logistic regression \nWhen we apply softmax for logistic regression, the input will (just as for the sigmoid) be the dot product between a weight vector $pmb { w }$ and an input vector $pmb { x }$ (plus a bias). But now we’ll need separate weight vectors $pmb { w } _ { k }$ and bias $b _ { k }$ for each of the $K$ classes. The probability of each of our output classes $hat { y } _ { k }$ can thus be computed as: \nThe form of Eq. 5.17 makes it seem that we would compute each output separately. Instead, it’s more common to set up the equation for more efficient computation by modern vector processing hardware. We’ll do this by representing the set of $K$ weight vectors as a weight matrix $W$ and a bias vector b. Each row $k$ of $boldsymbol { mathsf { w } }$ corresponds to the vector of weights $w _ { k }$ . $boldsymbol { mathsf { w } }$ thus has shape $[ K times f ]$ , for $K$ the number of output classes and $f$ the number of input features. The bias vector $mathbf { b }$ has one value for each of the $K$ output classes. If we represent the weights in this way, we can compute $hat { pmb y }$ , the vector of output probabilities for each of the $K$ classes, by a single elegant equation: \nIf you work out the matrix arithmetic, you can see that the estimated score of the first output class $hat { y } _ { 1 }$ (before we take the softmax) will correctly turn out to be $mathbf { w } _ { 1 } cdot mathbf { x } + b _ { 1 }$ . \n5.3.3 Features in Multinomial Logistic Regression \nFeatures in multinomial logistic regression function similarly to binary logistic regression, with the difference mentioned above that we’ll need separate weight vectors and biases for each of the $K$ classes. Recall our binary exclamation point feature",
        "chapter": "Logistic Regression",
        "section": "Multinomial logistic regression",
        "subsection": "Applying softmax in logistic regression",
        "subsubsection": "N/A"
    },
    {
        "content": "If you work out the matrix arithmetic, you can see that the estimated score of the first output class $hat { y } _ { 1 }$ (before we take the softmax) will correctly turn out to be $mathbf { w } _ { 1 } cdot mathbf { x } + b _ { 1 }$ . \n5.3.3 Features in Multinomial Logistic Regression \nFeatures in multinomial logistic regression function similarly to binary logistic regression, with the difference mentioned above that we’ll need separate weight vectors and biases for each of the $K$ classes. Recall our binary exclamation point feature \n$x _ { 5 }$ from page 81: \nIn binary classification a positive weight ${ pmb w } _ { 5 }$ on a feature influences the classifier toward $y = 1$ (positive sentiment) and a negative weight influences it toward $y = 0$ (negative sentiment) with the absolute value indicating how important the feature is. For multinomial logistic regression, by contrast, with separate weights for each class, a feature can be evidence for or against each individual class. \nIn 3-way multiclass sentiment classification, for example, we must assign each document one of the 3 classes $^ +$ , , or 0 (neutral). Now a feature related to exclamation marks might have a negative weight for 0 documents, and a positive weight for $^ +$ or documents: \nBecause these feature weights are dependent both on the input text and the output class, we sometimes make this dependence explicit and represent the features themselves as $f ( x , y )$ : a function of both the input and the class. Using such a notation $f _ { 5 } ( x )$ above could be represented as three features $f _ { 5 } ( x , + )$ , $f _ { 5 } ( x , - )$ , and $f _ { 5 } ( x , 0 )$ , each of which has a single weight. We’ll use this kind of notation in our description of the CRF in Chapter 8. \n5.4 Learning in Logistic Regression \nloss \nHow are the parameters of the model, the weights $pmb { w }$ and bias $b$ , learned? Logistic regression is an instance of supervised classification in which we know the correct label $y$ (either 0 or 1) for each observation $x$ . What the system produces via Eq. 5.5 is $hat { y }$ , the system’s estimate of the true $y$ . We want to learn parameters (meaning $pmb { w }$ and $b$ ) that make $hat { y }$ for each training observation as close as possible to the true $y$ . \nThis requires two components that we foreshadowed in the introduction to the chapter. The first is a metric for how close the current label $( hat { y } )$ is to the true gold label $y$ . Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. \nThe second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section. \nWe’ll describe these algorithms for the simpler case of binary logistic regression in the next two sections, and then turn to multinomial logistic regression in Section 5.8.",
        "chapter": "Logistic Regression",
        "section": "Multinomial logistic regression",
        "subsection": "Features in Multinomial Logistic Regression",
        "subsubsection": "N/A"
    },
    {
        "content": "$x _ { 5 }$ from page 81: \nIn binary classification a positive weight ${ pmb w } _ { 5 }$ on a feature influences the classifier toward $y = 1$ (positive sentiment) and a negative weight influences it toward $y = 0$ (negative sentiment) with the absolute value indicating how important the feature is. For multinomial logistic regression, by contrast, with separate weights for each class, a feature can be evidence for or against each individual class. \nIn 3-way multiclass sentiment classification, for example, we must assign each document one of the 3 classes $^ +$ , , or 0 (neutral). Now a feature related to exclamation marks might have a negative weight for 0 documents, and a positive weight for $^ +$ or documents: \nBecause these feature weights are dependent both on the input text and the output class, we sometimes make this dependence explicit and represent the features themselves as $f ( x , y )$ : a function of both the input and the class. Using such a notation $f _ { 5 } ( x )$ above could be represented as three features $f _ { 5 } ( x , + )$ , $f _ { 5 } ( x , - )$ , and $f _ { 5 } ( x , 0 )$ , each of which has a single weight. We’ll use this kind of notation in our description of the CRF in Chapter 8. \n5.4 Learning in Logistic Regression \nloss \nHow are the parameters of the model, the weights $pmb { w }$ and bias $b$ , learned? Logistic regression is an instance of supervised classification in which we know the correct label $y$ (either 0 or 1) for each observation $x$ . What the system produces via Eq. 5.5 is $hat { y }$ , the system’s estimate of the true $y$ . We want to learn parameters (meaning $pmb { w }$ and $b$ ) that make $hat { y }$ for each training observation as close as possible to the true $y$ . \nThis requires two components that we foreshadowed in the introduction to the chapter. The first is a metric for how close the current label $( hat { y } )$ is to the true gold label $y$ . Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. \nThe second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section. \nWe’ll describe these algorithms for the simpler case of binary logistic regression in the next two sections, and then turn to multinomial logistic regression in Section 5.8.",
        "chapter": "Logistic Regression",
        "section": "Learning in Logistic Regression",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "$x _ { 5 }$ from page 81: \nIn binary classification a positive weight ${ pmb w } _ { 5 }$ on a feature influences the classifier toward $y = 1$ (positive sentiment) and a negative weight influences it toward $y = 0$ (negative sentiment) with the absolute value indicating how important the feature is. For multinomial logistic regression, by contrast, with separate weights for each class, a feature can be evidence for or against each individual class. \nIn 3-way multiclass sentiment classification, for example, we must assign each document one of the 3 classes $^ +$ , , or 0 (neutral). Now a feature related to exclamation marks might have a negative weight for 0 documents, and a positive weight for $^ +$ or documents: \nBecause these feature weights are dependent both on the input text and the output class, we sometimes make this dependence explicit and represent the features themselves as $f ( x , y )$ : a function of both the input and the class. Using such a notation $f _ { 5 } ( x )$ above could be represented as three features $f _ { 5 } ( x , + )$ , $f _ { 5 } ( x , - )$ , and $f _ { 5 } ( x , 0 )$ , each of which has a single weight. We’ll use this kind of notation in our description of the CRF in Chapter 8. \n5.4 Learning in Logistic Regression \nloss \nHow are the parameters of the model, the weights $pmb { w }$ and bias $b$ , learned? Logistic regression is an instance of supervised classification in which we know the correct label $y$ (either 0 or 1) for each observation $x$ . What the system produces via Eq. 5.5 is $hat { y }$ , the system’s estimate of the true $y$ . We want to learn parameters (meaning $pmb { w }$ and $b$ ) that make $hat { y }$ for each training observation as close as possible to the true $y$ . \nThis requires two components that we foreshadowed in the introduction to the chapter. The first is a metric for how close the current label $( hat { y } )$ is to the true gold label $y$ . Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. \nThe second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section. \nWe’ll describe these algorithms for the simpler case of binary logistic regression in the next two sections, and then turn to multinomial logistic regression in Section 5.8. \n5.5 The cross-entropy loss function \nWe need a loss function that expresses, for an observation $x$ , how close the classifier output $( widehat { boldsymbol { y } } = sigma ( boldsymbol { mathsf { w } } cdot boldsymbol { mathsf { x } } + b ) )$ is to the correct output ( $mathrm { Delta } _ { mathrm { { y } } }$ , which is 0 or 1). We’ll call this: \nWe do this via a loss function that prefers the correct class labels of the training examples to be more likely. This is called conditional maximum likelihood estimation: we choose the parameters $w , b$ that maximize the log probability of the true $y$ labels in the training data given the observations $x$ . The resulting loss function is the negative log likelihood loss, generally called the cross-entropy loss. \nLet’s derive this loss function, applied to a single observation $x$ . We’d like to learn weights that maximize the probability of the correct label $p ( y | x )$ . Since there are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can express the probability $p ( y | x )$ that our classifier produces for one observation as the following (keeping in mind that if $scriptstyle mathbf { y } = 1$ , Eq. 5.20 simplifies to $hat { y }$ ; if $scriptstyle mathbf { y = } 0$ , Eq. 5.20 simplifies to $1 - { hat { y } } )$ : \nNow we take the log of both sides. This will turn out to be handy mathematically, and doesn’t hurt us; whatever values maximize a probability will also maximize the log of the probability: \nEq. 5.21 describes a log likelihood that should be maximized. In order to turn this into loss function (something that we need to minimize), we’ll just flip the sign on Eq. 5.21. The result is the cross-entropy loss $L _ { mathrm { C E } }$ : \nFinally, we can plug in the definition of $hat { y } = sigma ( { boldsymbol { mathsf { w } } } cdot { boldsymbol { mathsf { x } } } + b )$ : \nLet’s see if this loss function does the right thing for our example from Fig. 5.2. We want the loss to be smaller if the model’s estimate is close to correct, and bigger if the model is confused. So first let’s suppose the correct gold label for the sentiment example in Fig. 5.2 is positive, i.e., $y = 1$ . In this case our model is doing well, since from Eq. 5.7 it indeed gave the example a higher probability of being positive (.70) than negative (.30). If we plug $sigma ( { boldsymbol { mathsf { w } } } cdot { boldsymbol { mathsf { x } } } + b ) = . 7 0$ and $y = 1$ into Eq. 5.23, the right side of the equation drops out, leading to the following loss (we’ll use log to mean natural log when the base is not specified): \nBy contrast, let’s pretend instead that the example in Fig. 5.2 was actually negative, i.e., $y = 0$ (perhaps the reviewer went on to say “But bottom line, the movie is terrible! I beg you not to see it!”). In this case our model is confused and we’d want the loss to be higher. Now if we plug $y = 0$ and $1 - sigma ( { boldsymbol { mathsf { w } } } cdot { boldsymbol { mathsf { x } } } + b ) = . 3 1$ from Eq. 5.7 into Eq. 5.23, the left side of the equation drops out: \nSure enough, the loss for the first classifier (.36) is less than the loss for the second classifier (1.2). \nWhy does minimizing this negative log probability do what we want? A perfect classifier would assign probability 1 to the correct outcome $mathrm { check { y } } { = } 1$ or $scriptstyle mathbf { y } = 0$ ) and probability 0 to the incorrect outcome. That means if y equals 1, the higher $hat { y }$ is (the closer it is to 1), the better the classifier; the lower $hat { y }$ is (the closer it is to 0), the worse the classifier. If y equals 0, instead, the higher $1 - { hat { y } }$ is (closer to 1), the better the classifier. The negative log of $hat { y }$ (if the true $y$ equals 1) or $1 - hat { y }$ (if the true $y$ equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss) to infinity (negative log of 0, infinite loss). This loss function also ensures that as the probability of the correct answer is maximized, the probability of the incorrect answer is minimized; since the two sum to one, any increase in the probability of the correct answer is coming at the expense of the incorrect answer. It’s called the cross-entropy loss, because Eq. 5.21 is also the formula for the cross-entropy between the true probability distribution $y$ and our estimated distribution $hat { y }$ . \nNow we know what we want to minimize; in the next section, we’ll see how to find the minimum. \n5.6 Gradient Descent \nOur goal with gradient descent is to find the optimal weights: minimize the loss function we’ve defined for the model. In Eq. 5.24 below, we’ll explicitly represent the fact that the loss function $L$ is parameterized by the weights, which we’ll refer to in machine learning in general as $theta$ (in the case of logistic regression $theta = w , b$ ). So the goal is to find the set of weights which minimizes the loss function, averaged over all examples: \nHow shall we find the minimum of this (or any) loss function? Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters $theta$ ) the function’s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself 360 degrees, find the direction where the ground is sloping the steepest, and walk downhill in that direction. \nFor logistic regression, this loss function is conveniently convex. A convex function has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)",
        "chapter": "Logistic Regression",
        "section": "The cross-entropy loss function",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "In an actual logistic regression, the parameter vector $w$ is much longer than 1 or 2, since the input feature vector $x$ can be quite long, and we need a weight $w _ { i }$ for each $x _ { i }$ . For each dimension/variable $w _ { i }$ in $w$ (plus the bias $b$ ), the gradient will have a component that tells us the slope with respect to that variable. In each dimension $w _ { i }$ , we express the slope as a partial derivative ∂∂w of the loss function. Essentially we’re asking: “How much would a small change in that variable $w _ { i }$ influence the total loss function $L$ ?” \nFormally, then, the gradient of a multi-variable function $f$ is a vector in which each component expresses the partial derivative of $f$ with respect to one of the variables. We’ll use the inverted Greek delta symbol $nabla$ to refer to the gradient, and represent $hat { y }$ as $f ( x ; theta )$ to make the dependence on $theta$ more obvious: \nThe final equation for updating $theta$ based on the gradient is thus \n5.6.1 The Gradient for Logistic Regression \nIn order to update $theta$ , we need a definition for the gradient $nabla L ( f ( x ; theta ) , y )$ . Recall that for logistic regression, the cross-entropy loss function is: \nIt turns out that the derivative of this function for one observation vector $x$ is Eq. 5.29 (the interested reader can see Section 5.10 for the derivation of this equation): \nYou’ll also sometimes see this equation in the equivalent form: \nNote in these equations that the gradient with respect to a single weight $boldsymbol { mathsf { w } } _ { j }$ represents a very intuitive value: the difference between the true $y$ and our estimated $hat { y } = sigma ( { boldsymbol w } cdot { boldsymbol x } + b )$ for that observation, multiplied by the corresponding input value $x _ { j }$ . \n5.6.2 The Stochastic Gradient Descent Algorithm \nStochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging $theta$ in the right direction (the opposite direction of the gradient). (An “online algorithm” is one that processes its input example by example, rather than waiting until it sees the entire input.) Fig. 5.6 shows the algorithm. \nFigure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used mainly to report how well we are doing on the current tuple; we don’t need to compute the loss in order to compute the gradient. The algorithm can terminate when it converges (or when the gradient norm $< epsilon dot { }$ ), or when progress halts (for example when the loss starts going up on a held-out set). \nThe learning rate $eta$ is a hyperparameter that must be adjusted. If it’s too high, the learner will take steps that are too large, overshooting the minimum of the loss function. If it’s too low, the learner will take steps that are too small, and take too long to get to the minimum. It is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration $k$ of training; the notation $eta _ { k }$ can be used to mean the value of the learning rate at iteration $k$ .",
        "chapter": "Logistic Regression",
        "section": "Gradient Descent",
        "subsection": "The Gradient for Logistic Regression",
        "subsubsection": "N/A"
    },
    {
        "content": "It turns out that the derivative of this function for one observation vector $x$ is Eq. 5.29 (the interested reader can see Section 5.10 for the derivation of this equation): \nYou’ll also sometimes see this equation in the equivalent form: \nNote in these equations that the gradient with respect to a single weight $boldsymbol { mathsf { w } } _ { j }$ represents a very intuitive value: the difference between the true $y$ and our estimated $hat { y } = sigma ( { boldsymbol w } cdot { boldsymbol x } + b )$ for that observation, multiplied by the corresponding input value $x _ { j }$ . \n5.6.2 The Stochastic Gradient Descent Algorithm \nStochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging $theta$ in the right direction (the opposite direction of the gradient). (An “online algorithm” is one that processes its input example by example, rather than waiting until it sees the entire input.) Fig. 5.6 shows the algorithm. \nFigure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used mainly to report how well we are doing on the current tuple; we don’t need to compute the loss in order to compute the gradient. The algorithm can terminate when it converges (or when the gradient norm $< epsilon dot { }$ ), or when progress halts (for example when the loss starts going up on a held-out set). \nThe learning rate $eta$ is a hyperparameter that must be adjusted. If it’s too high, the learner will take steps that are too large, overshooting the minimum of the loss function. If it’s too low, the learner will take steps that are too small, and take too long to get to the minimum. It is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration $k$ of training; the notation $eta _ { k }$ can be used to mean the value of the learning rate at iteration $k$ . \nWe’ll discuss hyperparameters in more detail in Chapter 7, but briefly they are a special kind of parameter for any machine learning model. Unlike regular parameters of a model (weights like $w$ and $b$ ), which are learned by the algorithm from the training set, hyperparameters are special parameters chosen by the algorithm designer that affect how the algorithm works. \n5.6.3 Working through an example \nLet’s walk though a single step of the gradient descent algorithm. We’ll use a simplified version of the example in Fig. 5.2 as it sees a single observation $x$ , whose correct value is $y = 1$ (this is a positive review), and with a feature vector $pmb { x } = [ pmb { x } _ { 1 } , pmb { x } _ { 2 } ]$ consisting of these two features: \nLet’s assume the initial weights and bias in $theta ^ { 0 }$ are all set to 0, and the initial learning rate $eta$ is 0.1: \nThe single update step requires that we compute the gradient, multiplied by the learning rate \nIn our mini example there are three parameters, so the gradient vector has 3 dimensions, for $mathbf { w } _ { 1 } , mathbf { w } _ { 2 }$ , and $b$ . We can compute the first gradient as follows: \nNow that we have a gradient, we compute the new parameter vector $theta ^ { 1 }$ by moving $theta ^ { 0 }$ in the opposite direction from the gradient: \nSo after one step of gradient descent, the weights have shifted to be: $pmb { w } _ { 1 } = . 1 5$ , $pmb { w } _ { 2 } = . 1$ , and $b = . 0 5$ . \nNote that this observation $x$ happened to be a positive example. We would expect that after seeing more negative examples with high counts of negative words, that the weight $mathbf { w } _ { 2 }$ would shift to have a negative value. \n5.6.4 Mini-batch training \nStochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. That can result in very choppy movements, so it’s common to compute the gradient over batches of training instances rather than a single instance.",
        "chapter": "Logistic Regression",
        "section": "Gradient Descent",
        "subsection": "The Stochastic Gradient Descent Algorithm",
        "subsubsection": "N/A"
    },
    {
        "content": "We’ll discuss hyperparameters in more detail in Chapter 7, but briefly they are a special kind of parameter for any machine learning model. Unlike regular parameters of a model (weights like $w$ and $b$ ), which are learned by the algorithm from the training set, hyperparameters are special parameters chosen by the algorithm designer that affect how the algorithm works. \n5.6.3 Working through an example \nLet’s walk though a single step of the gradient descent algorithm. We’ll use a simplified version of the example in Fig. 5.2 as it sees a single observation $x$ , whose correct value is $y = 1$ (this is a positive review), and with a feature vector $pmb { x } = [ pmb { x } _ { 1 } , pmb { x } _ { 2 } ]$ consisting of these two features: \nLet’s assume the initial weights and bias in $theta ^ { 0 }$ are all set to 0, and the initial learning rate $eta$ is 0.1: \nThe single update step requires that we compute the gradient, multiplied by the learning rate \nIn our mini example there are three parameters, so the gradient vector has 3 dimensions, for $mathbf { w } _ { 1 } , mathbf { w } _ { 2 }$ , and $b$ . We can compute the first gradient as follows: \nNow that we have a gradient, we compute the new parameter vector $theta ^ { 1 }$ by moving $theta ^ { 0 }$ in the opposite direction from the gradient: \nSo after one step of gradient descent, the weights have shifted to be: $pmb { w } _ { 1 } = . 1 5$ , $pmb { w } _ { 2 } = . 1$ , and $b = . 0 5$ . \nNote that this observation $x$ happened to be a positive example. We would expect that after seeing more negative examples with high counts of negative words, that the weight $mathbf { w } _ { 2 }$ would shift to have a negative value. \n5.6.4 Mini-batch training \nStochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. That can result in very choppy movements, so it’s common to compute the gradient over batches of training instances rather than a single instance.",
        "chapter": "Logistic Regression",
        "section": "Gradient Descent",
        "subsection": "Working through an example",
        "subsubsection": "N/A"
    },
    {
        "content": "We’ll discuss hyperparameters in more detail in Chapter 7, but briefly they are a special kind of parameter for any machine learning model. Unlike regular parameters of a model (weights like $w$ and $b$ ), which are learned by the algorithm from the training set, hyperparameters are special parameters chosen by the algorithm designer that affect how the algorithm works. \n5.6.3 Working through an example \nLet’s walk though a single step of the gradient descent algorithm. We’ll use a simplified version of the example in Fig. 5.2 as it sees a single observation $x$ , whose correct value is $y = 1$ (this is a positive review), and with a feature vector $pmb { x } = [ pmb { x } _ { 1 } , pmb { x } _ { 2 } ]$ consisting of these two features: \nLet’s assume the initial weights and bias in $theta ^ { 0 }$ are all set to 0, and the initial learning rate $eta$ is 0.1: \nThe single update step requires that we compute the gradient, multiplied by the learning rate \nIn our mini example there are three parameters, so the gradient vector has 3 dimensions, for $mathbf { w } _ { 1 } , mathbf { w } _ { 2 }$ , and $b$ . We can compute the first gradient as follows: \nNow that we have a gradient, we compute the new parameter vector $theta ^ { 1 }$ by moving $theta ^ { 0 }$ in the opposite direction from the gradient: \nSo after one step of gradient descent, the weights have shifted to be: $pmb { w } _ { 1 } = . 1 5$ , $pmb { w } _ { 2 } = . 1$ , and $b = . 0 5$ . \nNote that this observation $x$ happened to be a positive example. We would expect that after seeing more negative examples with high counts of negative words, that the weight $mathbf { w } _ { 2 }$ would shift to have a negative value. \n5.6.4 Mini-batch training \nStochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. That can result in very choppy movements, so it’s common to compute the gradient over batches of training instances rather than a single instance. \nbatch training \nFor example in batch training we compute the gradient over the entire dataset. By seeing so many examples, batch training offers a superb estimate of which direction to move the weights, at the cost of spending a lot of time processing every single example in the training set to compute this perfect direction. \nA compromise is mini-batch training: we train on a group of $m$ examples (perhaps 512, or 1024) that is less than the whole dataset. (If $m$ is the size of the dataset, then we are doing batch gradient descent; if $m = 1$ , we are back to doing stochastic gradient descent). Mini-batch training also has the advantage of computational efficiency. The mini-batches can easily be vectorized, choosing the size of the minibatch based on the computational resources. This allows us to process all the examples in one mini-batch in parallel and then accumulate the loss, something that’s not possible with individual or batch training. \nWe just need to define mini-batch versions of the cross-entropy loss function we defined in Section 5.5 and the gradient in Section 5.6.1. Let’s extend the crossentropy loss for one example from Eq. 5.22 to mini-batches of size $m$ . We’ll continue to use the notation that $x ^ { ( i ) }$ and $y ^ { ( i ) }$ mean the ith training features and training label, respectively. We make the assumption that the training examples are independent: \nNow the cost function for the mini-batch of $m$ examples is the average loss for each example: \nThe mini-batch gradient is the average of the individual gradients from Eq. 5.29: \nInstead of using the sum notation, we can more efficiently compute the gradient in its matrix form, following the vectorization we saw on page 84, where we have a matrix $pmb { x }$ of size $[ m times f ]$ representing the $m$ inputs in the batch, and a vector $pmb {  y }$ of size $[ m times 1 ]$ representing the correct outputs: \n5.7 Regularization \nNumquam ponenda est pluralitas sine necessitate ‘Plurality should never be proposed unless needed’ William of Occam \noverfitting generalize regularization \nThere is a problem with learning weights that make the model perfectly match the training data. If a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight. The weights for features will attempt to perfectly fit details of the training set, in fact too perfectly, modeling noisy factors that just accidentally correlate with the class. This problem is called overfitting. A good model should be able to generalize well from the training data to the unseen test set, but a model that overfits will have poor generalization. \nTo avoid overfitting, a new regularization term $R ( theta )$ is added to the objective function in Eq. 5.24, resulting in the following objective for a batch of $m$ examples (slightly rewritten from Eq. 5.24 to be maximizing log probability rather than minimizing loss, and removing the $frac { 1 } { m }$ term which doesn’t affect the argmax): \nL2 regularization \nThe new regularization term $R ( theta )$ is used to penalize large weights. Thus a setting of the weights that matches the training data perfectly— but uses many weights with high values to do so—will be penalized more than a setting that matches the data a little less well, but does so using smaller weights. There are two common ways to compute this regularization term $R ( theta )$ . L2 regularization is a quadratic function of the weight values, named because it uses the (square of the) L2 norm of the weight values. The L2 norm, $| | theta | | _ { 2 }$ , is the same as the Euclidean distance of the vector $theta$ from the origin. If $theta$ consists of $n$ weights, then: \nL1 regularization \nThe L2 regularized objective function becomes: \nL1 regularization is a linear function of the weight values, named after the L1 norm $| | W | | _ { 1 }$ , the sum of the absolute values of the weights, or Manhattan distance (the Manhattan distance is the distance you’d have to walk between two points in a city with a street grid like New York): \nThe L1 regularized objective function becomes:",
        "chapter": "Logistic Regression",
        "section": "Gradient Descent",
        "subsection": "Mini-batch training",
        "subsubsection": "N/A"
    },
    {
        "content": "5.7 Regularization \nNumquam ponenda est pluralitas sine necessitate ‘Plurality should never be proposed unless needed’ William of Occam \noverfitting generalize regularization \nThere is a problem with learning weights that make the model perfectly match the training data. If a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight. The weights for features will attempt to perfectly fit details of the training set, in fact too perfectly, modeling noisy factors that just accidentally correlate with the class. This problem is called overfitting. A good model should be able to generalize well from the training data to the unseen test set, but a model that overfits will have poor generalization. \nTo avoid overfitting, a new regularization term $R ( theta )$ is added to the objective function in Eq. 5.24, resulting in the following objective for a batch of $m$ examples (slightly rewritten from Eq. 5.24 to be maximizing log probability rather than minimizing loss, and removing the $frac { 1 } { m }$ term which doesn’t affect the argmax): \nL2 regularization \nThe new regularization term $R ( theta )$ is used to penalize large weights. Thus a setting of the weights that matches the training data perfectly— but uses many weights with high values to do so—will be penalized more than a setting that matches the data a little less well, but does so using smaller weights. There are two common ways to compute this regularization term $R ( theta )$ . L2 regularization is a quadratic function of the weight values, named because it uses the (square of the) L2 norm of the weight values. The L2 norm, $| | theta | | _ { 2 }$ , is the same as the Euclidean distance of the vector $theta$ from the origin. If $theta$ consists of $n$ weights, then: \nL1 regularization \nThe L2 regularized objective function becomes: \nL1 regularization is a linear function of the weight values, named after the L1 norm $| | W | | _ { 1 }$ , the sum of the absolute values of the weights, or Manhattan distance (the Manhattan distance is the distance you’d have to walk between two points in a city with a street grid like New York): \nThe L1 regularized objective function becomes: \nlasso ridge \nThese kinds of regularization come from statistics, where L1 regularization is called lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression, and both are commonly used in language processing. L2 regularization is easier to optimize because of its simple derivative (the derivative of $theta ^ { 2 }$ is just $2 theta$ ), while L1 regularization is more complex (the derivative of $| theta |$ is non-continuous at zero). But where L2 prefers weight vectors with many small weights, L1 prefers sparse solutions with some larger weights but many more weights set to zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features. \nBoth L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplace prior on the weights. L2 regularization corresponds to assuming that weights are distributed according to a Gaussian distribution with mean $mu = 0$ . In a Gaussian or normal distribution, the further away a value is from the mean, the lower its probability (scaled by the variance $sigma$ ). By using a Gaussian prior on the weights, we are saying that weights prefer to have the value 0. A Gaussian for a weight $theta _ { j }$ is \nIf we multiply each weight by a Gaussian prior on the weight, we are thus maximizing the following constraint: \nwhich in log space, with $mu = 0$ , and assuming $2 sigma ^ { 2 } = 1$ , corresponds to \nwhich is in the same form as Eq. 5.37. \n5.8 Learning in Multinomial Logistic Regression \nThe loss function for multinomial logistic regression generalizes the loss function for binary logistic regression from 2 to $K$ classes. Recall that that the cross-entropy loss for binary logistic regression (repeated from Eq. 5.22) is: \nThe loss function for multinomial logistic regression generalizes the two terms in Eq. 5.43 (one that is non-zero when $y = 1$ and one that is non-zero when $y = 0$ ) to $K$ terms. As we mentioned above, for multinomial regression we’ll represent both $pmb {  y }$ and $hat { pmb y }$ as vectors. The true label $pmb {  y }$ is a vector with $K$ elements, each corresponding to a class, with $y _ { c } = 1$ if the correct class is $c$ , with all other elements of $pmb {  y }$ being 0. And our classifier will produce an estimate vector with $K$ elements $hat { pmb y }$ , each element $hat { mathbf { y } } _ { k }$ of which represents the estimated probability $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ . \nThe loss function for a single example $boldsymbol { mathsf { x } }$ , generalizing from binary logistic regression, is the sum of the logs of the $K$ output classes, each weighted by their probability $pmb { y } _ { k }$ (Eq. 5.44). This turns out to be just the negative log probability of the correct class $c$ (Eq. 5.45):",
        "chapter": "Logistic Regression",
        "section": "Regularization",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "lasso ridge \nThese kinds of regularization come from statistics, where L1 regularization is called lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression, and both are commonly used in language processing. L2 regularization is easier to optimize because of its simple derivative (the derivative of $theta ^ { 2 }$ is just $2 theta$ ), while L1 regularization is more complex (the derivative of $| theta |$ is non-continuous at zero). But where L2 prefers weight vectors with many small weights, L1 prefers sparse solutions with some larger weights but many more weights set to zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features. \nBoth L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplace prior on the weights. L2 regularization corresponds to assuming that weights are distributed according to a Gaussian distribution with mean $mu = 0$ . In a Gaussian or normal distribution, the further away a value is from the mean, the lower its probability (scaled by the variance $sigma$ ). By using a Gaussian prior on the weights, we are saying that weights prefer to have the value 0. A Gaussian for a weight $theta _ { j }$ is \nIf we multiply each weight by a Gaussian prior on the weight, we are thus maximizing the following constraint: \nwhich in log space, with $mu = 0$ , and assuming $2 sigma ^ { 2 } = 1$ , corresponds to \nwhich is in the same form as Eq. 5.37. \n5.8 Learning in Multinomial Logistic Regression \nThe loss function for multinomial logistic regression generalizes the loss function for binary logistic regression from 2 to $K$ classes. Recall that that the cross-entropy loss for binary logistic regression (repeated from Eq. 5.22) is: \nThe loss function for multinomial logistic regression generalizes the two terms in Eq. 5.43 (one that is non-zero when $y = 1$ and one that is non-zero when $y = 0$ ) to $K$ terms. As we mentioned above, for multinomial regression we’ll represent both $pmb {  y }$ and $hat { pmb y }$ as vectors. The true label $pmb {  y }$ is a vector with $K$ elements, each corresponding to a class, with $y _ { c } = 1$ if the correct class is $c$ , with all other elements of $pmb {  y }$ being 0. And our classifier will produce an estimate vector with $K$ elements $hat { pmb y }$ , each element $hat { mathbf { y } } _ { k }$ of which represents the estimated probability $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ . \nThe loss function for a single example $boldsymbol { mathsf { x } }$ , generalizing from binary logistic regression, is the sum of the logs of the $K$ output classes, each weighted by their probability $pmb { y } _ { k }$ (Eq. 5.44). This turns out to be just the negative log probability of the correct class $c$ (Eq. 5.45): \n\nHow did we get from Eq. 5.44 to Eq. 5.45? Because only one class (let’s call it $c$ ) is the correct one, the vector $pmb {  y }$ takes the value 1 only for this value of $k$ , i.e., has $y _ { c } = 1$ and $y _ { j } = 0 forall j neq c$ . That means the terms in the sum in Eq. 5.44 will all be 0 except for the term corresponding to the true class $c$ . Hence the cross-entropy loss is simply the log of the output probability corresponding to the correct class, and we therefore also call Eq. 5.45 the negative log likelihood loss. \nOf course for gradient descent we don’t need the loss, we need its gradient. The gradient for a single example turns out to be very similar to the gradient for binary logistic regression, $( { hat { y } } - y ) x$ , that we saw in Eq. 5.29. Let’s consider one piece of the gradient, the derivative for a single weight. For each class $k$ , the weight of the ith element of input $pmb { x }$ is $boldsymbol { mathsf { w } } _ { k , i }$ . What is the partial derivative of the loss with respect to $mathbf { w } _ { k , i } ?$ This derivative turns out to be just the difference between the true value for the class $k$ (which is either 1 or 0) and the probability the classifier outputs for class $k$ , weighted by the value of the input $mathbf { x } _ { i }$ corresponding to the ith element of the weight vector for class $k$ : \nWe’ll return to this case of the gradient for softmax regression when we introduce neural networks in Chapter 7, and at that time we’ll also discuss the derivation of this gradient in equations Eq. 7.35–Eq. 7.43. \n5.9 Interpreting models \ninterpretable \nOften we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want our decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier’s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the Wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight $w$ associated with the feature?) can help us interpret why the classifier made the decision it makes. This is enormously important for building transparent models.",
        "chapter": "Logistic Regression",
        "section": "Learning in Multinomial Logistic Regression",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "How did we get from Eq. 5.44 to Eq. 5.45? Because only one class (let’s call it $c$ ) is the correct one, the vector $pmb {  y }$ takes the value 1 only for this value of $k$ , i.e., has $y _ { c } = 1$ and $y _ { j } = 0 forall j neq c$ . That means the terms in the sum in Eq. 5.44 will all be 0 except for the term corresponding to the true class $c$ . Hence the cross-entropy loss is simply the log of the output probability corresponding to the correct class, and we therefore also call Eq. 5.45 the negative log likelihood loss. \nOf course for gradient descent we don’t need the loss, we need its gradient. The gradient for a single example turns out to be very similar to the gradient for binary logistic regression, $( { hat { y } } - y ) x$ , that we saw in Eq. 5.29. Let’s consider one piece of the gradient, the derivative for a single weight. For each class $k$ , the weight of the ith element of input $pmb { x }$ is $boldsymbol { mathsf { w } } _ { k , i }$ . What is the partial derivative of the loss with respect to $mathbf { w } _ { k , i } ?$ This derivative turns out to be just the difference between the true value for the class $k$ (which is either 1 or 0) and the probability the classifier outputs for class $k$ , weighted by the value of the input $mathbf { x } _ { i }$ corresponding to the ith element of the weight vector for class $k$ : \nWe’ll return to this case of the gradient for softmax regression when we introduce neural networks in Chapter 7, and at that time we’ll also discuss the derivation of this gradient in equations Eq. 7.35–Eq. 7.43. \n5.9 Interpreting models \ninterpretable \nOften we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want our decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier’s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the Wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight $w$ associated with the feature?) can help us interpret why the classifier made the decision it makes. This is enormously important for building transparent models. \n\nFurthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it’s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. \n5.10 Advanced: Deriving the Gradient Equation \nIn this section we give the derivation of the gradient of the cross-entropy loss function $L _ { mathrm { C E } }$ for logistic regression. Let’s start with some quick calculus refreshers. First, the derivative of $ln ( x )$ : \nSecond, the (very elegant) derivative of the sigmoid: \nchain rule \nFinally, the chain rule of derivatives. Suppose we are computing the derivative of a composite function $f ( x ) = u ( nu ( x ) )$ . The derivative of $f ( x )$ is the derivative of $u ( x )$ with respect to $nu ( x )$ times the derivative of $nu ( x )$ with respect to $x$ : \nFirst, we want to know the derivative of the loss function with respect to a single weight $w _ { j }$ (we’ll need to compute it for each weight, and for the bias): \nNext, using the chain rule, and relying on the derivative of log:",
        "chapter": "Logistic Regression",
        "section": "Interpreting models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it’s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. \n5.10 Advanced: Deriving the Gradient Equation \nIn this section we give the derivation of the gradient of the cross-entropy loss function $L _ { mathrm { C E } }$ for logistic regression. Let’s start with some quick calculus refreshers. First, the derivative of $ln ( x )$ : \nSecond, the (very elegant) derivative of the sigmoid: \nchain rule \nFinally, the chain rule of derivatives. Suppose we are computing the derivative of a composite function $f ( x ) = u ( nu ( x ) )$ . The derivative of $f ( x )$ is the derivative of $u ( x )$ with respect to $nu ( x )$ times the derivative of $nu ( x )$ with respect to $x$ : \nFirst, we want to know the derivative of the loss function with respect to a single weight $w _ { j }$ (we’ll need to compute it for each weight, and for the bias): \nNext, using the chain rule, and relying on the derivative of log: \nRearranging terms: \nAnd now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq. 5.54: \n5.11 Summary \nThis chapter introduced the logistic regression model of classification. \n• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.   \n• Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).   \n• Multinomial logistic regression uses the softmax function to compute probabilities.   \n• The weights (vector $w$ and bias $b$ ) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.   \n• Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.   \n• Regularization is used to avoid overfitting.   \n• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features. \nBibliographical and Historical Notes \nLogistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). \nNonetheless, logistic regression didn’t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and",
        "chapter": "Logistic Regression",
        "section": "Advanced: Deriving the Gradient Equation",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Rearranging terms: \nAnd now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq. 5.54: \n5.11 Summary \nThis chapter introduced the logistic regression model of classification. \n• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.   \n• Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).   \n• Multinomial logistic regression uses the softmax function to compute probabilities.   \n• The weights (vector $w$ and bias $b$ ) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.   \n• Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.   \n• Regularization is used to avoid overfitting.   \n• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features. \nBibliographical and Historical Notes \nLogistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). \nNonetheless, logistic regression didn’t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and",
        "chapter": "Logistic Regression",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Rearranging terms: \nAnd now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq. 5.54: \n5.11 Summary \nThis chapter introduced the logistic regression model of classification. \n• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.   \n• Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).   \n• Multinomial logistic regression uses the softmax function to compute probabilities.   \n• The weights (vector $w$ and bias $b$ ) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.   \n• Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.   \n• Regularization is used to avoid overfitting.   \n• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features. \nBibliographical and Historical Notes \nLogistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). \nNonetheless, logistic regression didn’t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and",
        "chapter": "Logistic Regression",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Rearranging terms: \nAnd now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq. 5.54: \n5.11 Summary \nThis chapter introduced the logistic regression model of classification. \n• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.   \n• Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).   \n• Multinomial logistic regression uses the softmax function to compute probabilities.   \n• The weights (vector $w$ and bias $b$ ) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.   \n• Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.   \n• Regularization is used to avoid overfitting.   \n• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features. \nBibliographical and Historical Notes \nLogistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). \nNonetheless, logistic regression didn’t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and \nmaximum entropy \nspeech processing, both of which had made use of regression, and both of which lent many other statistical techniques to NLP. Indeed a very early use of logistic regression for document routing was one of the first NLP applications to use (LSI) embeddings as word representations (Schu¨tze et al., 1995). \nAt the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). \nMore on classification can be found in machine learning textbooks (Hastie et al. 2001, Witten and Frank 2005, Bishop 2006, Murphy 2012). \nExercises \n6 \nVector Semantics and Embeddings \n荃者所以在鱼，得鱼而忘荃 Nets are for fish; Once you get the fish, you can forget the net.   \n言者所以在意，得意而忘言 Words are for meaning; Once you get the meaning, you can forget the words \n庄子(Zhuangzi), Chapter 26 \nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly recognizable by its long canines. Five million years ago or so, a completely different sabre-tooth tiger called Thylacosmilus lived in Argentina and other parts of South America. Thylacosmilus was a marsupial whereas Smilodon was a placental mammal, but Thylacosmilus had the same long upper canines and, like Smilodon, had a protective bone flange on the lower jaw. The similarity of these two mammals is one of many examples of parallel or convergent evolution, in which particular contexts or environments lead to the evolution of very similar structures in different species (Gould, 1980). \n\ndistributional hypothesis \nThe role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), who noticed that words which are synonyms (like oculist and eye-doctor) tended to occur in the same environment (e.g., near words like eye or examined) with the amount of meaning difference between two words “corresponding roughly to the amount of difference in their environments” (Harris, 1954, 157). \nvector semantics embeddings \nIn this chapter we introduce vector semantics, which instantiates this linguistic hypothesis by learning representations of the meaning of words, called embeddings, directly from their distributions in texts. These representations are used in every natural language processing application that makes use of meaning, and the static embeddings we introduce here underlie the more powerful dynamic or contextualized embeddings like BERT that we will see in Chapter 11. \nrepresentation learning \nThese word representations are also the first example in this book of representation learning, automatically learning useful representations of the input text. Finding such self-supervised ways to learn representations of the input, instead of creating representations by hand via feature engineering, is an important focus of NLP research (Bengio et al., 2013).",
        "chapter": "Logistic Regression",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "6 \nVector Semantics and Embeddings \n荃者所以在鱼，得鱼而忘荃 Nets are for fish; Once you get the fish, you can forget the net.   \n言者所以在意，得意而忘言 Words are for meaning; Once you get the meaning, you can forget the words \n庄子(Zhuangzi), Chapter 26 \nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly recognizable by its long canines. Five million years ago or so, a completely different sabre-tooth tiger called Thylacosmilus lived in Argentina and other parts of South America. Thylacosmilus was a marsupial whereas Smilodon was a placental mammal, but Thylacosmilus had the same long upper canines and, like Smilodon, had a protective bone flange on the lower jaw. The similarity of these two mammals is one of many examples of parallel or convergent evolution, in which particular contexts or environments lead to the evolution of very similar structures in different species (Gould, 1980). \n\ndistributional hypothesis \nThe role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), who noticed that words which are synonyms (like oculist and eye-doctor) tended to occur in the same environment (e.g., near words like eye or examined) with the amount of meaning difference between two words “corresponding roughly to the amount of difference in their environments” (Harris, 1954, 157). \nvector semantics embeddings \nIn this chapter we introduce vector semantics, which instantiates this linguistic hypothesis by learning representations of the meaning of words, called embeddings, directly from their distributions in texts. These representations are used in every natural language processing application that makes use of meaning, and the static embeddings we introduce here underlie the more powerful dynamic or contextualized embeddings like BERT that we will see in Chapter 11. \nrepresentation learning \nThese word representations are also the first example in this book of representation learning, automatically learning useful representations of the input text. Finding such self-supervised ways to learn representations of the input, instead of creating representations by hand via feature engineering, is an important focus of NLP research (Bengio et al., 2013). \n6.1 Lexical Semantics \nLet’s begin by introducing some basic principles of word meaning. How should we represent the meaning of a word? In the n-gram models of Chapter 3, and in classical NLP applications, our only representation of a word is as a string of letters, or an index in a vocabulary list. This representation is not that different from a tradition in philosophy, perhaps you’ve seen it in introductory logic classes, in which the meaning of words is represented by just spelling the word with small capital letters; representing the meaning of “dog” as DOG, and “cat” as CAT, or by using an apostrophe (DOG’). \nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory model. You might have seen a version of a joke due originally to semanticist Barbara Partee (Carlson, 1977): \nQ: What’s the meaning of life? A: LIFE’ \nlexical semantics \nSurely we can do better than this! After all, we’ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event (If I buy something from you, you’ve probably sold it to me, and I likely paid you). More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. \nIn this section we summarize some of these desiderata, drawing on results in the linguistic study of word meaning, which is called lexical semantics; we’ll return to and expand on this list in Chapter 18 and Chapter 10. \nLemmas and Senses Let’s start by looking at how one word (we’ll choose mouse) might be defined in a dictionary (simplified from the online dictionary WordNet): mouse (N)   \n1. any of numerous small rodents..   \n2. a hand-operated device that controls a cursor. \nlemma citation form \nwordform \nHere the form mouse is the lemma, also called the citation form. The form mouse would also be the lemma for the word mice; dictionaries don’t have separate definitions for inflected forms like mice. Similarly sing is the lemma for sing, sang, sung. In many languages the infinitive form is used as the lemma for the verb, so Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The specific forms sung or carpets or sing or duermes are called wordforms. \nAs the example above shows, each lemma can have multiple meanings; the lemma mouse can refer to the rodent or the cursor control device. We call each of these aspects of the meaning of mouse a word sense. The fact that lemmas can be polysemous (have multiple senses) can make interpretation difficult (is someone who types “mouse info” into a search engine looking for a pet or a tool?). Chapter 18 will discuss the problem of polysemy, and introduce word sense disambiguation, the task of determining which sense of a word is being used in a particular context. \nSynonymy One important component of word meaning is the relationship between word senses. For example when one word has a sense whose meaning is identical to a sense of another word, or nearly identical, we say the two senses of those two words are synonyms. Synonyms include such pairs as \n\ncouch/sofa vomit/throw up filbert/hazelnut car/automobile \npropositional meaning \nA more formal definition of synonymy (between words rather than senses) is that two words are synonymous if they are substitutable for one another in any sentence without changing the truth conditions of the sentence, the situations in which the sentence would be true. We often say in this case that the two words have the same propositional meaning. \nprinciple of contrast \nWhile substitutions between some pairs of words like car / automobile or water / $H _ { 2 } O$ are truth preserving, the words are still not identical in meaning. Indeed, probably no two words are absolutely identical in meaning. One of the fundamental tenets of semantics, called the principle of contrast (Girard 1718, Bre´al 1897, Clark 1987), states that a difference in linguistic form is always associated with some difference in meaning. For example, the word $H _ { 2 } O$ is used in scientific contexts and would be inappropriate in a hiking guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy. \nsimilarity \nWord Similarity While words don’t have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. In moving from synonymy to similarity, it will be useful to shift from talking about relations between word senses (like synonymy) to relations between words (like similarity). Dealing with words avoids having to commit to a particular representation of word senses, which will turn out to simplify our task. \nThe notion of word similarity is very useful in larger semantic tasks. Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): \nrelatedness association \nWord Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology. \nConsider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel). \nsemantic field \nOne common kind of relatedness between words is if they belong to the same semantic field. A semantic field is a set of words which cover a particular semantic \ntopic models \ndomain and bear structured relations with each other. For example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, kitchen, family, bed). Semantic fields are also related to topic models, like Latent Dirichlet Allocation, LDA, which apply unsupervised learning on large sets of texts to induce sets of associated words from text. Semantic fields and topic models are very useful tools for discovering topical structure in documents. \nIn Chapter 18 we’ll introduce more relations between senses like hypernymy or IS-A, antonymy (opposites) and meronymy (part-whole relations). \nsemantic frame \nSemantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. \nKnowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. \nconnotations \nConnotation Finally, words have affective meanings or connotations. The word connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. For example some words have positive connotations (happy) while others have negative connotations (sad). Even words whose meanings are similar in other ways can vary in connotation; consider the difference in connotations between fake, knockoff, forgery, on the one hand, and copy, replica, reproduction on the other, or innocent (positive connotation) and naive (negative connotation). Some words describe positive evaluation (great, love) and others negative evaluation (terrible, hate). Positive or negative evaluation language is called sentiment, as we saw in Chapter 4, and word sentiment plays a role in important tasks like sentiment analysis, stance detection, and applications of NLP to the language of politics and consumer reviews. \nsentiment \nEarly work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning: \nvalence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus \nThus words like happy or satisfied are high on valence, while unhappy or annoyed are low on valence. Excited is high on arousal, while calm is low on arousal. Controlling is high on dominance, while awed or influenced are low on dominance. Each word is thus represented by three numbers, corresponding to its value on each of the three dimensions: \nOsgood et al. (1957) noticed that in using these 3 numbers to represent the meaning of a word, the model was representing each word as a point in a threedimensional space, a vector whose three dimensions corresponded to the word’s rating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be represented as the point [2.45, 5.65, 3.58]) was the first expression of the vector semantics models that we introduce next. \n6.2 Vector Semantics \nvector semantics \nVectors semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. The roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. \nFor example, suppose you didn’t know the meaning of the word ongchoi (a recent borrowing from Cantonese) but you see it in the following contexts: \n(6.1) Ongchoi is delicious sauteed with garlic.   \n(6.2) Ongchoi is superb over rice.   \n(6.3) ...ongchoi leaves with salty sauces... \nAnd suppose that you had seen many of these context words in other contexts: \n(6.4) ...spinach sauteed with garlic over rice... (6.5) ...chard stems and leaves are delicious... (6.6) ...collard greens and other salty leafy greens \nThe fact that ongchoi occurs with words like rice and garlic and delicious and salty, as do words like spinach, chard, and collard greens might suggest that ongchoi is a leafy green similar to these other leafy greens.1 We can do the same thing computationally by just counting words in the context of ongchoi. \nThe idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we’ll see) from the distributions of word neighbors. Vectors for representing words are called embeddings (although the term is sometimes more strictly applied only to dense vectors like word2vec (Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)). The word “embedding” derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of the chapter.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Lexical Semantics",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Osgood et al. (1957) noticed that in using these 3 numbers to represent the meaning of a word, the model was representing each word as a point in a threedimensional space, a vector whose three dimensions corresponded to the word’s rating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be represented as the point [2.45, 5.65, 3.58]) was the first expression of the vector semantics models that we introduce next. \n6.2 Vector Semantics \nvector semantics \nVectors semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. The roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. \nFor example, suppose you didn’t know the meaning of the word ongchoi (a recent borrowing from Cantonese) but you see it in the following contexts: \n(6.1) Ongchoi is delicious sauteed with garlic.   \n(6.2) Ongchoi is superb over rice.   \n(6.3) ...ongchoi leaves with salty sauces... \nAnd suppose that you had seen many of these context words in other contexts: \n(6.4) ...spinach sauteed with garlic over rice... (6.5) ...chard stems and leaves are delicious... (6.6) ...collard greens and other salty leafy greens \nThe fact that ongchoi occurs with words like rice and garlic and delicious and salty, as do words like spinach, chard, and collard greens might suggest that ongchoi is a leafy green similar to these other leafy greens.1 We can do the same thing computationally by just counting words in the context of ongchoi. \nThe idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we’ll see) from the distributions of word neighbors. Vectors for representing words are called embeddings (although the term is sometimes more strictly applied only to dense vectors like word2vec (Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)). The word “embedding” derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of the chapter. \nFig. 6.1 shows a visualization of embeddings learned for sentiment analysis, showing the location of selected words projected down from 60-dimensional space into a two dimensional space. Notice the distinct regions containing positive words, negative words, and neutral function words. \nThe fine-grained model of word similarity of vector semantics offers enormous power to NLP applications. NLP applications like the sentiment classifiers of Chapter 4 or Chapter 5 depend on the same words appearing in the training and test sets. But by representing words as embeddings, classifiers can assign sentiment as long as it sees some words with similar meanings. And as we’ll see, vector semantic models can be learned automatically from text without supervision. \nIn this chapter we’ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We’ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We’ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. \n6.3 Words and Vectors \n“The most important attributes of a vector in 3-space are Location, Location, Location ” Randall Munroe, https://xkcd.com/2358/ \nVector or distributional models of meaning are generally based on a co-occurrence matrix, a way of representing how often words co-occur. We’ll look at two popular matrices: the term-document matrix and the term-term matrix. \nterm-document matrix \n6.3.1 Vectors and documents \nIn a term-document matrix, each row represents a word in the vocabulary and each column represents a document from some collection of documents. Fig. 6.2 shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Vector Semantics",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Fig. 6.1 shows a visualization of embeddings learned for sentiment analysis, showing the location of selected words projected down from 60-dimensional space into a two dimensional space. Notice the distinct regions containing positive words, negative words, and neutral function words. \nThe fine-grained model of word similarity of vector semantics offers enormous power to NLP applications. NLP applications like the sentiment classifiers of Chapter 4 or Chapter 5 depend on the same words appearing in the training and test sets. But by representing words as embeddings, classifiers can assign sentiment as long as it sees some words with similar meanings. And as we’ll see, vector semantic models can be learned automatically from text without supervision. \nIn this chapter we’ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We’ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We’ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. \n6.3 Words and Vectors \n“The most important attributes of a vector in 3-space are Location, Location, Location ” Randall Munroe, https://xkcd.com/2358/ \nVector or distributional models of meaning are generally based on a co-occurrence matrix, a way of representing how often words co-occur. We’ll look at two popular matrices: the term-document matrix and the term-term matrix. \nterm-document matrix \n6.3.1 Vectors and documents \nIn a term-document matrix, each row represents a word in the vocabulary and each column represents a document from some collection of documents. Fig. 6.2 shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times \nvector space model \nvector \nvector space dimension \nThe term-document matrix of Fig. 6.2 was first defined as part of the vector space model of information retrieval (Salton, 1971). In this model, a document is represented as a count vector, a column in Fig. 6.3. \nTo review some basic linear algebra, a vector is, at heart, just a list or array of numbers. So As You Like $I t$ is represented as the list [1,114,36,20] (the first column vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third column vector). A vector space is a collection of vectors, characterized by their dimension. In the example in Fig. 6.3, the document vectors are of dimension 4, just so they fit on the page; in real term-document matrices, the vectors representing each document would have dimensionality $| V |$ , the vocabulary size. \nThe ordering of the numbers in a vector space indicates different meaningful dimensions on which documents vary. Thus the first dimension for both these vectors corresponds to the number of times the word battle occurs, and we can compare each dimension, noting for example that the vectors for $A s$ You Like It and Twelfth Night have similar values (1 and 0, respectively) for the first dimension. \nWe can think of the vector for a document as a point in $| V |$ -dimensional space; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we’ve arbitrarily chosen the dimensions corresponding to the words battle and fool. \nTerm-document matrices were originally defined as a means of finding similar documents for the task of document information retrieval. Two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar. The vectors for the comedies As You Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other (more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or Henry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition more formally. \nA real term-document matrix, of course, wouldn’t just have 4 rows and columns, let alone 2. More generally, the term-document matrix has $| V |$ rows (one for each word type in the vocabulary) and $D$ columns (one for each document in the collection); as we’ll see, vocabulary sizes are generally in the tens of thousands, and the number of documents can be enormous (think about all the pages on the web). \n\nInformation retrieval (IR) is the task of finding the document $d$ from the $D$ documents in some collection that best matches a query $q$ . For IR we’ll therefore also represent a query by a vector, also of length $| V |$ , and we’ll need a way to compare two vectors to find how similar they are. (Doing IR will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse, i.e., mostly zeros). \nLater in the chapter we’ll introduce some of the components of this vector comparison process: the tf-idf term weighting, and the cosine similarity metric. \n6.3.2 Words as vectors: document dimensions \nWe’ve seen that documents can be represented as vectors in a vector space. But vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word counts in the same four dimensions are used to form the vectors for the other 3 words: wit, [20,15,2,3]; battle, [1,0,7,13]; and good [114,80,62,89]. \nFor documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Words and Vectors",
        "subsection": "Vectors and documents",
        "subsubsection": "N/A"
    },
    {
        "content": "Information retrieval (IR) is the task of finding the document $d$ from the $D$ documents in some collection that best matches a query $q$ . For IR we’ll therefore also represent a query by a vector, also of length $| V |$ , and we’ll need a way to compare two vectors to find how similar they are. (Doing IR will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse, i.e., mostly zeros). \nLater in the chapter we’ll introduce some of the components of this vector comparison process: the tf-idf term weighting, and the cosine similarity metric. \n6.3.2 Words as vectors: document dimensions \nWe’ve seen that documents can be represented as vectors in a vector space. But vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word counts in the same four dimensions are used to form the vectors for the other 3 words: wit, [20,15,2,3]; battle, [1,0,7,13]; and good [114,80,62,89]. \nFor documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Words and Vectors",
        "subsection": "Words as vectors: document dimensions",
        "subsubsection": "N/A"
    },
    {
        "content": "Information retrieval (IR) is the task of finding the document $d$ from the $D$ documents in some collection that best matches a query $q$ . For IR we’ll therefore also represent a query by a vector, also of length $| V |$ , and we’ll need a way to compare two vectors to find how similar they are. (Doing IR will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse, i.e., mostly zeros). \nLater in the chapter we’ll introduce some of the components of this vector comparison process: the tf-idf term weighting, and the cosine similarity metric. \n6.3.2 Words as vectors: document dimensions \nWe’ve seen that documents can be represented as vectors in a vector space. But vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word counts in the same four dimensions are used to form the vectors for the other 3 words: wit, [20,15,2,3]; battle, [1,0,7,13]; and good [114,80,62,89]. \nFor documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in. \nword-word matrix \n6.3.3 Words as vectors: word dimensions \nAn alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix, also called the word-word matrix or the term-context matrix, in which the columns are labeled by words rather than documents. This matrix is thus of dimensionality $| V | times | V |$ and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be the document, in which case the cell represents the number of times the two words appear in the same document. It is most common, however, to use smaller contexts, generally a window around the word, for example of 4 words to the left and 4 words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a $pm 4$ word window around the row word. For example here is one example each of some words in their windows: \nis traditionally followed by cherry pie, a traditional dessert often mixed, such as strawberry rhubarb pie. Apple pie computer peripherals and personal digital assistants. These devices usually a computer. This includes information available on the internet \nIf we then take every occurrence of each word (say strawberry) and count the context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a simplified subset of the word-word co-occurrence matrix for these four words computed from the Wikipedia corpus (Davies, 2015). \nNote in Fig. 6.6 that the two words cherry and strawberry are more similar to each other (both pie and sugar tend to occur in their window) than they are to other words like digital; conversely, digital and information are more similar to each other than, say, to strawberry. Fig. 6.7 shows a spatial visualization. \nNote that $| V |$ , the dimensionality of the vector, is generally the size of the vocabulary, often between 10,000 and 50,000 words (using the most frequent words in the training corpus; keeping words after about the most frequent 50,000 or so is generally not helpful). Since most of these numbers are zero these are sparse vector representations; there are efficient algorithms for storing and computing with sparse matrices. \n\nNow that we have some intuitions, let’s move on to examine the details of computing word similarity. Afterwards we’ll discuss methods for weighting cells. \n6.4 Cosine for measuring similarity \ndot product inner product \nTo measure similarity between two target words $nu$ and $w$ , we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, hence of length $| V |$ , or both with documents as dimensions as documents, of length $| D | )$ and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. \nThe cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product: \nThe dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. \nvector length \nThis raw dot product, however, has a problem as a similarity metric: it favors long vectors. The vector length is defined as \nThe dot product is higher if a vector is longer, with higher values in each dimension. More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency. \nWe modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors, following from the definition of the dot product between two vectors a and $mathbf { b }$ : \nThe cosine similarity metric between two vectors $pmb { v }$ and $pmb { w }$ thus can be computed as:",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Words and Vectors",
        "subsection": "Words as vectors: word dimensions",
        "subsubsection": "N/A"
    },
    {
        "content": "Now that we have some intuitions, let’s move on to examine the details of computing word similarity. Afterwards we’ll discuss methods for weighting cells. \n6.4 Cosine for measuring similarity \ndot product inner product \nTo measure similarity between two target words $nu$ and $w$ , we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, hence of length $| V |$ , or both with documents as dimensions as documents, of length $| D | )$ and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. \nThe cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product: \nThe dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. \nvector length \nThis raw dot product, however, has a problem as a similarity metric: it favors long vectors. The vector length is defined as \nThe dot product is higher if a vector is longer, with higher values in each dimension. More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency. \nWe modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors, following from the definition of the dot product between two vectors a and $mathbf { b }$ : \nThe cosine similarity metric between two vectors $pmb { v }$ and $pmb { w }$ thus can be computed as: \nunit vector \nFor some applications we pre-normalize each vector, by dividing it by its length, creating a unit vector of length 1. Thus we could compute a unit vector from a by dividing it by a . For unit vectors, the dot product is the same as the cosine. \nThe cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1. \nLet’s see how the cosine computes which of the words cherry or digital is closer in meaning to information, just using raw counts from the following shortened table: \nThe model decides that information is way closer to digital than it is to cherry, a result that seems sensible. Fig. 6.8 shows a visualization. \n6.5 TF-IDF: Weighing terms in the vector \nThe co-occurrence matrices above represent each cell by frequencies, either of words with documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency is not the best measure of association between words. Raw frequency is very skewed and not very discriminative. If we want to know what kinds of contexts are shared by cherry and strawberry but not by digital and information, we’re not going to get good discrimination from words like the, it, or they, which occur frequently with all sorts of words and aren’t informative about any particular word. We saw this also in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not very discriminative between plays; good is simply a frequent word and has roughly equivalent high frequencies in each of the plays.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Cosine for measuring similarity",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "unit vector \nFor some applications we pre-normalize each vector, by dividing it by its length, creating a unit vector of length 1. Thus we could compute a unit vector from a by dividing it by a . For unit vectors, the dot product is the same as the cosine. \nThe cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1. \nLet’s see how the cosine computes which of the words cherry or digital is closer in meaning to information, just using raw counts from the following shortened table: \nThe model decides that information is way closer to digital than it is to cherry, a result that seems sensible. Fig. 6.8 shows a visualization. \n6.5 TF-IDF: Weighing terms in the vector \nThe co-occurrence matrices above represent each cell by frequencies, either of words with documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency is not the best measure of association between words. Raw frequency is very skewed and not very discriminative. If we want to know what kinds of contexts are shared by cherry and strawberry but not by digital and information, we’re not going to get good discrimination from words like the, it, or they, which occur frequently with all sorts of words and aren’t informative about any particular word. We saw this also in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not very discriminative between plays; good is simply a frequent word and has roughly equivalent high frequencies in each of the plays. \n\nIt’s a bit of a paradox. Words that occur nearby frequently (maybe pie nearby cherry) are more important than words that only appear once or twice. Yet words that are too frequent—ubiquitous, like the or good— are unimportant. How can we balance these two conflicting constraints? \nThere are two common solutions to this problem: in this section we’ll describe the tf-idf weighting, usually used when the dimensions are documents. In the next we introduce the PPMI algorithm (usually used when the dimensions are words). \nThe tf-idf weighting (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, each term capturing one of these two intuitions: \nThe first is the term frequency (Luhn, 1957): the frequency of the word $t$ in the document $d$ . We can just use the raw count as the term frequency: \nterm frequency \nMore commonly we squash the raw frequency a bit, by using the $log _ { 1 0 }$ of the frequency instead. The intuition is that a word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document. Because we can’t take the log of 0, we normally add 1 to the count:2 \nIf we use log weighting, terms which occur 0 times in a document would have ${ mathrm { t f } } = log _ { 1 0 } ( 1 ) = 0$ , 10 times in a document ${ mathrm { t f } } = log _ { 1 0 } ( 1 1 ) = 1 . 0 4 $ , 100 times tf $ O =$ $log _ { 1 0 } ( 1 0 1 ) = 2 . 0 0 4$ , 1000 times $mathrm { t f } = 3 . 0 0 0 4 4$ , and so on. \nThe second factor in tf-idf is used to give a higher weight to words that occur only in a few documents. Terms that are limited to a few documents are useful for discriminating those documents from the rest of the collection; terms that occur frequently across the entire collection aren’t as helpful. The document frequency $operatorname { d f } _ { t }$ of a term $t$ is the number of documents it occurs in. Document frequency is not the same as the collection frequency of a term, which is the total number of times the word appears in the whole collection in any document. Consider in the collection of Shakespeare’s 37 plays the two words Romeo and action. The words have identical collection frequencies (they both occur 113 times in all the plays) but very different document frequencies, since Romeo only occurs in a single play. If our goal is to find documents about the romantic tribulations of Romeo, the word Romeo should be highly weighted, but not action: \nWe emphasize discriminative words like Romeo via the inverse document frequency or idf term weight (Sparck Jones, 1972). The idf is defined using the fraction $N / mathrm { d f } _ { t }$ , where $N$ is the total number of documents in the collection, and $operatorname { d f } _ { t }$ is the number of documents in which term $t$ occurs. The fewer documents in which a term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that occur in all the documents. It’s usually clear what counts as a document: in Shakespeare we would use a play; when processing a collection of encyclopedia articles like Wikipedia, the document is a Wikipedia page; in processing newspaper articles, the document is a single article. Occasionally your corpus might not have appropriate document divisions and you might need to break up the corpus into documents yourself for the purposes of computing idf. \n\nBecause of the large number of documents in many collections, this measure too is usually squashed with a log function. The resulting definition for inverse document frequency (idf) is thus \nHere are some idf values for some words in the Shakespeare corpus, ranging from extremely informative words which occur in only one play like Romeo, to those that occur in a few like salad or Falstaff, to those which are very common like fool or so common as to be completely non-discriminative since they occur in all 37 plays like good or sweet.3 \ntf-idf \nThe tf-idf weighted value $w _ { t , d }$ for word $t$ in document $d$ thus combines term frequency $mathbf { t f } _ { t , d }$ (defined either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13: \nFig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2, using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corresponding to the word good have now all become 0; since this word appears in every document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which appears in 36 out of the 37 plays, has a much lower weight. \nThe tf-idf weighting is the way for weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing. It’s also a great baseline, the simple thing to try first. We’ll look at other weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "TF-IDF: Weighing terms in the vector",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Because of the large number of documents in many collections, this measure too is usually squashed with a log function. The resulting definition for inverse document frequency (idf) is thus \nHere are some idf values for some words in the Shakespeare corpus, ranging from extremely informative words which occur in only one play like Romeo, to those that occur in a few like salad or Falstaff, to those which are very common like fool or so common as to be completely non-discriminative since they occur in all 37 plays like good or sweet.3 \ntf-idf \nThe tf-idf weighted value $w _ { t , d }$ for word $t$ in document $d$ thus combines term frequency $mathbf { t f } _ { t , d }$ (defined either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13: \nFig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2, using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corresponding to the word good have now all become 0; since this word appears in every document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which appears in 36 out of the 37 plays, has a much lower weight. \nThe tf-idf weighting is the way for weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing. It’s also a great baseline, the simple thing to try first. We’ll look at other weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6. \nFigure 6.9 A tf-idf weighted term-document matrix for four words in four Shakespeare plays, using the counts in Fig. 6.2. For example the 0.049 value for $w i t$ in $A s$ You Like It is the product of $mathrm { t f } = log _ { 1 0 } ( 2 0 + 1 ) = 1 . 3 2 2$ and idf $mathbf { tau } = . 0 3 7$ . Note that the idf weighting has eliminated the importance of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word fool. \n6.6 Pointwise Mutual Information (PMI) \nAn alternative weighting function to tf-idf, PPMI (positive pointwise mutual information), is used for term-term-matrices, when the vector dimensions correspond to words rather than documents. PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance. \nPointwise mutual information (Fano, $1 9 6 1 ) ^ { 4 }$ is one of the most important concepts in NLP. It is a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent: \nThe pointwise mutual information between a target word $w$ and a context word $c$ (Church and Hanks 1989, Church and Hanks 1990) is then defined as: \nThe numerator tells us how often we observed the two words together (assuming we compute probability by using the MLE). The denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently; recall that the probability of two independent events both occurring is just the product of the probabilities of the two events. Thus, the ratio gives us an estimate of how much more the two words co-occur than we expect by chance. PMI is a useful tool whenever we need to find words that are strongly associated. \nPMI values range from negative to positive infinity. But negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable unless our corpora are enormous. To distinguish whether two words whose individual probability is each $1 0 ^ { - 6 }$ occur together less often than chance, we would need to be certain that the probability of the two occurring together is significantly different than $1 0 ^ { - 1 2 }$ , and this kind of granularity would require an enormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate such scores of ‘unrelatedness’ with human judgments. For this reason it \nPPMI is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta $1 9 9 4 ) ^ { 5 }$ : \nMore formally, let’s assume we have a co-occurrence matrix $mathrm { ~ F ~ }$ with $mathrm { ~ w ~ }$ rows (words) and C columns (contexts), where $f _ { i j }$ gives the number of times word $w _ { i }$ occurs in context $c _ { j }$ . This can be turned into a PPMI matrix where $mathrm { P P M I } _ { i j }$ gives the PPMI value of word $w _ { i }$ with context $c _ { j }$ (which we can also express as $mathrm { P P M I } ( w _ { i } , c _ { j } )$ or PPMI $boldsymbol { w } = i , boldsymbol { c } = j )$ ) as follows: \nLet’s see some PPMI calculations. We’ll use Fig. 6.10, which repeats Fig. 6.6 plus all the count marginals, and let’s pretend for ease of calculation that these are the only words/contexts that matter. \nThus for example we could compute PPMI(information,data), assuming we pretended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as follows: \nFig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and Fig. 6.12 shows the PPMI values. Not surprisingly, cherry and strawberry are highly associated with both pie and sugar, and data is mildly associated with information. PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values. One way to reduce this bias toward low frequency \nFigure 6.12 The PPMI matrix showing the association between words and context words, computed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had a negative PMI; for example PMI(cherry,computer) $= - 6 . 7$ , meaning that cherry and computer co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace negative values by zero. \nevents is to slightly change the computation for $P ( c )$ , using a different function $P _ { alpha } ( c )$ that raises the probability of the context word to the power of $alpha$ : \nLevy et al. (2015) found that a setting of $alpha = 0 . 7 5$ improved performance of embeddings on a wide range of tasks (drawing on a similar weighting used for skipgrams described below in Eq. 6.32). This works because raising the count to $alpha =$ 0.75 increases the probability assigned to rare contexts, and hence lowers their PMI $( P _ { alpha } ( c ) > P ( c )$ when $c$ is rare). \nAnother possible solution is Laplace smoothing: Before computing PMI, a small constant $k$ (values of 0.1-3 are common) is added to each of the counts, shrinking (discounting) all the non-zero values. The larger the $k$ , the more the non-zero counts are discounted. \n6.7 Applications of the tf-idf or PPMI vector models \nIn summary, the vector semantics model we’ve described so far represents a target word as a vector with dimensions corresponding either to the documents in a large collection (the term-document matrix) or to the counts of words in some neighboring window (the term-term matrix). The values in each dimension are counts, weighted by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the vectors are sparse (since most values are zero). \nThe model computes the similarity between two words $x$ and $y$ by taking the cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model is sometimes referred to as the tf-idf model or the PPMI model, after the weighting function.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Pointwise Mutual Information (PMI)",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Figure 6.12 The PPMI matrix showing the association between words and context words, computed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had a negative PMI; for example PMI(cherry,computer) $= - 6 . 7$ , meaning that cherry and computer co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace negative values by zero. \nevents is to slightly change the computation for $P ( c )$ , using a different function $P _ { alpha } ( c )$ that raises the probability of the context word to the power of $alpha$ : \nLevy et al. (2015) found that a setting of $alpha = 0 . 7 5$ improved performance of embeddings on a wide range of tasks (drawing on a similar weighting used for skipgrams described below in Eq. 6.32). This works because raising the count to $alpha =$ 0.75 increases the probability assigned to rare contexts, and hence lowers their PMI $( P _ { alpha } ( c ) > P ( c )$ when $c$ is rare). \nAnother possible solution is Laplace smoothing: Before computing PMI, a small constant $k$ (values of 0.1-3 are common) is added to each of the counts, shrinking (discounting) all the non-zero values. The larger the $k$ , the more the non-zero counts are discounted. \n6.7 Applications of the tf-idf or PPMI vector models \nIn summary, the vector semantics model we’ve described so far represents a target word as a vector with dimensions corresponding either to the documents in a large collection (the term-document matrix) or to the counts of words in some neighboring window (the term-term matrix). The values in each dimension are counts, weighted by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the vectors are sparse (since most values are zero). \nThe model computes the similarity between two words $x$ and $y$ by taking the cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model is sometimes referred to as the tf-idf model or the PPMI model, after the weighting function. \n\ncentroid \nThe tf-idf model of meaning is often used for document functions like deciding if two documents are similar. We represent a document by taking the vectors of all the words in the document, and computing the centroid of all those vectors. The centroid is the multidimensional version of the mean; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set. Given $k$ word vectors $w _ { 1 } , w _ { 2 } , . . . , w _ { k }$ , the centroid document vector $d$ is: \ndocument vector \nGiven two documents, we can then compute their document vectors $d _ { 1 }$ and $d _ { 2 }$ , and estimate the similarity between the two documents by $cos ( d _ { 1 } , d _ { 2 } )$ . Document similarity is also useful for all sorts of applications; information retrieval, plagiarism detection, news recommender systems, and even for digital humanities tasks like comparing different versions of a text to see which are similar to each other. \nEither the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word $w$ by computing the cosines between w and each of the $V - 1$ other words, sorting, and looking at the top 10. \n6.8 Word2vec \nIn the previous sections we saw how to represent a word as a sparse, long vector with dimensions corresponding to words in the vocabulary or documents in a collection. We now introduce a more powerful word representation: embeddings, short dense vectors. Unlike the vectors we’ve seen so far, embeddings are short, with number of dimensions $d$ ranging from 50-1000, rather than the much larger vocabulary size $| V |$ or number of documents $D$ we’ve seen. These $d$ dimensions don’t have a clear interpretation. And the vectors are dense: instead of vector entries being sparse, mostly-zero counts or functions of counts, the values will be real-valued numbers that can be negative. \nIt turns out that dense vectors work better in every NLP task than sparse vectors. While we don’t completely understand all the reasons for this, we have some intuitions. Representing words as 300-dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50,000-dimensional vectors, and the smaller parameter space possibly helps with generalization and avoiding overfitting. Dense vectors may also do a better job of capturing synonymy. For example, in a sparse vector representation, dimensions for synonyms like car and automobile dimension are distinct and unrelated; sparse vectors may thus fail to capture the similarity between a word with car as a neighbor and a word with automobile as a neighbor. \nIn this section we introduce one method for computing embeddings: skip-gram with negative sampling, sometimes called SGNS. The skip-gram algorithm is one of two algorithms in a software package called word2vec, and so sometimes the algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al. 2013b). The word2vec methods are fast, efficient to train, and easily available on",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Applications of the tf-idf or PPMI vector models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "static embeddings \nline with code and pretrained embeddings. Word2vec embeddings are static embeddings, meaning that the method learns one fixed embedding for each word in the vocabulary. In Chapter 11 we’ll introduce methods for learning dynamic contextual embeddings like the popular family of BERT representations, in which the vector for each word is different in different contexts. \nThe intuition of word2vec is that instead of counting how often each word $w$ occurs near, say, apricot, we’ll instead train a classifier on a binary prediction task: “Is word $w$ likely to show up near apricot?” We don’t actually care about this prediction task; instead we’ll take the learned classifier weights as the word embeddings. \nself-supervision \nThe revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word $c$ that occurs near the target word apricot acts as gold ‘correct answer’ to the question “Is word $c$ likely to show up near apricot?” This method, often called self-supervision, avoids the need for any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model (a neural network that learned to predict the next word from prior words) could just use the next word in running text as its supervision signal, and could be used to learn an embedding representation for each word as part of doing this prediction task. \nWe’ll see how to do neural networks in the next chapter, but word2vec is a much simpler model than the neural network language model, in two ways. First, word2vec simplifies the task (making it binary classification instead of word prediction). Second, word2vec simplifies the architecture (training a logistic regression classifier instead of a multi-layer neural network with hidden layers that demand more sophisticated training algorithms). The intuition of skip-gram is: \n1. Treat the target word and a neighboring context word as positive examples.   \n2. Randomly sample other words in the lexicon to get negative samples.   \n3. Use logistic regression to train a classifier to distinguish those two cases.   \n4. Use the learned weights as the embeddings. \n6.8.1 The classifier \nLet’s start by thinking about the classification task, and then turn to how to train. Imagine a sentence like the following, with a target word apricot, and assume we’re using a window of $pm 2$ context words: \nlemon, a [tablespoon of apricot jam, a] pinch c1 c2 w c3 c4 \nOur goal is to train a classifier such that, given a tuple $scriptstyle ( w , c )$ of a target word $w$ paired with a candidate context word $c$ (for example (apricot, jam), or perhaps (apricot, aardvark)) it will return the probability that $c$ is a real context word (true for jam, false for aardvark): \nThe probability that word $c$ is not a real context word for $w$ is just 1 minus Eq. 6.24: \nHow does the classifier compute the probability $P ?$ The intuition of the skipgram model is to base this probability on embedding similarity: a word is likely to occur near the target if its embedding vector is similar to the target embedding. To compute similarity between these dense embeddings, we rely on the intuition that two vectors are similar if they have a high dot product (after all, cosine is just a normalized dot product). In other words: \n\nThe dot product $circled { times } .$ is not a probability, it’s just a number ranging from $- infty$ to $infty$ (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we’ll use the logistic or sigmoid function $sigma ( x )$ , the fundamental core of logistic regression: \nWe model the probability that word $c$ is a real context word for target word $w$ as: \nThe sigmoid function returns a number between 0 and 1, but to make it a probability we’ll also need the total probability of the two possible events $dot { } c$ is a context word, and $c$ isn’t a context word) to sum to 1. We thus estimate the probability that word $c$ is not a real context word for $w$ as: \nEquation 6.28 gives us the probability for one word, but there are many context words in the window. Skip-gram makes the simplifying assumption that all context words are independent, allowing us to just multiply their probabilities: \nIn summary, skip-gram trains a probabilistic classifier that, given a test target word $w$ and its context window of $L$ words $c _ { 1 : L }$ , assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. \nFig. 6.13 shows the intuition of the parameters we’ll need. Skip-gram actually stores two embeddings for each word, one for the word as a target, and one for the word considered as context. Thus the parameters we need to learn are two matrices W and C, each containing an embedding for every one of the $| V |$ words in the vocabulary V .6 Let’s now turn to learning these embeddings (which is the real goal of training this classifier in the first place). \n6.8.2 Learning skip-gram embeddings \nThe learning algorithm for skip-gram embeddings takes as input a corpus of text, and a chosen vocabulary size N. It begins by assigning a random embedding vector for each of the $mathbf { N }$ vocabulary words, and then proceeds to iteratively shift the embedding of each word $w$ to be more like the embeddings of words that occur nearby in texts, and less like the embeddings of words that don’t occur nearby. Let’s start by considering a single piece of training data: \nThis example has a target word $w$ (apricot), and 4 context words in the $L = pm 2$ window, resulting in 4 positive training instances (on the left below): \nFor training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter $k$ ). So for each of these $( w , c _ { p o s } )$ training instances we’ll create $k$ negative samples, each consisting of the target $w$ plus a ‘noise word’ $c _ { n e g }$ . A noise word is a random word from the lexicon, constrained not to be the target word $w$ . The right above shows the setting where $k = 2$ , so we’ll have 2 negative examples in the negative training set for each positive example $w , c _ { p o s }$ . \nThe noise words are chosen according to their weighted unigram frequency $p _ { alpha } ( w )$ , where $alpha$ is a weight. If we were sampling according to unweighted frequency $p ( w )$ , it would mean that with unigram probability $p ( { } ^ { bullet } t h e ^ { prime prime } )$ we would choose the word the as a noise word, with unigram probability $p ( mathfrak { cdots } a a r d nu a r k ^ { mathfrak { W } } )$ we would choose aardvark, and so on. But in practice it is common to set $alpha = . 7 5$ , i.e. use the",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Word2vec",
        "subsection": "The classifier",
        "subsubsection": "N/A"
    },
    {
        "content": "6.8.2 Learning skip-gram embeddings \nThe learning algorithm for skip-gram embeddings takes as input a corpus of text, and a chosen vocabulary size N. It begins by assigning a random embedding vector for each of the $mathbf { N }$ vocabulary words, and then proceeds to iteratively shift the embedding of each word $w$ to be more like the embeddings of words that occur nearby in texts, and less like the embeddings of words that don’t occur nearby. Let’s start by considering a single piece of training data: \nThis example has a target word $w$ (apricot), and 4 context words in the $L = pm 2$ window, resulting in 4 positive training instances (on the left below): \nFor training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter $k$ ). So for each of these $( w , c _ { p o s } )$ training instances we’ll create $k$ negative samples, each consisting of the target $w$ plus a ‘noise word’ $c _ { n e g }$ . A noise word is a random word from the lexicon, constrained not to be the target word $w$ . The right above shows the setting where $k = 2$ , so we’ll have 2 negative examples in the negative training set for each positive example $w , c _ { p o s }$ . \nThe noise words are chosen according to their weighted unigram frequency $p _ { alpha } ( w )$ , where $alpha$ is a weight. If we were sampling according to unweighted frequency $p ( w )$ , it would mean that with unigram probability $p ( { } ^ { bullet } t h e ^ { prime prime } )$ we would choose the word the as a noise word, with unigram probability $p ( mathfrak { cdots } a a r d nu a r k ^ { mathfrak { W } } )$ we would choose aardvark, and so on. But in practice it is common to set $alpha = . 7 5$ , i.e. use the \nweighting $p ^ { frac { 3 } { 4 } } ( w )$ : \nSetting $alpha = . 7 5$ gives better performance because it gives rare noise words slightly higher probability: for rare words, $P _ { alpha } ( w ) > P ( w )$ . To illustrate this intuition, it might help to work out the probabilities for an example with two events, $P ( a ) = . 9 9$ and $P ( b ) = . 0 1$ : \nGiven the set of positive and negative training instances, and an initial set of embeddings, the goal of the learning algorithm is to adjust those embeddings to \n• Maximize the similarity of the target word, context word pairs $( w , c _ { p o s } )$ drawn from the positive examples • Minimize the similarity of the $left( w , c _ { n e g } right)$ pairs from the negative examples. \nIf we consider one word/context pair (w, cpos) with its k noise words cneg ...cnegk , we can express these two goals as the following loss function $L$ to be minimized (hence the −); here the first term expresses that we want the classifier to assign the real context word $c _ { p o s }$ a high probability of being a neighbor, and the second term expresses that we want to assign each of the noise words $c _ { n e g _ { i } }$ a high probability of being a non-neighbor, all multiplied because we assume independence: \nThat is, we want to maximize the dot product of the word with the actual context words, and minimize the dot products of the word with the $k$ negative sampled nonneighbor words. \nWe minimize this loss function using stochastic gradient descent. Fig. 6.14 shows the intuition of one step of learning. \nTo get the gradient, we need to take the derivative of Eq. 6.34 with respect to the different embeddings. It turns out the derivatives are the following (we leave the \nproof as an exercise at the end of the chapter): \nThe update equations going from time step $t$ to $t + 1$ in stochastic gradient descent are thus: \nJust as in logistic regression, then, the learning algorithm starts with randomly initialized $boldsymbol { mathsf { w } }$ and C matrices, and then walks through the training corpus using gradient descent to move $boldsymbol { mathsf { w } }$ and C so as to maximize the objective in Eq. 6.34 by making the updates in (Eq. 6.39)-(Eq. 6.40). \nRecall that the skip-gram model learns two separate embeddings for each word $i$ : the target embedding $pmb { w } _ { i }$ and the context embedding $mathbf { c } _ { i }$ , stored in two matrices, the target matrix $boldsymbol { mathsf { w } }$ and the context matrix C. It’s common to just add them together, representing word $i$ with the vector $pmb { w } _ { i } + pmb { c } _ { i }$ . Alternatively we can throw away the C matrix and just represent each word $i$ by the vector $pmb { w } _ { i }$ . \nAs with the simple count-based methods like tf-idf, the context window size $L$ affects the performance of skip-gram embeddings, and experiments often tune the parameter $L$ on a devset. \n6.8.3 Other kinds of static embeddings \nfasttext \nThere are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus. A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols $<$ and $>$ added to each word. For example, with $n = 3$ the word where would be represented by the sequence <where> plus the character n-grams: \n<wh, whe, her, ere, re> \nThen a skipgram embedding is learned for each constituent n-gram, and the word where is represented by the sum of all of the embeddings of its constituent n-grams. Unknown words can then be presented only by the sum of the constituent n-grams. A fastMtoedxeltinogpWeonrd- sMoeuanricneg UlisibnrgaLreyx, cianl cClo-uOdcicnurgrenpcretrained embeddings for 157 languages, is available at https://fasttext.cc. \nAnother very widely used static embedding model is GloVe (Pennington et al., 2014), short for Global Vectors, because the model is based on capturing global corpus statistics. HFIGRNAlNCoEVe is based on ratios of probabilities from the word-word cooccurrence matrix,EcURoOPmE bining the intuitions of count-based models like PPMI while also capturing the lAiMEnRIeCAar structures used by methods like word2vec. \nIt turns out that dMeOSnCsOWe embeddings like word2vec actually have an elegant mathematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly HOoAKWYpAOItIimizing a shifted version of a PPMI matrix (Levy and Goldberg, 2014c). \nDO6CGAT.9 Visualizing Embeddings \n“I see well in many dimensions as long as the dimensions are around two.” The late economist Martin Shubik \naVliinsgufoarltihzriene gnouenmclbaseseds.dings is an important goal in helping understand, apply, and improve these models of word meaning. But how can we visualize a (for example) 100-dimensional vector? \nThe simplest way to visualize the meaning of a word $w$ embedded in a space is to list the most similar words to $w$ by sorting the vectors for all words in the vocabulary by their cosine with the vector for $w$ . For example the 7 closest words to frog using the GloVe embeddings are: frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (Pennington et al., 2014). \nYet another visualization method is to use a clustering algorithm to show a hierarchical representation of which words are similar to others in the embedding space. The uncaptioned figure on the left uses hierarchical clustering of some embedding vectors for nouns as a visualization method (Rohde et al., 2006).",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Word2vec",
        "subsection": "Learning skip-gram embeddings",
        "subsubsection": "N/A"
    },
    {
        "content": "6.8.3 Other kinds of static embeddings \nfasttext \nThere are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus. A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols $<$ and $>$ added to each word. For example, with $n = 3$ the word where would be represented by the sequence <where> plus the character n-grams: \n<wh, whe, her, ere, re> \nThen a skipgram embedding is learned for each constituent n-gram, and the word where is represented by the sum of all of the embeddings of its constituent n-grams. Unknown words can then be presented only by the sum of the constituent n-grams. A fastMtoedxeltinogpWeonrd- sMoeuanricneg UlisibnrgaLreyx, cianl cClo-uOdcicnurgrenpcretrained embeddings for 157 languages, is available at https://fasttext.cc. \nAnother very widely used static embedding model is GloVe (Pennington et al., 2014), short for Global Vectors, because the model is based on capturing global corpus statistics. HFIGRNAlNCoEVe is based on ratios of probabilities from the word-word cooccurrence matrix,EcURoOPmE bining the intuitions of count-based models like PPMI while also capturing the lAiMEnRIeCAar structures used by methods like word2vec. \nIt turns out that dMeOSnCsOWe embeddings like word2vec actually have an elegant mathematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly HOoAKWYpAOItIimizing a shifted version of a PPMI matrix (Levy and Goldberg, 2014c). \nDO6CGAT.9 Visualizing Embeddings \n“I see well in many dimensions as long as the dimensions are around two.” The late economist Martin Shubik \naVliinsgufoarltihzriene gnouenmclbaseseds.dings is an important goal in helping understand, apply, and improve these models of word meaning. But how can we visualize a (for example) 100-dimensional vector? \nThe simplest way to visualize the meaning of a word $w$ embedded in a space is to list the most similar words to $w$ by sorting the vectors for all words in the vocabulary by their cosine with the vector for $w$ . For example the 7 closest words to frog using the GloVe embeddings are: frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (Pennington et al., 2014). \nYet another visualization method is to use a clustering algorithm to show a hierarchical representation of which words are similar to others in the embedding space. The uncaptioned figure on the left uses hierarchical clustering of some embedding vectors for nouns as a visualization method (Rohde et al., 2006).",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Word2vec",
        "subsection": "Other kinds of static embeddings",
        "subsubsection": "N/A"
    },
    {
        "content": "6.8.3 Other kinds of static embeddings \nfasttext \nThere are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus. A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols $<$ and $>$ added to each word. For example, with $n = 3$ the word where would be represented by the sequence <where> plus the character n-grams: \n<wh, whe, her, ere, re> \nThen a skipgram embedding is learned for each constituent n-gram, and the word where is represented by the sum of all of the embeddings of its constituent n-grams. Unknown words can then be presented only by the sum of the constituent n-grams. A fastMtoedxeltinogpWeonrd- sMoeuanricneg UlisibnrgaLreyx, cianl cClo-uOdcicnurgrenpcretrained embeddings for 157 languages, is available at https://fasttext.cc. \nAnother very widely used static embedding model is GloVe (Pennington et al., 2014), short for Global Vectors, because the model is based on capturing global corpus statistics. HFIGRNAlNCoEVe is based on ratios of probabilities from the word-word cooccurrence matrix,EcURoOPmE bining the intuitions of count-based models like PPMI while also capturing the lAiMEnRIeCAar structures used by methods like word2vec. \nIt turns out that dMeOSnCsOWe embeddings like word2vec actually have an elegant mathematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly HOoAKWYpAOItIimizing a shifted version of a PPMI matrix (Levy and Goldberg, 2014c). \nDO6CGAT.9 Visualizing Embeddings \n“I see well in many dimensions as long as the dimensions are around two.” The late economist Martin Shubik \naVliinsgufoarltihzriene gnouenmclbaseseds.dings is an important goal in helping understand, apply, and improve these models of word meaning. But how can we visualize a (for example) 100-dimensional vector? \nThe simplest way to visualize the meaning of a word $w$ embedded in a space is to list the most similar words to $w$ by sorting the vectors for all words in the vocabulary by their cosine with the vector for $w$ . For example the 7 closest words to frog using the GloVe embeddings are: frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (Pennington et al., 2014). \nYet another visualization method is to use a clustering algorithm to show a hierarchical representation of which words are similar to others in the embedding space. The uncaptioned figure on the left uses hierarchical clustering of some embedding vectors for nouns as a visualization method (Rohde et al., 2006). \nProbably the most common visualization method, however, is to project the 100 dimensions of a word down into 2 dimensions. Fig. 6.1 showed one such visualization, as does Fig. 6.16, using a projection method called t-SNE (van der Maaten and Hinton, 2008). \n6.10 Semantic properties of embeddings \nIn this section we briefly summarize some of the semantic properties of embeddings that have been studied. \nDifferent types of similarity or association: One parameter of vector semantic models that is relevant to both sparse tf-idf vectors and dense word2vec vectors is the size of the context window used to collect counts. This is generally between 1 and 10 words on each side of the target word (for a total context of 2-20 words). \nThe choice depends on the goals of the representation. Shorter context windows tend to lead to representations that are a bit more syntactic, since the information is coming from immediately nearby words. When the vectors are computed from short context windows, the most similar words to a target word $w$ tend to be semantically similar words with the same parts of speech. When vectors are computed from long context windows, the highest cosine words to a target word $w$ tend to be words that are topically related but not similar. \nFor example Levy and Goldberg (2014a) showed that using skip-gram with a window of $pm 2$ , the most similar words to the word Hogwarts (from the Harry Potter series) were names of other fictional schools: Sunnydale (from Buffy the Vampire Slayer) or Evernight (from a vampire series). With a window of $pm 5$ , the most similar words to Hogwarts were other words topically related to the Harry Potter series: Dumbledore, Malfoy, and half-blood. \nfirst-order co-occurrence \nIt’s also often useful to distinguish two kinds of similarity or association between words (Schu¨tze and Pedersen, 1993). Two words have first-order co-occurrence (sometimes called syntagmatic association) if they are typically nearby each other. Thus wrote is a first-order associate of book or poem. Two words have second-order co-occurrence (sometimes called paradigmatic association) if they have similar neighbors. Thus wrote is a second-order associate of words like said or remarked. \nparallelogram model \nAnalogy/Relational Similarity: Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to $b$ as $a ^ { * }$ is to what?. In such problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as grape is to , and must fill in the word vine. In the parallelogram model,#illu»strated in Fig. 6.15, the vector from the word apple to the word tree $( = { overrightarrow { mathrm { t r e e } } } - { overrightarrow { mathrm { a p p l e } } } )$ is added to the vector for grape (grape); the nearest word to that point is returned. \nIn early work with sparse embeddings, scholars showed that sparse vector models of meaning could solve such analogy problems (Turney and Littman, 2005), but the parallelogram method received more modern attention because of its success with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and G#ol d»berg 2014b, Pennington et al. 2014). For example, the result of the expression $( overrightarrow { mathrm { k i n g } } ) -$ $overrightarrow { mathrm { m a n } } + overrightarrow { mathrm { w o m a n } }$ is a vector c#l o se»to queen. Similarly, $overrightarrow { mathrm { P a r i s } } - overrightarrow { mathrm { F r a n c e } } + overrightarrow { mathrm { I t a l y } }$ y results in a vector that is close to Rome. The embedding model thus seems to be extracting representations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or even COMPARATIVE/SUPERLATIVE, as shown in Fig. 6.16 from GloVe.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Visualizing Embeddings",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "6.10.1 Embeddings and Historical Semantics \nEmbeddings can also be a useful tool for studying how meaning changes over time, by computing multiple embedding spaces, each from texts written in a particular time period. For example Fig. 6.17 shows a visualization of changes in meaning in English words over the last two centuries, computed by building separate embeddCiHngAsPpTacEeRs f5.  eDaYchN AdeMcaIdCeSfrOoCmIAhiLstRorEicPaRl EcoSrEpoNrTaAliTkIeOGNoSogOleF nW-gOraRmDs  (MLEinAeNtIaNl., 2012b) and the Corpus of Historical American English (Davies, 2012). \nA daft gay (1900s) B C awful (1850s) cirulaeastsses gay (1950s) bright /broadcast (1900s) appalling terrible awful (1900s) gay (1990s) awful (1990s) bbc broadcast (1990s) awfullweird \n6.11 Biwahs eatnhedy sEhifmt frboemdobdjiecntigves \nallocational harm \nIn addition to their ability to learn word meaning from text, embeddings, alas, also reproduce the implicit biases and stereotypes that were latent in the text. As t5h.e2.p2rior sCecotimonpjusttasthiowneadl, leimnbgeuddiisntgisc csatnurdouieghsly model relational similarity: ‘queen’ as the closest word to ‘king’ - ‘man’ $^ +$ ‘woman’ implies the analogy man:woman::king:queen. But these same embedding analogies also exhibit gender smtertehotdysp.e[s2. 0F]orusexlamtepnlte sBeomlaunktbiacsianeta ayls. s(2t0o1a6n)alfiynzde hoatw hweorcdlomsesatnoinccgsupbartoioand taond‘mnanr’ro-w‘covmerputtiemrep. [o1g1r3a]mumserr’ $^ +$ ‘cwoo-omcacnu’rrienncweorvde2cvtoercsetmobpeedrdfionrgms taraninuemdboern news text is ‘homemaker’, and that the embeddings similarly suggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. This could result in what Crawford (2017) and Blodgett et al. (2020) call an allocational harm, when a system alloicnaftoersmreastiounr-cbeass(ejdobesmobrecdrdeidnitg)suanfad rfloyutnoddtifhfaetresnetmgarnotuipcs.chFaonrgexsaumnpcloevaelrgeodribtyhmths tmheathuosde heamdbredadsiongasblaes apgareteomfeanstewairtch hfourmhainrijnugdpgomtentisa.  p1r2o9g]ranmdm[1e1rs9 oursdeo“cntoerusr might thus incorrectly downweight documents with women’s names. \nbias amplification \nIt turns out that embeddings don’t just reflect the statistics of their input, but also amplify bias; gendered terms become more gendered in embedding space than they were in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al. 2020), and biases are more exaggerated than in actual labor employment statistics (Garg et al., 2018). \nEmbeddings also encode the implicit associations that are a property of human reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Semantic properties of embeddings",
        "subsection": "Embeddings and Historical Semantics",
        "subsubsection": "N/A"
    },
    {
        "content": "6.10.1 Embeddings and Historical Semantics \nEmbeddings can also be a useful tool for studying how meaning changes over time, by computing multiple embedding spaces, each from texts written in a particular time period. For example Fig. 6.17 shows a visualization of changes in meaning in English words over the last two centuries, computed by building separate embeddCiHngAsPpTacEeRs f5.  eDaYchN AdeMcaIdCeSfrOoCmIAhiLstRorEicPaRl EcoSrEpoNrTaAliTkIeOGNoSogOleF nW-gOraRmDs  (MLEinAeNtIaNl., 2012b) and the Corpus of Historical American English (Davies, 2012). \nA daft gay (1900s) B C awful (1850s) cirulaeastsses gay (1950s) bright /broadcast (1900s) appalling terrible awful (1900s) gay (1990s) awful (1990s) bbc broadcast (1990s) awfullweird \n6.11 Biwahs eatnhedy sEhifmt frboemdobdjiecntigves \nallocational harm \nIn addition to their ability to learn word meaning from text, embeddings, alas, also reproduce the implicit biases and stereotypes that were latent in the text. As t5h.e2.p2rior sCecotimonpjusttasthiowneadl, leimnbgeuddiisntgisc csatnurdouieghsly model relational similarity: ‘queen’ as the closest word to ‘king’ - ‘man’ $^ +$ ‘woman’ implies the analogy man:woman::king:queen. But these same embedding analogies also exhibit gender smtertehotdysp.e[s2. 0F]orusexlamtepnlte sBeomlaunktbiacsianeta ayls. s(2t0o1a6n)alfiynzde hoatw hweorcdlomsesatnoinccgsupbartoioand taond‘mnanr’ro-w‘covmerputtiemrep. [o1g1r3a]mumserr’ $^ +$ ‘cwoo-omcacnu’rrienncweorvde2cvtoercsetmobpeedrdfionrgms taraninuemdboern news text is ‘homemaker’, and that the embeddings similarly suggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. This could result in what Crawford (2017) and Blodgett et al. (2020) call an allocational harm, when a system alloicnaftoersmreastiounr-cbeass(ejdobesmobrecdrdeidnitg)suanfad rfloyutnoddtifhfaetresnetmgarnotuipcs.chFaonrgexsaumnpcloevaelrgeodribtyhmths tmheathuosde heamdbredadsiongasblaes apgareteomfeanstewairtch hfourmhainrijnugdpgomtentisa.  p1r2o9g]ranmdm[1e1rs9 oursdeo“cntoerusr might thus incorrectly downweight documents with women’s names. \nbias amplification \nIt turns out that embeddings don’t just reflect the statistics of their input, but also amplify bias; gendered terms become more gendered in embedding space than they were in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al. 2020), and biases are more exaggerated than in actual labor employment statistics (Garg et al., 2018). \nEmbeddings also encode the implicit associations that are a property of human reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo\nrepresentational harm \nple’s associations between concepts (like ‘flowers’ or ‘insects’) and attributes (like ‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with which they label words in the various categories.7 Using such methods, people in the United States have been shown to associate African-American names with unpleasant words (more than European-American names), male names more with mathematics and female names with the arts, and old people’s names with unpleasant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan et al. (2017) replicated all these findings of implicit associations using GloVe vectors and cosine similarity instead of human latencies. For example African-American names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine with pleasant words. These problems with embeddings are an example of a representational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by a system demeaning or even ignoring some social groups. Any embedding-aware algorithm that made use of word sentiment could thus exacerbate bias against African Americans. \ndebiasing \nRecent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016, Zhao et al. 2017) or changing the training procedure (Zhao et al., 2018b). However, although these sorts of debiasing may reduce bias in embeddings, they do not eliminate it (Gonen and Goldberg, 2019), and this remains an open problem. \nHistorical embeddings are also being used to measure biases in the past. Garg et al. (2018) used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women’s names versus men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century. They found that the cosines correlate with the empirical historical percentages of women or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in 1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese ethnicity, correlates with the cosine between Chinese last names and those adjectives using embeddings trained on 1930s text. They also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence (‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We return in later chapters to this question about the role of bias in natural language processing. \n6.12 Evaluating Vector Models \nThe most important evaluation metric for vector models is extrinsic evaluation on tasks, i.e., using vectors in an NLP task and seeing whether this improves performance over some other model.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Bias and Embeddings",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "representational harm \nple’s associations between concepts (like ‘flowers’ or ‘insects’) and attributes (like ‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with which they label words in the various categories.7 Using such methods, people in the United States have been shown to associate African-American names with unpleasant words (more than European-American names), male names more with mathematics and female names with the arts, and old people’s names with unpleasant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan et al. (2017) replicated all these findings of implicit associations using GloVe vectors and cosine similarity instead of human latencies. For example African-American names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine with pleasant words. These problems with embeddings are an example of a representational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by a system demeaning or even ignoring some social groups. Any embedding-aware algorithm that made use of word sentiment could thus exacerbate bias against African Americans. \ndebiasing \nRecent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016, Zhao et al. 2017) or changing the training procedure (Zhao et al., 2018b). However, although these sorts of debiasing may reduce bias in embeddings, they do not eliminate it (Gonen and Goldberg, 2019), and this remains an open problem. \nHistorical embeddings are also being used to measure biases in the past. Garg et al. (2018) used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women’s names versus men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century. They found that the cosines correlate with the empirical historical percentages of women or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in 1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese ethnicity, correlates with the cosine between Chinese last names and those adjectives using embeddings trained on 1930s text. They also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence (‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We return in later chapters to this question about the role of bias in natural language processing. \n6.12 Evaluating Vector Models \nThe most important evaluation metric for vector models is extrinsic evaluation on tasks, i.e., using vectors in an NLP task and seeing whether this improves performance over some other model. \nNonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm’s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more difficult dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and including both concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. \nSlightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Section 18.5.3. The semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores. \nAnother task used for evaluation is the analogy task, discussed on page 125, where the system has to solve problems of the form a is to b as a* is to $b ^ { * }$ , given a, b, and $a ^ { * }$ and having to find $b ^ { * }$ (Turney and Littman, 2005). A number of sets of tuples have been created for this task, (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova et al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout::teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland), some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012). \nAll embedding algorithms suffer from inherent variability. For example because of randomness in the initialization and the random negative sampling, algorithms like word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings (Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is best practice to train multiple embeddings with bootstrap sampling over documents and average the results (Antoniak and Mimno, 2018). \n6.13 Summary \n• In vector semantics, a word is modeled as a vector—a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding. • Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary $V$ and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information), which is most common for for word-context matrices.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Evaluating Vector Models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm’s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more difficult dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and including both concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. \nSlightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Section 18.5.3. The semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores. \nAnother task used for evaluation is the analogy task, discussed on page 125, where the system has to solve problems of the form a is to b as a* is to $b ^ { * }$ , given a, b, and $a ^ { * }$ and having to find $b ^ { * }$ (Turney and Littman, 2005). A number of sets of tuples have been created for this task, (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova et al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout::teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland), some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012). \nAll embedding algorithms suffer from inherent variability. For example because of randomness in the initialization and the random negative sampling, algorithms like word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings (Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is best practice to train multiple embeddings with bootstrap sampling over documents and average the results (Antoniak and Mimno, 2018). \n6.13 Summary \n• In vector semantics, a word is modeled as a vector—a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding. • Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary $V$ and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information), which is most common for for word-context matrices.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm’s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more difficult dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and including both concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. \nSlightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Section 18.5.3. The semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores. \nAnother task used for evaluation is the analogy task, discussed on page 125, where the system has to solve problems of the form a is to b as a* is to $b ^ { * }$ , given a, b, and $a ^ { * }$ and having to find $b ^ { * }$ (Turney and Littman, 2005). A number of sets of tuples have been created for this task, (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova et al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout::teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland), some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012). \nAll embedding algorithms suffer from inherent variability. For example because of randomness in the initialization and the random negative sampling, algorithms like word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings (Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is best practice to train multiple embeddings with bootstrap sampling over documents and average the results (Antoniak and Mimno, 2018). \n6.13 Summary \n• In vector semantics, a word is modeled as a vector—a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding. • Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary $V$ and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information), which is most common for for word-context matrices.",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm’s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more difficult dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and including both concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. \nSlightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Section 18.5.3. The semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores. \nAnother task used for evaluation is the analogy task, discussed on page 125, where the system has to solve problems of the form a is to b as a* is to $b ^ { * }$ , given a, b, and $a ^ { * }$ and having to find $b ^ { * }$ (Turney and Littman, 2005). A number of sets of tuples have been created for this task, (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova et al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout::teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland), some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012). \nAll embedding algorithms suffer from inherent variability. For example because of randomness in the initialization and the random negative sampling, algorithms like word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings (Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is best practice to train multiple embeddings with bootstrap sampling over documents and average the results (Antoniak and Mimno, 2018). \n6.13 Summary \n• In vector semantics, a word is modeled as a vector—a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding. • Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary $V$ and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information), which is most common for for word-context matrices. \n\n• Dense vector models have dimensionality 50–1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are ‘likely to occur nearby in text’. This probability is computed from the dot product between the embeddings for the two words.   \n• Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.   \n• Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.   \n• Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors—a normalized dot product—is the most popular such metric. \nBibliographical and Historical Notes \nThe idea of vector semantics arose out of research in the 1950s in three distinct fields: linguistics, psychology, and computer science, each of which contributed a fundamental aspect of the model. \nThe idea that meaning is related to the distribution of words in context was widespread in linguistic theory of the 1950s, among distributionalists like Zellig Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos (1950) put it, \nmechanical indexing \nthe linguist’s “meaning” of a morpheme. . . is by definition the set of conditional probabilities of its occurrence in context with all other morphemes. \nThe idea that the meaning of a word might be modeled as a point in a multidimensional semantic space came from psychologists like Charles E. Osgood, who had been studying how people responded to the meaning of words by assigning values along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the meaning of a word in general could be modeled as a point in a multidimensional Euclidean space, and that the similarity of meaning between two words could be modeled as the distance between these points in the space. \nA final intellectual source in the 1950s and early 1960s was the field then called mechanical indexing, now known as information retrieval. In what became known as the vector space model for information retrieval (Salton 1971, Sparck Jones 1986), researchers demonstrated new ways to define the meaning of words in terms of vectors (Switzer, 1965), and refined methods for word similarity based on measures of statistical association between words like mutual information (Giuliano, 1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents could be represented in the same vector spaces used for words. \nSome of the philosophical underpinning of the distributional way of thinking came from the late writings of the philosopher Wittgenstein, who was skeptical of the possibility of building a completely formal theory of meaning definitions for each word, suggesting instead that “the meaning of a word is its use in the language” (Wittgenstein, 1953, PI 43). That is, instead of using some logical language to define each word, or drawing on denotations or truth values, Wittgenstein’s idea is that we should define a word by how it is used by people in speaking and understanding in their day-to-day interactions, thus prefiguring the movement toward embodied and experiential models in linguistics and NLP (Glenberg and Robertson 2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020). \n\nMore distantly related is the idea of defining words by a vector of discrete features, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992, Wierzbicka 1996). By the middle of the 20th century, beginning with the work of Hjelmslev (Hjelmslev, 1969) (originally 1943) and fleshed out in early models of generative grammar (Katz and Fodor, 1963), the idea arose of representing meaning with semantic features, symbols that represent some sort of primitive meaning. For example words like hen, rooster, or chick, have something in common (they all describe chickens) and something different (their age and sex), representable as: \nhen $^ +$ female, $^ +$ chicken, $^ +$ adult rooster -female, $^ +$ chicken, $^ +$ adult chick +chicken, -adult \nThe dimensions used by vector models of meaning to define words, however, are only abstractly related to this idea of a small fixed number of hand-built dimensions. Nonetheless, there has been some attempt to show that certain dimensions of embedding models do contribute some specific compositional aspect of meaning like these early semantic features. \nThe use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decomposition—SVD— is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000) morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schu¨tze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schu¨tze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schu¨tze (1992b). Schu¨tze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schu¨tze (1997a). \nA number of alternative matrix models followed on from the early SVD work, including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999). \nThe LSA community seems to have first used the word “embedding” in Landauer et al. (1997), in a variant of its mathematical meaning as a mapping from one space or mathematical structure to another. In LSA, the word embedding seems to have described the mapping from the space of sparse count vectors to the latent space of SVD dense vectors. Although the word thus originally meant the mapping from one space to another, it has metonymically shifted to mean the resulting dense vector in the latent space. and it is in this sense that we currently use the word. \nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). \nSee Manning et al. (2008) for a deeper understanding of the role of vectors in information retrieval, including how to compare queries with documents, more details on tf-idf, and issues of scaling to very large datasets. See Kim (2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful introductory linguistic text on lexical semantics. \nExercises \n7 \nNeural Networks and Neural Language Models \nfeedforward deep learning \n\n“[M]achines of this character can behave in a very complicated manner when the number of units is large.” \nAlan Turing (1948) “Intelligent Machines”, page 6 \nNeural networks are a fundamental computational tool for language processing, and a very old one. They are called neural because their origins lie in the McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic. But the modern use in language processing no longer draws on these early biological inspirations. \nInstead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). \nNeural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single ‘hidden layer’) can be shown to learn any function. \nNeural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for large scale problems that offer sufficient data to learn features automatically. \nIn this chapter we’ll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we’ll introduce many other aspects of neural models, such as recurrent neural networks and the Transformer (Chapter 9), contextual embeddings like BERT (Chapter 11), and encoder-decoder models and attention (Chapter 10).",
        "chapter": "Vector Semantics and Embeddings",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "7 \nNeural Networks and Neural Language Models \nfeedforward deep learning \n\n“[M]achines of this character can behave in a very complicated manner when the number of units is large.” \nAlan Turing (1948) “Intelligent Machines”, page 6 \nNeural networks are a fundamental computational tool for language processing, and a very old one. They are called neural because their origins lie in the McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic. But the modern use in language processing no longer draws on these early biological inspirations. \nInstead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). \nNeural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single ‘hidden layer’) can be shown to learn any function. \nNeural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for large scale problems that offer sufficient data to learn features automatically. \nIn this chapter we’ll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we’ll introduce many other aspects of neural models, such as recurrent neural networks and the Transformer (Chapter 9), contextual embeddings like BERT (Chapter 11), and encoder-decoder models and attention (Chapter 10). \n7.1 Units \nbias term \nThe building block of a neural network is a single computational unit. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output. \nAt its heart, a neural unit is taking a weighted sum of its inputs, with one additional term in the sum called a bias term. Given a set of inputs $x _ { 1 } . . . x _ { n }$ , a unit has a set of corresponding weights $w _ { 1 } . . . w _ { n }$ and a bias $b$ , so the weighted sum z can be represented as: \nvector \nOften it’s more convenient to express this weighted sum using vector notation; recall from linear algebra that a vector is, at heart, just a list or array of numbers. Thus we’ll talk about $z$ in terms of a weight vector $w$ , a scalar bias $b$ , and an input vector $x$ , and we’ll replace the sum with the convenient dot product: \nactivation \nAs defined in Eq. 7.2, $z$ is just a real valued number. \nFinally, instead of using $z$ , a linear function of $x$ , as the output, neural units apply a non-linear function $f$ to $z$ . We will refer to the output of this function as the activation value for the unit, $a$ . Since we are just modeling a single unit, the activation for the node is in fact the final output of the network, which we’ll generally call y. So the value $y$ is defined as: \nsigmoid \nWe’ll discuss three popular non-linear functions $f ( )$ below (the sigmoid, the tanh, and the rectified linear unit or ReLU) but it’s pedagogically convenient to start with the sigmoid function since we saw it in Chapter 5: \nThe sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output into the range $[ 0 , 1 ]$ , which is useful in squashing outliers toward 0 or 1. And it’s differentiable, which as we saw in Section 5.10 will be handy for learning. \nSubstituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit: \nFig. 7.2 shows a final schematic of a basic neural unit. In this example the unit takes 3 input values $x _ { 1 } , x _ { 2 }$ , and $x _ { 3 }$ , and computes a weighted sum, multiplying each value by a weight $( w _ { 1 } , w _ { 2 }$ , and $w _ { 3 }$ , respectively), adds them to a bias term $b$ , and then passes the resulting sum through a sigmoid function to result in a number between 0 and 1. \nLet’s walk through an example just to get an intuition. Let’s suppose we have a unit with the following weight vector and bias: \nWhat would this unit do with the following input vector: \nThe resulting output $y$ would be: \ntanh \nIn practice, the sigmoid is not commonly used as an activation function. A function that is very similar but almost always better is the tanh function shown in Fig. 7.3a; tanh is a variant of the sigmoid that ranges from $^ { - 1 }$ to $+ 1$ : \nThe simplest activation function, and perhaps the most commonly used, is the recReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as $z$ when $z$ is positive, and 0 otherwise: \nThese activation functions have different properties that make them useful for different language applications or network architectures. For example, the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean. The rectifier function, on the other hand has nice properties that \nsaturated \nvanishing gradient \nresult from it being very close to linear. In the sigmoid or tanh functions, very high values of $z$ result in values of $y$ that are saturated, i.e., extremely close to 1, and have derivatives very close to 0. Zero derivatives cause problems for learning, because as we’ll see in Section 7.6, we’ll train networks by propagating an error signal backwards, multiplying gradients (partial derivatives) from each layer of the network; gradients that are almost 0 cause the error signal to get smaller and smaller until it is too small to be used for training, a problem called the vanishing gradient problem. Rectifiers don’t have this problem, since the derivative of ReLU for high values of $z$ is 1 rather than very close to 0. \n7.2 The XOR problem \nEarly in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these units into larger networks. \nOne of the most clever demonstrations of the need for multi-layer networks was the proof by Minsky and Papert (1969) that a single neural unit cannot compute some very simple functions of its input. Consider the task of computing elementary logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are the truth tables for those functions: \nperceptron \nThis example was first shown for the perceptron, which is a very simple neural unit that has a binary output and does not have a non-linear activation function. The output $y$ of a perceptron is 0 or 1, and is computed as follows (using the same weight $pmb { w }$ , input $pmb {  x }$ , and bias $b$ as in Eq. 7.2):",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Units",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "It’s very easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs; Fig. 7.4 shows the necessary weights. \nIt turns out, however, that it’s not possible to build a perceptron to compute logical XOR! (It’s worth spending a moment to give it a try!) \nThe intuition behind this important result relies on understanding that a perceptron is a linear classifier. For a two-dimensional input $x _ { 1 }$ and $x _ { 2 }$ , the perceptron equation, $w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } + b = 0$ is the equation of a line. (We can see this by putting it in the standard linear format: $x _ { 2 } = ( - w _ { 1 } / w _ { 2 } ) x _ { 1 } + ( - b / w _ { 2 } )$ .) This line acts as a decision boundary in two-dimensional space in which the output 0 is assigned to all inputs lying on one side of the line, and the output 1 to all input points lying on the other side of the line. If we had more than 2 inputs, the decision boundary becomes a hyperplane instead of a line, but the idea is the same, separating the space into two categories. \nFig. 7.5 shows the possible logical inputs $( mathbb { 0 } mathbb { 0 } , mathbb { 0 } 1 , 1 mathbb { 0 }$ , and 11) and the line drawn by one possible set of parameters for an AND and an OR classifier. Notice that there is simply no way to draw a line that separates the positive cases of XOR (01 and 10) from the negative cases (00 and 11). We say that XOR is not a linearly separable function. Of course we could draw a boundary with a curve, or some other function, but not a single line. \n7.2.1 The solution: neural networks \nWhile the XOR function cannot be calculated by a single perceptron, it can be calculated by a layered network of perceptron units. Rather than see this with networks of simple perceptrons, however, let’s see how to compute XOR using two layers of ReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a figure with the input being processed by two layers of neural units. The middle layer (called $h$ ) has two units, and the output layer (called $y$ ) has one unit. A set of weights and biases are shown for each ReLU that correctly computes the XOR function. \nLet’s walk through what happens with the input $mathbf { x } = [ 0 , 0 ]$ . If we multiply each input value by the appropriate weight, sum, and then add the bias $b$ , we get the vector [0, -1], and we then apply the rectified linear transformation to give the output of the $mathbf { h }$ layer as [0, 0]. Now we once again multiply by the weights, sum, and add the bias (0 in this case) resulting in the value 0. The reader should work through the computation of the remaining 3 possible input pairs to see that the resulting $y$ values are 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1]. \nIt’s also instructive to look at the intermediate results, the outputs of the two hidden nodes $h _ { 1 }$ and $h _ { 2 }$ . We showed in the previous paragraph that the $mathbf { h }$ vector for the inputs $mathbf { x } = [ 0 , 0 ]$ was [0, 0]. Fig. 7.7b shows the values of the $mathbf { h }$ layer for all 4 inputs. Notice that hidden representations of the two input points $mathbf { x } = [ 0 , 1 ]$ and ${ bf x } = [ 1 , 0 ]$ (the two cases with XOR output ${ bf mu } = 1 { bf rho }$ ) are merged to the single point $boldsymbol { mathsf { h } } =$ [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. \nIn this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.6. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "The XOR problem",
        "subsection": "The solution: neural networks",
        "subsubsection": "N/A"
    },
    {
        "content": "7.3.1 More details on feedforward networks \nLet’s now set up some notation to make it easier to talk about deeper networks of depth more than 2. We’ll use superscripts in square brackets to mean layer numbers, starting at 0 for the input layer. $mathbf { S o } mathbf { overline { { w } } } ^ { [ 1 ] }$ will mean the weight matrix for the (first) hidden layer, and $mathbf { b } ^ { [ 1 ] }$ will mean the bias vector for the (first) hidden layer. $n _ { j }$ will mean the number of units at layer $j$ . We’ll use $g ( cdot )$ to stand for the activation function, which will tend to be ReLU or tanh for intermediate layers and softmax for output layers. We’ll use $mathbf { a } ^ { [ i ] }$ to mean the output from layer $i$ , and $pmb { z } ^ { [ i ] }$ to mean the combination of weights and biases $mathsf { mathbf { W } } ^ { [ i ] } mathsf { mathbf { a } } ^ { [ i - 1 ] } + mathsf { mathbf { b } } ^ { [ i ] }$ . The 0th layer is for inputs, so we’ll refer to the inputs $pmb { x }$ more generally as $mathbf { a } ^ { [ 0 ] }$ . \nThus we can re-represent our 2-layer net from Eq. 7.12 as follows: \nNote that with this notation, the equations for the computation done at each layer are the same. The algorithm for computing the forward step in an $mathfrak { n }$ -layer feedforward network, given the input vector $a ^ { [ 0 ] }$ is thus simply: \nThe activation functions $g ( cdot )$ are generally different at the final layer. Thus $g ^ { [ 2 ] }$ might be softmax for multinomial classification or sigmoid for binary classification, while ReLU or tanh might be the activation function $g ( cdot )$ at the internal layers. \nThe need for non-linear activation functions One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network. Let’s see why this is true. Imagine the first two layers of such a network of purely linear layers: \nWe can rewrite the function that the network is computing as: \nThis generalizes to any number of layers. So without non-linear activation functions, a multilayer network is just a notational variant of a single layer network with a different set of weights, and we lose all the representational power of multilayer networks. \nReplacing the bias unit In describing networks, we will often use a slightly simplified notation that represents exactly the same function without referring to an explicit bias node $b$ . Instead, we add a dummy node ${ bf a } _ { 0 }$ to each layer whose value will always be 1. Thus layer 0, the input layer, will have a dummy node $mathbf { a } _ { 0 } ^ { [ 0 ] } = 1$ , layer 1 will have a[01 = 1, and so on. This dummy node still has an associated weight, and that weight represents the bias value $b$ . For example instead of an equation like \nwe’ll use: \nBut now instead of our vector $pmb { x }$ having $n _ { 0 }$ values: $mathbf { x } = mathbf { x } _ { 1 } , ldots , mathbf { x } _ { n _ { 0 } }$ , it will have $n _ { 0 } +$ 1 values, with a new 0th dummy value $mathbf { x } _ { 0 } = 1$ : $mathbf { x } = mathbf { x } _ { 0 } , ldots , mathbf { x } _ { n _ { 0 } }$ . And instead of computing each $mathbf { h } _ { j }$ as follows: \nwe’ll instead use: \nwhere the value $boldsymbol { mathsf { W } } _ { j 0 }$ replaces what had been $mathbf { b } _ { j }$ . Fig. 7.9 shows a visualization. \nWe’ll continue showing the bias as $b$ when we go over the learning algorithm in Section 7.6, but then we’ll switch to this simplified notation without explicit bias terms for the rest of the book. \n7.4 Feedforward networks for NLP: Classification \nLet’s see how to apply feedforward networks to NLP tasks! In this section we’ll look at classification tasks like sentiment analysis; in the next section we’ll introduce neural language modeling.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Feedforward Neural Networks",
        "subsection": "More details on feedforward networks",
        "subsubsection": "N/A"
    },
    {
        "content": "Replacing the bias unit In describing networks, we will often use a slightly simplified notation that represents exactly the same function without referring to an explicit bias node $b$ . Instead, we add a dummy node ${ bf a } _ { 0 }$ to each layer whose value will always be 1. Thus layer 0, the input layer, will have a dummy node $mathbf { a } _ { 0 } ^ { [ 0 ] } = 1$ , layer 1 will have a[01 = 1, and so on. This dummy node still has an associated weight, and that weight represents the bias value $b$ . For example instead of an equation like \nwe’ll use: \nBut now instead of our vector $pmb { x }$ having $n _ { 0 }$ values: $mathbf { x } = mathbf { x } _ { 1 } , ldots , mathbf { x } _ { n _ { 0 } }$ , it will have $n _ { 0 } +$ 1 values, with a new 0th dummy value $mathbf { x } _ { 0 } = 1$ : $mathbf { x } = mathbf { x } _ { 0 } , ldots , mathbf { x } _ { n _ { 0 } }$ . And instead of computing each $mathbf { h } _ { j }$ as follows: \nwe’ll instead use: \nwhere the value $boldsymbol { mathsf { W } } _ { j 0 }$ replaces what had been $mathbf { b } _ { j }$ . Fig. 7.9 shows a visualization. \nWe’ll continue showing the bias as $b$ when we go over the learning algorithm in Section 7.6, but then we’ll switch to this simplified notation without explicit bias terms for the rest of the book. \n7.4 Feedforward networks for NLP: Classification \nLet’s see how to apply feedforward networks to NLP tasks! In this section we’ll look at classification tasks like sentiment analysis; in the next section we’ll introduce neural language modeling. \nLet’s begin with a simple 2-layer sentiment classifier. You might imagine taking our logistic regression classifier from Chapter 5, which corresponds to a 1-layer network, and just adding a hidden layer. The input element $mathbf { x } _ { i }$ could be scalar features like those in Fig. 5.2, e.g., $mathbf { x } _ { 1 } =$ count(words $in$ doc), $mathbf { x } _ { 2 } = $ count(positive lexicon words $ r in mathrm { d o c } ,$ ), $mathbf { x } _ { 3 } = 1$ if “no” $in$ doc, and so on. And the output layer $hat { pmb y }$ could have two nodes (one each for positive and negative), or 3 nodes (positive, negative, neutral), in which case $hat { bf y } _ { 1 }$ would be the estimated probability of positive sentiment, $hat { bf y } _ { 2 }$ the probability of negative and $hat { bf y } _ { 3 }$ the probability of neutral. The resulting equations would be just what we saw above for a 2-layer network (as always, we’ll continue to use the $sigma$ to stand for any non-linearity, whether sigmoid, ReLU or other). \nMost neural NLP applications do something different, however. Instead of using hand-built human-engineered features as the input to our classifier, we draw on deep learning’s ability to learn features from the data by representing words as embeddings, like the word2vec or GloVe embeddings we saw in Chapter 6. There are various ways to represent an input for classification. One simple baseline is to apply some sort of pooling function to the embeddings of all the words in the input. For example, for a text with $n$ input words/tokens $w _ { 1 } , . . . , w _ { n }$ , we can turn the $n$ embeddings $mathbf { e } ( w _ { 1 } ) , . . . , mathbf { e } ( w _ { n } )$ (each of dimensionality $d$ ) into a single embedding also of dimensionality $d$ by just summing the embeddings, or by taking their mean (summing and then dividing by $n$ ): \nThere are many other options, like taking the element-wise max. The element-wise max of a set of $n$ vectors is a new vector whose kth element is the max of the kth elements of all the $n$ vectors. Here are the equations for this classifier assuming mean pooling; the architecture is sketched in Fig. 7.11: \nWhile Eq. 7.21 shows how to a classify a single example $x$ , in practice we want to efficiently classify an entire test set of $m$ examples. We do this by vectoring the process, just as we saw with logistic regression; instead of using for-loops to go through each example, we’ll use matrix multiplication to do the entire computation of an entire test set at once. First, we pack all the input feature vectors for each input $x$ into a single input matrix $pmb { x }$ , with each row $i$ a row vector consisting of the pooled embedding for input example $x ^ { ( i ) }$ (i.e., the vector $mathbf { x } ^ { ( i ) }$ ). If the dimensionality of our pooled input embedding is $d$ , $pmb { x }$ will be a matrix of shape $[ m times d ]$ . \nWe will then need to slightly modify Eq. 7.21. $boldsymbol { mathsf { x } }$ is of shape $[ m times d ]$ and $boldsymbol { mathsf { w } }$ is of shape $[ d _ { h } times d ]$ , so we’ll have to reorder how we multiply $pmb { x }$ and $boldsymbol { mathsf { W } }$ and transpose $boldsymbol { mathsf { w } }$ so they correctly multiply to yield a matrix $H$ of shape $[ m times d _ { h } ]$ . The bias vector b from Eq. 7.21 of shape $[ 1 times d _ { h } ]$ will now have to be replicated into a matrix of shape $[ m times d _ { h } ]$ . We’ll need to similarly reorder the next step and transpose U. Finally, our output matrix $hat { pmb { mathsf { Y } } }$ will be of shape $[ m times 3 ]$ (or more generally $[ m times d _ { o } ]$ , where $d _ { o }$ is the number of output classes), with each row $i$ of our output matrix $hat { pmb { mathsf { Y } } }$ consisting of the output vector $bar { pmb y } ^ { ( i ) }$ .‘ Here are the final equations for computing the output class distribution for an entire test set: \nThe idea of using word2vec or GloVe embeddings as our input representation— and more generally the idea of relying on another algorithm to have already learned \npretraining \nan embedding representation for our input words—is called pretraining. Using pretrained embedding representations, whether simple static word embeddings like word2vec or the much more powerful contextual embeddings we’ll introduce in Chapter 11, is one of the central ideas of deep learning. (It’s also, possible, however, to train the word embeddings as part of an NLP task; we’ll talk about how to do this in Section 7.7 in the context of the neural language modeling task.) \n7.5 Feedforward Neural Language Modeling \nAs our second application of feedforward networks, let’s consider language modeling: predicting upcoming words from prior word context. Neural language modeling is an important NLP task in itself, and it plays a role in many important algorithms for tasks like machine translation, summarization, speech recognition, grammar correction, and dialogue. We’ll describe simple feedforward neural language models, first introduced by Bengio et al. (2003). While modern neural language models use more powerful architectures like the recurrent nets or transformer networks to be introduced in Chapter 9, the feedforward language model introduces many of the important concepts of neural language modeling. \nNeural language models have many advantages over the n-gram language models of Chapter 3. Compared to n-gram models, neural language models can handle much longer histories, can generalize better over contexts of similar words, and are more accurate at word-prediction. On the other hand, neural net language models are much more complex, are slower and need more energy to train, and are less interpretable than n-gram models, so for many (especially smaller) tasks an n-gram language model is still the right tool. \nA feedforward neural LM is a feedforward network that takes as input at time t a representation of some number of previous words $( w _ { t - 1 } , w _ { t - 2 }$ , etc.) and outputs a probability distribution over possible next words. Thus—like the n-gram LM—the feedforward neural LM approximates the probability of a word given the entire prior context $P ( w _ { t } | w _ { 1 : t - 1 } )$ by approximating based on the $N - 1$ previous words: \nIn the following examples we’ll use a 4-gram example, so we’ll show a net to estimate the probability $P ( w _ { t } = i | w _ { t - 3 } , w _ { t - 2 } , w _ { t - 1 } )$ . \nNeural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in $mathfrak { n }$ -gram language models. Using embeddings allows neural language models to generalize better to unseen data. For example, suppose we’ve seen this sentence in training: \nI have to make sure that the cat gets fed. \nbut have never seen the words “gets fed” after the word “dog”. Our test set has the prefix “I forgot to make sure that the dog gets”. What’s the next word? An n-gram language model will predict “fed” after “that the cat gets”, but not after “that the dog gets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will be able to generalize from the “cat” context to assign a high enough probability to “fed” even after seeing “dog”. \n7.5.1 Forward inference in the neural language model \nLet’s walk through forward inference or decoding for neural language models.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Feedforward networks for NLP: Classification",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "pretraining \nan embedding representation for our input words—is called pretraining. Using pretrained embedding representations, whether simple static word embeddings like word2vec or the much more powerful contextual embeddings we’ll introduce in Chapter 11, is one of the central ideas of deep learning. (It’s also, possible, however, to train the word embeddings as part of an NLP task; we’ll talk about how to do this in Section 7.7 in the context of the neural language modeling task.) \n7.5 Feedforward Neural Language Modeling \nAs our second application of feedforward networks, let’s consider language modeling: predicting upcoming words from prior word context. Neural language modeling is an important NLP task in itself, and it plays a role in many important algorithms for tasks like machine translation, summarization, speech recognition, grammar correction, and dialogue. We’ll describe simple feedforward neural language models, first introduced by Bengio et al. (2003). While modern neural language models use more powerful architectures like the recurrent nets or transformer networks to be introduced in Chapter 9, the feedforward language model introduces many of the important concepts of neural language modeling. \nNeural language models have many advantages over the n-gram language models of Chapter 3. Compared to n-gram models, neural language models can handle much longer histories, can generalize better over contexts of similar words, and are more accurate at word-prediction. On the other hand, neural net language models are much more complex, are slower and need more energy to train, and are less interpretable than n-gram models, so for many (especially smaller) tasks an n-gram language model is still the right tool. \nA feedforward neural LM is a feedforward network that takes as input at time t a representation of some number of previous words $( w _ { t - 1 } , w _ { t - 2 }$ , etc.) and outputs a probability distribution over possible next words. Thus—like the n-gram LM—the feedforward neural LM approximates the probability of a word given the entire prior context $P ( w _ { t } | w _ { 1 : t - 1 } )$ by approximating based on the $N - 1$ previous words: \nIn the following examples we’ll use a 4-gram example, so we’ll show a net to estimate the probability $P ( w _ { t } = i | w _ { t - 3 } , w _ { t - 2 } , w _ { t - 1 } )$ . \nNeural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in $mathfrak { n }$ -gram language models. Using embeddings allows neural language models to generalize better to unseen data. For example, suppose we’ve seen this sentence in training: \nI have to make sure that the cat gets fed. \nbut have never seen the words “gets fed” after the word “dog”. Our test set has the prefix “I forgot to make sure that the dog gets”. What’s the next word? An n-gram language model will predict “fed” after “that the cat gets”, but not after “that the dog gets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will be able to generalize from the “cat” context to assign a high enough probability to “fed” even after seeing “dog”. \n7.5.1 Forward inference in the neural language model \nLet’s walk through forward inference or decoding for neural language models. \nForward inference is the task, given an input, of running a forward pass on the network to produce a probability distribution over possible outputs, in this case next words. \nWe first represent each of the $N$ previous words as a one-hot vector of length $| V |$ , i.e., with one dimension for each word in the vocabulary. A one-hot vector is a vector that has one element equal to 1—in the dimension corresponding to that word’s index in the vocabulary— while all the other elements are set to zero. Thus in a one-hot representation for the word “toothpaste”, supposing it is $V _ { 5 }$ , i.e., index 5 in the vocabulary, $x _ { 5 } = 1$ , and $x _ { i } = 0 ~ forall i neq 5$ , as shown here: \nThe feedforward neural language model (sketched in Fig. 7.13) has a moving window that can see $mathbf { N }$ words into the past. We’ll let $mathbf { N }$ equal 3, so the 3 words $w _ { t - 1 } , w _ { t - 2 }$ , and $w _ { t - 3 }$ are each represented as a one-hot vector. We then multiply these one-hot vectors by the embedding matrix E. The embedding weight matrix E has a column for each word, each a column vector of $d$ dimensions, and hence has dimensionality $d times lvert V rvert$ . Multiplying by a one-hot vector that has only one non-zero element $x _ { i } = 1$ simply selects out the relevant column vector for word $i$ , resulting in the embedding for word $i$ , as shown in Fig. 7.12. \nThe 3 resulting embedding vectors are concatenated to produce e, the embedding layer. This is followed by a hidden layer and an output layer whose softmax produces a probability distribution over words. For example $y _ { 4 2 }$ , the value of output node 42, is the probability of the next word $w _ { t }$ being $V _ { 4 2 }$ , the vocabulary word with index 42 (which is the word ‘fish’ in our example). \nHere’s the algorithm in detail for our mini example: \n1. Select three embeddings from E: Given the three previous words, we look up their indices, create 3 one-hot vectors, and then multiply each by the embedding matrix E. Consider $w _ { t - 3 }$ . The one-hot vector for ‘for’ (index 35) is multiplied by the embedding matrix E, to give the first part of the first hidden layer, the embedding layer. Since each column of the input matrix E is an embedding for a word, and the input is a one-hot column vector $mathbf { x _ { i } }$ for word $V _ { i }$ , the embedding layer for input $w$ will be $pmb { mathsf { E x } } _ { mathrm { i } } = mathbf { e } _ { mathrm { i } }$ , the embedding for word $i$ . We now concatenate the three embeddings for the three context words to produce the embedding layer e.   \n2. Multiply by W: We multiply by $W$ (and add $b$ ) and pass through the ReLU (or other) activation function to get the hidden layer $h$ .   \n3. Multiply by U: $h$ is now multiplied by $U$   \n4. Apply softmax: After the softmax, each node $i$ in the output layer estimates the probability $P ( w _ { t } = i | w _ { t - 1 } , w _ { t - 2 } , w _ { t - 3 } )$ \nIn summary, the equations for a neural language model with a window size of 3, given one-hot input vectors for each input context word, are: \nNote that we formed the embedding layer e by concatenating the 3 embeddings for the three context vectors; we’ll often use semicolons to mean concatenation of vectors. \nIn the next section we’ll introduce a general algorithm for training neural networks, and then return to how to specifically train the neural language model in Section 7.7. \n7.6 Training Neural Nets \nA feedforward neural net is an instance of supervised machine learning in which we know the correct output $y$ for each observation $x$ . What the system produces, via Eq. 7.13, is $hat { y }$ , the system’s estimate of the true $y$ . The goal of the training procedure is to learn parameters $boldsymbol { mathsf { w } } ^ { [ i ] }$ and $mathbf { b } ^ { [ i ] }$ for each layer $i$ that make $hat { y }$ for each training observation as close as possible to the true $y$ .",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Feedforward Neural Language Modeling",
        "subsection": "Forward inference in the neural language model",
        "subsubsection": "N/A"
    },
    {
        "content": "In general, we do all this by drawing on the methods we introduced in Chapter 5 for logistic regression, so the reader should be comfortable with that chapter before proceeding. \nFirst, we’ll need a loss function that models the distance between the system output and the gold output, and it’s common to use the loss function used for logistic regression, the cross-entropy loss. \nSecond, to find the parameters that minimize this loss function, we’ll use the gradient descent optimization algorithm introduced in Chapter 5. \nThird, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual $w$ or $b$ . But for neural networks, with millions of parameters in many layers, it’s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. \n7.6.1 Loss function \nThe cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.22: \nIf we are using the network to classify into 3 or more classes, the loss function is exactly the same as the loss for multinomial regression that we saw in Chapter 5 on page 98. Let’s briefly summarize the explanation here for convenience. First, when we have more than 2 classes we’ll need to represent both $pmb {  y }$ and $hat { pmb y }$ as vectors. Let’s assume we’re doing hard classification, where only one class is the correct one. The true label $pmb {  y }$ is then a vector with $K$ elements, each corresponding to a class, with $mathbf { y } _ { c } = 1$ if the correct class is $c$ , with all other elements of $pmb {  y }$ being 0. Recall that a vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector. And our classifier will produce an estimate vector with $K$ elements $hat { pmb y }$ , each element $hat { mathbf { y } } _ { k }$ of which represents the estimated probability $p ( mathbf { y } _ { k } = 1 | mathbf { x } )$ . \nThe loss function for a single example $pmb { x }$ is the negative sum of the logs of the $K$ output classes, each weighted by their probability $y _ { k }$ : \nWe can simplify this equation further; let’s first rewrite the equation using the function $mathbb { 1 } { }$ which evaluates to 1 if the condition in the brackets is true and to 0 otherwise. This makes it more obvious that the terms in the sum in Eq. 7.26 will be 0 except for the term corresponding to the true class for which $mathbf { y } _ { k } = 1$ : \nIn other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss: \nPlugging in the softmax formula from Eq. 7.9, and with $K$ the number of classes: \n7.6.2 Computing the Gradient \nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.29 (and derived in Section 5.10): \nOr for a network with one weight layer and softmax output ( $dot { } = mathbf { nabla }$ multinomial logistic regression), we could use the derivative of the softmax loss from Eq. 5.47, shown for a particular weight $pmb { w } _ { k }$ and input $mathbf { x } _ { i }$ \nBut these derivatives only give correct updates for one weight layer: the last one! For deep networks, computing the gradients for each weight is much more complex, since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network, even though the loss is computed only at the very end of the network. \nThe solution to computing this gradient is an algorithm called error backpropagation or backprop (Rumelhart et al., 1986). While backprop was invented specially for neural networks, it turns out to be the same as a more general procedure called backward differentiation, which depends on the notion of computation graphs. Let’s see how that works in the next subsection. \n7.6.3 Computation Graphs \nA computation graph is a representation of the process of computing a mathematical expression, in which the computation is broken down into separate operations, each of which is modeled as a node in a graph. \nConsider computing the function $L ( a , b , c ) = c ( a + 2 b )$ . If we make each of the component addition and multiplication operations explicit, and add names $d$ and $e$ )",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Training Neural Nets",
        "subsection": "Loss function",
        "subsubsection": "N/A"
    },
    {
        "content": "In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss: \nPlugging in the softmax formula from Eq. 7.9, and with $K$ the number of classes: \n7.6.2 Computing the Gradient \nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.29 (and derived in Section 5.10): \nOr for a network with one weight layer and softmax output ( $dot { } = mathbf { nabla }$ multinomial logistic regression), we could use the derivative of the softmax loss from Eq. 5.47, shown for a particular weight $pmb { w } _ { k }$ and input $mathbf { x } _ { i }$ \nBut these derivatives only give correct updates for one weight layer: the last one! For deep networks, computing the gradients for each weight is much more complex, since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network, even though the loss is computed only at the very end of the network. \nThe solution to computing this gradient is an algorithm called error backpropagation or backprop (Rumelhart et al., 1986). While backprop was invented specially for neural networks, it turns out to be the same as a more general procedure called backward differentiation, which depends on the notion of computation graphs. Let’s see how that works in the next subsection. \n7.6.3 Computation Graphs \nA computation graph is a representation of the process of computing a mathematical expression, in which the computation is broken down into separate operations, each of which is modeled as a node in a graph. \nConsider computing the function $L ( a , b , c ) = c ( a + 2 b )$ . If we make each of the component addition and multiplication operations explicit, and add names $d$ and $e$ )",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Training Neural Nets",
        "subsection": "Computing the Gradient",
        "subsubsection": "N/A"
    },
    {
        "content": "In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss: \nPlugging in the softmax formula from Eq. 7.9, and with $K$ the number of classes: \n7.6.2 Computing the Gradient \nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.29 (and derived in Section 5.10): \nOr for a network with one weight layer and softmax output ( $dot { } = mathbf { nabla }$ multinomial logistic regression), we could use the derivative of the softmax loss from Eq. 5.47, shown for a particular weight $pmb { w } _ { k }$ and input $mathbf { x } _ { i }$ \nBut these derivatives only give correct updates for one weight layer: the last one! For deep networks, computing the gradients for each weight is much more complex, since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network, even though the loss is computed only at the very end of the network. \nThe solution to computing this gradient is an algorithm called error backpropagation or backprop (Rumelhart et al., 1986). While backprop was invented specially for neural networks, it turns out to be the same as a more general procedure called backward differentiation, which depends on the notion of computation graphs. Let’s see how that works in the next subsection. \n7.6.3 Computation Graphs \nA computation graph is a representation of the process of computing a mathematical expression, in which the computation is broken down into separate operations, each of which is modeled as a node in a graph. \nConsider computing the function $L ( a , b , c ) = c ( a + 2 b )$ . If we make each of the component addition and multiplication operations explicit, and add names $d$ and $e$ ) \nfor the intermediate outputs, the resulting series of computations is: \nWe can now represent this as a graph, with nodes for each operation, and directed edges showing the outputs from each operation as the inputs to the next, as in Fig. 7.14. The simplest use of computation graphs is to compute the value of the function with some given inputs. In the figure, we’ve assumed the inputs $a = 3$ , $b = 1$ , $c = - 2$ , and we’ve shown the result of the forward pass to compute the result $L ( 3 , 1 , - 2 ) = - 1 0$ . In the forward pass of a computation graph, we apply each operation left to right, passing the outputs of each computation as the input to the next node. \n7.6.4 Backward differentiation on computation graphs \nThe importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we’ll need for the weight update. In this to each of the input variables, i.e., example our goal is to compute the derivative of the output function $begin{array} { r } { frac { partial L } { partial a } , frac { partial L } { partial b } } end{array}$ , and $frac { partial L } { partial c }$ . The derivative $textstyle { frac { partial L } { partial a } }$ $L$ , tells us how with respect much a small change in $a$ affects . \nBackwards differentiation makes use of the chain rule in calculus, so let’s remind ourselves of that. Suppose we are computing the derivative of a composite function $f ( x ) = u ( nu ( x ) )$ . The derivative of $f ( x )$ is the derivative of $u ( x )$ with respect to $nu ( x )$ times the derivative of $nu ( x )$ with respect to $x$ : \nThe chain rule extends to more than two functions. If computing the derivative of a composite function $f ( x ) = u ( nu ( w ( x ) ) )$ , the derivative of $f ( x )$ is: \nThe intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. Fig. 7.15 shows part of the backward computation at one node $e$ . Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Training Neural Nets",
        "subsection": "Computation Graphs",
        "subsubsection": "N/A"
    },
    {
        "content": "for the intermediate outputs, the resulting series of computations is: \nWe can now represent this as a graph, with nodes for each operation, and directed edges showing the outputs from each operation as the inputs to the next, as in Fig. 7.14. The simplest use of computation graphs is to compute the value of the function with some given inputs. In the figure, we’ve assumed the inputs $a = 3$ , $b = 1$ , $c = - 2$ , and we’ve shown the result of the forward pass to compute the result $L ( 3 , 1 , - 2 ) = - 1 0$ . In the forward pass of a computation graph, we apply each operation left to right, passing the outputs of each computation as the input to the next node. \n7.6.4 Backward differentiation on computation graphs \nThe importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we’ll need for the weight update. In this to each of the input variables, i.e., example our goal is to compute the derivative of the output function $begin{array} { r } { frac { partial L } { partial a } , frac { partial L } { partial b } } end{array}$ , and $frac { partial L } { partial c }$ . The derivative $textstyle { frac { partial L } { partial a } }$ $L$ , tells us how with respect much a small change in $a$ affects . \nBackwards differentiation makes use of the chain rule in calculus, so let’s remind ourselves of that. Suppose we are computing the derivative of a composite function $f ( x ) = u ( nu ( x ) )$ . The derivative of $f ( x )$ is the derivative of $u ( x )$ with respect to $nu ( x )$ times the derivative of $nu ( x )$ with respect to $x$ : \nThe chain rule extends to more than two functions. If computing the derivative of a composite function $f ( x ) = u ( nu ( w ( x ) ) )$ , the derivative of $f ( x )$ is: \nThe intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. Fig. 7.15 shows part of the backward computation at one node $e$ . Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. \n\nLet’s now compute the 3 derivatives we need. Since in the computation graph $L = c e$ , we can directly compute the derivative $frac { partial L } { partial c }$ : \nFor the other two, we’ll need to use the chain rule: \nEq. 7.34 and Eq. 7.33 thus require five intermediate derivatives: $begin{array} { r } { frac { partial L } { partial e } , frac { partial L } { partial c } , frac { partial e } { partial a } , frac { partial e } { partial d } } end{array}$ , and   \n$frac { partial d } { partial b }$   \nsum of the derivatives): \nIn the backward pass, we compute each of these partials along each edge of the graph from right to left, using the chain rule just as we did above. Thus we begin by computing the downstream gradients from node $L$ , which are $frac { partial L } { partial e }$ and $frac { partial L } { partial c }$ . For node $e$ we then multiply this upstream gradient $frac { partial L } { partial e }$ by the local gradient (the gradient of the output with respect to the input), $frac { partial e } { partial d }$ to get the output we send back to node d: $frac { partial L } { partial d }$ And so on, until we have annotated the graph all the way to all the input variables. The forward pass conveniently already will have computed the values of the forward intermediate variables we need (like $d$ and $e$ ) to compute these derivatives. Fig. 7.16 shows the backward pass. \nBackward differentiation for a neural network \nOf course computation graphs for real neural networks are much more complex. Fig. 7.17 shows a sample computation graph for a 2-layer neural network with $n _ { 0 } =$ \nFigure 7.16 Computation graph for the function $L ( a , b , c ) = c ( a + 2 b )$ , showing the backward pass computation of ∂ La , ∂ Lb , and ∂ Lc . \n2, $n _ { 1 } = 2$ , and $n _ { 2 } = 1$ , assuming binary classification and hence using a sigmoid output unit for simplicity. The function that the computation graph is computing is: \nFor the backward pass we’ll also need to compute the loss $L$ . The loss function for binary sigmoid output from Eq. 7.25 is \nOur output $hat { y } = a ^ { [ 2 ] }$ , so we can rephrase this as \nThe weights that need updating (those for which we need to know the partial derivative of the loss function) are shown in teal. In order to do the backward pass, we’ll need to know the derivatives of all the functions in the graph. We already saw in Section 5.10 the derivative of the sigmoid $sigma$ : \nWe’ll also need the derivatives of each of the other activation functions. The derivative of tanh is: \nThe derivative of the ReLU is \nWe’ll give the start of the computation, computing the derivative of the loss function $L$ with respect to $z$ , or $frac { partial L } { partial z }$ (and leaving the rest of the computation as an exercise for the reader). By the chain rule: \nSo let’s first compute ∂∂a[L2] , taking the derivative of Eq. 7.37, repeated here: \nNext, by the derivative of the sigmoid: \nFinally, we can use the chain rule: \nContinuing the backward computation of the gradients (next by passing the gradients over $b _ { 1 } ^ { [ 2 ] }$ and the two product nodes, and so on, back to all the teal nodes), is left as an exercise for the reader. \n7.6.5 More details on learning \nhyperparameter \nOptimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best practices for successful learning. \nFor logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It’s also helpful to normalize the input values to have 0 mean and unit variance. \nVarious forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning of hyperparameters is also important. The parameters of a neural network are the weights $W$ and biases $b$ ; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate $eta$ , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015). \nFinally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter. \n7.7 Training the neural language model \nfreeze \nNow that we’ve seen how to train a generic neural net, let’s talk about the architecture for training a neural language model, setting the parameters $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . \nFor some tasks, it’s ok to freeze the embedding layer E with initial word2vec values. Freezing means we use word2vec or some other pretraining algorithm to compute the initial embedding matrix E, and then hold it constant while we only modify W, U, and b, i.e., we don’t update E during language model training. However, often we’d like to learn the embeddings simultaneously with training the network. This is useful when the task the network is designed for (like sentiment classification, translation, or parsing) places strong constraints on what makes a good representation for words. \nLet’s see how to train the entire model including E, i.e. to set all the parameters $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . We’ll do this via gradient descent (Fig. 5.6), using error backpropagation on the computation graph to compute the gradient. Training thus not only sets the weights $boldsymbol { mathsf { w } }$ and U of the network, but also as we’re predicting upcoming words, we’re learning the embeddings E for each word that best predict upcoming words.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Training Neural Nets",
        "subsection": "Backward differentiation on computation graphs",
        "subsubsection": "N/A"
    },
    {
        "content": "Continuing the backward computation of the gradients (next by passing the gradients over $b _ { 1 } ^ { [ 2 ] }$ and the two product nodes, and so on, back to all the teal nodes), is left as an exercise for the reader. \n7.6.5 More details on learning \nhyperparameter \nOptimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best practices for successful learning. \nFor logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It’s also helpful to normalize the input values to have 0 mean and unit variance. \nVarious forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning of hyperparameters is also important. The parameters of a neural network are the weights $W$ and biases $b$ ; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate $eta$ , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015). \nFinally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter. \n7.7 Training the neural language model \nfreeze \nNow that we’ve seen how to train a generic neural net, let’s talk about the architecture for training a neural language model, setting the parameters $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . \nFor some tasks, it’s ok to freeze the embedding layer E with initial word2vec values. Freezing means we use word2vec or some other pretraining algorithm to compute the initial embedding matrix E, and then hold it constant while we only modify W, U, and b, i.e., we don’t update E during language model training. However, often we’d like to learn the embeddings simultaneously with training the network. This is useful when the task the network is designed for (like sentiment classification, translation, or parsing) places strong constraints on what makes a good representation for words. \nLet’s see how to train the entire model including E, i.e. to set all the parameters $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . We’ll do this via gradient descent (Fig. 5.6), using error backpropagation on the computation graph to compute the gradient. Training thus not only sets the weights $boldsymbol { mathsf { w } }$ and U of the network, but also as we’re predicting upcoming words, we’re learning the embeddings E for each word that best predict upcoming words.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Training Neural Nets",
        "subsection": "More details on learning",
        "subsubsection": "N/A"
    },
    {
        "content": "Continuing the backward computation of the gradients (next by passing the gradients over $b _ { 1 } ^ { [ 2 ] }$ and the two product nodes, and so on, back to all the teal nodes), is left as an exercise for the reader. \n7.6.5 More details on learning \nhyperparameter \nOptimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best practices for successful learning. \nFor logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It’s also helpful to normalize the input values to have 0 mean and unit variance. \nVarious forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning of hyperparameters is also important. The parameters of a neural network are the weights $W$ and biases $b$ ; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate $eta$ , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015). \nFinally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter. \n7.7 Training the neural language model \nfreeze \nNow that we’ve seen how to train a generic neural net, let’s talk about the architecture for training a neural language model, setting the parameters $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . \nFor some tasks, it’s ok to freeze the embedding layer E with initial word2vec values. Freezing means we use word2vec or some other pretraining algorithm to compute the initial embedding matrix E, and then hold it constant while we only modify W, U, and b, i.e., we don’t update E during language model training. However, often we’d like to learn the embeddings simultaneously with training the network. This is useful when the task the network is designed for (like sentiment classification, translation, or parsing) places strong constraints on what makes a good representation for words. \nLet’s see how to train the entire model including E, i.e. to set all the parameters $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . We’ll do this via gradient descent (Fig. 5.6), using error backpropagation on the computation graph to compute the gradient. Training thus not only sets the weights $boldsymbol { mathsf { w } }$ and U of the network, but also as we’re predicting upcoming words, we’re learning the embeddings E for each word that best predict upcoming words. \nFig. 7.18 shows the set up for a window size of $Nu { = } 3$ context words. The input x consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instantiations of the embedding matrix $E$ . We don’t want to learn separate weight matrices for mapping each of the 3 previous words to the projection layer. We want one single embedding dictionary $E$ that’s shared among these three. That’s because over time, many different words will appear as $w _ { t - 2 }$ or $w _ { t - 1 }$ , and we’d like to just represent each word with one vector, whichever context position it appears in. Recall that the embedding weight matrix $E$ has a column for each word, each a column vector of $d$ dimensions, and hence has dimensionality $d times lvert V rvert$ . \nGenerally training proceeds by taking as input a very long text, concatenating all the sentences, starting with random weights, and then iteratively moving through the text predicting each word $w _ { t }$ . At each word $w _ { t }$ , we use the cross-entropy (negative log likelihood) loss. Recall that the general form for this (repeated from Eq. 7.27 is: \nFor language modeling, the classes are the words in the vocabulary, so $hat { y } _ { i }$ here means the probability that the model assigns to the correct next word $w _ { t }$ : \nThe parameter update for stochastic gradient descent for this loss from step $s$ to $s + 1$ is then: \nThis gradient can be computed in any standard neural network framework which will then backpropagate through $pmb { theta } = pmb { mathsf { E } } , pmb { mathsf { W } } , pmb { mathsf { U } } , mathbf { b }$ . \nTraining the parameters to minimize loss will result both in an algorithm for language modeling (a word predictor) but also a new set of embeddings E that can be used as word representations for other tasks. \n7.8 Summary \n• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.   \n• Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.   \n• In a fully-connected, feedforward network, each unit in layer $i$ is connected to each unit in layer $i + 1$ , and there are no cycles.   \n• The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.   \n• Neural networks are trained by optimization algorithms like gradient descent.   \n• Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.   \n• Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous $n$ words.   \n• Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling. \nBibliographical and Historical Notes \nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). \nThe field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn’t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, \nconnectionist",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Training the neural language model",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Training the parameters to minimize loss will result both in an algorithm for language modeling (a word predictor) but also a new set of embeddings E that can be used as word representations for other tasks. \n7.8 Summary \n• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.   \n• Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.   \n• In a fully-connected, feedforward network, each unit in layer $i$ is connected to each unit in layer $i + 1$ , and there are no cycles.   \n• The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.   \n• Neural networks are trained by optimization algorithms like gradient descent.   \n• Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.   \n• Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous $n$ words.   \n• Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling. \nBibliographical and Historical Notes \nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). \nThe field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn’t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, \nconnectionist",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Training the parameters to minimize loss will result both in an algorithm for language modeling (a word predictor) but also a new set of embeddings E that can be used as word representations for other tasks. \n7.8 Summary \n• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.   \n• Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.   \n• In a fully-connected, feedforward network, each unit in layer $i$ is connected to each unit in layer $i + 1$ , and there are no cycles.   \n• The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.   \n• Neural networks are trained by optimization algorithms like gradient descent.   \n• Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.   \n• Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous $n$ words.   \n• Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling. \nBibliographical and Historical Notes \nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). \nThe field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn’t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, \nconnectionist \n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality (Smolensky, 1990). \nBy the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 9 and Chapter 26. \nThere are a number of excellent books on the subject. Goldberg (2017) has superb coverage of neural networks for natural language processing. For neural networks in general see Goodfellow et al. (2016) and Nielsen (2015). \n8 \nSequence Labeling for Parts of Speech and Named Entities \nTo each word a warbling note A Midsummer Night’s Dream, V.I \nparts of speech \nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language. \nnamed entity \nProper names are another important and anciently studied linguistic category. While parts of speech are generally assigned to individual words or morphemes, a proper name is often an entire multiword phrase, like the name “Marie Curie”, the location “New York City”, or the organization “Stanford University”. We’ll use the term named entity for, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization, although as we’ll see the term is commonly extended to include things that aren’t entities per se. \nParts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. \nIn this chapter we’ll introduce the task of part-of-speech tagging, taking a sequence of words and assigning each word a part of speech like NOUN or VERB, and the task of named entity recognition (NER), assigning words or phrases tags like PERSON, LOCATION, or ORGANIZATION. \nsequence labeling \nSuch tasks in which we assign, to each word $x _ { i }$ in an input word sequence, a label $y _ { i }$ , so that the output sequence $Y$ has the same length as the input sequence $X$ are called sequence labeling tasks. We’ll introduce classic sequence labeling algorithms, one generative— the Hidden Markov Model (HMM)—and one discriminative— the Conditional Random Field (CRF). In following chapters we’ll introduce modern sequence labelers based on RNNs and Transformers.",
        "chapter": "Neural Networks and Neural Language Models",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "8 \nSequence Labeling for Parts of Speech and Named Entities \nTo each word a warbling note A Midsummer Night’s Dream, V.I \nparts of speech \nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language. \nnamed entity \nProper names are another important and anciently studied linguistic category. While parts of speech are generally assigned to individual words or morphemes, a proper name is often an entire multiword phrase, like the name “Marie Curie”, the location “New York City”, or the organization “Stanford University”. We’ll use the term named entity for, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization, although as we’ll see the term is commonly extended to include things that aren’t entities per se. \nParts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. \nIn this chapter we’ll introduce the task of part-of-speech tagging, taking a sequence of words and assigning each word a part of speech like NOUN or VERB, and the task of named entity recognition (NER), assigning words or phrases tags like PERSON, LOCATION, or ORGANIZATION. \nsequence labeling \nSuch tasks in which we assign, to each word $x _ { i }$ in an input word sequence, a label $y _ { i }$ , so that the output sequence $Y$ has the same length as the input sequence $X$ are called sequence labeling tasks. We’ll introduce classic sequence labeling algorithms, one generative— the Hidden Markov Model (HMM)—and one discriminative— the Conditional Random Field (CRF). In following chapters we’ll introduce modern sequence labelers based on RNNs and Transformers. \n8.1 (Mostly) English Word Classes \nUntil now we have been using part-of-speech terms like noun and verb rather freely. In this section we give more complete definitions. While word classes do have semantic tendencies—adjectives, for example, often describe properties and nouns people— parts of speech are defined instead based on their grammatical relationship with neighboring words or the morphological properties about their affixes. \nclosed class open class \nfunction word \nParts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions— new prepositions are rarely coined. By contrast, nouns and verbs are open classes— new nouns and verbs like iPhone or to fax are continually being created or borrowed. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. \nFour major open classes occur in the languages of the world: nouns (including proper nouns), verbs, adjectives, and adverbs, as well as the smaller open class of interjections. English has all five, although not every language does. \nnoun common noun count noun mass noun \n\nNouns are words for people, places, or things, but include others as well. Common nouns include concrete terms like cat and mango, abstractions like algorithm and beauty, and verb-like terms like pacing as in His pacing to and fro became quite annoying. Nouns in English can occur with determiners (a goat, this bandwidth) take possessives (IBM’s annual revenue), and may occur in the plural (goats, abaci). Many languages, including English, divide common nouns into count nouns and mass nouns. Count nouns can occur in the singular and plural (goat/goats, relationship/relationships) and can be counted (one goat, two goats). Mass nouns are used when something is conceptualized as a homogeneous group. So snow, salt, and communism are not counted (i.e., *two snows or *two communisms). Proper nouns, like Regina, Colorado, and IBM, are names of specific persons or entities. \nverb \nVerbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-personsingular (eats), progressive (eating), past participle (eaten)). While many scholars believe that all human languages have the categories of noun and verb, others have argued that some languages, such as Riau Indonesian and Tongan, don’t even make this distinction (Broschart 1997; Evans 2000; Gil 2000) . \nadjective \nAdjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), but there are languages without adjectives. In Korean, for example, the words corresponding to English adjectives act as a subclass of verbs, so what is in English an adjective “beautiful” acts in Korean like a verb meaning “to be beautiful”. \nadverb \nAdverbs are a hodge-podge. All the italicized words in this example are adverbs: \nActually, I ran home extremely quickly yesterday \nlocative degree \nmanner temporal \nAdverbs generally modify something (often verbs, hence the name “adverb”, but also other adverbs and entire verb phrases). Directional adverbs or locative adverbs (home, here, downhill) specify the direction or location of some action; degree adverbs (extremely, very, somewhat) specify the extent of some action, process, or property; manner adverbs (slowly, slinkily, delicately) describe the manner of some action or process; and temporal adverbs describe the time that some action or event took place (yesterday, Monday). \ninterjection \nInterjections (oh, hey, alas, uh, um) are a smaller open class that also includes greetings (hello, goodbye) and question responses (yes, no, uh-huh). \npreposition \nEnglish adpositions occur before nouns, hence are called prepositions. They can indicate spatial or temporal relations, whether literal (on it, before then, by the house) or metaphorical (on time, with gusto, beside herself), and relations like marking the agent in Hamlet was written by Shakespeare. \nparticle \nphrasal verb \nA particle resembles a preposition or an adverb and is used in combination with a verb. Particles often have extended meanings that aren’t quite the same as the prepositions they resemble, as in the particle over in she turned the paper over. A verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional—not predictable from the individual meanings of the verb and the particle. Thus, turn down means ‘reject’, rule out ‘eliminate’, and go on ‘continue’. \ndeterminer article \nDeterminers like this and that (this chapter, that page) can mark the start of an English noun phrase. Articles like a, an, and the, are a type of determiner that mark discourse properties of the noun and are quite frequent; the is the most common word in written English, with $a$ and an right behind. \nconjunction \ncomplementizer pronoun \nConjunctions join two phrases, clauses, or sentences. Coordinating conjunctions like and, or, and but join two elements of equal status. Subordinating conjunctions are used when one of the elements has some embedded status. For example, the subordinating conjunction that in “I thought that you might like some milk” links the main clause I thought with the subordinate clause you might like some milk. This clause is called subordinate because this entire clause is the “content” of the main verb thought. Subordinating conjunctions like that which link a verb to its argument in this way are also called complementizers. \nPronouns act as a shorthand for referring to an entity or event. Personal pronouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that indicate either actual possession or more often just an abstract relation between the person and some object (my, your, his, her, its, one’s, our, their). Wh-pronouns (what, who, whom, whoever) are used in certain question \nauxiliary \nforms, or act as complementizers (Frida, who married Diego. . . ). \nAuxiliary verbs mark semantic features of a main verb such as its tense, whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to mark the mood associated with the event depicted by the main verb: can indicates ability or possibility, may permission or possibility, must necessity. \ncopula modal \nAn English-specific tagset, the 45-tag Penn Treebank tagset (Marcus et al., 1993), shown in Fig. 8.2, has been used to label many syntactically annotated corpora like the Penn Treebank corpora, so is worth knowing about. \nBelow we show some examples with each word tagged according to both the UD and Penn tagsets. Notice that the Penn tagset distinguishes tense and participles on verbs, and has a special tag for the existential there construction in English. Note that since New England Journal of Medicine is a proper noun, both tagsets mark its component nouns as NNP, including journal and medicine, which might otherwise be labeled as common nouns (NOUN/NN). \n(8.1) There/PRO/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNS there/ADV/RB ./PUNC/.   \n(8.2) Preliminary/ADJ/JJ findings/NOUN/NNS were/AUX/VBD reported/VERB/VBN in/ADP/IN today/NOUN/NN ’s/PART/POS New/PROPN/NNP England/PROPN/NNP Journal/PROPN/NNP of/ADP/IN Medicine/PROPN/NNP \n8.2 Part-of-Speech Tagging \npart-of-speech tagging \nPart-of-speech tagging is the process of assigning a part-of-speech to each word in a text. The input is a sequence $x _ { 1 } , x _ { 2 } , . . . , x _ { n }$ of (tokenized) words and a tagset, and the output is a sequence $y _ { 1 } , y _ { 2 } , . . . , y _ { n }$ of tags, each output $y _ { i }$ corresponding exactly to one input $x _ { i }$ , as shown in the intuition in Fig. 8.3. \nambiguous \nTagging is a disambiguation task; words are ambiguous —have more than one possible part-of-speech—and the goal is to find the correct tag for the situation. For example, book can be a verb (book that flight) or a noun (hand me that book). That can be a determiner (Does that flight serve dinner) or a complementizer (I thought that your flight was earlier). The goal of POS-tagging is to resolve these ambiguities, choosing the proper tag for the context.",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "(Mostly) English Word Classes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "auxiliary \nforms, or act as complementizers (Frida, who married Diego. . . ). \nAuxiliary verbs mark semantic features of a main verb such as its tense, whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to mark the mood associated with the event depicted by the main verb: can indicates ability or possibility, may permission or possibility, must necessity. \ncopula modal \nAn English-specific tagset, the 45-tag Penn Treebank tagset (Marcus et al., 1993), shown in Fig. 8.2, has been used to label many syntactically annotated corpora like the Penn Treebank corpora, so is worth knowing about. \nBelow we show some examples with each word tagged according to both the UD and Penn tagsets. Notice that the Penn tagset distinguishes tense and participles on verbs, and has a special tag for the existential there construction in English. Note that since New England Journal of Medicine is a proper noun, both tagsets mark its component nouns as NNP, including journal and medicine, which might otherwise be labeled as common nouns (NOUN/NN). \n(8.1) There/PRO/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNS there/ADV/RB ./PUNC/.   \n(8.2) Preliminary/ADJ/JJ findings/NOUN/NNS were/AUX/VBD reported/VERB/VBN in/ADP/IN today/NOUN/NN ’s/PART/POS New/PROPN/NNP England/PROPN/NNP Journal/PROPN/NNP of/ADP/IN Medicine/PROPN/NNP \n8.2 Part-of-Speech Tagging \npart-of-speech tagging \nPart-of-speech tagging is the process of assigning a part-of-speech to each word in a text. The input is a sequence $x _ { 1 } , x _ { 2 } , . . . , x _ { n }$ of (tokenized) words and a tagset, and the output is a sequence $y _ { 1 } , y _ { 2 } , . . . , y _ { n }$ of tags, each output $y _ { i }$ corresponding exactly to one input $x _ { i }$ , as shown in the intuition in Fig. 8.3. \nambiguous \nTagging is a disambiguation task; words are ambiguous —have more than one possible part-of-speech—and the goal is to find the correct tag for the situation. For example, book can be a verb (book that flight) or a noun (hand me that book). That can be a determiner (Does that flight serve dinner) or a complementizer (I thought that your flight was earlier). The goal of POS-tagging is to resolve these ambiguities, choosing the proper tag for the context. \n\nThe accuracy of part-of-speech tagging algorithms (the percentage of test set tags that match human gold labels) is extremely high. One study found accuracies over $9 7 %$ across 15 languages from the Universal Dependency (UD) treebank ( $mathrm { W u }$ and Dredze, 2019). Accuracies on various English treebanks are also $9 7 %$ (no matter the algorithm; HMMs, CRFs, BERT perform similarly). This $9 7 %$ number is also about the human performance on this task, at least for English (Manning, 2011). \nWe’ll introduce algorithms for the task in the next few sections, but first let’s explore the task. Exactly how hard is it? Fig. 8.4 shows that most word types $( 8 5 - 8 6 % )$ are unambiguous (Janet is always NNP, hesitantly is always RB). But the ambiguous words, though accounting for only $14 %$ of the vocabulary, are very common, and $5 5 %$ of word tokens in running text are ambiguous. Particularly ambiguous common words include that, back, down, put and set; here are some examples of the 6 different parts of speech for the word back: \nearnings growth took a back/JJ seat a small building in the back/NN a clear majority of senators back/VBP the bill Dave began to back/VB toward the door enable the country to buy back/RP debt I was twenty-one back/RB then \nNonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter $a$ , but the determiner sense is much more likely. \nThis idea suggests a useful baseline: given an ambiguous word, choose the tag which is most frequent in the training corpus. This is a key concept: \nMost Frequent Class Baseline: Always compare a classifier against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set). \nThe most-frequent-tag baseline has an accuracy of about $9 2 % ^ { 1 }$ . The baseline thus differs from the state-of-the-art and human ceiling $( 9 7 % )$ by only $5 %$ . \n8.3 Named Entities and Named Entity Tagging \nnamed entity \nnamed entity recognition NER \nPart of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location. \nA named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger: \nCiting high fuel prices, [ORG United Airlines] said $mathrm { Delta [ T I M E }$ Friday] it has increased fares by [MONEY $$ 6]$ per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect [TIME Thursday] and applies to most routes where it competes against discount carriers, such as $mathrm { I _ { L O C } }$ Chicago] to [LOC Dallas] and [LOC Denver] to $operatorname { I _ { L O C } }$ San Francisco]. \nThe text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 8.5 shows typical generic named entity types. Many applications will also need to use specific entity types like proteins, genes, commercial products, or works of art. \nNamed entity tagging is a useful first step in lots of natural language processing tasks. In sentiment analysis we might want to know a consumer’s sentiment toward a particular entity. Entities are a useful first stage in question answering, or for linking text to information in structured knowledge sources like Wikipedia. And named entity tagging is also central to tasks involving building semantic representations, like extracting events and the relationship between participants. \nUnlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what’s an entity and what isn’t, and where the boundaries are. Indeed, most words in a text will not be named entities. Another difficulty is caused by type ambiguity. The mention JFK can refer to a person, the airport in New York, or any number of schools, bridges, and streets around the United States. Some examples of this kind of cross-type confusion are given in Figure 8.6.",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Part-of-Speech Tagging",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The most-frequent-tag baseline has an accuracy of about $9 2 % ^ { 1 }$ . The baseline thus differs from the state-of-the-art and human ceiling $( 9 7 % )$ by only $5 %$ . \n8.3 Named Entities and Named Entity Tagging \nnamed entity \nnamed entity recognition NER \nPart of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location. \nA named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger: \nCiting high fuel prices, [ORG United Airlines] said $mathrm { Delta [ T I M E }$ Friday] it has increased fares by [MONEY $$ 6]$ per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect [TIME Thursday] and applies to most routes where it competes against discount carriers, such as $mathrm { I _ { L O C } }$ Chicago] to [LOC Dallas] and [LOC Denver] to $operatorname { I _ { L O C } }$ San Francisco]. \nThe text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 8.5 shows typical generic named entity types. Many applications will also need to use specific entity types like proteins, genes, commercial products, or works of art. \nNamed entity tagging is a useful first step in lots of natural language processing tasks. In sentiment analysis we might want to know a consumer’s sentiment toward a particular entity. Entities are a useful first stage in question answering, or for linking text to information in structured knowledge sources like Wikipedia. And named entity tagging is also central to tasks involving building semantic representations, like extracting events and the relationship between participants. \nUnlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what’s an entity and what isn’t, and where the boundaries are. Indeed, most words in a text will not be named entities. Another difficulty is caused by type ambiguity. The mention JFK can refer to a person, the airport in New York, or any number of schools, bridges, and streets around the United States. Some examples of this kind of cross-type confusion are given in Figure 8.6. \n\nThe standard approach to sequence labeling for a span-recognition problem like NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us to treat NER like a word-by-word sequence labeling task, via tags that capture both the boundary and the named entity type. Consider the following sentence: \n[PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route. \nFigure 8.7 shows the same excerpt represented with BIO tagging, as well as variants called IO tagging and BIOES tagging. In BIO tagging we label any token that begins a span of interest with the label B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O. While there is only one O tag, we’ll have distinct B and I tags for each named entity class. The number of tags is thus $2 n + 1$ tags, where $n$ is the number of entity types. BIO tagging can represent exactly the same information as the bracketed notation, but has the advantage that we can represent the task in the same simple sequence modeling way as part-of-speech tagging: assigning a single label $y _ { i }$ to each input word $x _ { i }$ : \nWe’ve also shown two variant tagging schemes: IO tagging, which loses some information by eliminating the B tag, and BIOES tagging, which adds an end tag E for the end of a span, and a span tag S for a span consisting of only one word. A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each token in a text with tags that indicate the presence (or absence) of particular kinds of named entities.",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Named Entities and Named Entity Tagging",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "8.4 HMM Part-of-Speech Tagging \nIn this section we introduce our first sequence labeling algorithm, the Hidden Markov Model, and show how to apply it to part-of-speech tagging. Recall that a sequence labeler is a model whose job is to assign a label to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels of the same length. The HMM is a classic model that introduces many of the key concepts of sequence modeling that we will see again in more modern models. \nAn HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), it computes a probability distribution over possible sequences of labels and chooses the best label sequence. \n8.4.1 Markov Chains \nMarkov chain \nThe HMM is based on augmenting the Markov chain. A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state. It’s as if to predict tomorrow’s weather you could examine today’s weather but you weren’t allowed to look at yesterday’s weather. \nMore formally, consider a sequence of state variables $q _ { 1 } , q _ { 2 } , . . . , q _ { i }$ . A Markov model embodies the Markov assumption on the probabilities of this sequence: that when predicting the future, the past doesn’t matter, only the present. \nFigure 8.8a shows a Markov chain for assigning a probability to a sequence of weather events, for which the vocabulary consists of HOT, COLD, and WARM. The states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given state must sum to 1. Figure $8 . 8  { mathrm { b } }$ shows a Markov chain for assigning a probability to a sequence of words $w _ { 1 } . . . w _ { t }$ . This Markov chain should be familiar; in fact, it represents a bigram language model, with each edge expressing the probability $p ( w _ { i } | w _ { j } ) !$ Given the two models in Fig. 8.8, we can assign a probability to any sequence from our vocabulary. \nBefore you go on, use the sample probabilities in Fig. 8.8a (with $pi = [ 0 . 1 , 0 . 7 , 0 .$ .2]) to compute the probability of each of the following sequences: \n(8.4) hot hot hot hot (8.5) cold hot cold hot \nWhat does the difference in these probabilities tell you about a real-world weather fact encoded in Fig. 8.8a? \n8.4.2 The Hidden Markov Model \nhidden \nA Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don’t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. \nA hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components: \nA first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state: \nSecond, the probability of an output observation $o _ { i }$ depends only on the state that produced the observation $q _ { i }$ and not on any other states or any other observations: \nOutput Independence: $P ( o _ { i } | q _ { 1 } , dots q _ { i } , dots , q _ { T } , o _ { 1 } , dots , o _ { i } , dots , o _ { T } ) = P ( o _ { i } | q _ { i } )$",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "HMM Part-of-Speech Tagging",
        "subsection": "Markov Chains",
        "subsubsection": "N/A"
    },
    {
        "content": "Before you go on, use the sample probabilities in Fig. 8.8a (with $pi = [ 0 . 1 , 0 . 7 , 0 .$ .2]) to compute the probability of each of the following sequences: \n(8.4) hot hot hot hot (8.5) cold hot cold hot \nWhat does the difference in these probabilities tell you about a real-world weather fact encoded in Fig. 8.8a? \n8.4.2 The Hidden Markov Model \nhidden \nA Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don’t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. \nA hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components: \nA first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state: \nSecond, the probability of an output observation $o _ { i }$ depends only on the state that produced the observation $q _ { i }$ and not on any other states or any other observations: \nOutput Independence: $P ( o _ { i } | q _ { 1 } , dots q _ { i } , dots , q _ { T } , o _ { 1 } , dots , o _ { i } , dots , o _ { T } ) = P ( o _ { i } | q _ { i } )$ \n8.4.3 The components of an HMM tagger \nLet’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use it to tag. An HMM has two components, the $A$ and $B$ probabilities. \nThe A matrix contains the tag transition probabilities $P ( t _ { i } | t _ { i - 1 } )$ which represent the probability of a tag occurring given the previous tag. For example, modal verbs like will are very likely to be followed by a verb in the base form, a VB, like race, so we expect this probability to be high. We compute the maximum likelihood estimate of this transition probability by counting, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second: \nIn the WSJ corpus, for example, MD occurs 13124 times of which it is followed by VB 10471, for an MLE estimate of \nLet’s walk through an example, seeing how these probabilities are estimated and used in a sample tagging task, before we return to the algorithm for decoding. \nIn HMM tagging, the probabilities are estimated by counting on a tagged training corpus. For this example we’ll use the tagged WSJ corpus. \nThe $B$ emission probabilities, $P ( w _ { i } | t _ { i } )$ , represent the probability, given a tag (say MD), that it will be associated with a given word (say will). The MLE of the emission probability is \nOf the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046 times: \nWe saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood term is not asking “which is the most likely tag for the word will?” That would be the posterior $P ( mathrm { M D } | mathrm { w i l l } )$ . Instead, $P ( mathrm { w i l l } | mathbf { M D } )$ answers the slightly counterintuitive question “If we were going to generate a MD, how likely is it that this modal would be will?” \nThe $A$ transition probabilities, and $B$ observation likelihoods of the HMM are illustrated in Fig. 8.9 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag. \n8.4.4 HMM tagging as decoding \ndecoding \nFor any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, \nDecoding: Given as input an HMM $lambda = ( A , B )$ and a sequence of observations $O = o _ { 1 } , o _ { 2 } , . . . , o _ { T }$ , find the most probable sequence of states $Q = q _ { 1 } q _ { 2 } q _ { 3 } ldots q _ { T }$ .",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "HMM Part-of-Speech Tagging",
        "subsection": "The Hidden Markov Model",
        "subsubsection": "N/A"
    },
    {
        "content": "8.4.3 The components of an HMM tagger \nLet’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use it to tag. An HMM has two components, the $A$ and $B$ probabilities. \nThe A matrix contains the tag transition probabilities $P ( t _ { i } | t _ { i - 1 } )$ which represent the probability of a tag occurring given the previous tag. For example, modal verbs like will are very likely to be followed by a verb in the base form, a VB, like race, so we expect this probability to be high. We compute the maximum likelihood estimate of this transition probability by counting, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second: \nIn the WSJ corpus, for example, MD occurs 13124 times of which it is followed by VB 10471, for an MLE estimate of \nLet’s walk through an example, seeing how these probabilities are estimated and used in a sample tagging task, before we return to the algorithm for decoding. \nIn HMM tagging, the probabilities are estimated by counting on a tagged training corpus. For this example we’ll use the tagged WSJ corpus. \nThe $B$ emission probabilities, $P ( w _ { i } | t _ { i } )$ , represent the probability, given a tag (say MD), that it will be associated with a given word (say will). The MLE of the emission probability is \nOf the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046 times: \nWe saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood term is not asking “which is the most likely tag for the word will?” That would be the posterior $P ( mathrm { M D } | mathrm { w i l l } )$ . Instead, $P ( mathrm { w i l l } | mathbf { M D } )$ answers the slightly counterintuitive question “If we were going to generate a MD, how likely is it that this modal would be will?” \nThe $A$ transition probabilities, and $B$ observation likelihoods of the HMM are illustrated in Fig. 8.9 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag. \n8.4.4 HMM tagging as decoding \ndecoding \nFor any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, \nDecoding: Given as input an HMM $lambda = ( A , B )$ and a sequence of observations $O = o _ { 1 } , o _ { 2 } , . . . , o _ { T }$ , find the most probable sequence of states $Q = q _ { 1 } q _ { 2 } q _ { 3 } ldots q _ { T }$ .",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "HMM Part-of-Speech Tagging",
        "subsection": "The components of an HMM tagger",
        "subsubsection": "N/A"
    },
    {
        "content": "8.4.3 The components of an HMM tagger \nLet’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use it to tag. An HMM has two components, the $A$ and $B$ probabilities. \nThe A matrix contains the tag transition probabilities $P ( t _ { i } | t _ { i - 1 } )$ which represent the probability of a tag occurring given the previous tag. For example, modal verbs like will are very likely to be followed by a verb in the base form, a VB, like race, so we expect this probability to be high. We compute the maximum likelihood estimate of this transition probability by counting, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second: \nIn the WSJ corpus, for example, MD occurs 13124 times of which it is followed by VB 10471, for an MLE estimate of \nLet’s walk through an example, seeing how these probabilities are estimated and used in a sample tagging task, before we return to the algorithm for decoding. \nIn HMM tagging, the probabilities are estimated by counting on a tagged training corpus. For this example we’ll use the tagged WSJ corpus. \nThe $B$ emission probabilities, $P ( w _ { i } | t _ { i } )$ , represent the probability, given a tag (say MD), that it will be associated with a given word (say will). The MLE of the emission probability is \nOf the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046 times: \nWe saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood term is not asking “which is the most likely tag for the word will?” That would be the posterior $P ( mathrm { M D } | mathrm { w i l l } )$ . Instead, $P ( mathrm { w i l l } | mathbf { M D } )$ answers the slightly counterintuitive question “If we were going to generate a MD, how likely is it that this modal would be will?” \nThe $A$ transition probabilities, and $B$ observation likelihoods of the HMM are illustrated in Fig. 8.9 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag. \n8.4.4 HMM tagging as decoding \ndecoding \nFor any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, \nDecoding: Given as input an HMM $lambda = ( A , B )$ and a sequence of observations $O = o _ { 1 } , o _ { 2 } , . . . , o _ { T }$ , find the most probable sequence of states $Q = q _ { 1 } q _ { 2 } q _ { 3 } ldots q _ { T }$ . \nFor part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence $t _ { 1 } ldots t _ { n }$ that is most probable given the observation sequence of $n$ words $w _ { 1 } ldots w _ { n }$ : \nThe way we’ll do this in the HMM is to use Bayes’ rule to instead compute: \nFurthermore, we simplify Eq. 8.13 by dropping the denominator $P ( w _ { 1 } ^ { n } )$ : \nHMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags: \nThe second assumption, the bigram assumption, is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence; \nPlugging the simplifying assumptions from Eq. 8.15 and Eq. 8.16 into Eq. 8.14 results in the following equation for the most probable tag sequence from a bigram tagger: \nThe two parts of Eq. 8.17 correspond neatly to the $B$ emission probability and $A$ transition probability that we just defined above! \n8.4.5 The Viterbi Algorithm \nViterbi algorithm \nThe decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.10. As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm of Chapter 2. \nfunction VITERBI(observations of len T,state-graph of len $N$ ) returns best-path, path-prob   \ncreate a path probability matrix viterbi[N,T]   \nfor each state $s$ from 1 to $N$ do ; initialization step $begin{array} { l } { nu i t e r b i [ s , 1 ] gets pi _ { s } * b _ { s } ( o _ { 1 } ) }  { b a c k p o i n t e r [ mathrm { s } , 1 ] gets 0 } end{array}$   \nfor each time step $t$ from 2 to $T$ do ; recursion step for each state $s$ from 1 to $N$ do   \n$begin{array} { r l } & { ~ nu i t e r b i [ { mathsf { s } } , { mathsf { t } } ] gets underset {  s ^ { prime } = 1 } { overset { N } { operatorname* { m a x } } } ~ nu i t e r b i [ s ^ { prime } , t - 1 ]  * { mathsf { a } } _ { s ^ { prime } , s } * b _ { s } ( o _ { t } ) }  & { ~ b a c k p o i n t e r [ { mathsf { s } } , { mathsf { t } } ] gets underset {  s ^ { prime } = 1 } { overset { N } { operatorname* { m a x } } } ~ nu i t e r b i [ s ^ { prime } , t - 1 ] * a _ { s ^ { prime } , s } * b _ { s } ( o _ { t } ) }  & { ~ b e s t p a t h p r o b gets underset {  s = 1 } { overset { N } { operatorname* { m a x } } } ~ nu i t e r b i [ s , T ] qquad quad ; mathrm { t e r m i n a t i o n ~ s t e p } }  & { b e s t p a t h p o i n t e r gets  underset {  s = 1 } { operatorname* { m a x } } ~ nu i t e r b i [ s , T ] qquad quad ; mathrm { t e r m i n a t i o n ~ s t e r } } end{array}$ 0   \nbestpath $$ the path starting at state bestpathpointer, that follows backpointer[] to states back in time   \nreturn bestpath, bestpathprob \nFigure 8.10 Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM $lambda = ( A , B )$ , the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence. \nThe Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation $o _ { t }$ and one row for each state in the state graph. Each column thus has a cell for each state $q _ { i }$ in the single combined automaton. Figure 8.11 shows an intuition of this lattice for the sentence Janet will back the bill. \nEach cell of the lattice, $nu _ { t } ( j )$ , represents the probability that the HMM is in state $j$ after seeing the first $t$ observations and passing through the most probable state sequence $q _ { 1 } , . . . , q _ { t - 1 }$ , given the HMM $lambda$ . The value of each cell $nu _ { t } ( j )$ is computed by recursively taking the most probable path that could lead us to this cell. Formally, each cell expresses the probability \nWe represent the most probable path by taking the maximum over all possible previous state sequences max . Like other dynamic programming algorithms, $q _ { 1 } , . . . , q _ { t - 1 }$ Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time $t - 1$ , we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state $q _ { j }$ at time $t$ , the value $nu _ { t } ( j )$ is computed as \nThe three factors that are multiplied in Eq. 8.19 for extending the previous paths to compute the Viterbi probability at time $t$ are",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "HMM Part-of-Speech Tagging",
        "subsection": "HMM tagging as decoding",
        "subsubsection": "N/A"
    },
    {
        "content": "8.4.5 The Viterbi Algorithm \nViterbi algorithm \nThe decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.10. As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm of Chapter 2. \nfunction VITERBI(observations of len T,state-graph of len $N$ ) returns best-path, path-prob   \ncreate a path probability matrix viterbi[N,T]   \nfor each state $s$ from 1 to $N$ do ; initialization step $begin{array} { l } { nu i t e r b i [ s , 1 ] gets pi _ { s } * b _ { s } ( o _ { 1 } ) }  { b a c k p o i n t e r [ mathrm { s } , 1 ] gets 0 } end{array}$   \nfor each time step $t$ from 2 to $T$ do ; recursion step for each state $s$ from 1 to $N$ do   \n$begin{array} { r l } & { ~ nu i t e r b i [ { mathsf { s } } , { mathsf { t } } ] gets underset {  s ^ { prime } = 1 } { overset { N } { operatorname* { m a x } } } ~ nu i t e r b i [ s ^ { prime } , t - 1 ]  * { mathsf { a } } _ { s ^ { prime } , s } * b _ { s } ( o _ { t } ) }  & { ~ b a c k p o i n t e r [ { mathsf { s } } , { mathsf { t } } ] gets underset {  s ^ { prime } = 1 } { overset { N } { operatorname* { m a x } } } ~ nu i t e r b i [ s ^ { prime } , t - 1 ] * a _ { s ^ { prime } , s } * b _ { s } ( o _ { t } ) }  & { ~ b e s t p a t h p r o b gets underset {  s = 1 } { overset { N } { operatorname* { m a x } } } ~ nu i t e r b i [ s , T ] qquad quad ; mathrm { t e r m i n a t i o n ~ s t e p } }  & { b e s t p a t h p o i n t e r gets  underset {  s = 1 } { operatorname* { m a x } } ~ nu i t e r b i [ s , T ] qquad quad ; mathrm { t e r m i n a t i o n ~ s t e r } } end{array}$ 0   \nbestpath $$ the path starting at state bestpathpointer, that follows backpointer[] to states back in time   \nreturn bestpath, bestpathprob \nFigure 8.10 Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM $lambda = ( A , B )$ , the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence. \nThe Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation $o _ { t }$ and one row for each state in the state graph. Each column thus has a cell for each state $q _ { i }$ in the single combined automaton. Figure 8.11 shows an intuition of this lattice for the sentence Janet will back the bill. \nEach cell of the lattice, $nu _ { t } ( j )$ , represents the probability that the HMM is in state $j$ after seeing the first $t$ observations and passing through the most probable state sequence $q _ { 1 } , . . . , q _ { t - 1 }$ , given the HMM $lambda$ . The value of each cell $nu _ { t } ( j )$ is computed by recursively taking the most probable path that could lead us to this cell. Formally, each cell expresses the probability \nWe represent the most probable path by taking the maximum over all possible previous state sequences max . Like other dynamic programming algorithms, $q _ { 1 } , . . . , q _ { t - 1 }$ Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time $t - 1$ , we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state $q _ { j }$ at time $t$ , the value $nu _ { t } ( j )$ is computed as \nThe three factors that are multiplied in Eq. 8.19 for extending the previous paths to compute the Viterbi probability at time $t$ are \n$overline { { nu _ { t - 1 } ( i ) } }$ the previous Viterbi path probability from the previous time step ai j the transition probability from previous state $q _ { i }$ to current state $q _ { j }$ $b _ { j } ( o _ { t } )$ the state observation likelihood of the observation symbol $o _ { t }$ given the current state $j$ \n8.4.6 Working through an example \nLet’s tag the sentence Janet will back the bill; the goal is the correct series of tags (see also Fig. 8.11): \n(8.20) Janet/NNP will/MD back/VB the/DT bill/NN \nLet the HMM be defined by the two tables in Fig. 8.12 and Fig. 8.13. Figure 8.12 lists the $a _ { i j }$ probabilities for transitioning between the hidden states (part-of-speech tags). Figure 8.13 expresses the $b _ { i } ( o _ { t } )$ probabilities, the observation likelihoods of words given tags. This table is (slightly simplified) from counts in the WSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts of speech, and the word the can appear as a determiner or as an NNP (in titles like “Somewhere Over the Rainbow” all words are tagged as NNP).",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "HMM Part-of-Speech Tagging",
        "subsection": "The Viterbi Algorithm",
        "subsubsection": "N/A"
    },
    {
        "content": "$overline { { nu _ { t - 1 } ( i ) } }$ the previous Viterbi path probability from the previous time step ai j the transition probability from previous state $q _ { i }$ to current state $q _ { j }$ $b _ { j } ( o _ { t } )$ the state observation likelihood of the observation symbol $o _ { t }$ given the current state $j$ \n8.4.6 Working through an example \nLet’s tag the sentence Janet will back the bill; the goal is the correct series of tags (see also Fig. 8.11): \n(8.20) Janet/NNP will/MD back/VB the/DT bill/NN \nLet the HMM be defined by the two tables in Fig. 8.12 and Fig. 8.13. Figure 8.12 lists the $a _ { i j }$ probabilities for transitioning between the hidden states (part-of-speech tags). Figure 8.13 expresses the $b _ { i } ( o _ { t } )$ probabilities, the observation likelihoods of words given tags. This table is (slightly simplified) from counts in the WSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts of speech, and the word the can appear as a determiner or as an NNP (in titles like “Somewhere Over the Rainbow” all words are tagged as NNP). \n\nThere are $N = 5$ state columns. We begin in column 1 (for the word Janet) by setting the Viterbi value in each cell to the product of the $pi$ transition probability (the start probability for that state $i$ , which we get from the ${ < s > }$ entry of Fig. 8.12), and the observation likelihood of the word Janet given the tag for that cell. Most of the cells in the column are zero since the word Janet cannot be any of those tags. The reader should find this in Fig. 8.14. \n\nNext, each cell in the will column gets updated. For each state, we compute the value viterb $big [ s , t big ]$ by taking the maximum over the extensions of all the paths from the previous column that lead to the current cell according to Eq. 8.19. We have shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 values from the previous column, multiplied by the appropriate transition probability; as it happens in this case, most of them are zero from the previous column. The remaining value is multiplied by the relevant observation probability, and the (trivial) max is taken. In this case the final value, 2.772e-8, comes from the NNP state at the previous column. The reader should fill in the rest of the lattice in Fig. 8.14 and backtrace to see whether or not the Viterbi algorithm returns the gold state sequence NNP MD VB DT NN. \n8.5 Conditional Random Fields (CRFs) \nunknown words \nWhile the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate. It would be great to have ways to add arbitrary features to help with this, perhaps based on capitalization or morphology (words starting with capital letters are likely to be proper nouns, words ending with -ed tend to be past tense (VBD or VBN), etc.) Or knowing the previous or following words might be a useful feature (if the previous word is the, the current tag is unlikely to be a verb). \nCRF \nAlthough we could try to hack the HMM to find ways to incorporate some of these, in general it’s hard for generative models like HMMs to add arbitrary features directly into the model in a clean way. We’ve already seen a model for combining arbitrary features in a principled way: log-linear models like the logistic regression model of Chapter 5! But logistic regression isn’t a sequence model; it assigns a class to a single observation. \nLuckily, there is a discriminative sequence model based on log-linear models: the conditional random field (CRF). We’ll describe here the linear chain CRF, the version of the CRF most commonly used for language processing, and the one whose conditioning closely matches the HMM. \nAssuming we have a sequence of input words $X = x _ { 1 } . . . x _ { n }$ and want to compute a sequence of output tags $Y = y _ { 1 } . . . y _ { n }$ . In an HMM to compute the best tag sequence that maximizes $P ( { Y } | { X } )$ we rely on Bayes’ rule and the likelihood $P ( X | Y )$ : \nIn a CRF, by contrast, we compute the posterior $p ( Y | X )$ directly, training the CRF",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "HMM Part-of-Speech Tagging",
        "subsection": "Working through an example",
        "subsubsection": "N/A"
    },
    {
        "content": "8.5.1 Features in a CRF POS Tagger \nLet’s look at some of these features in detail, since the reason to use a discriminative sequence model is that it’s easier to incorporate a lot of features.2 \nAgain, in a linear-chain CRF, each local feature $f _ { k }$ at position $i$ can depend on any information from: $( y _ { i - 1 } , y _ { i } , X , i )$ . So some legal features representing common situations might be the following: \nFor simplicity, we’ll assume all CRF features take on the value 1 or 0. Above, we explicitly use the notation $mathbb { 1 } { x }$ to mean $^ { ast } 1$ if $x$ is true, and 0 otherwise”. From now on, we’ll leave off the $mathbb { 1 }$ when we define features, but you can assume each feature has it there implicitly. \nAlthough the idea of what features to use is done by the system designer by hand, the specific features are automatically populated by using feature templates as we briefly mentioned in Chapter 5. Here are some templates that only use information from $y _ { i - 1 } , y _ { i } , X , i )$ : \nThese templates automatically populate the set of features from every instance in the training and test set. Thus for our example Janet/NNP will/MD back/VB the/DT bill/NN, when $x _ { i }$ is the word back, the following features would be generated and have the value 1 (we’ve assigned them arbitrary feature numbers): \nword shape \nIt’s also important to have features that help with unknown words. One of the most important is word shape features, which represent the abstract letter pattern of the word by mapping lower-case letters to $mathbf { dot { rho } } _ { mathbf { X } } ,$ , upper-case to $mathbf { partial } ^ { cdot } mathbf { X } ^ { prime }$ , numbers to ’d’, and retaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would map to XXdd-dd. A second class of shorter word shape features is also used. In these features consecutive character types are removed, so words in all caps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to Xd-d but I.M.F would still map to X.X.X. Prefix and suffix features are also useful. In summary, here are some sample feature templates that help with unknown words: \n$x _ { i }$ contains a particular prefix (perhaps from all prefixes of length $leq 2$ ) $x _ { i }$ contains a particular suffix (perhaps from all suffixes of length $leq 2$ ) $x _ { i }$ ’s word shape   \n$x _ { i }$ ’s short word shape \nFor example the word well-dressed might generate the following non-zero valued feature values: \nThe known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count $< 5$ in the training set. \nRemember that in a CRF we don’t learn weights for each of these local features $f _ { k }$ . Instead, we first sum the values of each local feature (for example feature $f _ { 3 7 4 3 } mathrm { dot { } }$ ) over the entire sentence, to create each global feature (for example $F _ { 3 7 4 3 }$ ). It is those global features that will then be multiplied by weight $w _ { 3 7 4 3 }$ . Thus for training and inference there is always a fixed set of $K$ features with $K$ weights, even though the length of each sentence is different. \n8.5.2 Features for CRF Named Entity Recognizers \nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15. \ngazetteer \nOne feature that is especially useful for locations is a gazetteer, a list of place names, often providing millions of entries for locations with detailed geographical and political information.3 This can be implemented as a binary feature indicating a phrase appears in the list. Other related resources like name-lists, for example from the United States Census Bureau4, can be used, as can other entity dictionaries like lists of corporations or products, although they may not be as helpful as a gazetteer (Mikheev et al., 1999). \nThe sample named entity token L’Occitane would generate the following nonzero valued feature values (assuming that L’Occitane is neither in the gazetteer nor the census).",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Conditional Random Fields (CRFs)",
        "subsection": "Features in a CRF POS Tagger",
        "subsubsection": "N/A"
    },
    {
        "content": "The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count $< 5$ in the training set. \nRemember that in a CRF we don’t learn weights for each of these local features $f _ { k }$ . Instead, we first sum the values of each local feature (for example feature $f _ { 3 7 4 3 } mathrm { dot { } }$ ) over the entire sentence, to create each global feature (for example $F _ { 3 7 4 3 }$ ). It is those global features that will then be multiplied by weight $w _ { 3 7 4 3 }$ . Thus for training and inference there is always a fixed set of $K$ features with $K$ weights, even though the length of each sentence is different. \n8.5.2 Features for CRF Named Entity Recognizers \nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15. \ngazetteer \nOne feature that is especially useful for locations is a gazetteer, a list of place names, often providing millions of entries for locations with detailed geographical and political information.3 This can be implemented as a binary feature indicating a phrase appears in the list. Other related resources like name-lists, for example from the United States Census Bureau4, can be used, as can other entity dictionaries like lists of corporations or products, although they may not be as helpful as a gazetteer (Mikheev et al., 1999). \nThe sample named entity token L’Occitane would generate the following nonzero valued feature values (assuming that L’Occitane is neither in the gazetteer nor the census). \nprefix $left( x _ { i } right) = mathbf { L }$ $begin{array} { r l } & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { t a n } mathfrak { e } }  & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { a n e } }  & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { n e } }  & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { e } } end{array}$ e   \nprefix $( x _ { i } ) = mathbf { L }$ ’   \nprefi $mathtt { Psi } ( x _ { i } ) = mathtt { L } ^ { prime } 0$   \nprefix $mathtt { Gamma } ( x _ { i } ) = mathtt { L } ^ { prime } 0 mathtt { c }$   \nword-shape $( x _ { i } ) = mathbf { X } ^ { prime }$ Xxxxxxxx short-word-shape $( x _ { i } ) = mathbf { X } ^ { prime } mathbf { X } mathbf { x }$ \n8.5.3 Inference and Training for CRFs \nHow do we find the best tag sequence $hat { Y }$ for a given input $X ?$ We start with Eq. 8.22: \nWe can ignore the exp function and the denominator $Z ( X )$ , as we do above, because exp doesn’t change the argmax, and the denominator $Z ( X )$ is constant for a given observation sequence $X$ . \nHow should we decode to find this optimal tag sequence $hat { y } ?$ Just as with HMMs, we’ll turn to the Viterbi algorithm, which works because, like the HMM, the linearchain CRF depends at each timestep on only one previous output token $y _ { i - 1 }$ . \nConcretely, this involves filling an $N times T$ array with the appropriate values, maintaining backpointers as we proceed. As with HMM Viterbi, when the table is filled, we simply follow pointers back from the maximum value in the final column to retrieve the desired set of labels.",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Conditional Random Fields (CRFs)",
        "subsection": "Features for CRF Named Entity Recognizers",
        "subsubsection": "N/A"
    },
    {
        "content": "prefix $left( x _ { i } right) = mathbf { L }$ $begin{array} { r l } & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { t a n } mathfrak { e } }  & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { a n e } }  & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { n e } }  & { mathrm { s u f f i x } ( x _ { i } ) = mathrm { e } } end{array}$ e   \nprefix $( x _ { i } ) = mathbf { L }$ ’   \nprefi $mathtt { Psi } ( x _ { i } ) = mathtt { L } ^ { prime } 0$   \nprefix $mathtt { Gamma } ( x _ { i } ) = mathtt { L } ^ { prime } 0 mathtt { c }$   \nword-shape $( x _ { i } ) = mathbf { X } ^ { prime }$ Xxxxxxxx short-word-shape $( x _ { i } ) = mathbf { X } ^ { prime } mathbf { X } mathbf { x }$ \n8.5.3 Inference and Training for CRFs \nHow do we find the best tag sequence $hat { Y }$ for a given input $X ?$ We start with Eq. 8.22: \nWe can ignore the exp function and the denominator $Z ( X )$ , as we do above, because exp doesn’t change the argmax, and the denominator $Z ( X )$ is constant for a given observation sequence $X$ . \nHow should we decode to find this optimal tag sequence $hat { y } ?$ Just as with HMMs, we’ll turn to the Viterbi algorithm, which works because, like the HMM, the linearchain CRF depends at each timestep on only one previous output token $y _ { i - 1 }$ . \nConcretely, this involves filling an $N times T$ array with the appropriate values, maintaining backpointers as we proceed. As with HMM Viterbi, when the table is filled, we simply follow pointers back from the maximum value in the final column to retrieve the desired set of labels. \nThe requisite changes from HMM Viterbi have to do only with how we fill each cell. Recall from Eq. 8.19 that the recursive step of the Viterbi equation computes the Viterbi value of time $t$ for state $j$ as \nwhich is the HMM implementation of \nThe CRF requires only a slight change to this latter formula, replacing the $a$ and $b$ prior and likelihood probabilities with the CRF features: \nLearning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important. \n8.6 Evaluation of Named Entity Recognition \nPart-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and $mathbf { F } _ { 1 }$ measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and $F _ { mathrm { ell } }$ -measure is the harmonic mean of the two. \nTo know if the difference between the $mathrm { F } _ { 1 }$ scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test (Section 4.9). \nFor named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response. \nThe fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions. \n8.7 Further Details \nIn this section we summarize a few remaining details of the data and models for part-of-speech tagging and NER, beginning with data. Since the algorithms we have presented are supervised, having labeled data is essential for training and test. A wide variety of datasets exist for part-of-speech tagging and/or NER. The Universal Dependencies (UD) dataset (Nivre et al., 2016b) has POS tagged corpora in 92 languages at the time of this writing, as do the Penn Treebanks in English, Chinese, and Arabic. OntoNotes has corpora labeled for named entities in English, Chinese, and Arabic (Hovy et al., 2006). Named entity tagged corpora also available in particular domains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al., 2019).",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Conditional Random Fields (CRFs)",
        "subsection": "Inference and Training for CRFs",
        "subsubsection": "N/A"
    },
    {
        "content": "The requisite changes from HMM Viterbi have to do only with how we fill each cell. Recall from Eq. 8.19 that the recursive step of the Viterbi equation computes the Viterbi value of time $t$ for state $j$ as \nwhich is the HMM implementation of \nThe CRF requires only a slight change to this latter formula, replacing the $a$ and $b$ prior and likelihood probabilities with the CRF features: \nLearning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important. \n8.6 Evaluation of Named Entity Recognition \nPart-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and $mathbf { F } _ { 1 }$ measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and $F _ { mathrm { ell } }$ -measure is the harmonic mean of the two. \nTo know if the difference between the $mathrm { F } _ { 1 }$ scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test (Section 4.9). \nFor named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response. \nThe fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions. \n8.7 Further Details \nIn this section we summarize a few remaining details of the data and models for part-of-speech tagging and NER, beginning with data. Since the algorithms we have presented are supervised, having labeled data is essential for training and test. A wide variety of datasets exist for part-of-speech tagging and/or NER. The Universal Dependencies (UD) dataset (Nivre et al., 2016b) has POS tagged corpora in 92 languages at the time of this writing, as do the Penn Treebanks in English, Chinese, and Arabic. OntoNotes has corpora labeled for named entities in English, Chinese, and Arabic (Hovy et al., 2006). Named entity tagged corpora also available in particular domains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al., 2019).",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Evaluation of Named Entity Recognition",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "8.7.1 Rule-based Methods \nWhile machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013). For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018). \nOne common approach is to make repeated rule-based passes over a text, starting with rules with very high precision but low recall, and, in subsequent stages, using machine learning methods that take the output of the first pass into account (an approach first worked out for coreference (Lee et al., 2017a)): \n1. First, use high-precision rules to tag unambiguous entity mentions.   \n2. Then, search for substring matches of the previously detected names.   \n3. Use application-specific name lists to find likely domain-specific mentions.   \n4. Finally, apply supervised sequence labeling techniques that use tags from pre  \nvious stages as additional features. \nRule-based methods were also the earliest methods for part-of-speech tagging. Rule-based taggers like the English Constraint Grammar system (Karlsson et al. 1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s: (1) a morphological analyzer with tens of thousands of word stem entries returns all parts of speech for a word, then (2) a large set of thousands of constraints are applied to the input sentence to rule out parts of speech inconsistent with the context. \n8.7.2 POS Tagging for Morphologically Rich Languages \nAugmentations to tagging algorithms become necessary when dealing with languages with rich morphology like Czech, Hungarian and Turkish. \nThese productive word-formation processes result in a large vocabulary for these languages: a 250,000 word token corpus of Hungarian has more than twice as many word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while a 10 million word token corpus of Turkish contains four times as many word types as a similarly sized English corpus (Hakkani-Tu¨r et al., 2002). Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages (including Czech, Slovene, Estonian, and Romanian) (Hajicˇ, 2000). \nHighly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Here’s a Turkish example, in which the word izin has three possible morphological/part-of-speech tags and meanings (Hakkani-T¨ur et al., 2002):",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Further Details ",
        "subsection": "Rule-based Methods",
        "subsubsection": "N/A"
    },
    {
        "content": "8.7.1 Rule-based Methods \nWhile machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013). For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018). \nOne common approach is to make repeated rule-based passes over a text, starting with rules with very high precision but low recall, and, in subsequent stages, using machine learning methods that take the output of the first pass into account (an approach first worked out for coreference (Lee et al., 2017a)): \n1. First, use high-precision rules to tag unambiguous entity mentions.   \n2. Then, search for substring matches of the previously detected names.   \n3. Use application-specific name lists to find likely domain-specific mentions.   \n4. Finally, apply supervised sequence labeling techniques that use tags from pre  \nvious stages as additional features. \nRule-based methods were also the earliest methods for part-of-speech tagging. Rule-based taggers like the English Constraint Grammar system (Karlsson et al. 1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s: (1) a morphological analyzer with tens of thousands of word stem entries returns all parts of speech for a word, then (2) a large set of thousands of constraints are applied to the input sentence to rule out parts of speech inconsistent with the context. \n8.7.2 POS Tagging for Morphologically Rich Languages \nAugmentations to tagging algorithms become necessary when dealing with languages with rich morphology like Czech, Hungarian and Turkish. \nThese productive word-formation processes result in a large vocabulary for these languages: a 250,000 word token corpus of Hungarian has more than twice as many word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while a 10 million word token corpus of Turkish contains four times as many word types as a similarly sized English corpus (Hakkani-Tu¨r et al., 2002). Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages (including Czech, Slovene, Estonian, and Romanian) (Hajicˇ, 2000). \nHighly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Here’s a Turkish example, in which the word izin has three possible morphological/part-of-speech tags and meanings (Hakkani-T¨ur et al., 2002): \n\n1. Yerdeki izin temizlenmesi gerek. $mathrm { i z } + tt N o u n { + } tt A 3 s g { + } tt P n o n { + } tt G e n$ The trace on the floor should be cleaned.   \n2. ¨ zerinde parmak izin kalmi¸s iz + Noun+A3sg+P2sg+Nom Your finger print is left on (it).   \n3. I¸ceri girmek ic¸in izin alman gerekiyor. izin + Noun+A3sg+Pnon+Nom You need permission to enter. \nUsing a morphological parse sequence like $mathtt { N o u n } + mathtt { A } 3 mathtt { s g } + mathtt { P n o n } + mathtt { G e n }$ as the partof-speech tag greatly increases the number of parts of speech, and so tagsets can be 4 to 10 times larger than the 50–100 tags we have seen for English. With such large tagsets, each word needs to be morphologically analyzed to generate the list of possible morphological tag sequences (part-of-speech tags) for the word. The role of the tagger is then to disambiguate among these tags. This method also helps with unknown words since morphological parsers can accept unknown stems and still segment the affixes properly. \n8.8 Summary \nThis chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition: \n• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.   \n• Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.   \n• Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren’t strictly entities or even proper nouns.   \n• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.   \n• The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence   \n• Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Further Details ",
        "subsection": "POS Tagging for Morphologically Rich Languages",
        "subsubsection": "N/A"
    },
    {
        "content": "1. Yerdeki izin temizlenmesi gerek. $mathrm { i z } + tt N o u n { + } tt A 3 s g { + } tt P n o n { + } tt G e n$ The trace on the floor should be cleaned.   \n2. ¨ zerinde parmak izin kalmi¸s iz + Noun+A3sg+P2sg+Nom Your finger print is left on (it).   \n3. I¸ceri girmek ic¸in izin alman gerekiyor. izin + Noun+A3sg+Pnon+Nom You need permission to enter. \nUsing a morphological parse sequence like $mathtt { N o u n } + mathtt { A } 3 mathtt { s g } + mathtt { P n o n } + mathtt { G e n }$ as the partof-speech tag greatly increases the number of parts of speech, and so tagsets can be 4 to 10 times larger than the 50–100 tags we have seen for English. With such large tagsets, each word needs to be morphologically analyzed to generate the list of possible morphological tag sequences (part-of-speech tags) for the word. The role of the tagger is then to disambiguate among these tags. This method also helps with unknown words since morphological parsers can accept unknown stems and still segment the affixes properly. \n8.8 Summary \nThis chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition: \n• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.   \n• Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.   \n• Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren’t strictly entities or even proper nouns.   \n• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.   \n• The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence   \n• Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "1. Yerdeki izin temizlenmesi gerek. $mathrm { i z } + tt N o u n { + } tt A 3 s g { + } tt P n o n { + } tt G e n$ The trace on the floor should be cleaned.   \n2. ¨ zerinde parmak izin kalmi¸s iz + Noun+A3sg+P2sg+Nom Your finger print is left on (it).   \n3. I¸ceri girmek ic¸in izin alman gerekiyor. izin + Noun+A3sg+Pnon+Nom You need permission to enter. \nUsing a morphological parse sequence like $mathtt { N o u n } + mathtt { A } 3 mathtt { s g } + mathtt { P n o n } + mathtt { G e n }$ as the partof-speech tag greatly increases the number of parts of speech, and so tagsets can be 4 to 10 times larger than the 50–100 tags we have seen for English. With such large tagsets, each word needs to be morphologically analyzed to generate the list of possible morphological tag sequences (part-of-speech tags) for the word. The role of the tagger is then to disambiguate among these tags. This method also helps with unknown words since morphological parsers can accept unknown stems and still segment the affixes properly. \n8.8 Summary \nThis chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition: \n• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.   \n• Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.   \n• Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren’t strictly entities or even proper nouns.   \n• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.   \n• The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence   \n• Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "1. Yerdeki izin temizlenmesi gerek. $mathrm { i z } + tt N o u n { + } tt A 3 s g { + } tt P n o n { + } tt G e n$ The trace on the floor should be cleaned.   \n2. ¨ zerinde parmak izin kalmi¸s iz + Noun+A3sg+P2sg+Nom Your finger print is left on (it).   \n3. I¸ceri girmek ic¸in izin alman gerekiyor. izin + Noun+A3sg+Pnon+Nom You need permission to enter. \nUsing a morphological parse sequence like $mathtt { N o u n } + mathtt { A } 3 mathtt { s g } + mathtt { P n o n } + mathtt { G e n }$ as the partof-speech tag greatly increases the number of parts of speech, and so tagsets can be 4 to 10 times larger than the 50–100 tags we have seen for English. With such large tagsets, each word needs to be morphologically analyzed to generate the list of possible morphological tag sequences (part-of-speech tags) for the word. The role of the tagger is then to disambiguate among these tags. This method also helps with unknown words since morphological parsers can accept unknown stems and still segment the affixes properly. \n8.8 Summary \nThis chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition: \n• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.   \n• Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.   \n• Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren’t strictly entities or even proper nouns.   \n• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.   \n• The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence   \n• Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training, \nBibliographical and Historical Notes \nWhat is probably the earliest part-of-speech tagger was part of the parser in Zellig Harris’s Transformations and Discourse Analysis Project (TDAP), implemented between June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962), although earlier systems had used part-of-speech dictionaries. TDAP used 14 handwritten rules for part-of-speech disambiguation; the use of part-of-speech tag sequences and the relative frequency of tags for a word prefigures modern algorithms. The parser was implemented essentially as a cascade of finite-state transducers; see Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation. \nThe Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUNADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucˇera (1982, p. 9), accurately tagged $7 7 %$ of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word. \nProbabilities were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The Lancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Marshall 1987; Garside 1987), a probabilistic algorithm that approximated a simplified HMM tagger. The algorithm used tag bigram probabilities, but instead of storing the word likelihood of each tag, the algorithm marked tags either as rare $( P ( log | mathrm { w o r d } ) <$ .01) infrequent $( P ( mathrm { t a g } | mathrm { w o r d } ) < . 1 0 )$ or normally frequent $( P ( mathrm { t a g } | mathrm { w o r d } ) > . 1 0 )$ . \nDeRose (1988) developed a quasi-HMM algorithm, including the use of dynamic programming, although computing $P ( t | w ) P ( w )$ instead of $P ( w | t ) P ( w )$ . The same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the first implemented HMM tagger, described correctly in Church (1989), although Church (1988) also described the computation incorrectly as $P ( t | w ) P ( w )$ instead of $P ( w | t ) P ( w )$ . Church (p.c.) explained that he had simplified for pedagogical purposes because using the probability $P ( t | w )$ made the idea seem more understandable as “storing a lexicon in an almost standard form”. \nLater taggers explicitly introduced the use of the hidden Markov model (Kupiec 1992; Weischedel et al. 1993; Schu¨tze and Singer 1994). Merialdo (1994) showed that fully unsupervised EM didn’t work well for the tagging task and that reliance on hand-labeled data was important. Charniak et al. (1993) showed the importance of the most frequent tag baseline; the $9 2 . 3 %$ number we give above was from Abney et al. (1999). See Brants (2000) for HMM tagger implementation details, including the extension to trigram contexts, and the use of sophisticated unknown word features; its performance is still close to state of the art taggers. \nLog-linear models for POS tagging were introduced by Ratnaparkhi (1996), who introduced a system called MXPOST which implemented a maximum entropy Markov model (MEMM), a slightly simpler version of a CRF. Around the same time, sequence labelers were applied to the task of named entity tagging, first with HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once CRFs were developed (Lafferty et al. 2001), they were also applied to NER (McCallum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005). Neural approaches to NER mainly follow from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and character-based embeddings as input followed shortly and became a standard neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016) followed by the more recent use of Transformers and BERT. \nThe idea of using letter suffixes for unknown words is quite old; the early Klein and Simmons (1963) system checked all final letter suffixes of lengths 1-5. The unknown word features described on page 175 come mainly from Ratnaparkhi (1996), with augmentations from Toutanova et al. (2003) and Manning (2011). \nState of the art POS taggers use neural algorithms, either bidirectional RNNs or Transformers like BERT; see Chapter 9 and Chapter 11. HMM (Brants 2000; Thede and Harper 1999) and CRF tagger accuracies are likely just a tad lower. \nManning (2011) investigates the remaining $2 . 7 %$ of errors in a high-performing tagger (Toutanova et al., 2003). He suggests that a third or half of these remaining errors are due to errors or inconsistencies in the training data, a third might be solvable with richer linguistic models, and for the remainder the task is underspecified or unclear. \nSupervised tagging relies heavily on in-domain training data hand-labeled by experts. Ways to relax this assumption include unsupervised algorithms for clustering words into part-of-speech-like classes, summarized in Christodoulopoulos et al. (2010), and ways to combine labeled and unlabeled data, for example by co-training (Clark et al. 2003; Søgaard 2010). \nSee Householder (1995) for historical notes on parts of speech, and Sampson (1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets. \nExercises \n8.1 Find one tagging error in each of the following sentences that are tagged with the Penn Treebank tagset: \n1. I/PRP need/VBP a/DT flight/NN from/IN Atlanta/NN   \n2. Does/VBZ this/DT flight/NN serve/VB dinner/NNS   \n3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP   \n4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN flights/NNS \n8.2 Use the Penn Treebank tagset to tag each word in the following sentences from Damon Runyon’s short stories. You may ignore punctuation. Some of these are quite difficult; do your best. \n1. It is a nice night.   \n2. This crap game is over a garage in Fifty-second Street. . .   \n3. . . . Nobody ever takes the newspapers she sells . .   \n4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a   \nmournful voice. \n5. . . . I am sitting in Mindy’s restaurant putting on the gefillte fish, which is a dish I am very fond of, .   \n6. When a guy and a doll get to taking peeks back and forth at each other, why there you are indeed. \n8.3 Now compare your tags from the previous exercise with one or two friend’s answers. On which words did you disagree the most? Why? \n8.4 Implement the “most likely tag” baseline. Find a POS-tagged training set, and use it to compute for each word the tag that maximizes $p ( t | w )$ . You will need to implement a simple tokenizer to deal with sentence boundaries. Start by assuming that all unknown words are NN and compute your error rate on known and unknown words. Now write at least five rules to do a better job of tagging unknown words, and show the difference in error rates. \n8.5 Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus. First split the corpus into a training set and test set. From the labeled training set, train the transition and observation probabilities of the HMM tagger directly on the hand-tagged data. Then implement the Viterbi algorithm so you can decode a test sentence. Now run your algorithm on the test set. Report its error rate and compare its performance to the most frequent tag baseline. \n8.6 Do an error analysis of your tagger. Build a confusion matrix and investigate the most frequent errors. Propose some features for improving the performance of your tagger on these errors. \n8.7 Develop a set of regular expressions to recognize the character shape features described on page 175. \n8.8 The BIO and other labeling schemes given in this chapter aren’t the only possible one. For example, the B tag can be reserved only for those situations where an ambiguity exists between adjacent entities. Propose a new set of BIO tags for use with your NER system. Experiment with it and compare its performance with the schemes presented in this chapter. \n8.9 Names of works of art (books, movies, video games, etc.) are quite different from the kinds of named entities we’ve discussed in this chapter. Collect a list of names of works of art from a particular category from a Web-based source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list and give examples of ways that the names in it are likely to be problematic for the techniques described in this chapter. \n8.10 Develop an NER system specific to the category of names that you collected in the last exercise. Evaluate your system on a collection of text likely to contain instances of these named entities. \n9 \nDeep Learning Architectures for Sequence Processing \nTime will explain. Jane Austen, Persuasion \nLanguage is an inherently temporal phenomenon. Spoken language is a sequence of acoustic events over time, and we comprehend and produce both spoken and written language as a continuous input stream. The temporal nature of language is reflected in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter streams, all of which emphasize that language is a sequence that unfolds in time. \nThis temporal nature is reflected in some of the algorithms we use to process language. For example, the Viterbi algorithm applied to HMM part-of-speech tagging, proceeds through the input a word at a time, carrying forward information gleaned along the way. Yet other machine learning approaches, like those we’ve studied for sentiment analysis or other text classification tasks don’t have this temporal nature – they assume simultaneous access to all aspects of their input. \nThe feedforward networks of Chapter 7 also assumed simultaneous access, although they also had a simple model for time. Recall that we applied feedforward networks to language modeling by having them look only at a fixed-size window of words, and then sliding this window over the input, making independent predictions along the way. Fig. 9.1, reproduced from Chapter 7, shows a neural language model with window size 3 predicting what word follows the input for all the. Subsequent words are predicted by sliding the window forward a word at a time. \nThe simple feedforward sliding-window is promising, but isn’t a completely satisfactory solution to temporality. By using embeddings as inputs, it does solve the main problem of the simple n-gram models of Chapter 3 (recall that n-grams were based on words rather than embeddings, making them too literal, unable to generalize across contexts of similar words). But feedforward networks still share another weakness of n-gram approaches: limited context. Anything outside the context window has no impact on the decision being made. Yet many language tasks require access to information that can be arbitrarily distant from the current word. Second, the use of windows makes it difficult for networks to learn systematic patterns arising from phenomena like constituency and compositionality: the way the meaning of words in phrases combine together. For example, in Fig. 9.1 the phrase all the appears in one window in the second and third positions, and in the next window in the first and second positions, forcing the network to learn two separate patterns for what should be the same item. \nThis chapter introduces two important deep learning architectures designed to address these challenges: recurrent neural networks and transformer networks. Both approaches have mechanisms to deal directly with the sequential nature of language that allow them to capture and exploit the temporal nature of language. The recurrent network offers a new way to represent the prior context, allowing the model’s decision to depend on information from hundreds of words in the past. The transformer offers new mechanisms (self-attention and positional encodings) that help represent time and help focus on how words relate to each other over long distances. We’ll see how to apply both models to the task of language modeling, to sequence modeling tasks like part-of-speech tagging, and to text classification tasks like sentiment analysis.",
        "chapter": "Sequence Labeling for Parts of Speech and Named Entities",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "9.1 Language Models Revisited \nIn this chapter, we’ll begin exploring the RNN and transformer architectures through the lens of probabilistic language models, so let’s briefly remind ourselves of the framework for language modeling. Recall from Chapter 3 that probabilistic language models predict the next word in a sequence given some preceding context. For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “fish” we would compute: \nLanguage models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. We can also assign probabilities to entire sequences by using these conditional probabilities in combination with the chain rule: \nRecall that we evaluate language models by examining how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words). \nperplexity \nWe instantiate this intuition by using perplexity to measure the quality of a language model. Recall from page 37 that the perplexity (PP) of a model $theta$ on an unseen test set is the inverse probability that $theta$ assigns to the test set, normalized by the test set length. For a test set $w _ { 1 : n }$ , the perplexity is \nTo visualize how perplexity can be computed as a function of the probabilities our LM will compute for each new word, we can use the chain rule to expand the computation of probability of the test set: \n9.2 Recurrent Neural Networks \nElman Networks \nA recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. While powerful, such networks are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely effective when applied to language. In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent networks. These networks are useful in their own right and serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks discussed later in this chapter. In this chapter when we use the term RNN we’ll be referring to these simpler more constrained networks (although you will often see the term RNN to mean any net with recurrent properties including LSTMs). \nFig. 9.2 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, $mathbf { x } _ { t }$ , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, $mathbf { y } _ { t }$ . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We’ll use subscripts to represent time, thus $mathbf { x } _ { t }$ will mean the input vector $pmb { x }$ at time $t$ . The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Language Models Revisited",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. \nAdding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. But in reality, they’re not all that different. Given an input vector and the values for the hidden layer from the previous time step, we’re still performing the standard feedforward calculation introduced in Chapter 7. To see this, consider Fig. 9.3 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights determine how the network makes use of past context in calculating the output for the current input. As with the other weights in the network, these connections are trained via backpropagation. \n9.2.1 Inference in RNNs \nForward inference (mapping a sequence of inputs to a sequence of outputs) in an RNN is nearly identical to what we’ve already seen with feedforward networks. To compute an output $mathbf { y } _ { t }$ for an input $mathbf { x } _ { t }$ , we need the activation value for the hidden layer $mathbf { h } _ { t }$ . To calculate this, we multiply the input $mathbf { x } _ { t }$ with the weight matrix $boldsymbol { mathsf { w } }$ , and the hidden layer from the previous time step $mathbf { h } _ { t - 1 }$ with the weight matrix U. We add these values together and pass them through a suitable activation function, $g$ , to arrive at the activation value for the current hidden layer, $mathbf { h } _ { t }$ . Once we have the values for the hidden layer, we proceed with the usual computation to generate the output vector. \nIt’s worthwhile here to be careful about specifying the dimensions of the input, hidden and output layers, as well as the weight matrices to make sure these calculations \nare correct. Let’s refer to the input, hidden and output layer dimensions as $d _ { i n } , d _ { h }$ , and $d _ { o u t }$ respectively. Given this, our three parameter matrices are: $W in mathbb { R } ^ { d _ { h } times d _ { i n } }$ , $mathbf { U } in mathbb { R } ^ { d _ { h } times d _ { h } }$ , and $boldsymbol { mathsf { V } } in mathbb { R } ^ { d _ { o u t } times d _ { h } }$ . \nIn the commonly encountered case of soft classification, computing $y _ { t }$ consists of a softmax computation that provides a probability distribution over the possible output classes. \nThe fact that the computation at time $t$ requires the value of the hidden layer from time $t - 1$ mandates an incremental inference algorithm that proceeds from the start of the sequence to the end as illustrated in Fig. 9.4. The sequential nature of simple recurrent networks can also be seen by unrolling the network in time as is shown in Fig. 9.5. In this figure, the various layers of units are copied for each time step to illustrate that they will have differing values over time. However, the various weight matrices are shared across time. \n9.2.2 Training \nAs with feedforward networks, we’ll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks. As shown in Fig. 9.3, we now have 3 sets of weights to update: $boldsymbol { mathsf { w } }$ , the weights from the input layer to the hidden layer, U, the weights from the previous hidden layer to the current hidden layer, and finally $pmb {  v }$ , the weights from the hidden layer to the output layer. \nFig. 9.5 highlights two considerations that we didn’t have to worry about with backpropagation in feedforward networks. First, to compute the loss function for the output at time t we need the hidden layer from time $t - 1$ . Second, the hidden layer at time $t$ influences both the output at time $t$ and the hidden layer at time $t + 1$ (and hence the output and loss at $t + 1$ ). It follows from this that to assess the error accruing to $mathbf { h } _ { t }$ , we’ll need to know its influence on both the current output as well as the ones that follow. \nTailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing $mathbf { h } _ { t }$ , $mathbf { y } _ { t }$ , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as Backpropagation Through Time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Recurrent Neural Networks",
        "subsection": "Inference in RNNs",
        "subsubsection": "N/A"
    },
    {
        "content": "are correct. Let’s refer to the input, hidden and output layer dimensions as $d _ { i n } , d _ { h }$ , and $d _ { o u t }$ respectively. Given this, our three parameter matrices are: $W in mathbb { R } ^ { d _ { h } times d _ { i n } }$ , $mathbf { U } in mathbb { R } ^ { d _ { h } times d _ { h } }$ , and $boldsymbol { mathsf { V } } in mathbb { R } ^ { d _ { o u t } times d _ { h } }$ . \nIn the commonly encountered case of soft classification, computing $y _ { t }$ consists of a softmax computation that provides a probability distribution over the possible output classes. \nThe fact that the computation at time $t$ requires the value of the hidden layer from time $t - 1$ mandates an incremental inference algorithm that proceeds from the start of the sequence to the end as illustrated in Fig. 9.4. The sequential nature of simple recurrent networks can also be seen by unrolling the network in time as is shown in Fig. 9.5. In this figure, the various layers of units are copied for each time step to illustrate that they will have differing values over time. However, the various weight matrices are shared across time. \n9.2.2 Training \nAs with feedforward networks, we’ll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks. As shown in Fig. 9.3, we now have 3 sets of weights to update: $boldsymbol { mathsf { w } }$ , the weights from the input layer to the hidden layer, U, the weights from the previous hidden layer to the current hidden layer, and finally $pmb {  v }$ , the weights from the hidden layer to the output layer. \nFig. 9.5 highlights two considerations that we didn’t have to worry about with backpropagation in feedforward networks. First, to compute the loss function for the output at time t we need the hidden layer from time $t - 1$ . Second, the hidden layer at time $t$ influences both the output at time $t$ and the hidden layer at time $t + 1$ (and hence the output and loss at $t + 1$ ). It follows from this that to assess the error accruing to $mathbf { h } _ { t }$ , we’ll need to know its influence on both the current output as well as the ones that follow. \nTailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing $mathbf { h } _ { t }$ , $mathbf { y } _ { t }$ , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as Backpropagation Through Time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990). \nFortunately, with modern computational frameworks and adequate computing resources, there is no need for a specialized approach to training RNNs. As illustrated in Fig. 9.5, explicitly unrolling a recurrent network into a feedforward computational graph eliminates any explicit recurrences, allowing the network weights to be trained directly. In such an approach, we provide a template that specifies the basic structure of the network, including all the necessary parameters for the input, output, and hidden layers, the weight matrices, as well as the activation and output functions to be used. Then, when presented with a specific input sequence, we can generate an unrolled feedforward network specific to that input, and use that graph to perform forward inference or training via ordinary backpropagation. \nFor applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming of continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item. \n9.3 RNNs as Language Models \nRNN language models (Mikolov et al., 2010) process the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state. RNNs don’t have the limited context problem that $mathfrak { n }$ -gram models have, since the hidden state can in principle represent information about all of the preceding words all the way back to the beginning of the sequence. \nForward inference in a recurrent language model proceeds exactly as described in Section 9.2.1. The input sequence $pmb { mathsf { X } } = [ pmb { mathsf { x } } _ { 1 } ; . . . ; pmb { mathsf { x } } _ { t } ; . . . ; pmb { mathsf { x } } _ { N } ]$ consists of a series of word embeddings each represented as a one-hot vector of size $| V | times 1$ , and the output prediction, $pmb {  y }$ , is a vector representing a probability distribution over the vocabulary. At each step, the model uses the word embedding matrix E to retrieve the embedding for the current word, and then combines it with the hidden layer from the previous step to compute a new hidden layer. This hidden layer is then used to generate an output layer which is passed through a softmax layer to generate a probability distribution over the entire vocabulary. That is, at time $t$ :",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Recurrent Neural Networks",
        "subsection": "Training",
        "subsubsection": "N/A"
    },
    {
        "content": "Fortunately, with modern computational frameworks and adequate computing resources, there is no need for a specialized approach to training RNNs. As illustrated in Fig. 9.5, explicitly unrolling a recurrent network into a feedforward computational graph eliminates any explicit recurrences, allowing the network weights to be trained directly. In such an approach, we provide a template that specifies the basic structure of the network, including all the necessary parameters for the input, output, and hidden layers, the weight matrices, as well as the activation and output functions to be used. Then, when presented with a specific input sequence, we can generate an unrolled feedforward network specific to that input, and use that graph to perform forward inference or training via ordinary backpropagation. \nFor applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming of continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item. \n9.3 RNNs as Language Models \nRNN language models (Mikolov et al., 2010) process the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state. RNNs don’t have the limited context problem that $mathfrak { n }$ -gram models have, since the hidden state can in principle represent information about all of the preceding words all the way back to the beginning of the sequence. \nForward inference in a recurrent language model proceeds exactly as described in Section 9.2.1. The input sequence $pmb { mathsf { X } } = [ pmb { mathsf { x } } _ { 1 } ; . . . ; pmb { mathsf { x } } _ { t } ; . . . ; pmb { mathsf { x } } _ { N } ]$ consists of a series of word embeddings each represented as a one-hot vector of size $| V | times 1$ , and the output prediction, $pmb {  y }$ , is a vector representing a probability distribution over the vocabulary. At each step, the model uses the word embedding matrix E to retrieve the embedding for the current word, and then combines it with the hidden layer from the previous step to compute a new hidden layer. This hidden layer is then used to generate an output layer which is passed through a softmax layer to generate a probability distribution over the entire vocabulary. That is, at time $t$ : \n\nThe vector resulting from Vh can be thought of as a set of scores over the vocabulary given the evidence provided in h. Passing these scores through the softmax normalizes the scores into a probability distribution. The probability that a particular word $i$ in the vocabulary is the next word is represented by $mathbf { y } _ { t } [ i ]$ , the ith component of $mathbf { y } _ { t }$ : \nThe probability of an entire sequence is just the product of the probabilities of each item in the sequence, where we’ll use $mathbf { y } _ { i } [ boldsymbol { w } _ { i } ]$ to mean the probability of the true word $w _ { i }$ at time step $i$ . \nTo train an RNN as a language model, we use a corpus of text as training material, having the model predict the next word at each time step $t$ . We train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the correct distribution. \nIn the case of language modeling, the correct distribution $mathbf { y } _ { t }$ comes from knowing the next word. This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0. Thus, the cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word. So at time $t$ the CE loss is the negative log probability the model assigns to the next word in the training sequence. \nThus at each word position $t$ of the input, the model takes as input the correct sequence of tokens $w _ { 1 : t }$ , and uses them to compute a probability distribution over possible next words so as to compute the model’s loss for the next token $w _ { t + 1 }$ . Then we move to the next word, we ignore what the model predicted for the next word and instead use the correct sequence of tokens $w _ { 1 : t + 1 }$ to estimate the probability of token $w _ { t + 2 }$ . This idea that we always give the model the correct history sequence to \nteacher forcing \npredict the next word (rather than feeding the model its best case from the previous time step) is called teacher forcing. \nThe weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent. Fig. 9.6 illustrates this training regimen. \nCareful readers may have noticed that the input embedding matrix E and the final layer matrix $pmb {  v }$ , which feeds the output softmax, are quite similar. The columns of E represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar meaning and function will have similar embeddings. And, since the length of these embeddings corresponds to the size of the hidden layer $d _ { h }$ , the shape of the embedding matrix E is $d _ { h } times lvert V rvert$ . \nWeight tying \nThe final layer matrix $pmb { v }$ provides a way to score the likelihood of each word in the vocabulary given the evidence present in the final hidden layer of the network through the calculation of Vh. This results in a dimensionality $| V | times d _ { h }$ . That is, the rows of $V$ provide a second set of learned word embeddings that capture relevant aspects of word meaning and function. This leads to an obvious question – is it even necessary to have both? Weight tying is a method that dispenses with this redundancy and simply uses a single set of embeddings at the input and softmax layers. That is, we dispense with $V$ and use $E$ in both the start and end of the computation. \nIn addition to providing improved model perplexity, this approach significantly reduces the number of parameters required for the model. \n9.4 RNNs for other NLP tasks \nNow that we’ve seen the basic RNN architecture, let’s consider how to apply it to three types of NLP tasks: sequence classification tasks like sentiment analysis and",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "RNNs as Language Models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "topic classification, sequence labeling tasks like part-of-speech tagging, and text generation tasks. And we’ll see in Chapter 10 how to use them for encoder-decoder approaches to summarization, machine translation, and question answering. \n9.4.1 Sequence Labeling \nIn sequence labeling, the network’s task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from Chapter 8. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in Fig. 9.7. \nIn this figure, the inputs at each time step are pre-trained word embeddings corresponding to the input tokens. The RNN block is an abstraction that represents an unrolled simple recurrent network consisting of an input layer, hidden layer, and output layer at each time step, as well as the shared U, $pmb { v }$ and $boldsymbol { mathsf { w } }$ weight matrices that comprise the network. The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer. \nTo generate a sequence of tags for a given input, we run forward inference over the input sequence and select the most likely tag from the softmax at each step. Since we’re using a softmax layer to generate the probability distribution over the output tagset at each time step, we will again employ the cross-entropy loss during training. \n9.4.2 RNNs for Sequence Classification \nAnother use of RNNs is to classify entire sequences rather than the tokens within them. We’ve already encountered sentiment analysis in Chapter 4, in which we classify a text as positive or negative. Other sequence classification tasks for mapping sequences of text to one from a small set of categories include document-level topic classification, spam detection, or message routing for customer service applications. \nTo apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer at each time step. We can then take the hidden layer for the last token of the text, $ { mathbf { h } } _ { n }$ , to constitute a compressed representation of the entire sequence. We can pass this representation $ { mathbf { h } } _ { n }$ to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 9.8 illustrates this approach.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "RNNs for other NLP tasks",
        "subsection": "Sequence Labeling",
        "subsubsection": "N/A"
    },
    {
        "content": "topic classification, sequence labeling tasks like part-of-speech tagging, and text generation tasks. And we’ll see in Chapter 10 how to use them for encoder-decoder approaches to summarization, machine translation, and question answering. \n9.4.1 Sequence Labeling \nIn sequence labeling, the network’s task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from Chapter 8. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in Fig. 9.7. \nIn this figure, the inputs at each time step are pre-trained word embeddings corresponding to the input tokens. The RNN block is an abstraction that represents an unrolled simple recurrent network consisting of an input layer, hidden layer, and output layer at each time step, as well as the shared U, $pmb { v }$ and $boldsymbol { mathsf { w } }$ weight matrices that comprise the network. The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer. \nTo generate a sequence of tags for a given input, we run forward inference over the input sequence and select the most likely tag from the softmax at each step. Since we’re using a softmax layer to generate the probability distribution over the output tagset at each time step, we will again employ the cross-entropy loss during training. \n9.4.2 RNNs for Sequence Classification \nAnother use of RNNs is to classify entire sequences rather than the tokens within them. We’ve already encountered sentiment analysis in Chapter 4, in which we classify a text as positive or negative. Other sequence classification tasks for mapping sequences of text to one from a small set of categories include document-level topic classification, spam detection, or message routing for customer service applications. \nTo apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer at each time step. We can then take the hidden layer for the last token of the text, $ { mathbf { h } } _ { n }$ , to constitute a compressed representation of the entire sequence. We can pass this representation $ { mathbf { h } } _ { n }$ to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 9.8 illustrates this approach. \n\nNote that in this approach there don’t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 9.2.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. \nAnother option, instead of using just the last token $h _ { n }$ to represent the whole sequence, is to use some sort of pooling function of all the hidden states $h _ { i }$ for each word $i$ in the sequence. For example, we can create a representation that pools all the $n$ hidden states by taking their element-wise mean: \nOr we can take the element-wise max; the element-wise max of a set of $n$ vectors is a new vector whose $k$ th element is the max of the kth elements of all the $n$ vectors. \n9.4.3 Generation with RNN-Based Language Models \nRNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, and conversational dialogue; any ask where a system needs to produce text, conditioned on some other text. \nRecall back in Chapter 3 we saw how to generate text from an n-gram language model by adapting a technique suggested contemporaneously by Claude Shannon (Shannon, 1951) and the psychologists George Miller and Selfridge (Miller and Selfridge, 1950). We first randomly sample a word to begin a sequence based on its suitability as the start of a sequence. We then continue to sample words conditioned on our previous choices until we reach a pre-determined length, or an end of sequence token is generated.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "RNNs for other NLP tasks",
        "subsection": "RNNs for Sequence Classification",
        "subsubsection": "N/A"
    },
    {
        "content": "Note that in this approach there don’t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 9.2.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. \nAnother option, instead of using just the last token $h _ { n }$ to represent the whole sequence, is to use some sort of pooling function of all the hidden states $h _ { i }$ for each word $i$ in the sequence. For example, we can create a representation that pools all the $n$ hidden states by taking their element-wise mean: \nOr we can take the element-wise max; the element-wise max of a set of $n$ vectors is a new vector whose $k$ th element is the max of the kth elements of all the $n$ vectors. \n9.4.3 Generation with RNN-Based Language Models \nRNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, and conversational dialogue; any ask where a system needs to produce text, conditioned on some other text. \nRecall back in Chapter 3 we saw how to generate text from an n-gram language model by adapting a technique suggested contemporaneously by Claude Shannon (Shannon, 1951) and the psychologists George Miller and Selfridge (Miller and Selfridge, 1950). We first randomly sample a word to begin a sequence based on its suitability as the start of a sequence. We then continue to sample words conditioned on our previous choices until we reach a pre-determined length, or an end of sequence token is generated. \n\nToday, this approach of using a language model to incrementally generate words by repeatedly sampling the next word conditioned on our previous choices is called autoregressive generation. The procedure is basically the same as that described on 39, in a neural context: \n• Sample a word in the output from the softmax distribution that results from using the beginning of sentence marker, $< mathsf { s } >$ , as the first input. • Use the word embedding for that first word as the input to the network at the next time step, and then sample the next word in the same fashion. • Continue generating until the end of sentence marker, $< / mathsf { s } >$ , is sampled or a fixed length limit is reached. \nTechnically an autoregressive model is a model that predicts a value at time $t$ based on a linear function of the previous values at times $t - 1 , t - 2$ , and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 9.9 illustrates this approach. In this figure, the details of the RNN’s hidden layers and recurrent connections are hidden within the blue block. \nThis simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using $< mathsf { s } >$ to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it’s the long text we want to summarize. We’ll discuss the application of contextual generation to the problem of summarization in Section 9.9 in the context of transformer-based language models, and then again in Chapter 10 when we introduce encoder-decoder models.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "RNNs for other NLP tasks",
        "subsection": "Generation with RNN-Based Language Models",
        "subsubsection": "N/A"
    },
    {
        "content": "9.5 Stacked and Bidirectional RNN architectures \nStacked RNNs \nRecurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs. \n9.5.1 Stacked RNNs \nIn our examples thus far, the inputs to our RNNs have consisted of sequences of word or character embeddings (vectors) and the outputs have been vectors useful for predicting words, tags or sequence labels. However, nothing prevents us from using the entire sequence of outputs from one RNN as an input sequence to another one. Stacked RNNs consist of multiple networks where the output of one layer serves as the input to a subsequent layer, as shown in Fig. 9.10. \nStacked RNNs generally outperform single-layer networks. One reason for this success seems to be that the network induces representations at differing levels of abstraction across layers. Just as the early stages of the human visual system detect edges that are then used for finding larger regions and shapes, the initial layers of stacked networks can induce representations that serve as useful abstractions for further layers—representations that might prove difficult to induce in a single RNN. The optimal number of stacked RNNs is specific to each application and to each training set. However, as the number of stacks is increased the training costs rise quickly. \n9.5.2 Bidirectional RNNs \nThe RNN uses information from the left (prior) context to make its predictions at time $t$ . But in many applications we have access to the entire input sequence; in those cases we would like to use words from the context to the right of $t$ . One way to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and concatenate their representations.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Stacked and Bidirectional RNN architectures",
        "subsection": "Stacked RNNs",
        "subsubsection": "N/A"
    },
    {
        "content": "9.5 Stacked and Bidirectional RNN architectures \nStacked RNNs \nRecurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs. \n9.5.1 Stacked RNNs \nIn our examples thus far, the inputs to our RNNs have consisted of sequences of word or character embeddings (vectors) and the outputs have been vectors useful for predicting words, tags or sequence labels. However, nothing prevents us from using the entire sequence of outputs from one RNN as an input sequence to another one. Stacked RNNs consist of multiple networks where the output of one layer serves as the input to a subsequent layer, as shown in Fig. 9.10. \nStacked RNNs generally outperform single-layer networks. One reason for this success seems to be that the network induces representations at differing levels of abstraction across layers. Just as the early stages of the human visual system detect edges that are then used for finding larger regions and shapes, the initial layers of stacked networks can induce representations that serve as useful abstractions for further layers—representations that might prove difficult to induce in a single RNN. The optimal number of stacked RNNs is specific to each application and to each training set. However, as the number of stacks is increased the training costs rise quickly. \n9.5.2 Bidirectional RNNs \nThe RNN uses information from the left (prior) context to make its predictions at time $t$ . But in many applications we have access to the entire input sequence; in those cases we would like to use words from the context to the right of $t$ . One way to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and concatenate their representations. \nIn the left-to-right RNNs we’ve discussed so far, the hidden state at a given time $t$ represents everything the network knows about the sequence up to that point. The state is a function of the inputs $x _ { 1 } , . . . , x _ { t }$ and represents the context of the network to the left of the current time. \nThis new notation $ { mathbf { h } } _ { t } ^ { f }$ simply corresponds to the normal hidden state at time $t$ , representing everything the network has gleaned from the sequence so far. \nTo take advantage of context to the right of the current input, we can train an RNN on a reversed input sequence. With this approach, the hidden state at time $t$ represents information about the sequence to the right of the current input: \nHere, the hidden state $mathbf { h } _ { t } ^ { b }$ represents all the information we have discerned about the sequence from $t$ to the end of the sequence. \nA bidirectional RNN (Schuster and Paliwal, 1997) combines two independent RNNs, one where the input is processed from the start to the end, and the other from the end to the start. We then concatenate the two representations computed by the networks into a single vector that captures both the left and right contexts of an input at each point in time. Here we use either the semicolon ”;” or the equivalent symbol $oplus$ to mean vector concatenation: \nFig. 9.11 illustrates such a bidirectional network that concatenates the outputs of the forward and backward pass. Other simple ways to combine the forward and backward contexts include element-wise addition or multiplication. The output at each step in time thus captures information to the left and to the right of the current input. In sequence labeling applications, these concatenated outputs can serve as the basis for a local labeling decision. \nBidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 9.8 that for sequence classification we used the final hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 9.12, we simply combine the final hidden states from the forward and backward passes (for example by concatenation) and use that as input for follow-on processing. \n9.6 The LSTM \nIn practice, it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. Despite having access to the entire preceding sequence, the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. Yet distant information is critical to many language applications. Consider the following example in the context of language modeling.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Stacked and Bidirectional RNN architectures",
        "subsection": "Bidirectional RNNs",
        "subsubsection": "N/A"
    },
    {
        "content": "current context. \nNext, we add this to the modified context vector to get our new context vector. \noutput gate \nThe final gate we’ll use is the output gate which is used to decide what information is required for the current hidden state (as opposed to what information needs to be preserved for future decisions). \nFig. 9.13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output. The hidden layer, $h _ { t }$ , can be used as input to subsequent layers in a stacked RNN, or to generate an output for the final layer of a network. \n9.6.1 Gated Units, Layers and Networks \nThe neural units used in LSTMs are obviously much more complex than those used in basic feedforward networks. Fortunately, this complexity is encapsulated within the basic processing units, allowing us to maintain modularity and to easily experiment with different architectures. To see this, consider Fig. 9.14 which illustrates the inputs and outputs associated with each kind of unit. \nAt the far left, (a) is the basic feedforward unit where a single set of weights and a single activation function determine its output, and when arranged in a layer there are no connections among the units in the layer. Next, (b) represents the unit in a simple recurrent network. Now there are two inputs and an additional set of weights to go with it. However, there is still a single activation function and output. \nThe increased complexity of the LSTM units is encapsulated within the unit itself. The only additional external complexity for the LSTM over the basic recurrent unit (b) is the presence of the additional context vector as an input and output. \nThis modularity is key to the power and widespread applicability of LSTM units. LSTM units (or other varieties, like GRUs) can be substituted into any of the network architectures described in Section 9.5. And, as with simple RNNs, multi-layered networks making use of gated units can be unrolled into deep feedforward networks and trained in the usual fashion with backpropagation. \n9.7 Self-Attention Networks: Transformers \ntransformers \nWhile the addition of gates allows LSTMs to handle more distant information than RNNs, they don’t completely solve the underlying problem: passing information through an extended series of recurrent connections leads to information loss and difficulties in training. Moreover, the inherently sequential nature of recurrent networks makes it hard to do computation in parallel. These considerations led to the development of transformers – an approach to sequence processing that eliminates recurrent connections and returns to architectures reminiscent of the fully connected networks described earlier in Chapter 7. \nself-attention \nTransformers map sequences of input vectors $left( mathbf { x } _ { 1 } , . . . , mathbf { x } _ { n } right)$ to sequences of output vectors $( mathsf { y } _ { 1 } , . . . , mathsf { y } _ { n } )$ of the same length. Transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple linear layers, feedforward networks, and self-attention layers, the key innovation of transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. We’ll start by describing how self-attention works and then return to how it fits into larger transformer blocks. \nFig. 9.15 illustrates the flow of information in a single causal, or backward looking, self-attention layer. As with the overall transformer, a self-attention layer maps input sequences $( pmb { mathrm { x } } _ { 1 } , . . . , pmb { mathrm { x } } _ { n } )$ to output sequences of the same length $( pmb { mathsf { y } } _ { 1 } , . . . , pmb { mathsf { y } } _ { n } )$ . When processing each item in the input, the model has access to all of the inputs up to and including the one under consideration, but no access to information about inputs beyond the current one. In addition, the computation performed for each item is independent of all the other computations. The first point ensures that we can use this approach to create language models and use them for autoregressive generation, and the second point means that we can easily parallelize both forward inference and training of such models.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "The LSTM",
        "subsection": "Gated Units, Layers and Networks",
        "subsubsection": "N/A"
    },
    {
        "content": "to the following computation: \nUnfortunately, this process goes a bit too far since the calculation of the comparisons in QK⊺ results in a score for each query value to every key value, including those that follow the query. This is inappropriate in the setting of language modeling since guessing the next word is pretty simple if you already know it. To fix this, the elements in the upper-triangular portion of the matrix are zeroed out (set to $- infty )$ ), thus eliminating any knowledge of words that follow in the sequence. Fig. 9.17 depicts the $mathbf { Q } mathbf { K } ^ { intercal }$ matrix. (we’ll see in Chapter 11 how to make use of words in the future for tasks that need it). \nFig. 9.17 also makes it clear that attention is quadratic in the length of the input, since at each layer we need to compute dot products between each pair of tokens in the input. This makes it extremely expensive for the input to a transformer to consist of long documents (like entire Wikipedia pages, or novels), and so most applications have to limit the input length, for example to at most a page or a paragraph of text at a time. Finding more efficient attention mechanisms is an ongoing research direction. \n9.7.1 Transformer Blocks \nThe self-attention calculation lies at the core of what’s called a transformer block, which, in addition to the self-attention layer, includes additional feedforward layers, residual connections, and normalizing layers. The input and output dimensions of these blocks are matched so they can be stacked just as was the case for stacked RNNs. \nFig. 9.18 illustrates a standard transformer block consisting of a single attention layer followed by a fully-connected feedforward layer with residual connections and layer normalizations following each. We’ve already seen feedforward layers in Chapter 7, but what are residual connections and layer norm? In deep networks, residual connections are connections that pass information from a lower layer to a higher layer without going through the intermediate layer. Allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct access to information from lower layers (He et al., 2016). Residual connections in transformers are implemented by added a layer’s input vector to its output vector before passing it forward. In the transformer block shown in Fig. 9.18, residual connections are used with both the attention and feedforward sublayers. These summed vectors are then normalized using layer normalization (Ba et al., 2016). If we think of a layer as one long vector of units, the resulting function computed in a transformer block can be expressed as: \n\nlayer norm \nLayer normalization (or layer norm) is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the standard score, or z-score, from statistics applied to a single hidden layer. The first step in layer normalization is to calculate the mean, $mu$ , and standard deviation, $sigma$ , over the elements of the vector to be normalized. Given a hidden layer with dimensionality $d _ { h }$ , these values are calculated as follows. \nGiven these values, the vector components are normalized by subtracting the mean from each and dividing by the standard deviation. The result of this computation is a new vector with zero mean and a standard deviation of one. \nFinally, in the standard implementation of layer normalization, two learnable parameters, $gamma$ and $beta$ , representing gain and offset values, are introduced. \n9.7.2 Multihead Attention \nThe different words in a sentence can relate to each other in many different ways simultaneously. For example, distinct syntactic, semantic, and discourse relationships can hold between verbs and their arguments in a sentence. It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. Transformers address this issue with multihead selfattention layers. These are sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters. Given these distinct sets of parameters, each head can learn different aspects of the relationships that exist among inputs at the same level of abstraction. \nTo implement this notion, each head, $i$ , in a self-attention layer is provided with its own set of key, query and value matrices: $boldsymbol { mathsf { w } } _ { i } ^ { K } , boldsymbol { mathsf { w } } _ { i } ^ { Q }$ and $boldsymbol { mathsf { W } } _ { i } ^ { V }$ . These are used to project the inputs into separate key, value, and query embeddings separately for each head, with the rest of the self-attention computation remaining unchanged. In multi-head attention, instead of using the model dimension $d$ that’s used for the input and output from the model, the key and query embeddings have dimensionality $d _ { k }$ , and the value embeddings are dimensionality $d _ { nu }$ (in the original transformer paper $d _ { k } = d _ { nu } = 6 4 ;$ . Thus for each head $i$ , we have weight layers $boldsymbol { mathsf { W } } _ { i } ^ { Q } in mathbb { R } ^ { d times d _ { k } }$ , $pmb { mathsf { W } } _ { i } ^ { K } in$ $mathbb { R } ^ { d times d _ { k } }$ , and ${ pmb w } _ { i } ^ { V } in mathbb { R } ^ { d times d _ { nu } }$ , and these get multiplied by the inputs packed into $pmb { times }$ to produce $mathbf { Q } in mathbb { R } ^ { N times d _ { k } }$ , $pmb { kappa } in mathbb { R } ^ { N times d _ { k } }$ , and $pmb { mathsf { v } } in mathbb { R } ^ { N times d _ { nu } }$ . The output of each of the $h$ heads is of shape $N times d _ { nu }$ , and so the output of the multi-head layer with $h$ heads consists of $h$ vectors of shape $N times d _ { nu }$ . To make use of these vectors in further processing, they are combined and then reduced down to the original input dimension $d$ . This is accomplished by concatenating the outputs from each head and then using yet another linear projection, $boldsymbol { mathsf { W } } ^ { o } in mathbb { R } ^ { h d _ { nu } times d }$ , to reduce it to the original output dimension for each token, or a total $N times d$ output. \nFig. 9.19 illustrates this approach with 4 self-attention heads. This multihead layer replaces the single self-attention layer in the transformer block shown earlier in Fig. 9.18, the rest of the transformer block with its feedforward layer, residual connections, and layer norms remains the same. \n9.7.3 Modeling word order: positional embeddings \nHow does a transformer model the position of each token in the input sequence? With RNNs, information about the order of the inputs was built into the structure of the model. Unfortunately, the same isn’t true for transformers; the models as we’ve described them so far don’t have any notion of the relative, or absolute, positions of the tokens in the input. This can be seen from the fact that if you scramble the order of the inputs in the attention computation in Fig. 9.16 you get exactly the same answer. \nOne simple solution is to modify the input embeddings by combining them with positional embeddings specific to each position in an input sequence. \nWhere do we get these positional embeddings? The simplest method is to start with randomly initialized embeddings corresponding to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. This new embedding serves as the input for further processing. Fig. 9.20 shows the idea.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Self-Attention Networks: Transformers",
        "subsection": "Transformer Blocks",
        "subsubsection": "N/A"
    },
    {
        "content": "9.7.2 Multihead Attention \nThe different words in a sentence can relate to each other in many different ways simultaneously. For example, distinct syntactic, semantic, and discourse relationships can hold between verbs and their arguments in a sentence. It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. Transformers address this issue with multihead selfattention layers. These are sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters. Given these distinct sets of parameters, each head can learn different aspects of the relationships that exist among inputs at the same level of abstraction. \nTo implement this notion, each head, $i$ , in a self-attention layer is provided with its own set of key, query and value matrices: $boldsymbol { mathsf { w } } _ { i } ^ { K } , boldsymbol { mathsf { w } } _ { i } ^ { Q }$ and $boldsymbol { mathsf { W } } _ { i } ^ { V }$ . These are used to project the inputs into separate key, value, and query embeddings separately for each head, with the rest of the self-attention computation remaining unchanged. In multi-head attention, instead of using the model dimension $d$ that’s used for the input and output from the model, the key and query embeddings have dimensionality $d _ { k }$ , and the value embeddings are dimensionality $d _ { nu }$ (in the original transformer paper $d _ { k } = d _ { nu } = 6 4 ;$ . Thus for each head $i$ , we have weight layers $boldsymbol { mathsf { W } } _ { i } ^ { Q } in mathbb { R } ^ { d times d _ { k } }$ , $pmb { mathsf { W } } _ { i } ^ { K } in$ $mathbb { R } ^ { d times d _ { k } }$ , and ${ pmb w } _ { i } ^ { V } in mathbb { R } ^ { d times d _ { nu } }$ , and these get multiplied by the inputs packed into $pmb { times }$ to produce $mathbf { Q } in mathbb { R } ^ { N times d _ { k } }$ , $pmb { kappa } in mathbb { R } ^ { N times d _ { k } }$ , and $pmb { mathsf { v } } in mathbb { R } ^ { N times d _ { nu } }$ . The output of each of the $h$ heads is of shape $N times d _ { nu }$ , and so the output of the multi-head layer with $h$ heads consists of $h$ vectors of shape $N times d _ { nu }$ . To make use of these vectors in further processing, they are combined and then reduced down to the original input dimension $d$ . This is accomplished by concatenating the outputs from each head and then using yet another linear projection, $boldsymbol { mathsf { W } } ^ { o } in mathbb { R } ^ { h d _ { nu } times d }$ , to reduce it to the original output dimension for each token, or a total $N times d$ output. \nFig. 9.19 illustrates this approach with 4 self-attention heads. This multihead layer replaces the single self-attention layer in the transformer block shown earlier in Fig. 9.18, the rest of the transformer block with its feedforward layer, residual connections, and layer norms remains the same. \n9.7.3 Modeling word order: positional embeddings \nHow does a transformer model the position of each token in the input sequence? With RNNs, information about the order of the inputs was built into the structure of the model. Unfortunately, the same isn’t true for transformers; the models as we’ve described them so far don’t have any notion of the relative, or absolute, positions of the tokens in the input. This can be seen from the fact that if you scramble the order of the inputs in the attention computation in Fig. 9.16 you get exactly the same answer. \nOne simple solution is to modify the input embeddings by combining them with positional embeddings specific to each position in an input sequence. \nWhere do we get these positional embeddings? The simplest method is to start with randomly initialized embeddings corresponding to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. This new embedding serves as the input for further processing. Fig. 9.20 shows the idea.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Self-Attention Networks: Transformers",
        "subsection": "Multihead Attention",
        "subsubsection": "N/A"
    },
    {
        "content": "9.7.2 Multihead Attention \nThe different words in a sentence can relate to each other in many different ways simultaneously. For example, distinct syntactic, semantic, and discourse relationships can hold between verbs and their arguments in a sentence. It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. Transformers address this issue with multihead selfattention layers. These are sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters. Given these distinct sets of parameters, each head can learn different aspects of the relationships that exist among inputs at the same level of abstraction. \nTo implement this notion, each head, $i$ , in a self-attention layer is provided with its own set of key, query and value matrices: $boldsymbol { mathsf { w } } _ { i } ^ { K } , boldsymbol { mathsf { w } } _ { i } ^ { Q }$ and $boldsymbol { mathsf { W } } _ { i } ^ { V }$ . These are used to project the inputs into separate key, value, and query embeddings separately for each head, with the rest of the self-attention computation remaining unchanged. In multi-head attention, instead of using the model dimension $d$ that’s used for the input and output from the model, the key and query embeddings have dimensionality $d _ { k }$ , and the value embeddings are dimensionality $d _ { nu }$ (in the original transformer paper $d _ { k } = d _ { nu } = 6 4 ;$ . Thus for each head $i$ , we have weight layers $boldsymbol { mathsf { W } } _ { i } ^ { Q } in mathbb { R } ^ { d times d _ { k } }$ , $pmb { mathsf { W } } _ { i } ^ { K } in$ $mathbb { R } ^ { d times d _ { k } }$ , and ${ pmb w } _ { i } ^ { V } in mathbb { R } ^ { d times d _ { nu } }$ , and these get multiplied by the inputs packed into $pmb { times }$ to produce $mathbf { Q } in mathbb { R } ^ { N times d _ { k } }$ , $pmb { kappa } in mathbb { R } ^ { N times d _ { k } }$ , and $pmb { mathsf { v } } in mathbb { R } ^ { N times d _ { nu } }$ . The output of each of the $h$ heads is of shape $N times d _ { nu }$ , and so the output of the multi-head layer with $h$ heads consists of $h$ vectors of shape $N times d _ { nu }$ . To make use of these vectors in further processing, they are combined and then reduced down to the original input dimension $d$ . This is accomplished by concatenating the outputs from each head and then using yet another linear projection, $boldsymbol { mathsf { W } } ^ { o } in mathbb { R } ^ { h d _ { nu } times d }$ , to reduce it to the original output dimension for each token, or a total $N times d$ output. \nFig. 9.19 illustrates this approach with 4 self-attention heads. This multihead layer replaces the single self-attention layer in the transformer block shown earlier in Fig. 9.18, the rest of the transformer block with its feedforward layer, residual connections, and layer norms remains the same. \n9.7.3 Modeling word order: positional embeddings \nHow does a transformer model the position of each token in the input sequence? With RNNs, information about the order of the inputs was built into the structure of the model. Unfortunately, the same isn’t true for transformers; the models as we’ve described them so far don’t have any notion of the relative, or absolute, positions of the tokens in the input. This can be seen from the fact that if you scramble the order of the inputs in the attention computation in Fig. 9.16 you get exactly the same answer. \nOne simple solution is to modify the input embeddings by combining them with positional embeddings specific to each position in an input sequence. \nWhere do we get these positional embeddings? The simplest method is to start with randomly initialized embeddings corresponding to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. This new embedding serves as the input for further processing. Fig. 9.20 shows the idea. \n\nA potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly trained and may not generalize well during testing. An alternative approach to positional embeddings is to choose a static function that maps integer inputs to realvalued vectors in a way that captures the inherent relationships among the positions. That is, it captures the fact that position 4 in an input is more closely related to position 5 than it is to position 17. A combination of sine and cosine functions with differing frequencies was used in the original transformer work. Developing better position representations is an ongoing research topic. \n\n9.8 Transformers as Language Models \nNow that we’ve seen all the major components of transformers, let’s examine how to deploy them as language models via semi-supervised learning. To do this, we’ll proceed just as we did with the RNN-based approach: given a training corpus of plain text we’ll train a model to predict the next word in a sequence using teacher forcing. Fig. 9.21 illustrates the general approach. At each step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. As with RNNs, the loss for a training sequence is the average cross-entropy loss over the entire sequence. \nNote the key difference between this figure and the earlier RNN-based version shown in Fig. 9.6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. Once trained, we can compute the perplexity of the resulting model, or autoregressively generate novel text just as with RNN-based models.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Self-Attention Networks: Transformers",
        "subsection": "Modeling word order: positional embeddings",
        "subsubsection": "N/A"
    },
    {
        "content": "9.8 Transformers as Language Models \nNow that we’ve seen all the major components of transformers, let’s examine how to deploy them as language models via semi-supervised learning. To do this, we’ll proceed just as we did with the RNN-based approach: given a training corpus of plain text we’ll train a model to predict the next word in a sequence using teacher forcing. Fig. 9.21 illustrates the general approach. At each step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. As with RNNs, the loss for a training sequence is the average cross-entropy loss over the entire sequence. \nNote the key difference between this figure and the earlier RNN-based version shown in Fig. 9.6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. Once trained, we can compute the perplexity of the resulting model, or autoregressively generate novel text just as with RNN-based models.",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Transformers as Language Models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "9.9 Contextual Generation and Summarization \nA simple variation on autoregressive generation that underlies a number of practical applications uses a prior context to prime the autoregressive generation process. Fig. 9.22 illustrates this with the task of text completion. Here a standard language model is given the prefix to some text and is asked to generate a possible completion to it. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs. This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of these models. \nText summarization \nText summarization is a practical application of context-based autoregressive generation. The task is to take a full-length article and produce an effective summary of it. To train a transformer-based autoregressive model to perform this task, we start with a corpus consisting of full-length articles accompanied by their corresponding summaries. Fig. 9.23 shows an example of this kind of data from a widely used summarization corpus consisting of CNN and Daily Mirror news articles. \nA simple but surprisingly effective approach to applying transformers to summarization is to append a summary to each full-length article in a corpus, with a unique marker separating the two. More formally, each article-summary pair $( x _ { 1 } , . . . , x _ { m } )$ , $( y _ { 1 } , . . . , y _ { n } )$ in a training corpus is converted into a single training instance $( x _ { 1 } , . . . , x _ { m } , delta , y _ { 1 } , . . . y _ { n } )$ with an overall length of $n + m + 1$ . These training instances are treated as long sentences and then used to train an autoregressive language model using teacher forcing, exactly as we did earlier. \nOnce trained, full articles ending with the special marker are used as the context to prime the generation process to produce a summary as illustrated in Fig. 9.24. Note that, in contrast to RNNs, the model has access to the original article as well as to the newly generated text throughout the process. \nAs we’ll see in later chapters, variations on this simple scheme are the basis for successful text-to-text applications including machine translation, summarization and question answering. \nOriginal Article \nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff and offering it for sale online? People are actually buying it. For $$ 89$ , self-styled entrepreneur Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough for 10 to 15 snowballs, he says. \nBut not if you live in New England or surrounding states. “We will not ship snow to any states in the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging snow!” \nHis website and social media accounts claim to have filled more than 133 orders for snow – more than 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a record this winter for the snowiest month in its history. Most residents see the huge piles of snow choking their yards and sidewalks as a nuisance, but Waring saw an opportunity. \nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shoveling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He joked about shipping the stuff to friends and family in warmer states, and an idea was born. His business slogan: “Our nightmare is your dream!” At first, ShipSnowYo sold snow packed into empty 16.9-ounce water bottles for $$ 19.99$ , but the snow usually melted before it reached its destination... \nSummary \nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states. \nFigure 9.23 Examples of articles and summaries from the CNN/Daily Mail corpus (Hermann et al., 2015b), (Nallapati et al., 2016). \n9.9.1 Applying Transformers to other NLP tasks \npretraining \nTransformers can also be used for sequence labeling tasks (like part-of-speech tagging or named entity tagging) and sequence classification tasks (like sentiment classification), as we’ll see in detail in Chapter 11. Just to give a preview, however, we don’t directly train a raw transformer on these tasks. Instead, we use a technique called pretraining, in which we first train a transformer language model on a large corpus of text, in a normal self-supervised way, and only afterwards add a linear or feedforward layer on top that we finetune on a smaller dataset hand-labeled with part-of-speech or sentiment labels. Pretraining on large amounts of data via the \nfinetune \nself-supervised language model objective turns out to be a very useful way of incorporating rich information about language, and the resulting representations make it much easier to learn from the generally smaller supervised datasets for tagging or sentiment. \n9.10 Summary \nThis chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered: \n• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time $t$ based both on the current input at $t$ and the hidden layer from time $t - 1$ .   \n• RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).   \n• Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.   \n• Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word.   \n• A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.   \n• Common language-based applications for RNNs and transformers include: – Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. – Auto-regressive generation using a trained language model. – Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. – Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification. \nBibliographical and Historical Notes \nInfluential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the $1 9 8 0 ^ { 3 } mathrm { s } .$ . Much of this work was directed at human cognitive modeling rather than practical NLP applications Rumelhart and McClelland 1986c McClelland and Rumelhart 1986. Models using recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with the addition of a recurrent context layer prior to the hidden layer. The possibility of unrolling a recurrent network into an equivalent feedforward network is discussed in (Rumelhart and McClelland, 1986c).",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Contextual Generation and Summarization",
        "subsection": "Applying Transformers to other NLP tasks",
        "subsubsection": "N/A"
    },
    {
        "content": "self-supervised language model objective turns out to be a very useful way of incorporating rich information about language, and the resulting representations make it much easier to learn from the generally smaller supervised datasets for tagging or sentiment. \n9.10 Summary \nThis chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered: \n• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time $t$ based both on the current input at $t$ and the hidden layer from time $t - 1$ .   \n• RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).   \n• Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.   \n• Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word.   \n• A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.   \n• Common language-based applications for RNNs and transformers include: – Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. – Auto-regressive generation using a trained language model. – Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. – Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification. \nBibliographical and Historical Notes \nInfluential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the $1 9 8 0 ^ { 3 } mathrm { s } .$ . Much of this work was directed at human cognitive modeling rather than practical NLP applications Rumelhart and McClelland 1986c McClelland and Rumelhart 1986. Models using recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with the addition of a recurrent context layer prior to the hidden layer. The possibility of unrolling a recurrent network into an equivalent feedforward network is discussed in (Rumelhart and McClelland, 1986c).",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "self-supervised language model objective turns out to be a very useful way of incorporating rich information about language, and the resulting representations make it much easier to learn from the generally smaller supervised datasets for tagging or sentiment. \n9.10 Summary \nThis chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered: \n• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time $t$ based both on the current input at $t$ and the hidden layer from time $t - 1$ .   \n• RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).   \n• Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.   \n• Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, using attention heads that model how the surrounding words are relevant for the processing of the current word.   \n• A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.   \n• Common language-based applications for RNNs and transformers include: – Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. – Auto-regressive generation using a trained language model. – Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. – Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification. \nBibliographical and Historical Notes \nInfluential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the $1 9 8 0 ^ { 3 } mathrm { s } .$ . Much of this work was directed at human cognitive modeling rather than practical NLP applications Rumelhart and McClelland 1986c McClelland and Rumelhart 1986. Models using recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with the addition of a recurrent context layer prior to the hidden layer. The possibility of unrolling a recurrent network into an equivalent feedforward network is discussed in (Rumelhart and McClelland, 1986c). \n\nIn parallel with work in cognitive modeling, RNNs were investigated extensively in the continuous domain in the signal processing and speech communities (Giles et al. 1994, Robinson et al. 1996). Schuster and Paliwal (1997) introduced bidirectional RNNs and described results on the TIMIT phoneme transcription task. \nWhile theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013b). \nInterest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. They demonstrated near state-of-the-art performance on a number of standard shared tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling without the use of hand-engineered features. \nApproaches that married LSTMs with pre-trained collections of word-embeddings based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) quickly came to dominate many common tasks: part-of-speech tagging (Ling et al., 2015), syntactic chunking (Søgaard and Goldberg, 2016), named entity recognition (Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie, 2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and Martin, 2016). As with the earlier surge of progress involving statistical machine learning, these advances were made possible by the availability of training data provided by CONLL, SemEval, and other shared tasks, as well as shared resources such as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005). \nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior research: self-attention and memory networks. Encoder-decoder attention, the idea of using a soft weighting over the encodings of input words to inform a generative decoder (see Chapter 10) was developed by Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015) for MT. This idea was extended to self-attention by dropping the need for separate encoding and decoding sequences and instead seeing attention a way of weighting the tokens in collecting information passed from lower layers to higher layers (Ling et al., 2015; Cheng et al., 2016; Liu et al., 2016b). Other aspects of the transformer, including the terminology of key, query, and value, came from memory networks, a mechanism for adding an external read-write memory to networks, by using an embedding of a query to match keys representing content in an associative memory (Sukhbaatar et al., 2015; Weston et al., 2015; Graves et al., 2014). \n10 Machine Translation and Encoder-Decoder Models \nmachine translation MT \n“I want to talk the dialect of your people. It’s no use of talking unless people understand what you say.” \nZora Neale Hurston, Moses, Man of the Mountain 1939, p. 121 \nThis chapter introduces machine translation (MT), the use of computers to translate from one language to another. \nOf course translation, in its full generality, such as the translation of literature, or poetry, is a difficult, fascinating, and intensely human endeavor, as rich as any other area of human creativity. \ninformation access \nMachine translation in its present form therefore focuses on a number of very practical tasks. Perhaps the most common current use of machine translation is for information access. We might want to translate some instructions on the web, perhaps the recipe for a favorite dish, or the steps for putting together some furniture. Or we might want to read an article in a newspaper, or get information from an online resource like Wikipedia or a government webpage in a foreign language. \nMT for information access is probably one of the most common uses of NLP technology, and Googl \n$equiv$ Documents →   \nEnun recipiente hondo,mezclar el jugo de $times$ Inadeepbowl,mixtheorange juicewith the   \nnaranja con el azucar, jengibre,y nuez moscada. sugar, ginger,and nutmeg. \npost-editing \nTranslate alone (shown above) translates hundreds of billions of words a day between over 100 languages. \nAnother common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content or a product to a particular language community. \nlocalization \nencoderdecoder \nFinally, a more recent application of MT is to in-the-moment human communication needs. This includes incremental translation, translating speech on-the-fly before the entire sentence is complete, as is commonly used in simultaneous interpretation. Image-centric translation can be used for example to use OCR of the text on a phone camera image as input to an MT system to translate menus or street signs. \nThe standard algorithm for MT is the encoder-decoder network, also called the sequence to sequence network, an architecture that can be implemented with RNNs or with Transformers. We’ve seen in prior chapters that RNN or Transformer architecture can be used to do classification (for example to map a sentence to a positive or negative sentiment tag for sentiment analysis), or can be used to do sequence labeling (for example to assign each word in an input sentence with a part-of-speech, or with a named entity tag). For part-of-speech tagging, recall that the output tag is associated directly with each input word, and so we can just model the tag as output $y _ { t }$ for each input word $x _ { t }$ .",
        "chapter": "Deep Learning Architectures for Sequence Processing",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "10.1 Language Divergences and Typology \nuniversal \nSome aspects of human language seem to be universal, holding true for every language, or are statistical universals, holding true for most languages. Many universals arise from the functional role of language as a communicative system by humans. Every language, for example, seems to have words for referring to people, for talking about eating and drinking, for being polite or not. There are also structural linguistic universals; for example, every language seems to have nouns and verbs (Chapter 8), has ways to ask questions, or issue commands, linguistic mechanisms for indicating agreement or disagreement. \ntranslation divergence \ntypology \nYet languages also differ in many ways, and an understanding of what causes such translation divergences will help us build better MT models. We often distinguish the idiosyncratic and lexical differences that must be dealt with one by one (the word for ”dog” differs wildly from language to language), from systematic differences that we can model in a general way (many languages put the verb before the direct object; others put the verb after the direct object). The study of these systematic cross-linguistic similarities and differences is called linguistic typology. This section sketches some typological facts that impact machine translation; the interested reader should also look into WALS, the World Atlas of Language Structures, which gives many typological facts about languages (Dryer and Haspelmath, 2013). \n10.1.1 Word Order Typology \nAs we hinted it in our example above comparing English and Japanese, languages differ in the basic word order of verbs, subjects, and objects in simple declarative clauses. German, French, English, and Mandarin, for example, are all SVO (Subject-Verb-Object) languages, meaning that the verb tends to come between the subject and object. Hindi and Japanese, by contrast, are SOV languages, meaning that the verb tends to come at the end of basic clauses, and Irish and Arabic are VSO languages. Two languages that share their basic word order type often have other similarities. For example, VO languages generally have prepositions, whereas OV languages generally have postpositions. \nLet’s look in more detail at the example we saw above. In this SVO English sentence, the verb wrote is followed by its object a letter and the prepositional phrase to a friend, in which the preposition $t o$ is followed by its argument a friend. Arabic, with a VSO order, also has the verb before the object and prepositions. By contrast, in the Japanese example that follows, each of these orderings is reversed; the verb is preceded by its arguments, and the postposition follows its argument. \n(10.3) English: He wrote a letter to a friend Japanese: tomodachi ni tegami-o kaita friend to letter wrote Arabic: katabt ris¯ala li s˙adq wrote letter to friend \nOther kinds of ordering preferences vary idiosyncratically from language to language. In some SVO languages (like English and Mandarin) adjectives tend to appear before verbs, while in others languages like Spanish and Modern Hebrew, adjectives appear after the noun: \n(10.4) Spanish bruja verde English green witch \nThe green witch is at home this week cheng long dao xiang gang qu Diese Woche ist die grune Hexe zu Hause Jackie Chan went to Hong Kong (a) (b) \nFig. 10.1 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output. \n10.1.2 Lexical Divergences \nOf course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context. The English source-language word bass, for example, can appear in Spanish as the fish lubina or the musical instrument bajo. German uses two distinct words for what in English would be called a wall: Wand for walls inside a building, and Mauer for walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization, disambiguating the different uses of a word. For this reason the fields of MT and Word Sense Disambiguation (Chapter 18) are closely linked. \nSometimes one language places more grammatical constraints on word choice than another. We saw above that English marks nouns for whether they are singular or plural. Mandarin doesn’t. Or French and Spanish, for example, mark grammatical gender on adjectives, so an English translation into French requires specifying adjective gender. \nThe way that languages differ in lexically dividing up conceptual space may be more complex than this one-to-many translation problem, leading to many-to-many mappings. For example, Fig. 10.2 summarizes some of the complexities discussed by Hutchins and Somers (1992) in translating English leg, foot, and paw, to French. For example, when leg is used about an animal it’s translated as French jambe; but about the leg of a journey, as French etape; if the leg is of a chair, we use French pied. \nFurther, one language may have a lexical gap, where no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language. For example, English does not have a word that corresponds neatly to Mandarin xi`ao or Japanese oyak¯ok¯oo (in English one has to make do with awkward phrases like filial piety or loving child, or good son/daughter for both). \nFinally, languages differ systematically in how the conceptual properties of an event are mapped onto specific words. Talmy (1985, 1991) noted that languages can be characterized by whether direction of motion and manner of motion are marked on the verb or on the “satellites”: particles, prepositional phrases, or adverbial phrases. For example, a bottle floating out of a cave would be described in English with the direction marked on the particle out, while in Spanish the direction",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Language Divergences and Typology",
        "subsection": "Word Order Typology",
        "subsubsection": "N/A"
    },
    {
        "content": "The green witch is at home this week cheng long dao xiang gang qu Diese Woche ist die grune Hexe zu Hause Jackie Chan went to Hong Kong (a) (b) \nFig. 10.1 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output. \n10.1.2 Lexical Divergences \nOf course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context. The English source-language word bass, for example, can appear in Spanish as the fish lubina or the musical instrument bajo. German uses two distinct words for what in English would be called a wall: Wand for walls inside a building, and Mauer for walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization, disambiguating the different uses of a word. For this reason the fields of MT and Word Sense Disambiguation (Chapter 18) are closely linked. \nSometimes one language places more grammatical constraints on word choice than another. We saw above that English marks nouns for whether they are singular or plural. Mandarin doesn’t. Or French and Spanish, for example, mark grammatical gender on adjectives, so an English translation into French requires specifying adjective gender. \nThe way that languages differ in lexically dividing up conceptual space may be more complex than this one-to-many translation problem, leading to many-to-many mappings. For example, Fig. 10.2 summarizes some of the complexities discussed by Hutchins and Somers (1992) in translating English leg, foot, and paw, to French. For example, when leg is used about an animal it’s translated as French jambe; but about the leg of a journey, as French etape; if the leg is of a chair, we use French pied. \nFurther, one language may have a lexical gap, where no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language. For example, English does not have a word that corresponds neatly to Mandarin xi`ao or Japanese oyak¯ok¯oo (in English one has to make do with awkward phrases like filial piety or loving child, or good son/daughter for both). \nFinally, languages differ systematically in how the conceptual properties of an event are mapped onto specific words. Talmy (1985, 1991) noted that languages can be characterized by whether direction of motion and manner of motion are marked on the verb or on the “satellites”: particles, prepositional phrases, or adverbial phrases. For example, a bottle floating out of a cave would be described in English with the direction marked on the particle out, while in Spanish the direction \nwould be marked on the verb: \n(10.5) English: The bottle floated out. Spanish: La botella salio´ flotando. The bottle exited floating. \nverb-framed \nsatellite-framed \nVerb-framed languages mark the direction of motion on the verb (leaving the satellites to mark the manner of motion), like Spanish acercarse ‘approach’, alcanzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the direction of motion on the satellite (leaving the verb to mark the manner of motion), like English crawl out, float off, jump down, run after. Languages like Japanese, Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991, Slobin 1996). \n10.1.3 Morphological Typology \nisolating polysynthetic agglutinative fusion \n\nMorphologically, languages are often characterized along two dimensions of variation. The first is the number of morphemes per word, ranging from isolating languages like Vietnamese and Cantonese, in which each word generally has one morpheme, to polysynthetic languages like Siberian Yupik (“Eskimo”), in which a single word may have very many morphemes, corresponding to a whole sentence in English. The second dimension is the degree to which morphemes are segmentable, ranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTRDECL1), which fuses the distinct morphological categories instrumental, singular, and first declension. \nTranslating between languages with rich morphology requires dealing with structure below the word level, and for this reason modern systems generally use subword models like the wordpiece or BPE models of Section 10.7.1. \n10.1.4 Referential density \nFinally, languages vary along a typological dimension related to the things they tend to omit. Some languages, like English, require that we use an explicit pronoun when talking about a referent that is given in the discourse. In other languages, however, we can sometimes omit pronouns altogether, as the following example from Spanish shows1:",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Language Divergences and Typology",
        "subsection": "Lexical Divergences",
        "subsubsection": "N/A"
    },
    {
        "content": "would be marked on the verb: \n(10.5) English: The bottle floated out. Spanish: La botella salio´ flotando. The bottle exited floating. \nverb-framed \nsatellite-framed \nVerb-framed languages mark the direction of motion on the verb (leaving the satellites to mark the manner of motion), like Spanish acercarse ‘approach’, alcanzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the direction of motion on the satellite (leaving the verb to mark the manner of motion), like English crawl out, float off, jump down, run after. Languages like Japanese, Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991, Slobin 1996). \n10.1.3 Morphological Typology \nisolating polysynthetic agglutinative fusion \n\nMorphologically, languages are often characterized along two dimensions of variation. The first is the number of morphemes per word, ranging from isolating languages like Vietnamese and Cantonese, in which each word generally has one morpheme, to polysynthetic languages like Siberian Yupik (“Eskimo”), in which a single word may have very many morphemes, corresponding to a whole sentence in English. The second dimension is the degree to which morphemes are segmentable, ranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTRDECL1), which fuses the distinct morphological categories instrumental, singular, and first declension. \nTranslating between languages with rich morphology requires dealing with structure below the word level, and for this reason modern systems generally use subword models like the wordpiece or BPE models of Section 10.7.1. \n10.1.4 Referential density \nFinally, languages vary along a typological dimension related to the things they tend to omit. Some languages, like English, require that we use an explicit pronoun when talking about a referent that is given in the discourse. In other languages, however, we can sometimes omit pronouns altogether, as the following example from Spanish shows1:",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Language Divergences and Typology",
        "subsection": "Morphological Typology",
        "subsubsection": "N/A"
    },
    {
        "content": "would be marked on the verb: \n(10.5) English: The bottle floated out. Spanish: La botella salio´ flotando. The bottle exited floating. \nverb-framed \nsatellite-framed \nVerb-framed languages mark the direction of motion on the verb (leaving the satellites to mark the manner of motion), like Spanish acercarse ‘approach’, alcanzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the direction of motion on the satellite (leaving the verb to mark the manner of motion), like English crawl out, float off, jump down, run after. Languages like Japanese, Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991, Slobin 1996). \n10.1.3 Morphological Typology \nisolating polysynthetic agglutinative fusion \n\nMorphologically, languages are often characterized along two dimensions of variation. The first is the number of morphemes per word, ranging from isolating languages like Vietnamese and Cantonese, in which each word generally has one morpheme, to polysynthetic languages like Siberian Yupik (“Eskimo”), in which a single word may have very many morphemes, corresponding to a whole sentence in English. The second dimension is the degree to which morphemes are segmentable, ranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTRDECL1), which fuses the distinct morphological categories instrumental, singular, and first declension. \nTranslating between languages with rich morphology requires dealing with structure below the word level, and for this reason modern systems generally use subword models like the wordpiece or BPE models of Section 10.7.1. \n10.1.4 Referential density \nFinally, languages vary along a typological dimension related to the things they tend to omit. Some languages, like English, require that we use an explicit pronoun when talking about a referent that is given in the discourse. In other languages, however, we can sometimes omit pronouns altogether, as the following example from Spanish shows1: \n(10.6) [El jefe]i dio con un libro. ${ varnothing } _ { i }$ Mostro´ a un descifrador ambulante. [The boss] came upon a book. [He] showed it to a wandering decoder. \npro-drop \nreferential density \ncold language hot language \nLanguages that can omit pronouns are called pro-drop languages. Even among the pro-drop languages, there are marked differences in frequencies of omission. Japanese and Chinese, for example, tend to omit far more than does Spanish. This dimension of variation across languages is called the dimension of referential density. We say that languages that tend to use more pronouns are more referentially dense than those that use more zeros. Referentially sparse languages, like Chinese or Japanese, that require the hearer to do more inferential work to recover antecedents are also called cold languages. Languages that are more explicit and make it easier for the hearer are called hot languages. The terms hot and cold are borrowed from Marshall McLuhan’s 1964 distinction between hot media like movies, which fill in many details for the viewer, versus cold media like comics, which require the reader to do more inferential work to fill out the representation (Bickel, 2003). \nTranslating from languages with extensive pro-drop, like Chinese or Japanese, to non-pro-drop languages like English can be difficult since the model must somehow identify each zero and recover who or what is being talked about in order to insert the proper pronoun. \n10.2 The Encoder-Decoder Model \nencoderdecoder \nEncoder-decoder networks, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue. \nThe key idea underlying these networks is the use of an encoder network that takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a taskspecific output sequence. Fig. 10.3 illustrates the architecture \nEncoder-decoder networks consist of three components: \n1. An encoder that accepts an input sequence, $x _ { 1 } ^ { n }$ , and generates a corresponding sequence of contextualized representations, $h _ { 1 } ^ { n }$ . LSTMs, convolutional networks, and Transformers can all be employed as encoders.   \n2. A context vector, $c$ , which is a function of $h _ { 1 } ^ { n }$ , and conveys the essence of the input to the decoder.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Language Divergences and Typology",
        "subsection": "Referential density",
        "subsubsection": "N/A"
    },
    {
        "content": "(10.6) [El jefe]i dio con un libro. ${ varnothing } _ { i }$ Mostro´ a un descifrador ambulante. [The boss] came upon a book. [He] showed it to a wandering decoder. \npro-drop \nreferential density \ncold language hot language \nLanguages that can omit pronouns are called pro-drop languages. Even among the pro-drop languages, there are marked differences in frequencies of omission. Japanese and Chinese, for example, tend to omit far more than does Spanish. This dimension of variation across languages is called the dimension of referential density. We say that languages that tend to use more pronouns are more referentially dense than those that use more zeros. Referentially sparse languages, like Chinese or Japanese, that require the hearer to do more inferential work to recover antecedents are also called cold languages. Languages that are more explicit and make it easier for the hearer are called hot languages. The terms hot and cold are borrowed from Marshall McLuhan’s 1964 distinction between hot media like movies, which fill in many details for the viewer, versus cold media like comics, which require the reader to do more inferential work to fill out the representation (Bickel, 2003). \nTranslating from languages with extensive pro-drop, like Chinese or Japanese, to non-pro-drop languages like English can be difficult since the model must somehow identify each zero and recover who or what is being talked about in order to insert the proper pronoun. \n10.2 The Encoder-Decoder Model \nencoderdecoder \nEncoder-decoder networks, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue. \nThe key idea underlying these networks is the use of an encoder network that takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a taskspecific output sequence. Fig. 10.3 illustrates the architecture \nEncoder-decoder networks consist of three components: \n1. An encoder that accepts an input sequence, $x _ { 1 } ^ { n }$ , and generates a corresponding sequence of contextualized representations, $h _ { 1 } ^ { n }$ . LSTMs, convolutional networks, and Transformers can all be employed as encoders.   \n2. A context vector, $c$ , which is a function of $h _ { 1 } ^ { n }$ , and conveys the essence of the input to the decoder. \n3. A decoder, which accepts $c$ as input and generates an arbitrary length sequence of hidden states $h _ { 1 } ^ { m }$ , from which a corresponding sequence of output states $y _ { 1 } ^ { m }$ , can be obtained. Just as with encoders, decoders can be realized by any kind of sequence architecture. \n10.3 Encoder-Decoder with RNNs \nLet’s begin by describing an encoder-decoder network based on a pair of RNNs.2 Recall the conditional RNN language model from Chapter 9 for computing $p ( y )$ , the probability of a sequence $y$ . Like any language model, we can break down the probability as follows: \nAt a particular time $t$ , we pass the prefix of $t - 1$ tokens through the language model, using forward inference to produce a sequence of hidden states, ending with the hidden state corresponding to the last word of the prefix. We then use the final hidden state of the prefix as our starting point to generate the next token. \nMore formally, if $g$ is an activation function like tanh or ReLU, a function of the input at time $t$ and the hidden state at time $t - 1$ , and $f$ is a softmax over the set of possible vocabulary items, then at time $t$ the output $mathbf { y } _ { t }$ and hidden state $mathbf { h } _ { t }$ are computed as: \nWe only have to make one slight change to turn this language model with autoregressive generation into a translation model that can translate from a source text in one language to a target text in a second: add a sentence separation marker at the end of the source text, and then simply concatenate the target text. We briefly introduced this idea of a sentence separator token in Chapter 9 when we considered using a Transformer language model to do summarization, by training a conditional language model. \nIf we call the source text $x$ and the target text $y$ , we are computing the probability $p ( y | x )$ as follows: \nFig. 10.4 shows the setup for a simplified version of the encoder-decoder model (we’ll see the full model, which requires attention, in the next section). \nFig. 10.4 shows an English source text (“the green witch arrived”), a sentence separator token ( $< s >$ , and a Spanish target text (“lleg´o la bruja verde”). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "The Encoder-Decoder Model",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "One weakness of this approach as described so far is that the influence of the context vector, $c$ , will wane as the output sequence is generated. A solution is to make the context vector $c$ available at each step in the decoding process by adding it as a parameter to the computation of the current hidden state, using the following equation (illustrated in Fig. 10.6): \nNow we’re ready to see the full equations for this version of the decoder in the basic encoder-decoder model, with context available at each decoding timestep. Recall that $g$ is a stand-in for some flavor of RNN and $hat { y } _ { t - 1 }$ is the embedding for the output sampled from the softmax at the previous step: \nFinally, as shown earlier, the output $y$ at each time step consists of a softmax computation over the set of possible outputs (the vocabulary, in the case of language modeling or MT). We compute the most likely output at each time step by taking the argmax over the softmax output: \n10.3.1 Training the Encoder-Decoder Model \nEncoder-decoder architectures are trained end-to-end, just as with the RNN language models of Chapter 9. Each training example is a tuple of paired strings, a source and a target. Concatenated with a separator token, these source-target pairs can now serve as training data. \nFor MT, the training data typically consists of sets of sentences and their translations. These can be drawn from standard datasets of aligned sentence pairs, as we’ll discuss in Section 10.7.2. Once we have a training set, the training itself proceeds as with any RNN-based language model. The network is given the source text and then starting with the separator token is trained autoregressively to predict the next word, as shown in Fig. 10.7. \nteacher forcing \nNote the differences between training (Fig. 10.7) and inference (Fig. 10.4) with respect to the outputs at each time step. The decoder during inference uses its own estimated output $hat { y _ { t } }$ as the input for the next time step $x _ { t + 1 }$ . Thus the decoder will tend to deviate more and more from the gold target sentence as it keeps generating more tokens. In training, therefore, it is more common to use teacher forcing in the decoder. Teacher forcing means that we force the system to use the gold target token from training as the next input $x _ { t + 1 }$ , rather than allowing it to rely on the (possibly erroneous) decoder output $hat { y _ { t } }$ . This speeds up training. \n10.4 Attention \nattention mechanism \nThe simplicity of the encoder-decoder model is its clean separation of the encoder— which builds a representation of the source text—from the decoder, which uses this context to generate a target text. In the model as we’ve described it so far, this context vector is $h _ { n }$ , the hidden state of the last (nth) time step of the source text. This final hidden state is thus acting as a bottleneck: it must represent absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what’s in this context vector (Fig. 10.8). Information at the beginning of the sentence, especially for long sentences, may not be equally well represented in the context vector. \nThe attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state. \nIn the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder, that is, $mathbf { c } = f ( mathbf { h } _ { 1 } ^ { e } ldots mathbf { h } _ { n } ^ { e } )$ . Because the number of hidden states varies with the size of the input, we can’t use the entire tensor of encoder hidden state vectors directly as the context for the decoder.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Encoder-Decoder with RNNs",
        "subsection": "Training the Encoder-Decoder Model",
        "subsubsection": "N/A"
    },
    {
        "content": "teacher forcing \nNote the differences between training (Fig. 10.7) and inference (Fig. 10.4) with respect to the outputs at each time step. The decoder during inference uses its own estimated output $hat { y _ { t } }$ as the input for the next time step $x _ { t + 1 }$ . Thus the decoder will tend to deviate more and more from the gold target sentence as it keeps generating more tokens. In training, therefore, it is more common to use teacher forcing in the decoder. Teacher forcing means that we force the system to use the gold target token from training as the next input $x _ { t + 1 }$ , rather than allowing it to rely on the (possibly erroneous) decoder output $hat { y _ { t } }$ . This speeds up training. \n10.4 Attention \nattention mechanism \nThe simplicity of the encoder-decoder model is its clean separation of the encoder— which builds a representation of the source text—from the decoder, which uses this context to generate a target text. In the model as we’ve described it so far, this context vector is $h _ { n }$ , the hidden state of the last (nth) time step of the source text. This final hidden state is thus acting as a bottleneck: it must represent absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what’s in this context vector (Fig. 10.8). Information at the beginning of the sentence, especially for long sentences, may not be equally well represented in the context vector. \nThe attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state. \nIn the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder, that is, $mathbf { c } = f ( mathbf { h } _ { 1 } ^ { e } ldots mathbf { h } _ { n } ^ { e } )$ . Because the number of hidden states varies with the size of the input, we can’t use the entire tensor of encoder hidden state vectors directly as the context for the decoder. \n\nThe idea of attention is instead to create the single fixed-length vector $c$ by taking a weighted sum of all the encoder hidden states. The weights focus on (‘attend to’) a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, different for each token in decoding. \nThis context vector, $pmb { c } _ { i }$ , is generated anew with each decoding step $i$ and takes all of the encoder hidden states into account in its derivation. We then make this context available during decoding by conditioning the computation of the current decoder hidden state on it (along with the prior hidden state and the previous output generated by the decoder), as we see in this equation (and Fig. 10.9): \nThe first step in computing $pmb { c } _ { i }$ is to compute how much to focus on each encoder state, how relevant each encoder state is to the decoder state captured in $boldsymbol { mathsf { h } } _ { i - 1 } ^ { d }$ . We capture relevance by computing— at each state $i$ during decoding—a score $( mathsf { h } _ { i - 1 } ^ { d } , mathsf { h } _ { j } ^ { e } )$ for each encoder state $j$ . \nThe simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them: \nThe score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder. \nTo make use of these scores, we’ll normalize them with a softmax to create a vector of weights, $alpha _ { i j }$ , that tells us the proportional relevance of each encoder hidden state $j$ to the prior hidden decoder state, $h _ { i - 1 } ^ { d }$ . \n\nFinally, given the distribution in $alpha$ , we can compute a fixed-length context vector for the current decoder state by taking a weighted average over all the encoder hidden states. \nWith this, we finally have a fixed-length context vector that takes into account information from the entire encoder state that is dynamically updated to reflect the needs of the decoder at each step of decoding. Fig. 10.10 illustrates an encoderdecoder network with attention, focusing on the computation of one context vector $pmb { c } _ { i }$ . \nIt’s also possible to create more sophisticated scoring functions for attention models. Instead of simple dot product attention, we can get a more powerful function that computes the relevance of each encoder hidden state to the decoder hidden state by parameterizing the score with its own set of weights, $pmb { { mathsf { W } } } _ { s }$ . \nThe weights $W _ { s }$ , which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality. \n10.5 Beam Search \nThe decoding algorithm we gave above for generating translations has a problem (as does the autoregressive generation we introduced in Chapter 9 for generating from a conditional language model). Recall that algorithm: at each time step in decoding, the output $y _ { t }$ is chosen by computing a softmax over the set of possible outputs (the vocabulary, in the case of language modeling or MT), and then choosing the highest probability token (the argmax): \ngreedy \nChoosing the single most probable token to generate at each step is called greedy decoding; a greedy algorithm is one that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight. \nIndeed, greedy search is not optimal, and may not find the highest probability translation. The problem is that the token that looks good to the decoder now might turn out later to have been the wrong choice! \nLet’s see this by looking at the search tree, a graphical representation of the choices the decoder makes in searching for the best translation, in which we view the decoding problem as a heuristic state-space search and systematically explore the space of possible outputs. In such a search tree, the branches are the actions, in this case the action of generating a token, and the nodes are the states, in this case the state of having generated a particular prefix. We are searching for the best action sequence, i.e. the target string with the highest probability. Fig. 10.11 demonstrates the problem, using a made-up example. Notice that the most probable sequence is ok ok $< / { mathsf { s } } >$ (with a probability of $. 4 ^ { * } . 7 ^ { * } 1 . 0 )$ , but a greedy search algorithm will fail to find it, because it incorrectly chooses yes as the first word since it has the highest local probability. \nRecall from Chapter 8 that for part-of-speech tagging we used dynamic programming search (the Viterbi algorithm) to address this problem. Unfortunately, dynamic programming is not applicable to generation problems with long-distance dependencies between the output decisions. The only method guaranteed to find the",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Attention",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "10.5 Beam Search \nThe decoding algorithm we gave above for generating translations has a problem (as does the autoregressive generation we introduced in Chapter 9 for generating from a conditional language model). Recall that algorithm: at each time step in decoding, the output $y _ { t }$ is chosen by computing a softmax over the set of possible outputs (the vocabulary, in the case of language modeling or MT), and then choosing the highest probability token (the argmax): \ngreedy \nChoosing the single most probable token to generate at each step is called greedy decoding; a greedy algorithm is one that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight. \nIndeed, greedy search is not optimal, and may not find the highest probability translation. The problem is that the token that looks good to the decoder now might turn out later to have been the wrong choice! \nLet’s see this by looking at the search tree, a graphical representation of the choices the decoder makes in searching for the best translation, in which we view the decoding problem as a heuristic state-space search and systematically explore the space of possible outputs. In such a search tree, the branches are the actions, in this case the action of generating a token, and the nodes are the states, in this case the state of having generated a particular prefix. We are searching for the best action sequence, i.e. the target string with the highest probability. Fig. 10.11 demonstrates the problem, using a made-up example. Notice that the most probable sequence is ok ok $< / { mathsf { s } } >$ (with a probability of $. 4 ^ { * } . 7 ^ { * } 1 . 0 )$ , but a greedy search algorithm will fail to find it, because it incorrectly chooses yes as the first word since it has the highest local probability. \nRecall from Chapter 8 that for part-of-speech tagging we used dynamic programming search (the Viterbi algorithm) to address this problem. Unfortunately, dynamic programming is not applicable to generation problems with long-distance dependencies between the output decisions. The only method guaranteed to find the \nbeam search \nbeam width \nbest solution is exhaustive search: computing the probability of every one of the $V ^ { T }$ possible sentences (for some length value $T$ ) which is obviously too slow. \nInstead, decoding in MT and other sequence generation problems generally uses a method called beam search. In beam search, instead of choosing the best token to generate at each timestep, we keep $k$ possible tokens at each step. This fixed-size memory footprint $k$ is called the beam width, on the metaphor of a flashlight beam that can be parameterized to be wider or narrower. \nThus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the $k$ -best options from this softmax output. These initial $k$ outputs are the search frontier and these $k$ initial words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability. \nAt subsequent steps, each of the $k$ best hypotheses is extended incrementally by being passed to distinct decoders, which each generate a softmax over the entire vocabulary to extend the hypothesis to every possible next token. Each of these $k * V$ hypotheses is scored by $P ( y _ { i } | boldsymbol x , boldsymbol y _ { < i } )$ : the product of the probability of current word choice multiplied by the probability of the path that led to it. We then prune the $k * V$ hypotheses down to the $k$ best hypotheses, so there are never more than $k$ hypotheses at the frontier of the search, and never more than $k$ decoders. \n\nFig. 10.12 illustrates this process with a beam width of 2. \nThis process continues until a $< / mathsf { s } >$ is generated indicating that a complete candidate output has been found. At this point, the completed hypothesis is removed from the frontier and the size of the beam is reduced by one. The search continues until the beam has been reduced to 0. The result will be $k$ hypotheses. \nLet’s see how the scoring works in detail, scoring each node by its log probability. Recall from Eq. 10.10 that we can use the chain rule of probability to break down $p ( y | x )$ into the product of the probability of each word given its prior context, which we can turn into a sum of logs (for an output string of length $t$ ): \nThus at each step, to compute the probability of a partial translation, we simply add the log probability of the prefix translation so far to the log probability of generating the next token. Fig. 10.13 shows the scoring for the example sentence shown in Fig. 10.12, using some simple made-up probabilities. Log probabilities are negative or 0, and the max of two log probabilities is the one that is greater (closer to 0). \nlog P (arrived the|x) log P (“the green witch arrived”|x) = -2.3 = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x) the ① +logP(arrived|the,green,witch,x) +log P(END|the,green,witch,arrived,x) log P(arrived|x) -.69 log P(arrived witch|x) -3.2 -2.7 -1.6 = -3.9 mage -2.5 END arrived -2.3 witch -2.1 -.22 arrived -1.6 -1.6 -2.3 -4.8 log P(the green|x) -.36 -3.7 at start -.92 log P(the|x) =-.92 -.69 green $ O =$ 1.6 -.51 witch -1.6 came ① the -2.7 = o-g2.P1(the witch|x) -2.2 END -1.2 -.51 witch -.11 arrived -2.3 -4.4 -1.61 -b3.y8 ○ who log P(y1|x) log P(y2|y1,x) log P(y3|y2,y1,x) log P(y4|y3,y2,y1,x) log P(y5|y4,y3,y2,y1,x) y1 y2 y3 y4 y5 \nFigure 10.13 Scoring for beam search decoding with a beam width of $k = 2$ . We maintain the log probability of each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top $k$ paths are extended to the next step. \nOne problem arises from the fact that the completed hypotheses may have different lengths. Because models generally assign lower probabilities to longer strings, a naive algorithm would also choose shorter strings for y. This was not an issue during the earlier steps of decoding; due to the breadth-first nature of beam search all the hypotheses being compared had the same length. The usual solution to this is \nfunction BEAMDECODE(c, beam width) returns best paths $begin{array} { l } { { y _ { 0 } , h _ { 0 }  0 } }  { { p a t h  0 } } end{array}$ complete paths $ ( )$ $s t a t e gets ( c , y _ { 0 } , h _ { 0 } ,$ , path) ;initial state frontier $cdot gets langle s t a t e rangle$ ;initial frontier while frontier contains incomplete paths and beamwidth $> 0$ extended frontier $ langle rangle$ for each state $in$ frontier do $y gets mathrm { D E C O D E } ( s t a t e ,$ ) for each word $i in$ Vocabulary do successor $$ NEWSTATE(state, i, yi) extended frontier $$ ADDTOBEAM(successor, extended frontier, beam width) for each state in extended frontier do if state is complete do complete paths $$ APPEND(complete paths, state) extended frontier $$ REMOVE(extended frontier, state) beam width $$ beam width - 1 frontier $$ extended frontier return completed paths   \nfunction NEWSTATE(state, word, word prob) returns new state   \nfunction ADDTOBEAM(state, frontier, width) returns updated frontier if LENGTH(frontier) $<$ width then frontier $$ INSERT(state, frontier) else if SCORE(state) $>$ SCORE(WORSTOF(frontier)) frontier $$ REMOVE(WORSTOF(frontier)) frontier $$ INSERT(state, frontier) return frontier \nto apply some form of length normalization to each of the hypotheses, for example simply dividing the negative log probability by the number of words: \nBeam search is common in large production MT systems, generally with beam widths $k$ between 5 and 10. What do we do with the resulting $k$ hypotheses? In some cases, all we need from our MT algorithm is the single best hypothesis, so we can return that. In other cases our downstream application might want to look at all $k$ hypotheses, so we can pass them all (or a subset) to the downstream application with their respective scores.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Beam Search",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "function BEAMDECODE(c, beam width) returns best paths $begin{array} { l } { { y _ { 0 } , h _ { 0 }  0 } }  { { p a t h  0 } } end{array}$ complete paths $ ( )$ $s t a t e gets ( c , y _ { 0 } , h _ { 0 } ,$ , path) ;initial state frontier $cdot gets langle s t a t e rangle$ ;initial frontier while frontier contains incomplete paths and beamwidth $> 0$ extended frontier $ langle rangle$ for each state $in$ frontier do $y gets mathrm { D E C O D E } ( s t a t e ,$ ) for each word $i in$ Vocabulary do successor $$ NEWSTATE(state, i, yi) extended frontier $$ ADDTOBEAM(successor, extended frontier, beam width) for each state in extended frontier do if state is complete do complete paths $$ APPEND(complete paths, state) extended frontier $$ REMOVE(extended frontier, state) beam width $$ beam width - 1 frontier $$ extended frontier return completed paths   \nfunction NEWSTATE(state, word, word prob) returns new state   \nfunction ADDTOBEAM(state, frontier, width) returns updated frontier if LENGTH(frontier) $<$ width then frontier $$ INSERT(state, frontier) else if SCORE(state) $>$ SCORE(WORSTOF(frontier)) frontier $$ REMOVE(WORSTOF(frontier)) frontier $$ INSERT(state, frontier) return frontier \nto apply some form of length normalization to each of the hypotheses, for example simply dividing the negative log probability by the number of words: \nBeam search is common in large production MT systems, generally with beam widths $k$ between 5 and 10. What do we do with the resulting $k$ hypotheses? In some cases, all we need from our MT algorithm is the single best hypothesis, so we can return that. In other cases our downstream application might want to look at all $k$ hypotheses, so we can pass them all (or a subset) to the downstream application with their respective scores. \n10.6 Encoder-Decoder with Transformers \nThe encoder-decoder architecture can also be implemented using transformers (rather than RNN/LSTMs) as the component modules. At a high-level, the architecture, sketched in Fig. 10.15, is quite similar to what we saw for RNNs. It consists of an encoder that takes the source language input words $pmb { mathsf { X } } = pmb { mathsf { x } } _ { 1 } , . . . , pmb { mathsf { x } } _ { T }$ and maps them to an output representation ${ mathsf { mathbf { mathbf { H } } } } ^ { e n c } = { mathsf { mathbf { h } } } _ { 1 } , . . . , { mathsf { mathbf { h } } } _ { T }$ ; usually via $N = 6$ stacked encoder blocks. The decoder, just like the encoder-decoder RNN, is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep conditioning on the source sentence and the previously generated target language words. \ncross-attention \nBut the components of the architecture differ somewhat from the RNN and also from the transformer block we’ve seen. First, in order to attend to the source language, the transformer blocks in the decoder has an extra cross-attention layer. Recall that the transformer block of Chapter 9 consists of a self-attention layer that attends to the input from the previous layer, followed by layer norm, a feed forward layer, and another layer norm. The decoder transformer block includes an extra layer with a special kind of attention, cross-attention (also sometimes called encoder-decoder attention or source attention). Cross-attention has the same form as the multi-headed self-attention in a normal transformer block, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder. \nThat is, the final output of the encoder ${ bf { sf H } } ^ { e n c } = { bf { h } } _ { 1 } , . . . , { bf { h } } _ { t }$ is multiplied by the cross-attention layer’s key weights $boldsymbol { mathsf { w } } ^ { kappa }$ and value weights $boldsymbol { mathsf { W } } ^ { boldsymbol { mathsf { v } } }$ , but the output from the prior decoder layer $dot { bf H } ^ { d e c [ i - 1 ] }$ is multiplied by the cross-attention layer’s query weights $boldsymbol { mathsf { W } ^ { mathsf { Q } } }$ : \nThe cross attention thus allows the decoder to attend to each of the source language words as projected into the the entire encoder final output representations. The other attention layer in each decoder block, the self-attention layer, is the same causal (leftto-right) self-attention that we saw in Chapter 9. The self-attention in the encoder, however, is allowed to look ahead at the entire source language text. \nIn training, just as for RNN encoder-decoders, we use teacher forcing, and train autoregressively, at each time step predicting the next token in the target language, using cross-entropy loss. \n10.7 Some practical details on building MT systems \nwordpiece \n10.7.1 Tokenization \nMachine translation systems generally use a fixed vocabulary, A common way to generate this vocabulary is with the BPE or wordpiece algorithms sketched in Chapter 2. Generally a shared vocabulary is used for the source and target languages, which makes it easy to copy tokens (like names) from source to target, so we build the wordpiece/BPE lexicon on a corpus that contains both source and target language data. Wordpieces use a special symbol at the beginning of each token; here’s a resulting tokenization from the Google MT system (Wu et al., 2016): \nwords: Jet makers feud over seat width with big orders at stake wordpieces: J et makers fe ud over seat width with big orders at stake \nWe gave the BPE algorithm in detail in Chapter 2; here are more details on the wordpiece algorithm, which is given a training corpus and a desired vocabulary size",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Encoder-Decoder with Transformers",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The cross attention thus allows the decoder to attend to each of the source language words as projected into the the entire encoder final output representations. The other attention layer in each decoder block, the self-attention layer, is the same causal (leftto-right) self-attention that we saw in Chapter 9. The self-attention in the encoder, however, is allowed to look ahead at the entire source language text. \nIn training, just as for RNN encoder-decoders, we use teacher forcing, and train autoregressively, at each time step predicting the next token in the target language, using cross-entropy loss. \n10.7 Some practical details on building MT systems \nwordpiece \n10.7.1 Tokenization \nMachine translation systems generally use a fixed vocabulary, A common way to generate this vocabulary is with the BPE or wordpiece algorithms sketched in Chapter 2. Generally a shared vocabulary is used for the source and target languages, which makes it easy to copy tokens (like names) from source to target, so we build the wordpiece/BPE lexicon on a corpus that contains both source and target language data. Wordpieces use a special symbol at the beginning of each token; here’s a resulting tokenization from the Google MT system (Wu et al., 2016): \nwords: Jet makers feud over seat width with big orders at stake wordpieces: J et makers fe ud over seat width with big orders at stake \nWe gave the BPE algorithm in detail in Chapter 2; here are more details on the wordpiece algorithm, which is given a training corpus and a desired vocabulary size \nV, and proceeds as follows: \n1. Initialize the wordpiece lexicon with characters (for example a subset of Unicode characters, collapsing all the remaining characters to a special unknown character token). \n2. Repeat until there are V wordpieces: \n(a) Train an n-gram language model on the training corpus, using the current set of wordpieces.   \n(b) Consider the set of possible new wordpieces made by concatenating two wordpieces from the current lexicon. Choose the one new wordpiece that most increases the language model probability of the training corpus. \nA vocabulary of 8K to 32K word pieces is commonly used. \n10.7.2 MT corpora \nparallel corpus \nEuroparl \nMachine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. Large numbers of parallel corpora are available. Some are governmental; the Europarl corpus (Koehn, 2005), extracted from the proceedings of the European Parliament, contains between 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from general web text, like the ParaCrawl corpus of 223 million sentence pairs between 23 EU languages and English extracted from the CommonCrawl Ban˜o´n et al. (2020). \nSentence alignment \nStandard training corpora for MT come as aligned pairs of sentences. When creating new corpora, for example for underresourced languages or new domains, these sentence alignments must be created. Fig. 10.17 gives a sample hypothetical sentence alignment. \nE1: “Good morning,\" said the little prince. F1: -Bonjour, dit le petit prince.   \nE2: “Good morning,\" said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionnées qui apaisent la soif.   \nE3: This was a merchant who sold pills that had F3: On en avale une par semaine et l'on n'éprouve plus le been perfected to quench thirst. besoin de boire.   \nE4: You just swallow one pill a week and you F4: -C’est une grosse économie de temps, dit le marchand. won’t feel the need for anything to drink.   \nE5: “They save a huge amount of time,\" said the merchant. F5: Les experts ont fait des calculs.   \nE6: “Fifty−three minutes a week.\" F6: On épargne cinquante-trois minutes par semaine. E7: “If I had  fifty−three minutes to spend?\" said the F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes little prince to himself. à dépenser, je marcherais tout doucement vers une fontaine...\" E8: “I would take a stroll to a spring of fresh water” \nFigure 10.17 A sample alignment between sentences in English and French, with sentences extracted from Antoine de Saint-Exupery’s $L e$ Petit Prince and a hypothetical translation. Sentence alignment takes sentences $e _ { 1 } , . . . , e _ { n }$ , and $f _ { 1 } , . . . , f _ { n }$ and finds minimal sets of sentences that are translations of each other, including single sentence mappings like $( mathrm { e } _ { 1 } , mathrm { f } _ { 1 } )$ , $mathbf { Gamma } ( mathbf { e } _ { 4 } , mathbf { f } _ { 3 } )$ , $( mathrm { e } _ { 5 } , mathrm { f } _ { 4 } )$ , $left( mathrm { e } _ { 6 } mathrm { , f } _ { 6 } right)$ as well as 2-1 alignments $( mathrm { e } _ { 2 } / mathrm { e } _ { 3 } , mathrm { f } _ { 2 } )$ , $mathrm { ( e _ { 7 } / e _ { 8 } , f _ { 7 } ) }$ , and null alignments $( mathrm { f } _ { 5 } )$ .",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Some practical details on building MT systems",
        "subsection": "Tokenization",
        "subsubsection": "N/A"
    },
    {
        "content": "V, and proceeds as follows: \n1. Initialize the wordpiece lexicon with characters (for example a subset of Unicode characters, collapsing all the remaining characters to a special unknown character token). \n2. Repeat until there are V wordpieces: \n(a) Train an n-gram language model on the training corpus, using the current set of wordpieces.   \n(b) Consider the set of possible new wordpieces made by concatenating two wordpieces from the current lexicon. Choose the one new wordpiece that most increases the language model probability of the training corpus. \nA vocabulary of 8K to 32K word pieces is commonly used. \n10.7.2 MT corpora \nparallel corpus \nEuroparl \nMachine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. Large numbers of parallel corpora are available. Some are governmental; the Europarl corpus (Koehn, 2005), extracted from the proceedings of the European Parliament, contains between 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from general web text, like the ParaCrawl corpus of 223 million sentence pairs between 23 EU languages and English extracted from the CommonCrawl Ban˜o´n et al. (2020). \nSentence alignment \nStandard training corpora for MT come as aligned pairs of sentences. When creating new corpora, for example for underresourced languages or new domains, these sentence alignments must be created. Fig. 10.17 gives a sample hypothetical sentence alignment. \nE1: “Good morning,\" said the little prince. F1: -Bonjour, dit le petit prince.   \nE2: “Good morning,\" said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionnées qui apaisent la soif.   \nE3: This was a merchant who sold pills that had F3: On en avale une par semaine et l'on n'éprouve plus le been perfected to quench thirst. besoin de boire.   \nE4: You just swallow one pill a week and you F4: -C’est une grosse économie de temps, dit le marchand. won’t feel the need for anything to drink.   \nE5: “They save a huge amount of time,\" said the merchant. F5: Les experts ont fait des calculs.   \nE6: “Fifty−three minutes a week.\" F6: On épargne cinquante-trois minutes par semaine. E7: “If I had  fifty−three minutes to spend?\" said the F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes little prince to himself. à dépenser, je marcherais tout doucement vers une fontaine...\" E8: “I would take a stroll to a spring of fresh water” \nFigure 10.17 A sample alignment between sentences in English and French, with sentences extracted from Antoine de Saint-Exupery’s $L e$ Petit Prince and a hypothetical translation. Sentence alignment takes sentences $e _ { 1 } , . . . , e _ { n }$ , and $f _ { 1 } , . . . , f _ { n }$ and finds minimal sets of sentences that are translations of each other, including single sentence mappings like $( mathrm { e } _ { 1 } , mathrm { f } _ { 1 } )$ , $mathbf { Gamma } ( mathbf { e } _ { 4 } , mathbf { f } _ { 3 } )$ , $( mathrm { e } _ { 5 } , mathrm { f } _ { 4 } )$ , $left( mathrm { e } _ { 6 } mathrm { , f } _ { 6 } right)$ as well as 2-1 alignments $( mathrm { e } _ { 2 } / mathrm { e } _ { 3 } , mathrm { f } _ { 2 } )$ , $mathrm { ( e _ { 7 } / e _ { 8 } , f _ { 7 } ) }$ , and null alignments $( mathrm { f } _ { 5 } )$ . \nGiven two documents that are translations of each other, we generally need two steps to produce sentence alignments: \n• a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations.   \n• an alignment algorithm that takes these scores to find a good alignment between the documents. \nSince it is possible to induce multilingual sentence embeddings (Artetxe and Schwenk, 2019), cosine similarity of such embeddings provides a natural scoring function (Schwenk, 2018). Thompson and Koehn (2019) give the following cost function between two sentences or spans $x , y$ from the source and target documents respectively: \nwhere nSents() gives the number of sentences (this biases the metric toward many alignments of single sentences instead of aligning very large spans). The denominator helps to normalize the similarities, and so $x _ { 1 } , . . . , x _ { S } , y _ { 1 } , . . . , y _ { S }$ , are randomly selected sentences sampled from the respective documents. \nUsually dynamic programming is used as the alignment algorithm (Gale and Church, 1993), in a simple extension of the minimum edit distance algorithm we introduced in Chapter 2. \nFinally, it’s helpful to do some corpus cleanup by removing noisy sentence pairs. This can involve handwritten rules to remove low-precision pairs (for example removing sentences that are too long, too short, have different URLs, or even pairs that are too similar, suggesting that they were copies rather than translations). Or pairs can be ranked by their multilingual embedding cosine score and low-scoring pairs discarded. \nbacktranslation \n10.7.3 Backtranslation \nWe’re often short of data for training MT models, since parallel corpora may be limited for particular languages or domains. However, often we can find a large monolingual corpus, to add to the smaller parallel corpora that are available. \nBacktranslation is a way of making use of monolingual corpora in the target language by creating synthetic bitexts. In backtranslation, we train an intermediate target-to-source MT system on the small bitext to translate the monolingual target data to the source language. Now we can add this synthetic bitext (natural target sentences, aligned with MT-produced source sentences) to our training data, and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of course we can find lots of monolingual English data. We use the small bitext to build an MT engine going the other way (from English to Navajo). Once we translate the monolingual English text to Navajo, we can add this synthetic Navajo/English bitext to our training data. \nMonte Carlo search \nBacktranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, or Monte Carlo search. In Monte Carlo decoding, at each timestep, instead of always generating the word with the highest softmax probability, we roll a weighted die, and use it to choose the next word according to its softmax probability. This works just like the sampling algorithm we saw in Chapter 3 for generating random sentences from n-gram language models. Imagine there are only 4 words and the softmax probability distribution at time t is (the: 0.6, green: 0.2, a: 0.1, witch: 0.1). We roll a weighted die, with the 4 sides weighted 0.6, 0.2, 0.1, and 0.1, and chose the word based on which side comes up. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence).",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Some practical details on building MT systems",
        "subsection": "MT corpora",
        "subsubsection": "N/A"
    },
    {
        "content": "Given two documents that are translations of each other, we generally need two steps to produce sentence alignments: \n• a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations.   \n• an alignment algorithm that takes these scores to find a good alignment between the documents. \nSince it is possible to induce multilingual sentence embeddings (Artetxe and Schwenk, 2019), cosine similarity of such embeddings provides a natural scoring function (Schwenk, 2018). Thompson and Koehn (2019) give the following cost function between two sentences or spans $x , y$ from the source and target documents respectively: \nwhere nSents() gives the number of sentences (this biases the metric toward many alignments of single sentences instead of aligning very large spans). The denominator helps to normalize the similarities, and so $x _ { 1 } , . . . , x _ { S } , y _ { 1 } , . . . , y _ { S }$ , are randomly selected sentences sampled from the respective documents. \nUsually dynamic programming is used as the alignment algorithm (Gale and Church, 1993), in a simple extension of the minimum edit distance algorithm we introduced in Chapter 2. \nFinally, it’s helpful to do some corpus cleanup by removing noisy sentence pairs. This can involve handwritten rules to remove low-precision pairs (for example removing sentences that are too long, too short, have different URLs, or even pairs that are too similar, suggesting that they were copies rather than translations). Or pairs can be ranked by their multilingual embedding cosine score and low-scoring pairs discarded. \nbacktranslation \n10.7.3 Backtranslation \nWe’re often short of data for training MT models, since parallel corpora may be limited for particular languages or domains. However, often we can find a large monolingual corpus, to add to the smaller parallel corpora that are available. \nBacktranslation is a way of making use of monolingual corpora in the target language by creating synthetic bitexts. In backtranslation, we train an intermediate target-to-source MT system on the small bitext to translate the monolingual target data to the source language. Now we can add this synthetic bitext (natural target sentences, aligned with MT-produced source sentences) to our training data, and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of course we can find lots of monolingual English data. We use the small bitext to build an MT engine going the other way (from English to Navajo). Once we translate the monolingual English text to Navajo, we can add this synthetic Navajo/English bitext to our training data. \nMonte Carlo search \nBacktranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, or Monte Carlo search. In Monte Carlo decoding, at each timestep, instead of always generating the word with the highest softmax probability, we roll a weighted die, and use it to choose the next word according to its softmax probability. This works just like the sampling algorithm we saw in Chapter 3 for generating random sentences from n-gram language models. Imagine there are only 4 words and the softmax probability distribution at time t is (the: 0.6, green: 0.2, a: 0.1, witch: 0.1). We roll a weighted die, with the 4 sides weighted 0.6, 0.2, 0.1, and 0.1, and chose the word based on which side comes up. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence). \n\nIn general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018). \n10.8 MT Evaluation \nTranslations are evaluated along two dimensions: \nadequacy \nfluency \n1. adequacy: how well the translation captures the exact meaning of the source sentence. Sometimes called faithfulness or fidelity.   \n2. fluency: how fluent the translation is in the target language (is it grammatical, clear, readable, natural). \nUsing humans to evaluate is most accurate, but automatic metrics are also used for convenience. \nranking \n10.8.1 Using Human Raters to Evaluate MT \nThe most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible, or 1 to 100, and ask them to rate each sentence or paragraph of the MT output. \nWe can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer. \nTraining of human raters (who are often online crowdworkers) is essential; raters without translation expertise find it difficult to separate fluency and adequacy, and so training includes examples carefully distinguishing these. Raters often disagree (sources sentences may be ambiguous, raters will have different world knowledge, raters may apply scales differently). It is therefore common to remove outlier raters, and (if we use a fine-grained enough scale) normalizing raters by subtracting the mean from their scores and dividing by the variance.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Some practical details on building MT systems",
        "subsection": "Backtranslation",
        "subsubsection": "N/A"
    },
    {
        "content": "In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018). \n10.8 MT Evaluation \nTranslations are evaluated along two dimensions: \nadequacy \nfluency \n1. adequacy: how well the translation captures the exact meaning of the source sentence. Sometimes called faithfulness or fidelity.   \n2. fluency: how fluent the translation is in the target language (is it grammatical, clear, readable, natural). \nUsing humans to evaluate is most accurate, but automatic metrics are also used for convenience. \nranking \n10.8.1 Using Human Raters to Evaluate MT \nThe most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible, or 1 to 100, and ask them to rate each sentence or paragraph of the MT output. \nWe can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer. \nTraining of human raters (who are often online crowdworkers) is essential; raters without translation expertise find it difficult to separate fluency and adequacy, and so training includes examples carefully distinguishing these. Raters often disagree (sources sentences may be ambiguous, raters will have different world knowledge, raters may apply scales differently). It is therefore common to remove outlier raters, and (if we use a fine-grained enough scale) normalizing raters by subtracting the mean from their scores and dividing by the variance.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "MT Evaluation",
        "subsection": "Using Human Raters to Evaluate MT",
        "subsubsection": "N/A"
    },
    {
        "content": "In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018). \n10.8 MT Evaluation \nTranslations are evaluated along two dimensions: \nadequacy \nfluency \n1. adequacy: how well the translation captures the exact meaning of the source sentence. Sometimes called faithfulness or fidelity.   \n2. fluency: how fluent the translation is in the target language (is it grammatical, clear, readable, natural). \nUsing humans to evaluate is most accurate, but automatic metrics are also used for convenience. \nranking \n10.8.1 Using Human Raters to Evaluate MT \nThe most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible, or 1 to 100, and ask them to rate each sentence or paragraph of the MT output. \nWe can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer. \nTraining of human raters (who are often online crowdworkers) is essential; raters without translation expertise find it difficult to separate fluency and adequacy, and so training includes examples carefully distinguishing these. Raters often disagree (sources sentences may be ambiguous, raters will have different world knowledge, raters may apply scales differently). It is therefore common to remove outlier raters, and (if we use a fine-grained enough scale) normalizing raters by subtracting the mean from their scores and dividing by the variance. \n10.8.2 Automatic Evaluation \nWhile humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character- or word-overlap and those based on embedding similarity. \nAutomatic Evaluation by Character Overlap: chrF \nchrF \nThe simplest and most robust metric for MT evaluation is called chrF, which stands for character F-score (Popovic´, 2015). chrF (along with many other earlier related metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which each source sentence has both a gold human target translation and a candidate MT translation we’d like to evaluate. The chrF metric ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation. \nGiven the hypothesis and the reference, chrF is given a parameter $k$ indicating the length of character n-grams to be considered, and computes the average of the $k$ precisions (unigram precision, bigram, and so on) and the average of the $k$ recalls (unigram recall, bigram recall, etc.): \nchrP percentage of character 1-grams, 2-grams, ..., $mathbf { k }$ -grams in the hypothesis that occur in the reference, averaged. \nchrR percentage of character 1-grams, 2-grams,..., $mathbf { k }$ -grams in the reference that occur in the hypothesis, averaged. \nThe metric then computes an F-score by combining chrP and chrR using a weighting parameter $beta$ . It is common to set $beta = 2$ , thus weighing recall twice as much as precision: \nFor $beta = 2$ , that would be: \nFor example, consider two hypotheses that we’d like to score against the reference translation witness for the past. Here are the hypotheses along with chrF values computed using parameters $k = beta = 2$ (in real examples, $k$ would be a higher number like 6): \nLet’s see how we computed that chrF value for HYP1 (we’ll leave the computation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores spaces, so we’ll remove them from both the reference and hypothesis: \nREF: witnessforthepast, (18 unigrams, 17 bigrams) HYP1: witnessofthepast, (17 unigrams, 16 bigrams) \nNext let’s see how many unigrams and bigrams match between the reference and hypothesis: \nunigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams) bigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams) \nWe use that to compute the unigram and bigram precisions and recalls: \nunigram P: $1 7 / 1 7 = 1$ unigram R: $1 7 / 1 8 = . 9 4 4$ bigram P: $1 3 / 1 6 = . 8 1 3$ bigram R: $1 3 / 1 7 = . 7 6 5$ \nFinally we average to get chrP and chrR, and compute the $mathrm { F }$ -score: \nchrF is simple, robust, and correlates very well with human judgments in many languages (Kocmi et al., 2021). There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole. Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it difficult to compare across situations, and doesn’t work as well in languages with complex morphology. \nStatistical Significance Testing for MT evals \nCharacter or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly used to compare two systems, with the goal of answering questions like: did the new algorithm we just invented improve our MT system? To know if the difference between the chrF scores of two MT systems is a significant difference, we use the paired bootstrap test, or the similar randomization test. \nTo get a confidence interval on a single chrF score using the bootstrap test, recall from Section 4.9 that we take our test set (or devset) and create thousands of pseudotestsets by repeatedly sampling with replacement from the original test set. We now compute the chrF score of each of the pseudo-testsets. If we drop the top $2 . 5 %$ and bottom $2 . 5 %$ of the scores, the remaining scores will give us the $9 5 %$ confidence interval for the chrF score of our system. \nTo compare two MT systems A and B, we draw the same set of pseudo-testsets, and compute the chrF scores for each of them. We then compute the percentage of pseudo-test-sets in which A has a higher chrF score than B. \nchrF: Limitations \nWhile automatic character and word-overlap metrics like chrF or BLEU are useful, they have important limitations. chrF is very local: a large phrase that is moved around might barely change the chrF score at all, and chrF can’t evaluate crosssentence properties of a document like its discourse coherence (Chapter 22). chrF and similar automatic metrics also do poorly at comparing very different kinds of systems, such as comparing human-aided translation against machine translation, or different machine translation architectures against each other (Callison-Burch et al., 2006). Instead, automatic overlap metrics like chrF are most appropriate when evaluating changes to a single system. \n\n10.8.3 Automatic Evaluation: Embedding-Based Methods \nThe chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common. However, this criterion is overly strict, since a good translation may use alternate words or paraphrases. A solution first pioneered in early metrics like METEOR (Banerjee and Lavie, 2005) was to allow synonyms to match between the reference $x$ and candidate $tilde { x }$ . More recent metrics use BERT or other embeddings to implement this intuition. \nFor example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples $( x , tilde { x } , r )$ , where $x = ( x _ { 1 } , ldots , x _ { n } ) $ is a reference translation, $tilde { x } = ( tilde { x } _ { 1 } , dots , tilde { x } _ { m } )$ is a candidate machine translation, and $r in mathbb { R }$ is a human rating that expresses the quality of $tilde { x }$ with respect to $x$ . Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing $x$ and $tilde { x }$ through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict $r$ . The output of such models correlates highly with human labels. \nIn other cases, however, we don’t have such human-labeled datasets. In that case we can measure the similarity of $x$ and $tilde { x }$ by the similarity of their embeddings. The BERTSCORE algorithm (Zhang et al., 2020) shown in Fig. 10.18, for example, passes the reference $x$ and the candidate $tilde { x }$ through BERT, computing a BERT embedding for each token $x _ { i }$ and $tilde { x } _ { j }$ . Each pair of tokens $( x _ { i } , tilde { x } _ { j } )$ is scored by its cosine $frac { boldsymbol { x } _ { i } { cdot } tilde { boldsymbol { x } } _ { j } } { | boldsymbol { x } _ { i } | | tilde { boldsymbol { x } } _ { j } | }$ . Each token in $x$ is matched to a token in $tilde { x }$ to compute recall, and each token in $tilde { x }$ is matched to a token in $x$ to compute precision (with each token greedily matched to the most similar token in the corresponding sentence). BERTSCORE provides precision and recall (and hence $mathrm { F } _ { 1 }$ ): \nContextual Pairwise Cosine Maximum Similarity Importance Weighting Embedding Similarity (Optional)   \nReference x 眀 the 0.5970.428 0.40   \nthe weather is 0.462 0.39   \ncold today 41.0.447 cold 0.479 0.45 RBERT $mathbf { Sigma } = mathbf { Sigma }$ (0.713×1.27)+(0.515×7.94)+... 1.27+7.94+1.82+7.90+8.88   \nCandidate xˆ today   \nit is freezing today idf weights Candidate",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "MT Evaluation",
        "subsection": "Automatic Evaluation",
        "subsubsection": "N/A"
    },
    {
        "content": "10.8.3 Automatic Evaluation: Embedding-Based Methods \nThe chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common. However, this criterion is overly strict, since a good translation may use alternate words or paraphrases. A solution first pioneered in early metrics like METEOR (Banerjee and Lavie, 2005) was to allow synonyms to match between the reference $x$ and candidate $tilde { x }$ . More recent metrics use BERT or other embeddings to implement this intuition. \nFor example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples $( x , tilde { x } , r )$ , where $x = ( x _ { 1 } , ldots , x _ { n } ) $ is a reference translation, $tilde { x } = ( tilde { x } _ { 1 } , dots , tilde { x } _ { m } )$ is a candidate machine translation, and $r in mathbb { R }$ is a human rating that expresses the quality of $tilde { x }$ with respect to $x$ . Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing $x$ and $tilde { x }$ through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict $r$ . The output of such models correlates highly with human labels. \nIn other cases, however, we don’t have such human-labeled datasets. In that case we can measure the similarity of $x$ and $tilde { x }$ by the similarity of their embeddings. The BERTSCORE algorithm (Zhang et al., 2020) shown in Fig. 10.18, for example, passes the reference $x$ and the candidate $tilde { x }$ through BERT, computing a BERT embedding for each token $x _ { i }$ and $tilde { x } _ { j }$ . Each pair of tokens $( x _ { i } , tilde { x } _ { j } )$ is scored by its cosine $frac { boldsymbol { x } _ { i } { cdot } tilde { boldsymbol { x } } _ { j } } { | boldsymbol { x } _ { i } | | tilde { boldsymbol { x } } _ { j } | }$ . Each token in $x$ is matched to a token in $tilde { x }$ to compute recall, and each token in $tilde { x }$ is matched to a token in $x$ to compute precision (with each token greedily matched to the most similar token in the corresponding sentence). BERTSCORE provides precision and recall (and hence $mathrm { F } _ { 1 }$ ): \nContextual Pairwise Cosine Maximum Similarity Importance Weighting Embedding Similarity (Optional)   \nReference x 眀 the 0.5970.428 0.40   \nthe weather is 0.462 0.39   \ncold today 41.0.447 cold 0.479 0.45 RBERT $mathbf { Sigma } = mathbf { Sigma }$ (0.713×1.27)+(0.515×7.94)+... 1.27+7.94+1.82+7.90+8.88   \nCandidate xˆ today   \nit is freezing today idf weights Candidate",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "MT Evaluation",
        "subsection": "Automatic Evaluation: Embedding-Based Methods",
        "subsubsection": "N/A"
    },
    {
        "content": "10.8.3 Automatic Evaluation: Embedding-Based Methods \nThe chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common. However, this criterion is overly strict, since a good translation may use alternate words or paraphrases. A solution first pioneered in early metrics like METEOR (Banerjee and Lavie, 2005) was to allow synonyms to match between the reference $x$ and candidate $tilde { x }$ . More recent metrics use BERT or other embeddings to implement this intuition. \nFor example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples $( x , tilde { x } , r )$ , where $x = ( x _ { 1 } , ldots , x _ { n } ) $ is a reference translation, $tilde { x } = ( tilde { x } _ { 1 } , dots , tilde { x } _ { m } )$ is a candidate machine translation, and $r in mathbb { R }$ is a human rating that expresses the quality of $tilde { x }$ with respect to $x$ . Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing $x$ and $tilde { x }$ through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict $r$ . The output of such models correlates highly with human labels. \nIn other cases, however, we don’t have such human-labeled datasets. In that case we can measure the similarity of $x$ and $tilde { x }$ by the similarity of their embeddings. The BERTSCORE algorithm (Zhang et al., 2020) shown in Fig. 10.18, for example, passes the reference $x$ and the candidate $tilde { x }$ through BERT, computing a BERT embedding for each token $x _ { i }$ and $tilde { x } _ { j }$ . Each pair of tokens $( x _ { i } , tilde { x } _ { j } )$ is scored by its cosine $frac { boldsymbol { x } _ { i } { cdot } tilde { boldsymbol { x } } _ { j } } { | boldsymbol { x } _ { i } | | tilde { boldsymbol { x } } _ { j } | }$ . Each token in $x$ is matched to a token in $tilde { x }$ to compute recall, and each token in $tilde { x }$ is matched to a token in $x$ to compute precision (with each token greedily matched to the most similar token in the corresponding sentence). BERTSCORE provides precision and recall (and hence $mathrm { F } _ { 1 }$ ): \nContextual Pairwise Cosine Maximum Similarity Importance Weighting Embedding Similarity (Optional)   \nReference x 眀 the 0.5970.428 0.40   \nthe weather is 0.462 0.39   \ncold today 41.0.447 cold 0.479 0.45 RBERT $mathbf { Sigma } = mathbf { Sigma }$ (0.713×1.27)+(0.515×7.94)+... 1.27+7.94+1.82+7.90+8.88   \nCandidate xˆ today   \nit is freezing today idf weights Candidate \n10.9 Bias and Ethical Issues \nMachine translation raises many of the same ethical issues that we’ve discussed in earlier chapters. For example, consider MT systems translating from Hungarian (which has the gender neutral pronoun $tilde { sigma }$ ) or Spanish (which often drops pronouns) into English (in which pronouns are obligatory, and they have grammatical gender). When translating a reference to a person described without specified gender, MT systems often default to male gender (Schiebinger 2014, Prates et al. 2019). And MT systems often assign gender according to culture stereotypes of the sort we saw in Section 6.11. Fig. 10.19 shows examples from Prates et al. (2019), in which Hungarian gender-neutral $tilde { sigma }$ is a nurse is translated with she, but gender-neutral $tilde { sigma }$ is $a$ $_ { C E O }$ is translated with he. Prates et al. (2019) find that these stereotypes can’t completely be accounted for by gender bias in US labor statistics, because the biases are amplified by MT systems, with pronouns being mapped to male or female gender with a probability higher than if the mapping was based on actual labor employment statistics. \nSimilarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019) shows that MT systems perform worse when they are asked to translate sentences that describe people with non-stereotypical gender roles, like “The doctor asked the nurse to help her in the operation”. \nconfidence \nMany ethical questions in MT require further research. One open problem is developing metrics for knowing what our systems don’t know. This is because MT systems can be used in urgent situations where human translators may be unavailable or delayed: in medical domains, to help translate when patients and doctors don’t speak the same language, or in legal domains, to help judges or lawyers communicate with witnesses or defendants. In order to ‘do no harm’, systems need ways to assign confidence values to candidate translations, so they can abstain from giving incorrect translations that may cause harm. \nlow-resourced languages \nAnother is the need for low-resource algorithms that can translate to and from all the world’s languages, the vast majority of which do not have large parallel training texts available. This problem is exacerbated by the tendency of many MT approaches to focus on the case where one of the languages is English (Anastasopoulos and Neubig, 2020). $forall$ et al. (2020) propose a participatory design process to encourage content creators, curators, and language technologists who speak these low-resourced languages to participate in developing MT algorithms. They provide online groups, mentoring, and infrastructure, and report on a case study on developing MT algorithms for low-resource African languages. \n10.10 Summary \nMachine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP. \n• Languages have divergences, both structural and lexical, that make translation difficult.   \n• The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.   \n• Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.   \n• The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.   \n• For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.   \n• In beam search, instead of choosing the best token to generate at each timestep, we keep $k$ possible tokens at each step. This fixed-size memory footprint $k$ is called the beam width.   \n• Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.   \n• Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.   \n• MT is evaluated by measuring a translation’s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used. \nBibliographical and Historical Notes \nMT was proposed seriously by the late 1940s, soon after the birth of the computer (Weaver, 1949/1955). In 1954, the first public demonstration of an MT system prototype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The next decade saw a great flowering of ideas, prefiguring most subsequent developments. But this work was ahead of its time—implementations were limited by, for example, the fact that pending the development of disks there was no good way to store dictionary information. \nAs high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus on the need for better evaluation and more basic research in the new fields of formal and computational linguistics. This consensus culminated in the famously critical ALPAC (Automatic Language Processing Advisory Committee) report of 1966 (Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT in the US. As MT research lost academic respectability, the Association for Machine Translation and Computational Linguistics dropped MT from its name. Some MT developers, however, persevered, and there were early MT systems like Me´te´o, which translated weather forecasts from English to French (Chandioux, 1976), and industrial systems like Systran.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Bias and Ethical Issues",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "10.10 Summary \nMachine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP. \n• Languages have divergences, both structural and lexical, that make translation difficult.   \n• The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.   \n• Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.   \n• The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.   \n• For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.   \n• In beam search, instead of choosing the best token to generate at each timestep, we keep $k$ possible tokens at each step. This fixed-size memory footprint $k$ is called the beam width.   \n• Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.   \n• Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.   \n• MT is evaluated by measuring a translation’s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used. \nBibliographical and Historical Notes \nMT was proposed seriously by the late 1940s, soon after the birth of the computer (Weaver, 1949/1955). In 1954, the first public demonstration of an MT system prototype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The next decade saw a great flowering of ideas, prefiguring most subsequent developments. But this work was ahead of its time—implementations were limited by, for example, the fact that pending the development of disks there was no good way to store dictionary information. \nAs high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus on the need for better evaluation and more basic research in the new fields of formal and computational linguistics. This consensus culminated in the famously critical ALPAC (Automatic Language Processing Advisory Committee) report of 1966 (Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT in the US. As MT research lost academic respectability, the Association for Machine Translation and Computational Linguistics dropped MT from its name. Some MT developers, however, persevered, and there were early MT systems like Me´te´o, which translated weather forecasts from English to French (Chandioux, 1976), and industrial systems like Systran.",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "10.10 Summary \nMachine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP. \n• Languages have divergences, both structural and lexical, that make translation difficult.   \n• The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.   \n• Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.   \n• The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.   \n• For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.   \n• In beam search, instead of choosing the best token to generate at each timestep, we keep $k$ possible tokens at each step. This fixed-size memory footprint $k$ is called the beam width.   \n• Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.   \n• Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.   \n• MT is evaluated by measuring a translation’s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used. \nBibliographical and Historical Notes \nMT was proposed seriously by the late 1940s, soon after the birth of the computer (Weaver, 1949/1955). In 1954, the first public demonstration of an MT system prototype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The next decade saw a great flowering of ideas, prefiguring most subsequent developments. But this work was ahead of its time—implementations were limited by, for example, the fact that pending the development of disks there was no good way to store dictionary information. \nAs high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus on the need for better evaluation and more basic research in the new fields of formal and computational linguistics. This consensus culminated in the famously critical ALPAC (Automatic Language Processing Advisory Committee) report of 1966 (Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT in the US. As MT research lost academic respectability, the Association for Machine Translation and Computational Linguistics dropped MT from its name. Some MT developers, however, persevered, and there were early MT systems like Me´te´o, which translated weather forecasts from English to French (Chandioux, 1976), and industrial systems like Systran. \n\nIn the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 10.20. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. \nStatistical methods began to be applied around 1990, enabled first by the development of large bilingual corpora like the Hansard corpus of the proceedings of the Canadian Parliament, which are kept in both French and English, and then by the growth of the Web. Early on, a number of researchers showed that it was possible to extract pairs of aligned sentences from bilingual corpora, using words or simple cues like sentence length (Kay and Ro¨scheisen 1988, Gale and Church 1991, Gale and Church 1993, Kay and Ro¨scheisen 1993). \nAt the same time, the IBM group, drawing directly on the noisy channel model for speech recognition, proposed two related paradigms for statistical MT. These include the generative algorithms that became known as IBM Models 1 through 5, implemented in the Candide system. The algorithms (except for the decoder) were published in full detail— encouraged by the US government who had par",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 10.20. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. \nStatistical methods began to be applied around 1990, enabled first by the development of large bilingual corpora like the Hansard corpus of the proceedings of the Canadian Parliament, which are kept in both French and English, and then by the growth of the Web. Early on, a number of researchers showed that it was possible to extract pairs of aligned sentences from bilingual corpora, using words or simple cues like sentence length (Kay and Ro¨scheisen 1988, Gale and Church 1991, Gale and Church 1993, Kay and Ro¨scheisen 1993). \nAt the same time, the IBM group, drawing directly on the noisy channel model for speech recognition, proposed two related paradigms for statistical MT. These include the generative algorithms that became known as IBM Models 1 through 5, implemented in the Candide system. The algorithms (except for the decoder) were published in full detail— encouraged by the US government who had par\nphrase-based translation \ntially funded the work— which gave them a huge impact on the research community (Brown et al. 1990, Brown et al. 1993). The group also developed a discriminative approach, called MaxEnt (for maximum entropy, an alternative formulation of logistic regression), which allowed many features to be combined discriminatively rather than generatively (Berger et al., 1996), which was further developed by Och and Ney (2002). \nBy the turn of the century, most academic research on machine translation used statistical MT. An extended approach, called phrase-based translation was developed, based on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia). Once automatic metrics like BLEU were developed (Papineni et al., 2002), use log linear formulation (Och and Ney, 2004) to directly optimize evaluation metrics like BLEU in a method known as Minimum Error Rate Training, or MERT (Och, 2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) were widely used. \nMERT \nMoses \ntransduction grammars \ninversion transduction grammar \nThere were also approaches around the turn of the century that were based on syntactic structure (Chapter 12). Models based on transduction grammars (also called synchronous grammars assign a parallel syntactic tree structure to a pair of sentences in different languages, with the goal of translating the sentences by applying reordering operations on the trees. From a generative perspective, we can view a transduction grammar as generating pairs of aligned sentences in two languages. Some of the most widely used models included the inversion transduction grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005), \nNeural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al. (2014) (who coined the name “encoder-decoder”) and Sutskever et al. (2014) then showed how to use extended RNNs for both encoder and decoder. The idea that a generative decoder should take as input a soft weighting of the inputs, the central idea of attention, was first developed by Graves (2013) in the context of handwriting recognition. Bahdanau et al. (2015) extended the idea, named it “attention” and applied it to MT. The transformer encoder-decoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). \nBeam-search has an interesting relationship with human language processing; (Meister et al., 2020) show that beam search enforces the cognitive property of uniform information density in text. Uniform information density is the hypothesis that human language processors tend to prefer to distribute information equally across the sentence (Jaeger and Levy, 2007). \nResearch on evaluation of machine translation began quite early. Miller and Beebe-Center (1956) proposed a number of methods drawing on work in psycholinguistics. These included the use of cloze and Shannon tasks to measure intelligibility as well as a metric of edit distance from a human translation, the intuition that underlies all modern overlap-based automatic evaluation metrics. The ALPAC report included an early evaluation study conducted by John Carroll that was extremely influential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for fidelity and intelligibility, and had raters score them subjectively on 9-point scales. Much early evaluation work focuses on automatic word-overlap metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate) (Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR (Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (Popovic´, 2015) came later. More recent evaluation work, echoing the ALPAC report, has emphasized the importance of careful statistical methodology and the use of human evaluation (Kocmi et al., 2021; Marie et al., 2021). \n\nThe early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al. (2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions to linguistic typology. \nExercises \n10.1 Compute by hand the chrF2,2 score for HYP2 on page 234 (the answer should round to .62). \n11 tTrainsefderLaLnegauranginegMwoidtehl ParnedContextual Embeddings \n“How much do we know at any time? Much more, or so I believe, than we know we know.” \nAgatha Christie, The Moving Finger \nFluent speakers bring an enormous amount of knowledge to bear during comprehension and production of language. This knowledge is embodied in many forms, perhaps most obviously in the vocabulary. That is, in the rich representations associated with the words we know, including their grammatical function, meaning, real-world reference, and pragmatic function. This makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines. \nEstimates of the size of adult vocabularies vary widely both within and across languages. For example, estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used to make the estimate and the definition of what it means to know a word. What is agreed upon is that the vast majority of words that mature speakers use in their dayto-day interactions are acquired early in life through spoken interactions in context with care givers and peers, usually well before the start of formal schooling. This active vocabulary is extremely limited compared to the size of the adult vocabulary (usually on the order of 2000 words for young speakers) and is quite stable, with very few additional words learned via casual conversation beyond this early stage. Obviously, this leaves a very large number of words to be acquired by some other means. \nA simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age. And indeed empirical estimates of vocabulary growth in late elementary through high school are consistent with this rate. How do children achieve this rate of vocabulary growth given their daily experiences during this period? We know that most of this growth is not happening through direct vocabulary instruction in school since these methods are largely ineffective, and are not deployed at a rate that would result in the reliable acquisition of words at the required rate. \nThe most likely remaining explanation is that the bulk of this knowledge acquisition happens as a by-product of reading. Research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate. But the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner! \nMany of these facts have motivated approaches to word learning based on the distributional hypothesis, introduced in Chapter 6. This is the idea that something about what we’re loosely calling word meanings can be learned even without any grounding in the real world, solely based on the content of the texts we’ve encountered over our lives. This knowledge is based on the complex association of words with the words they co-occur with (and with the words that those words occur with).",
        "chapter": "Machine Translation and Encoder-Decoder Models",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The crucial insight of the distributional hypothesis is that the knowledge that we acquire through this process can be brought to bear during language processing long after its initial acquisition in novel contexts. We saw in Chapter 6 that embeddings (static word representations) can be learned from text and then employed for other purposes like measuring word similarity or studying meaning change over time. \nIn this chapter, we expand on this idea in two large ways. First, we’ll introduce the idea of contextual embeddings: representations for words in context. The methods of Chapter 6 like word2vec or GloVe learned a single vector embedding for each unique word $w$ in the vocabulary. By contrast, with contextual embeddings, such as those learned by popular methods like BERT (Devlin et al., 2019) or GPT (Radford et al., 2019) or their descendants, each word $w$ will be represented by a different vector each time it appears in a different context. \npretraining fine-tuning \nSecond, we’ll introduce in this chapter the idea of pretraining and fine-tuning. We call pretraining the process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text. We’ll call these pretrained models pretrained language models, since they can take the form of the transformer language models we introduced in Chapter 9. We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some downstream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn (‘be fine-tuned to’) the requirements of a downstream language understanding task. \ntransfer learning \nThe pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. Of course, adding grounding from vision or from real-world interaction into pretrained models can help build even more powerful models, but even text alone is remarkably useful, and we will limit our attention here to purely textual models. \nThere are two common paradigms for pretrained language models. One is the causal or left-to-right transformer model we introduced in Chapter 9. In this chapter we’ll introduce a second paradigm, called the bidirectional transformer encoder, and the method of masked language modeling, introduced with the BERT model (Devlin et al., 2019) that allows the model to see entire texts at a time, including both the right and left context. \nFinally, we’ll show how the contextual embeddings from these pretrained language models can be used to transfer the knowledge embodied in these models to novel applications via fine-tuning. Indeed, in later chapters we’ll see pretrained language models fine-tuned to tasks from parsing to question answering, from information extraction to semantic parsing. \n11.1 Bidirectional Transformer Encoders \nLet’s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). In Chapter 9 we explored causal (left-to-right) transformers that can serve as the basis for powerful language models—models that can easily be applied to autoregressive generation problems such as contextual generation, summarization and machine translation. However, when applied to sequence classification and labeling problems causal models have obvious shortcomings since they are based on an incremental, left-to-right processing of their inputs. If we want to assign the correct named-entity tag to each word in a sentence, or other sophisticated linguistic labels like the parse tags we’ll introduce in later chapters, we’ll want to be able to take into account information from the right context as we process each element. Fig. 11.1, reproduced here from Chapter 9, illustrates the information flow in the purely left-to-right approach of Chapter 9. As can be seen, the hidden state computation at each point in time is based solely on the current and earlier elements of the input, ignoring potentially useful information located to the right of each tagging decision. \nBidirectional encoders overcome this limitation by allowing the self-attention mechanism to range over the entire input, as shown in Fig. 11.2. The focus of bidirectional encoders is on computing contextualized representations of the tokens in an input sequence that are generally useful across a range of downstream applications. Therefore, bidirectional encoders use self-attention to map sequences of input embeddings $( pmb { mathrm { x } } _ { 1 } , . . . , pmb { mathrm { x } } _ { n } )$ to sequences of output embeddings the same length $( pmb { mathsf { y } } _ { 1 } , . . . , pmb { mathsf { y } } _ { n } )$ , where the output vectors have been contextualized using information from the entire input sequence. \n\nThis contextualization is accomplished through the use of the same self-attention mechanism used in causal models. As with these models, the first step is to generate a set of key, query and value embeddings for each element of the input vector $pmb { x }$ through the use of learned weight matrices $boldsymbol { mathsf { W } ^ { mathsf { Q } } } , boldsymbol { mathsf { W } ^ { mathsf { K } } }$ , and $boldsymbol { mathsf { w } } ^ { boldsymbol { mathsf { v } } }$ . These weights project each input vector $mathbf { x } _ { i }$ into its specific role as a key, query, or value. \nThe output vector $pmb { y } _ { i }$ corresponding to each input element $mathbf { x } _ { i }$ is a weighted sum of all the input value vectors $pmb {  v }$ , as follows: \nThe $alpha$ weights are computed via a softmax over the comparison scores between every element of an input sequence considered as a query and every other element as a key, where the comparison scores are computed using dot products. \nSince each output vector, $pmb { y } _ { i }$ , is computed independently, the processing of an entire sequence can be parallelized via matrix operations. The first step is to pack the input embeddings $pmb { x } _ { i }$ into a matrix $pmb { chi } in mathbb { R } ^ { N times d _ { h } }$ . That is, each row of $pmb { x }$ is the embedding of one token of the input. We then multiply $pmb { x }$ by the key, query, and value weight matrices (all of dimensionality $d times d )$ to produce matrices $mathbf { Q } in mathbb { R } ^ { N times d }$ , $pmb { kappa } in mathbb { R } ^ { N times d }$ , and $pmb { mathsf { v } } in mathbb { R } ^ { N times d }$ , containing all the key, query, and value vectors in a single step. \nGiven these matrices we can compute all the requisite query-key comparisons simultaneously by multiplying $mathbf { Q }$ and $mathsf { pmb { K } ^ { top } }$ in a single operation. Fig. 11.3 illustrates the result of this operation for an input with length 5. \nFinally, we can scale these scores, take the softmax, and then multiply the result by $pmb { v }$ resulting in a matrix of shape $N times d$ where each row contains a contextualized output embedding corresponding to each token in the input. \nAs shown in Fig. 11.3, the full set of self-attention scores represented by $mathsf { mathbf { Q } } mathsf {  l } mathsf { K } ^ { T }$ constitute an all-pairs comparison between the keys and queries for each element of the input. In the case of causal language models in Chapter 9, we masked the upper triangular portion of this matrix (in Fig. 9.17) to eliminate information about future words since this would make the language modeling training task trivial. With bidirectional encoders we simply skip the mask, allowing the model to contextualize each token using information from the entire input. \n\nBeyond this simple change, all of the other elements of the transformer architecture remain the same for bidirectional encoder models. Inputs to the model are segmented using subword tokenization and are combined with positional embeddings before being passed through a series of standard transformer blocks consisting of self-attention and feedforward layers augmented with residual connections and layer normalization, as shown in Fig. 11.4. \nTo make this more concrete, the original bidirectional transformer encoder model, BERT (Devlin et al., 2019), consisted of the following: \n• A subword vocabulary consisting of 30,000 tokens generated using the WordPiece algorithm (Schuster and Nakajima, 2012),   \n• Hidden layers of size of 768,   \n• 12 layers of transformer blocks, with 12 multihead attention layers each. \nThe result is a model with over 100M parameters. The use of WordPiece (one of the large family of subword tokenization algorithms that includes the BPE algorithm we saw in Chapter 2) means that BERT and its descendants are based on subword tokens rather than words. Every input sentence first has to be tokenized, and then all further processing takes place on subword tokens rather than words. This will require, as we’ll see, that for some NLP tasks that require notions of words (like named entity tagging, or parsing) we will occasionally need to map subwords back to words. \n\nFinally, a fundamental issue with transformers is that the size of the input layer dictates the complexity of model. Both the time and memory requirements in a transformer grow quadratically with the length of the input. It’s necessary, therefore, to set a fixed input length that is long enough to provide sufficient context for the model to function and yet still be computationally tractable. For BERT, a fixed input size of 512 subword tokens was used. \n11.2 Training Bidirectional Encoders \ncloze task \nWe trained causal transformer language models in Chapter 9 by making them iteratively predict the next word in a text. But eliminating the causal mask makes the guess-the-next-word language modeling task trivial since the answer is now directly available from the context, so we’re in need of a new training scheme. Fortunately, the traditional learning objective suggests an approach that can be used to train bidirectional encoders. Instead of trying to predict the next word, the model learns to perform a fill-in-the-blank task, technically called the cloze task (Taylor, 1953). To see this, let’s return to the motivating example from Chapter 3. Instead of predicting which words are likely to come next in this example: \nPlease turn your homework we’re asked to predict a missing item given the rest of the sentence. \n\nPlease turn homework in. \nThat is, given an input sequence with one or more elements missing, the learning task is to predict the missing elements. More precisely, during training the model is deprived of one or more elements of an input sequence and must generate a probability distribution over the vocabulary for each of the missing items. We then use the cross-entropy loss from each of the model’s predictions to drive the learning process. \nThis approach can be generalized to any of a variety of methods that corrupt the training input and then asks the model to recover the original input. Examples of the kinds of manipulations that have been used include masks, substitutions, reorderings, deletions, and extraneous insertions into the training text. \n11.2.1 Masking Words \nThe original approach to training bidirectional encoders is called Masked Language Modeling (MLM) (Devlin et al., 2019). As with the language model training methods we’ve already seen, MLM uses unannotated text from a large corpus. Here, the model is presented with a series of sentences from the training corpus where a random sample of tokens from each training sequence is selected for use in the learning task. Once chosen, a token is used in one of three ways: \n• It is replaced with the unique vocabulary token [MASK].   \n• It is replaced with another token from the vocabulary, randomly sampled based on token unigram probabilities.   \n• It is left unchanged.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Bidirectional Transformer Encoders",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Finally, a fundamental issue with transformers is that the size of the input layer dictates the complexity of model. Both the time and memory requirements in a transformer grow quadratically with the length of the input. It’s necessary, therefore, to set a fixed input length that is long enough to provide sufficient context for the model to function and yet still be computationally tractable. For BERT, a fixed input size of 512 subword tokens was used. \n11.2 Training Bidirectional Encoders \ncloze task \nWe trained causal transformer language models in Chapter 9 by making them iteratively predict the next word in a text. But eliminating the causal mask makes the guess-the-next-word language modeling task trivial since the answer is now directly available from the context, so we’re in need of a new training scheme. Fortunately, the traditional learning objective suggests an approach that can be used to train bidirectional encoders. Instead of trying to predict the next word, the model learns to perform a fill-in-the-blank task, technically called the cloze task (Taylor, 1953). To see this, let’s return to the motivating example from Chapter 3. Instead of predicting which words are likely to come next in this example: \nPlease turn your homework we’re asked to predict a missing item given the rest of the sentence. \n\nPlease turn homework in. \nThat is, given an input sequence with one or more elements missing, the learning task is to predict the missing elements. More precisely, during training the model is deprived of one or more elements of an input sequence and must generate a probability distribution over the vocabulary for each of the missing items. We then use the cross-entropy loss from each of the model’s predictions to drive the learning process. \nThis approach can be generalized to any of a variety of methods that corrupt the training input and then asks the model to recover the original input. Examples of the kinds of manipulations that have been used include masks, substitutions, reorderings, deletions, and extraneous insertions into the training text. \n11.2.1 Masking Words \nThe original approach to training bidirectional encoders is called Masked Language Modeling (MLM) (Devlin et al., 2019). As with the language model training methods we’ve already seen, MLM uses unannotated text from a large corpus. Here, the model is presented with a series of sentences from the training corpus where a random sample of tokens from each training sequence is selected for use in the learning task. Once chosen, a token is used in one of three ways: \n• It is replaced with the unique vocabulary token [MASK].   \n• It is replaced with another token from the vocabulary, randomly sampled based on token unigram probabilities.   \n• It is left unchanged. \nIn BERT, $1 5 %$ of the input tokens in a training sequence are sampled for learning. Of these, $80 %$ are replaced with [MASK], $10 %$ are replaced with randomly selected tokens, and the remaining $10 %$ are left unchanged. \nThe MLM training objective is to predict the original inputs for each of the masked tokens using a bidirectional encoder of the kind described in the last section. The cross-entropy loss from these predictions drives the training process for all the parameters in the model. Note that all of the input tokens play a role in the selfattention process, but only the sampled tokens are used for learning. \nMore specifically, the original input sequence is first tokenized using a subword model. The sampled items which drive the learning process are chosen from among the set of tokenized inputs. Word embeddings for all of the tokens in the input are retrieved from the word embedding matrix and then combined with positional embeddings to form the input to the transformer. \nFig. 11.5 illustrates this approach with a simple example. Here, long, thanks and the have been sampled from the training sequence, with the first two masked and the replaced with the randomly sampled token apricot. The resulting embeddings are passed through a stack of bidirectional transformer blocks. To produce a probability distribution over the vocabulary for each of the masked tokens, the output vector from the final transformer layer for each of the masked tokens is multiplied by a learned set of classification weights ${ pmb w } _ { V } in mathbb { R } ^ { | V | times d _ { h } }$ and then through a softmax to yield the required predictions over the vocabulary. \nWith a predicted probability distribution for each masked item, we can use crossentropy to compute the loss for each masked item—the negative log probability assigned to the actual masked word, as shown in Fig. 11.5. The gradients that form \nthe basis for the weight updates are based on the average loss over the sampled learning items from a single training sequence (or batch of sequences). \n11.2.2 Masking Spans \nFor many NLP applications, the natural unit of interest may be larger than a single word (or token). Question answering, syntactic parsing, coreference and semantic role labeling applications all involve the identification and classification of constituents, or phrases. This suggests that a span-oriented masked learning objective might provide improved performance on such tasks. \nA span is a contiguous sequence of one or more words selected from a training text, prior to subword tokenization. In span-based masking, a set of randomly selected spans from a training sequence are chosen. In the SpanBERT work that originated this technique (Joshi et al., 2020), a span length is first chosen by sampling from a geometric distribution that is biased towards shorter spans an with upper bound of 10. Given this span length, a starting location consistent with the desired span length and the length of the input is sampled uniformly. \nOnce a span is chosen for masking, all the words within the span are substituted according to the same regime used in BERT: $80 %$ of the time the span elements are substituted with the [MASK] token, $10 %$ of the time they are replaced by randomly sampled words from the vocabulary, and $10 %$ of the time they are left as is. Note that this substitution process is done at the span level—all the tokens in a given span are substituted using the same method. As with BERT, the total token substitution is limited to $1 5 %$ of the training sequence input. Having selected and masked the training span, the input is passed through the standard transformer architecture to generate contextualized representations of the input tokens. \nDownstream span-based applications rely on span representations derived from the tokens within the span, as well as the start and end points, or the boundaries, of a span. Representations for these boundaries are typically derived from the first and last words of a span, the words immediately preceding and following the span, or some combination of them. The SpanBERT learning objective augments the MLM objective with a boundary oriented component called the Span Boundary Objective (SBO). The SBO relies on a model’s ability to predict the words within a masked span from the words immediately preceding and following it. This prediction is made using the output vectors associated with the words that immediately precede and follow the span being masked, along with positional embedding that signals which word in the span is being predicted: \nwhere $s$ denotes the position of the word before the span and $e$ denotes the word after the end. The prediction for a given position $i$ within the span is produced by concatenating the output embeddings for words $s$ and $e$ span boundary vectors with a positional embedding for position $i$ and passing the result through a 2-layer feedforward network. \nThe final loss is the sum of the BERT MLM loss and the SBO loss.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Training Bidirectional Encoders",
        "subsection": "Masking Words",
        "subsubsection": "N/A"
    },
    {
        "content": "the basis for the weight updates are based on the average loss over the sampled learning items from a single training sequence (or batch of sequences). \n11.2.2 Masking Spans \nFor many NLP applications, the natural unit of interest may be larger than a single word (or token). Question answering, syntactic parsing, coreference and semantic role labeling applications all involve the identification and classification of constituents, or phrases. This suggests that a span-oriented masked learning objective might provide improved performance on such tasks. \nA span is a contiguous sequence of one or more words selected from a training text, prior to subword tokenization. In span-based masking, a set of randomly selected spans from a training sequence are chosen. In the SpanBERT work that originated this technique (Joshi et al., 2020), a span length is first chosen by sampling from a geometric distribution that is biased towards shorter spans an with upper bound of 10. Given this span length, a starting location consistent with the desired span length and the length of the input is sampled uniformly. \nOnce a span is chosen for masking, all the words within the span are substituted according to the same regime used in BERT: $80 %$ of the time the span elements are substituted with the [MASK] token, $10 %$ of the time they are replaced by randomly sampled words from the vocabulary, and $10 %$ of the time they are left as is. Note that this substitution process is done at the span level—all the tokens in a given span are substituted using the same method. As with BERT, the total token substitution is limited to $1 5 %$ of the training sequence input. Having selected and masked the training span, the input is passed through the standard transformer architecture to generate contextualized representations of the input tokens. \nDownstream span-based applications rely on span representations derived from the tokens within the span, as well as the start and end points, or the boundaries, of a span. Representations for these boundaries are typically derived from the first and last words of a span, the words immediately preceding and following the span, or some combination of them. The SpanBERT learning objective augments the MLM objective with a boundary oriented component called the Span Boundary Objective (SBO). The SBO relies on a model’s ability to predict the words within a masked span from the words immediately preceding and following it. This prediction is made using the output vectors associated with the words that immediately precede and follow the span being masked, along with positional embedding that signals which word in the span is being predicted: \nwhere $s$ denotes the position of the word before the span and $e$ denotes the word after the end. The prediction for a given position $i$ within the span is produced by concatenating the output embeddings for words $s$ and $e$ span boundary vectors with a positional embedding for position $i$ and passing the result through a 2-layer feedforward network. \nThe final loss is the sum of the BERT MLM loss and the SBO loss. \nNext Sentence Prediction \n11.2.3 Next Sentence Prediction \nThe focus of masked-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These includes tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). \nTo capture the kind of knowledge required for applications such as these, BERT introduced a second learning objective called Next Sentence Prediction (NSP). In this task, the model is presented with pairs of sentences and is asked to predict whether each pair consists of an actual pair of adjacent sentences from the training corpus or a pair of unrelated sentences. In BERT, $5 0 %$ of the training pairs consisted of positive pairs, and in the other $50 %$ the second sentence of a pair was randomly selected from elsewhere in the corpus. The NSP loss is based on how well the model can distinguish true pairs from random pairs. \nTo facilitate NSP training, BERT introduces two new tokens to the input representation (tokens that will prove useful for fine-tuning as well). After tokenizing the input with the subword model, the token [CLS] is prepended to the input sentence pair, and the token [SEP] is placed between the sentences and after the final token of the second sentence. Finally, embeddings representing the first and second segments of the input are added to the word and positional embeddings to allow the model to more easily distinguish the input sentences.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Training Bidirectional Encoders",
        "subsection": "Masking Spans",
        "subsubsection": "N/A"
    },
    {
        "content": "Next Sentence Prediction \n11.2.3 Next Sentence Prediction \nThe focus of masked-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These includes tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). \nTo capture the kind of knowledge required for applications such as these, BERT introduced a second learning objective called Next Sentence Prediction (NSP). In this task, the model is presented with pairs of sentences and is asked to predict whether each pair consists of an actual pair of adjacent sentences from the training corpus or a pair of unrelated sentences. In BERT, $5 0 %$ of the training pairs consisted of positive pairs, and in the other $50 %$ the second sentence of a pair was randomly selected from elsewhere in the corpus. The NSP loss is based on how well the model can distinguish true pairs from random pairs. \nTo facilitate NSP training, BERT introduces two new tokens to the input representation (tokens that will prove useful for fine-tuning as well). After tokenizing the input with the subword model, the token [CLS] is prepended to the input sentence pair, and the token [SEP] is placed between the sentences and after the final token of the second sentence. Finally, embeddings representing the first and second segments of the input are added to the word and positional embeddings to allow the model to more easily distinguish the input sentences. \n\nDuring training, the output vector from the final layer associated with the [CLS] token represents the next sentence prediction. As with the MLM objective, a learned set of classification weights $boldsymbol { mathsf { W } } _ { mathsf { N S P } } mathbf { bar { in } } mathbb { R } ^ { 2 times d _ { h } }$ is used to produce a two-class prediction from the raw [CLS] vector. \nCross entropy is used to compute the NSP loss for each sentence pair presented to the model. Fig. 11.7 illustrates the overall NSP training setup. In BERT, the NSP loss was used in conjunction with the MLM training objective to form final loss. \n11.2.4 Training Regimes \nThe corpus used in training BERT and other early transformer-based language models consisted of an 800 million word corpus of book texts called BooksCorpus (Zhu et al., 2015) and a 2.5 Billion word corpus derived from the English Wikipedia, for a combined size of 3.3 Billion words. The BooksCorpus is no longer used (for intellectual property reasons), and in general, as we’ll discuss later, state-of-the-art models employ corpora that are orders of magnitude larger than these early efforts. \nTo train the original BERT models, pairs of sentences were selected from the training corpus according to the next sentence prediction 50/50 scheme. Pairs were sampled so that their combined length was less than the 512 token input. Tokens within these sentence pairs were then masked using the MLM approach with the combined loss from the MLM and NSP objectives used for a final loss. Approximately 40 passes (epochs) over the training data was required for the model to converge. \nThe result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Training Bidirectional Encoders",
        "subsection": "Next Sentence Prediction",
        "subsubsection": "N/A"
    },
    {
        "content": "During training, the output vector from the final layer associated with the [CLS] token represents the next sentence prediction. As with the MLM objective, a learned set of classification weights $boldsymbol { mathsf { W } } _ { mathsf { N S P } } mathbf { bar { in } } mathbb { R } ^ { 2 times d _ { h } }$ is used to produce a two-class prediction from the raw [CLS] vector. \nCross entropy is used to compute the NSP loss for each sentence pair presented to the model. Fig. 11.7 illustrates the overall NSP training setup. In BERT, the NSP loss was used in conjunction with the MLM training objective to form final loss. \n11.2.4 Training Regimes \nThe corpus used in training BERT and other early transformer-based language models consisted of an 800 million word corpus of book texts called BooksCorpus (Zhu et al., 2015) and a 2.5 Billion word corpus derived from the English Wikipedia, for a combined size of 3.3 Billion words. The BooksCorpus is no longer used (for intellectual property reasons), and in general, as we’ll discuss later, state-of-the-art models employ corpora that are orders of magnitude larger than these early efforts. \nTo train the original BERT models, pairs of sentences were selected from the training corpus according to the next sentence prediction 50/50 scheme. Pairs were sampled so that their combined length was less than the 512 token input. Tokens within these sentence pairs were then masked using the MLM approach with the combined loss from the MLM and NSP objectives used for a final loss. Approximately 40 passes (epochs) over the training data was required for the model to converge. \nThe result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs. \ncontextual embeddings \n11.2.5 Contextual Embeddings \nGiven a pretrained language model and a novel input sentence, we can think of the output of the model as constituting contextual embeddings for each token in the input. These contextual embeddings can be used as a contextual representation of the meaning of the input token for any task requiring the meaning of word. \nContextual embeddings are thus vectors representing some aspect of the meaning of a token in context. For example, given a sequence of input tokens $x _ { 1 } , . . . , x _ { n }$ , we can use the output vector $y _ { i }$ from the final layer of the model as a representation of the meaning of token $x _ { i }$ in the context of sentence $x _ { 1 } , . . . , x _ { n }$ . Or instead of just using the vector $y _ { i }$ from the final layer of the model, it’s common to compute a representation for $x _ { i }$ by averaging the output tokens $y _ { i }$ from each of the last four layers of the model. \nJust as we used static embeddings like word2vec to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word tokens: instances of a particular word type in a particular context. Contextual embeddings can thus by used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning. \nIn the next section, however, we’ll see the most common use of these representations: as embeddings of word or even entire sentences that are the inputs to classifiers in the fine-tuning process for downstream NLP applications. \n11.3 Transfer Learning through Fine-Tuning \nfine-tuning \nThe power of pretrained language models lies in their ability to extract generalizations from large amounts of text—generalizations that are useful for myriad downstream applications. To make practical use of these generalizations, we need to create interfaces from these models to downstream applications through a process called fine-tuning. Fine-tuning facilitates the creation of applications on top of pretrained models through the addition of a small set of application-specific parameters. The fine-tuning process consists of using labeled data from the application to train these additional application-specific parameters. Typically, this training will either freeze or make only minimal adjustments to the pretrained language model parameters. \nThe following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations. \n11.3.1 Sequence Classification \nsentence embedding \nSequence classification applications often represent an input sequence with a single consolidated representation. With RNNs, we used the hidden layer associated with the final input element to stand for the entire sequence. A similar approach is used with transformers. An additional vector is added to the model to stand for the entire sequence. This vector is sometimes called the sentence embedding since it refers to the entire sequence, although the term ‘sentence embedding’ is also used in other ways. In BERT, the [CLS] token plays the role of this embedding. This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Training Bidirectional Encoders",
        "subsection": "Training Regimes",
        "subsubsection": "N/A"
    },
    {
        "content": "contextual embeddings \n11.2.5 Contextual Embeddings \nGiven a pretrained language model and a novel input sentence, we can think of the output of the model as constituting contextual embeddings for each token in the input. These contextual embeddings can be used as a contextual representation of the meaning of the input token for any task requiring the meaning of word. \nContextual embeddings are thus vectors representing some aspect of the meaning of a token in context. For example, given a sequence of input tokens $x _ { 1 } , . . . , x _ { n }$ , we can use the output vector $y _ { i }$ from the final layer of the model as a representation of the meaning of token $x _ { i }$ in the context of sentence $x _ { 1 } , . . . , x _ { n }$ . Or instead of just using the vector $y _ { i }$ from the final layer of the model, it’s common to compute a representation for $x _ { i }$ by averaging the output tokens $y _ { i }$ from each of the last four layers of the model. \nJust as we used static embeddings like word2vec to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word tokens: instances of a particular word type in a particular context. Contextual embeddings can thus by used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning. \nIn the next section, however, we’ll see the most common use of these representations: as embeddings of word or even entire sentences that are the inputs to classifiers in the fine-tuning process for downstream NLP applications. \n11.3 Transfer Learning through Fine-Tuning \nfine-tuning \nThe power of pretrained language models lies in their ability to extract generalizations from large amounts of text—generalizations that are useful for myriad downstream applications. To make practical use of these generalizations, we need to create interfaces from these models to downstream applications through a process called fine-tuning. Fine-tuning facilitates the creation of applications on top of pretrained models through the addition of a small set of application-specific parameters. The fine-tuning process consists of using labeled data from the application to train these additional application-specific parameters. Typically, this training will either freeze or make only minimal adjustments to the pretrained language model parameters. \nThe following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations. \n11.3.1 Sequence Classification \nsentence embedding \nSequence classification applications often represent an input sequence with a single consolidated representation. With RNNs, we used the hidden layer associated with the final input element to stand for the entire sequence. A similar approach is used with transformers. An additional vector is added to the model to stand for the entire sequence. This vector is sometimes called the sentence embedding since it refers to the entire sequence, although the term ‘sentence embedding’ is also used in other ways. In BERT, the [CLS] token plays the role of this embedding. This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Training Bidirectional Encoders",
        "subsection": "Contextual Embeddings",
        "subsubsection": "N/A"
    },
    {
        "content": "contextual embeddings \n11.2.5 Contextual Embeddings \nGiven a pretrained language model and a novel input sentence, we can think of the output of the model as constituting contextual embeddings for each token in the input. These contextual embeddings can be used as a contextual representation of the meaning of the input token for any task requiring the meaning of word. \nContextual embeddings are thus vectors representing some aspect of the meaning of a token in context. For example, given a sequence of input tokens $x _ { 1 } , . . . , x _ { n }$ , we can use the output vector $y _ { i }$ from the final layer of the model as a representation of the meaning of token $x _ { i }$ in the context of sentence $x _ { 1 } , . . . , x _ { n }$ . Or instead of just using the vector $y _ { i }$ from the final layer of the model, it’s common to compute a representation for $x _ { i }$ by averaging the output tokens $y _ { i }$ from each of the last four layers of the model. \nJust as we used static embeddings like word2vec to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word tokens: instances of a particular word type in a particular context. Contextual embeddings can thus by used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning. \nIn the next section, however, we’ll see the most common use of these representations: as embeddings of word or even entire sentences that are the inputs to classifiers in the fine-tuning process for downstream NLP applications. \n11.3 Transfer Learning through Fine-Tuning \nfine-tuning \nThe power of pretrained language models lies in their ability to extract generalizations from large amounts of text—generalizations that are useful for myriad downstream applications. To make practical use of these generalizations, we need to create interfaces from these models to downstream applications through a process called fine-tuning. Fine-tuning facilitates the creation of applications on top of pretrained models through the addition of a small set of application-specific parameters. The fine-tuning process consists of using labeled data from the application to train these additional application-specific parameters. Typically, this training will either freeze or make only minimal adjustments to the pretrained language model parameters. \nThe following sections introduce fine-tuning methods for the most common applications including sequence classification, sequence labeling, sentence-pair inference, and span-based operations. \n11.3.1 Sequence Classification \nsentence embedding \nSequence classification applications often represent an input sequence with a single consolidated representation. With RNNs, we used the hidden layer associated with the final input element to stand for the entire sequence. A similar approach is used with transformers. An additional vector is added to the model to stand for the entire sequence. This vector is sometimes called the sentence embedding since it refers to the entire sequence, although the term ‘sentence embedding’ is also used in other ways. In BERT, the [CLS] token plays the role of this embedding. This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. \n\nAs an example, let’s return to the problem of sentiment classification. A simple approach to fine-tuning a classifier for this application involves learning a set of weights, $boldsymbol { mathsf { w } } _ { mathsf { c } }$ , to map the output vector for the [CLS] token, yCLS to a set of scores over the possible sentiment classes. Assuming a three-way sentiment classification task (positive, negative, neutral) and dimensionality $d _ { h }$ for the size of the language model hidden layers gives $pmb { mathsf { W } } _ { mathsf { C } } in mathbb { R } ^ { 3 times d _ { h } }$ . Classification of unseen documents proceeds by passing the input text through the pretrained language model to generate yCLS, multiplying it by $boldsymbol { mathsf { w } } _ { mathsf { c } }$ , and finally passing the resulting vector through a softmax. \nFinetuning the values in $boldsymbol { mathsf { W } } _ { mathsf { C } }$ requires supervised training data consisting of input sequences labeled with the appropriate class. Training proceeds in the usual way; cross-entropy loss between the softmax output and the correct answer is used to drive the learning that produces $boldsymbol { mathsf { w } } _ { mathsf { c } }$ . \nA key difference from what we’ve seen earlier with neural classifiers is that this loss can be used to not only learn the weights of the classifier, but also to update the weights for the pretrained language model itself. In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer. Fig. 11.8 illustrates this overall approach to sequence classification. \n11.3.2 Pair-Wise Sequence Classification \nAs mentioned in Section 11.2.3, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include logical entailment, paraphrase detection and discourse analysis.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning through Fine-Tuning",
        "subsection": "Sequence Classification",
        "subsubsection": "N/A"
    },
    {
        "content": "As an example, let’s return to the problem of sentiment classification. A simple approach to fine-tuning a classifier for this application involves learning a set of weights, $boldsymbol { mathsf { w } } _ { mathsf { c } }$ , to map the output vector for the [CLS] token, yCLS to a set of scores over the possible sentiment classes. Assuming a three-way sentiment classification task (positive, negative, neutral) and dimensionality $d _ { h }$ for the size of the language model hidden layers gives $pmb { mathsf { W } } _ { mathsf { C } } in mathbb { R } ^ { 3 times d _ { h } }$ . Classification of unseen documents proceeds by passing the input text through the pretrained language model to generate yCLS, multiplying it by $boldsymbol { mathsf { w } } _ { mathsf { c } }$ , and finally passing the resulting vector through a softmax. \nFinetuning the values in $boldsymbol { mathsf { W } } _ { mathsf { C } }$ requires supervised training data consisting of input sequences labeled with the appropriate class. Training proceeds in the usual way; cross-entropy loss between the softmax output and the correct answer is used to drive the learning that produces $boldsymbol { mathsf { w } } _ { mathsf { c } }$ . \nA key difference from what we’ve seen earlier with neural classifiers is that this loss can be used to not only learn the weights of the classifier, but also to update the weights for the pretrained language model itself. In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer. Fig. 11.8 illustrates this overall approach to sequence classification. \n11.3.2 Pair-Wise Sequence Classification \nAs mentioned in Section 11.2.3, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include logical entailment, paraphrase detection and discourse analysis. \nFine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model’s view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights. \nAs an example, let’s consider an entailment classification task with the MultiGenre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In the task of natural language inference or NLI, also called recognizing textual entailment, a model is presented with a pair of sentences and must classify the relationship between their meanings. For example in the MultiNLI corpus, pairs of sentences are given one of 3 labels: entails, contradicts and neutral. These labels describe a relationship between the meaning of the first sentence (the premise) and the meaning of the second sentence (the hypothesis). Here are representative examples of each class from the corpus: \n• Neutral a: Jon walked back to the town to the smithy. b: Jon traveled back to his hometown.   \n• Contradicts a: Tourist Information offices can be very helpful. b: Tourist Information offices are never of any help.   \n• Entails a: I’m confused. b: Not all of it is very clear to me. \nA relationship of contradicts means that the premise contradicts the hypothesis; entails means that the premise entails the hypothesis; neutral means that neither is necessarily true. The meaning of these labels is looser than strict logical entailment or contradiction indicating that a typical human reading the sentences would most likely interpret the meanings in this way. \nTo fine-tune a classifier for the MultiNLI task, we pass the premise/hypothesis pairs through a bidirectional encoder as described above and use the output vector for the [CLS] token as the input to the classification head. As with ordinary sequence classification, this head provides the input to a three-way classifier that can be trained on the MultiNLI training corpus. \n11.3.3 Sequence Labelling \nSequence labelling tasks, such as part-of-speech tagging or BIO-based named entity recognition, follow the same basic classification approach. Here, the final output vector corresponding to each input token is passed to a classifier that produces a softmax distribution over the possible set of tags. Again, assuming a simple classifier consisting of a single feedforward layer followed by a softmax, the set of weights to be learned for this additional layer is $boldsymbol { mathsf { W } } _ { mathsf { K } } in mathbb { R } ^ { dot { boldsymbol { k } } times boldsymbol { d } _ { h } }$ , where $k$ is the number of possible tags for the task. As with RNNs, a greedy approach, where the argmax tag for each token is taken as a likely answer, can be used to generate the final output tag sequence. Fig. 11.9 illustrates an example of this approach.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning through Fine-Tuning",
        "subsection": "Pair-Wise Sequence Classification",
        "subsubsection": "N/A"
    },
    {
        "content": "Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During fine-tuning, pairs of labeled sentences from the supervised training data are presented to the model. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model’s view of the input pair. And as with NSP training, the two inputs are separated by the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights. \nAs an example, let’s consider an entailment classification task with the MultiGenre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In the task of natural language inference or NLI, also called recognizing textual entailment, a model is presented with a pair of sentences and must classify the relationship between their meanings. For example in the MultiNLI corpus, pairs of sentences are given one of 3 labels: entails, contradicts and neutral. These labels describe a relationship between the meaning of the first sentence (the premise) and the meaning of the second sentence (the hypothesis). Here are representative examples of each class from the corpus: \n• Neutral a: Jon walked back to the town to the smithy. b: Jon traveled back to his hometown.   \n• Contradicts a: Tourist Information offices can be very helpful. b: Tourist Information offices are never of any help.   \n• Entails a: I’m confused. b: Not all of it is very clear to me. \nA relationship of contradicts means that the premise contradicts the hypothesis; entails means that the premise entails the hypothesis; neutral means that neither is necessarily true. The meaning of these labels is looser than strict logical entailment or contradiction indicating that a typical human reading the sentences would most likely interpret the meanings in this way. \nTo fine-tune a classifier for the MultiNLI task, we pass the premise/hypothesis pairs through a bidirectional encoder as described above and use the output vector for the [CLS] token as the input to the classification head. As with ordinary sequence classification, this head provides the input to a three-way classifier that can be trained on the MultiNLI training corpus. \n11.3.3 Sequence Labelling \nSequence labelling tasks, such as part-of-speech tagging or BIO-based named entity recognition, follow the same basic classification approach. Here, the final output vector corresponding to each input token is passed to a classifier that produces a softmax distribution over the possible set of tags. Again, assuming a simple classifier consisting of a single feedforward layer followed by a softmax, the set of weights to be learned for this additional layer is $boldsymbol { mathsf { W } } _ { mathsf { K } } in mathbb { R } ^ { dot { boldsymbol { k } } times boldsymbol { d } _ { h } }$ , where $k$ is the number of possible tags for the task. As with RNNs, a greedy approach, where the argmax tag for each token is taken as a likely answer, can be used to generate the final output tag sequence. Fig. 11.9 illustrates an example of this approach. \n\nAlternatively, the distribution over labels provided by the softmax for each input token can be passed to a conditional random field (CRF) layer which can take global tag-level transitions into account. \nA complication with this approach arises from the use of subword tokenization such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities: \n[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] . \nwould have the following set of per-word BIO tags. \n(11.14) Mt. Sanitas is in Sunshine Canyon . B-LOC I-LOC O O B-LOC I-LOC O \nUnfortunately, the WordPiece tokenization for this sentence yields the following sequence of tokens which doesn’t align directly with BIO tags in the ground truth annotation: \n’Mt’, , ’San’, ’##itas’, ’is’, ’in’, ’Sunshine’, ’Canyon’ \nTo deal with this misalignment, we need a way to assign BIO tags to subword tokens during training and a corresponding way to recover word-level tags from subwords during decoding. For training, we can just assign the gold-standard tag associated with each word to all of the subword tokens derived from it. \nFor decoding, the simplest approach is to use the argmax BIO tag associated with the first subword token of a word. Thus, in our example, the BIO tag assigned to “Mt” would be assigned to “Mt.” and the tag assigned to “San” would be assigned to “Sanitas”, effectively ignoring the information in the tags assigned to “.” and “##itas”. More complex approaches combine the distribution of tag probabilities across the subwords in an attempt to find an optimal word-level tag. \n\n11.3.4 Fine-tuning for Span-Based Applications \nSpan-oriented applications operate in a middle ground between sequence level and token level tasks. That is, in span-oriented applications the focus is on generating and operating with representations of contiguous sequences of tokens. Typical operations include identifying spans of interest, classifying spans according to some labeling scheme, and determining relations among discovered spans. Applications include named entity recognition, question answering, syntactic parsing, semantic role labeling and coreference resolution. \nFormally, given an input sequence $x$ consisting of $T$ tokens, $( x _ { 1 } , x _ { 2 } , . . . , x _ { T } )$ , a span is a contiguous sequence of tokens with start $i$ and end $j$ such that $1 < = i < =$ $j < = T$ . This formulation results in a total set of spans equal to $frac { T ( T - 1 ) } { 2 }$ . For practical purposes, span-based models often impose an application-specific length limit $L$ , so the legal spans are limited to those where $j - i < L$ . In the following, we’ll refer to the enumerated set of legal spans in $x$ as $S ( x )$ . \nThe first step in fine-tuning a pretrained language model for a span-based application using the contextualized input embeddings from the model to generate representations for all the spans in the input. Most schemes for representing spans make use of two primary components: representations of the span boundaries and summary representations of the contents of each span. To compute a unified span representation, we concatenate the boundary representations with the summary representation. \nIn the simplest possible approach, we can use the contextual embeddings of the start and end tokens of a span as the boundaries, and the average of the output embeddings within the span as the summary representation. \nA weakness of this approach is that it doesn’t distinguish the use of a word’s embedding as the beginning of a span from its use as the end of one. Therefore, more elaborate schemes for representing the span boundaries involve learned representations for start and end points through the use of two distinct feedforward networks: \nSimilarly, a simple average of the vectors in a span is unlikely to be an optimal representation of a span since it treats all of a span’s embeddings as equally important. For many applications, a more useful representation would be centered around the head of the phrase corresponding to the span. One method for getting at such information in the absence of a syntactic parse is to use a standard self-attention layer to generate a span representation.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning through Fine-Tuning",
        "subsection": "Sequence Labelling",
        "subsubsection": "N/A"
    },
    {
        "content": "11.3.4 Fine-tuning for Span-Based Applications \nSpan-oriented applications operate in a middle ground between sequence level and token level tasks. That is, in span-oriented applications the focus is on generating and operating with representations of contiguous sequences of tokens. Typical operations include identifying spans of interest, classifying spans according to some labeling scheme, and determining relations among discovered spans. Applications include named entity recognition, question answering, syntactic parsing, semantic role labeling and coreference resolution. \nFormally, given an input sequence $x$ consisting of $T$ tokens, $( x _ { 1 } , x _ { 2 } , . . . , x _ { T } )$ , a span is a contiguous sequence of tokens with start $i$ and end $j$ such that $1 < = i < =$ $j < = T$ . This formulation results in a total set of spans equal to $frac { T ( T - 1 ) } { 2 }$ . For practical purposes, span-based models often impose an application-specific length limit $L$ , so the legal spans are limited to those where $j - i < L$ . In the following, we’ll refer to the enumerated set of legal spans in $x$ as $S ( x )$ . \nThe first step in fine-tuning a pretrained language model for a span-based application using the contextualized input embeddings from the model to generate representations for all the spans in the input. Most schemes for representing spans make use of two primary components: representations of the span boundaries and summary representations of the contents of each span. To compute a unified span representation, we concatenate the boundary representations with the summary representation. \nIn the simplest possible approach, we can use the contextual embeddings of the start and end tokens of a span as the boundaries, and the average of the output embeddings within the span as the summary representation. \nA weakness of this approach is that it doesn’t distinguish the use of a word’s embedding as the beginning of a span from its use as the end of one. Therefore, more elaborate schemes for representing the span boundaries involve learned representations for start and end points through the use of two distinct feedforward networks: \nSimilarly, a simple average of the vectors in a span is unlikely to be an optimal representation of a span since it treats all of a span’s embeddings as equally important. For many applications, a more useful representation would be centered around the head of the phrase corresponding to the span. One method for getting at such information in the absence of a syntactic parse is to use a standard self-attention layer to generate a span representation. \nNow, given span representations $pmb { mathsf { g } }$ for each span in $S ( x )$ , classifiers can be finetuned to generate application-specific scores for various span-oriented tasks: binary span identification (is this a legitimate span of interest or not?), span classification (what kind of span is this?), and span relation classification (how are these two spans related?). \nTo ground this discussion, let’s return to named entity recognition (NER). Given a scheme for representing spans and set of named entity types, a span-based approach to NER is a straightforward classification problem where each span in an input is assigned a class label. More formally, given an input sequence $x$ , we want to assign a label $y$ , from the set of valid NER labels, to each of the spans in $S ( x )$ . Since most of the spans in a given input will not be named entities we’ll add the label NULL to the set of types in $Y$ . \nWith this approach, fine-tuning entails using supervised training data to learn the parameters of the final classifier, as well as the weights used to generate the boundary representations, and the weights in the self-attention layer that generates the span content representation. During training, the model’s predictions for all spans are compared to their gold-standard labels and cross-entropy loss is used to drive the training. \nDuring decoding, each span is scored using a softmax over the final classifier output to generate a distribution over the possible labels, with the argmax score for each span taken as the correct answer. Fig. 11.10 illustrates this approach with an example. A variation on this scheme designed to improve precision adds a calibrated threshold to the labeling of a span as anything other than NULL. \nThere are two significant advantages to a span-based approach to NER over a BIO-based per-word labeling approach. The first advantage is that BIO-based approaches are prone to a labeling mis-match problem. That is, every label in a longer named entity must be correct for an output to be judged correct. Returning to the example in Fig. 11.10, the following labeling would be judged entirely wrong due to the incorrect label on the first item. Span-based approaches only have to make one classification for each span. \n(11.22) Jane Villanueva of United Airlines Holding discussed ... B-PER I-PER O I-ORG I-ORG I-ORG O \nThe second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately. \n11.4 Transfer Learning via Prompting \n11.4.1 Text-to-Text Models   \n11.4.2 Prompting   \n11.4.3 Contextual (Few-Shot) Learning \n11.5 Training Corpora \n11.6 Potential Harms from Language Models \nLarge pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020). \nFor example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay. \nIndeed, language models are biased in a number of ways by the distributions of their training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. In addition to problems of toxicity, internet data is disproportionately generated by authors from developed countries, and many large language models train on data from Reddit, whose authors skew male and young. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. \nLanguage models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. \nFinally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2020). This is a problem if large language models are trained on private datasets such has electronic health records (EHRs).",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning through Fine-Tuning",
        "subsection": "Fine-tuning for Span-Based Applications",
        "subsubsection": "N/A"
    },
    {
        "content": "(11.22) Jane Villanueva of United Airlines Holding discussed ... B-PER I-PER O I-ORG I-ORG I-ORG O \nThe second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately. \n11.4 Transfer Learning via Prompting \n11.4.1 Text-to-Text Models   \n11.4.2 Prompting   \n11.4.3 Contextual (Few-Shot) Learning \n11.5 Training Corpora \n11.6 Potential Harms from Language Models \nLarge pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020). \nFor example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay. \nIndeed, language models are biased in a number of ways by the distributions of their training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. In addition to problems of toxicity, internet data is disproportionately generated by authors from developed countries, and many large language models train on data from Reddit, whose authors skew male and young. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. \nLanguage models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. \nFinally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2020). This is a problem if large language models are trained on private datasets such has electronic health records (EHRs).",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning via Prompting",
        "subsection": "Text-to-Text Models",
        "subsubsection": "N/A"
    },
    {
        "content": "(11.22) Jane Villanueva of United Airlines Holding discussed ... B-PER I-PER O I-ORG I-ORG I-ORG O \nThe second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately. \n11.4 Transfer Learning via Prompting \n11.4.1 Text-to-Text Models   \n11.4.2 Prompting   \n11.4.3 Contextual (Few-Shot) Learning \n11.5 Training Corpora \n11.6 Potential Harms from Language Models \nLarge pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020). \nFor example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay. \nIndeed, language models are biased in a number of ways by the distributions of their training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. In addition to problems of toxicity, internet data is disproportionately generated by authors from developed countries, and many large language models train on data from Reddit, whose authors skew male and young. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. \nLanguage models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. \nFinally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2020). This is a problem if large language models are trained on private datasets such has electronic health records (EHRs).",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning via Prompting",
        "subsection": "Prompting",
        "subsubsection": "N/A"
    },
    {
        "content": "(11.22) Jane Villanueva of United Airlines Holding discussed ... B-PER I-PER O I-ORG I-ORG I-ORG O \nThe second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately. \n11.4 Transfer Learning via Prompting \n11.4.1 Text-to-Text Models   \n11.4.2 Prompting   \n11.4.3 Contextual (Few-Shot) Learning \n11.5 Training Corpora \n11.6 Potential Harms from Language Models \nLarge pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020). \nFor example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay. \nIndeed, language models are biased in a number of ways by the distributions of their training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. In addition to problems of toxicity, internet data is disproportionately generated by authors from developed countries, and many large language models train on data from Reddit, whose authors skew male and young. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. \nLanguage models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. \nFinally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2020). This is a problem if large language models are trained on private datasets such has electronic health records (EHRs).",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Transfer Learning via Prompting",
        "subsection": "Contextual (Few-Shot) Learning",
        "subsubsection": "N/A"
    },
    {
        "content": "(11.22) Jane Villanueva of United Airlines Holding discussed ... B-PER I-PER O I-ORG I-ORG I-ORG O \nThe second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately. \n11.4 Transfer Learning via Prompting \n11.4.1 Text-to-Text Models   \n11.4.2 Prompting   \n11.4.3 Contextual (Few-Shot) Learning \n11.5 Training Corpora \n11.6 Potential Harms from Language Models \nLarge pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020). \nFor example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay. \nIndeed, language models are biased in a number of ways by the distributions of their training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. In addition to problems of toxicity, internet data is disproportionately generated by authors from developed countries, and many large language models train on data from Reddit, whose authors skew male and young. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. \nLanguage models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. \nFinally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2020). This is a problem if large language models are trained on private datasets such has electronic health records (EHRs).",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Training Corpora",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "(11.22) Jane Villanueva of United Airlines Holding discussed ... B-PER I-PER O I-ORG I-ORG I-ORG O \nThe second advantage to span-based approaches is that they naturally accommodate embedded named entities. For example, in this example both United Airlines and United Airlines Holding are legitimate named entities. The BIO approach has no way of encoding this embedded structure. But the span-based approach can naturally label both since the spans are labeled separately. \n11.4 Transfer Learning via Prompting \n11.4.1 Text-to-Text Models   \n11.4.2 Prompting   \n11.4.3 Contextual (Few-Shot) Learning \n11.5 Training Corpora \n11.6 Potential Harms from Language Models \nLarge pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are fine-tuned to downstream tasks, particularly those involving text generation, such as in assistive technologies like web search query completion, or predictive typing for email (Olteanu et al., 2020). \nFor example, language models can generate toxic language. Gehman et al. (2020) show that many kinds of completely non-toxic prompts can nonetheless lead large language models to output hate speech and abuse. Brown et al. (2020) and Sheng et al. (2019) showed that large language models generate sentences displaying negative attitudes toward minority identities such as being Black or gay. \nIndeed, language models are biased in a number of ways by the distributions of their training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. In addition to problems of toxicity, internet data is disproportionately generated by authors from developed countries, and many large language models train on data from Reddit, whose authors skew male and young. Such biased population samples likely skew the resulting generation away from the perspectives or topics of underrepresented populations. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. \nLanguage models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. \nFinally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2020). This is a problem if large language models are trained on private datasets such has electronic health records (EHRs). \n\nMitigating all these harms is an important but unsolved research question in NLP. Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model’s tendency to generate toxic language somewhat (Gehman et al., 2020). And analyzing the data used to pretrain large language models is important to understand toxicity and bias in generation, as well as privacy, making it extremely important that language models include datasheets (page 14) or model cards (page 75) giving full replicable information on the corpora used to train them. \n11.7 Summary \nThis chapter has introduced the topic of transfer learning from pretrained language models. Here’s a summary of the main points that we covered: \n• Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context.   \n• Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input.   \n• Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model. \nBibliographical and Historical Notes",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Potential Harms from Language Models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Mitigating all these harms is an important but unsolved research question in NLP. Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model’s tendency to generate toxic language somewhat (Gehman et al., 2020). And analyzing the data used to pretrain large language models is important to understand toxicity and bias in generation, as well as privacy, making it extremely important that language models include datasheets (page 14) or model cards (page 75) giving full replicable information on the corpora used to train them. \n11.7 Summary \nThis chapter has introduced the topic of transfer learning from pretrained language models. Here’s a summary of the main points that we covered: \n• Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context.   \n• Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input.   \n• Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model. \nBibliographical and Historical Notes",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Mitigating all these harms is an important but unsolved research question in NLP. Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model’s tendency to generate toxic language somewhat (Gehman et al., 2020). And analyzing the data used to pretrain large language models is important to understand toxicity and bias in generation, as well as privacy, making it extremely important that language models include datasheets (page 14) or model cards (page 75) giving full replicable information on the corpora used to train them. \n11.7 Summary \nThis chapter has introduced the topic of transfer learning from pretrained language models. Here’s a summary of the main points that we covered: \n• Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context.   \n• Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input.   \n• Pretrained language models can be fine-tuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model. \nBibliographical and Historical Notes \n12 Constituency Grammars \nBecause the Night by Bruce Springsteen and Patty Smith   \nThe Fire Next Time by James Baldwin   \nIf on a winter’s night a traveler by Italo Calvino   \nLove Actually by Richard Curtis   \nSuddenly Last Summer by Tennessee Williams   \nA Scanner Darkly by Philip K. Dick Six titles that are not constituents, from Geoffrey K. Pullum on Language Log (who was pointing out their incredible rarity). \nsyntax \nThe study of grammar has an ancient pedigree. The grammar of Sanskrit was described by the Indian grammarian Pa¯n. ini sometime between the 7th and 4th centuries BCE, in his famous treatise the A.st.a¯dhya¯y¯ı (‘8 books’). And our word syntax comes from the Greek s´yntaxis, meaning “setting out together or arrangement”, and refers to the way words are arranged together. We have seen various syntactic notions in previous chapters: ordering of sequences of words (Chapter 2), probabilities for these word sequences (Chapter 3), and the use of part-of-speech categories as a grammatical equivalence class for words (Chapter 8). In this chapter and the next three we introduce a variety of syntactic phenomena that go well beyond these simpler approaches, together with formal models for capturing them in a computationally useful manner. \nThe bulk of this chapter is devoted to context-free grammars. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). As such, they play a role in many computational applications, including grammar checking, semantic interpretation, dialogue understanding, and machine translation. They are powerful enough to express sophisticated relations among the words in a sentence, yet computationally tractable enough that efficient algorithms exist for parsing sentences with them (as we show in Chapter 13). And in Chapter 16 we show how they provide a systematic framework for semantic interpretation. Here we also introduce the concept of lexicalized grammars, focusing on one example, combinatory categorial grammar, or CCG. \nIn Chapter 14 we introduce a formal model of grammar called syntactic dependencies that is an alternative to these constituency grammars, and we’ll give algorithms for dependency parsing. Both constituency and dependency formalisms are important for language processing. \nFinally, we provide a brief overview of the grammar of English, illustrated from a domain with relatively simple sentences called ATIS (Air Traffic Information System) (Hemphill et al., 1990). ATIS systems were an early spoken language system for users to book flights, by expressing sentences like I’d like to fly to Atlanta.",
        "chapter": "Transfer Learning with Pretrained Language Models and Contextual Embeddings",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "12 Constituency Grammars \nBecause the Night by Bruce Springsteen and Patty Smith   \nThe Fire Next Time by James Baldwin   \nIf on a winter’s night a traveler by Italo Calvino   \nLove Actually by Richard Curtis   \nSuddenly Last Summer by Tennessee Williams   \nA Scanner Darkly by Philip K. Dick Six titles that are not constituents, from Geoffrey K. Pullum on Language Log (who was pointing out their incredible rarity). \nsyntax \nThe study of grammar has an ancient pedigree. The grammar of Sanskrit was described by the Indian grammarian Pa¯n. ini sometime between the 7th and 4th centuries BCE, in his famous treatise the A.st.a¯dhya¯y¯ı (‘8 books’). And our word syntax comes from the Greek s´yntaxis, meaning “setting out together or arrangement”, and refers to the way words are arranged together. We have seen various syntactic notions in previous chapters: ordering of sequences of words (Chapter 2), probabilities for these word sequences (Chapter 3), and the use of part-of-speech categories as a grammatical equivalence class for words (Chapter 8). In this chapter and the next three we introduce a variety of syntactic phenomena that go well beyond these simpler approaches, together with formal models for capturing them in a computationally useful manner. \nThe bulk of this chapter is devoted to context-free grammars. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). As such, they play a role in many computational applications, including grammar checking, semantic interpretation, dialogue understanding, and machine translation. They are powerful enough to express sophisticated relations among the words in a sentence, yet computationally tractable enough that efficient algorithms exist for parsing sentences with them (as we show in Chapter 13). And in Chapter 16 we show how they provide a systematic framework for semantic interpretation. Here we also introduce the concept of lexicalized grammars, focusing on one example, combinatory categorial grammar, or CCG. \nIn Chapter 14 we introduce a formal model of grammar called syntactic dependencies that is an alternative to these constituency grammars, and we’ll give algorithms for dependency parsing. Both constituency and dependency formalisms are important for language processing. \nFinally, we provide a brief overview of the grammar of English, illustrated from a domain with relatively simple sentences called ATIS (Air Traffic Information System) (Hemphill et al., 1990). ATIS systems were an early spoken language system for users to book flights, by expressing sentences like I’d like to fly to Atlanta. \n12.1 Constituency \nnoun phrase \nSyntactic constituency is the idea that groups of words can behave as single units, or constituents. Part of developing a grammar involves building an inventory of the constituents in the language. How do words group together in English? Consider the noun phrase, a sequence of words surrounding at least one noun. Here are some examples of noun phrases (thanks to Damon Runyon): \nWhat evidence do we have that these words group together (or “form constituents”)? One piece of evidence is that they can all appear in similar syntactic environments, for example, before a verb. \nthree parties from Brooklyn arrive. a high-class spot such as Mindy’s attracts. . . the Broadway coppers love. . . they sit \nBut while the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase. The following are not grammatical sentences of English (recall that we use an asterisk $( ^ { * } )$ to mark fragments that are not grammatical English sentences): \npreposed postposed \nThus, to correctly describe facts about the ordering of these words in English, we must be able to say things like “Noun Phrases can occur before verbs”. \nOther kinds of evidence for constituency come from what are called preposed or postposed constructions. For example, the prepositional phrase on September seventeenth can be placed in a number of different locations in the following examples, including at the beginning (preposed) or at the end (postposed): \nOn September seventeenth, I’d like to fly from Atlanta to Denver I’d like to fly on September seventeenth from Atlanta to Denver I’d like to fly from Atlanta to Denver on September seventeenth \nBut again, while the entire phrase can be placed differently, the individual words making up the phrase cannot be: \n*On September, I’d like to fly seventeenth from Atlanta to Denver *On I’d like to fly September seventeenth from Atlanta to Denver *I’d like to fly on September from Atlanta to Denver seventeenth \n12.2 Context-Free Grammars \nThe most widely used formal system for modeling constituent structure in English CFG and other natural languages is the Context-Free Grammar, or CFG. Contextfree grammars are also called Phrase-Structure Grammars, and the formalism is equivalent to Backus-Naur Form, or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt 1900 but was not formalized until Chomsky (1956) and, independently, Backus (1959).",
        "chapter": "Constituency Grammars",
        "section": "Constituency",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "This hard line between “in” and “out” characterizes all formal languages but is only a very simplified model of how natural languages really work. This is because determining whether a given sentence is part of a given natural language (say, English) often depends on the context. In linguistics, the use of formal languages to model natural languages is called generative grammar since the language is defined by the set of possible sentences “generated” by the grammar. \n12.2.1 Formal Definition of Context-Free Grammar \nWe conclude this section with a quick, formal description of a context-free grammar and the language it generates. A context-free grammar $G$ is defined by four parameters: $N , Sigma , R , S$ (technically this is a “4-tuple”). \n$N$ a set of non-terminal symbols (or variables)   \n$Sigma$ a set of terminal symbols (disjoint from $N$ )   \n$R$ a set of rules or productions, each of the form $A to beta$ , where $A$ is a non-terminal, $beta$ is a string of symbols from the infinite set of strings $( Sigma cup N ) ^ { * }$   \nS a designated start symbol and a member of $N$ \nFor the remainder of the book we adhere to the following conventions when discussing the formal properties of context-free grammars (as opposed to explaining particular facts about English or other languages). \nA language is defined through the concept of derivation. One string derives another one if it can be rewritten as the second one by some series of rule applications. More formally, following Hopcroft and Ullman (1979), \ndirectly derives \nif $A to beta$ is a production of $R$ and $alpha$ and $gamma$ are any strings in the set $( Sigma cup N ) ^ { * }$ , then we say that $alpha A gamma$ directly derives $alpha beta gamma .$ , or $alpha A gamma Rightarrow alpha beta gamma .$ \nDerivation is then a generalization of direct derivation: \nLet $alpha _ { 1 } , alpha _ { 2 } , ldots , alpha _ { m }$ be strings in $( Sigma cup N ) ^ { * } , m geq 1$ , such that \nderives \nWe say that $alpha _ { 1 }$ derives $alpha _ { m }$ , or $alpha _ { 1 } stackrel { * } { Rightarrow } alpha _ { m }$ . \nWe can then formally define the language $mathcal { L } _ { G }$ generated by a grammar $G$ as the set of strings composed of terminal symbols that can be derived from the designated start symbol $S$ . \nsyntactic parsing \nThe problem of mapping from a string of words to its parse tree is called syntactic parsing; we define algorithms for constituency parsing in Chapter 13. \n12.3 Some Grammar Rules for English \nIn this section, we introduce a few more aspects of the phrase structure of English; for consistency we will continue to focus on sentences from the ATIS domain. Because of space limitations, our discussion is necessarily limited to highlights. Readers are strongly advised to consult a good reference grammar of English, such as Huddleston and Pullum (2002). \n12.3.1 Sentence-Level Constructions \nIn the small grammar $mathcal { L } _ { 0 }$ , we provided only one sentence-level construction for declarative sentences like I prefer a morning flight. Among the large number of constructions for English sentences, four are particularly common and important: declaratives, imperatives, yes-no questions, and wh-questions. \ndeclarative \nSentences with declarative structure have a subject noun phrase followed by a verb phrase, like “I prefer a morning flight”. Sentences with this structure have a great number of different uses that we follow up on in Chapter 24. Here are a number of examples from the ATIS domain: \nI want a flight from Ontario to Chicago The flight should be eleven a.m. tomorrow The return flight should leave at around seven p.m. \nimperative \nSentences with imperative structure often begin with a verb phrase and have no subject. They are called imperative because they are almost always used for commands and suggestions; in the ATIS domain they are commands to the system. \nShow the lowest fare   \nGive me Sunday’s flights arriving in Las Vegas from New York City List all flights between five and seven p.m. \nWe can model this sentence structure with another rule for the expansion of $S$ : \nyes-no question \nSentences with yes-no question structure are often (though not always) used to ask questions; they begin with an auxiliary verb, followed by a subject $N P$ , followed by a $V P$ . Here are some examples. Note that the third example is not a question at all but a request; Chapter 24 discusses the uses of these question forms to perform different pragmatic functions such as asking, requesting, or suggesting. \nDo any of these flights have stops? Does American’s flight eighteen twenty five serve dinner? Can you give me the same information for United? \nHere’s the rule: \nwh-phrase wh-word \nThe most complex sentence-level structures we examine here are the various whstructures. These are so named because one of their constituents is a wh-phrase, that is, one that includes a wh-word (who, whose, when, where, what, which, how, why). These may be broadly grouped into two classes of sentence-level structures. The wh-subject-question structure is identical to the declarative structure, except that the first noun phrase contains some wh-word.",
        "chapter": "Constituency Grammars",
        "section": "Context-Free Grammars",
        "subsection": "Formal Definition of Context-Free Grammar",
        "subsubsection": "N/A"
    },
    {
        "content": "12.3 Some Grammar Rules for English \nIn this section, we introduce a few more aspects of the phrase structure of English; for consistency we will continue to focus on sentences from the ATIS domain. Because of space limitations, our discussion is necessarily limited to highlights. Readers are strongly advised to consult a good reference grammar of English, such as Huddleston and Pullum (2002). \n12.3.1 Sentence-Level Constructions \nIn the small grammar $mathcal { L } _ { 0 }$ , we provided only one sentence-level construction for declarative sentences like I prefer a morning flight. Among the large number of constructions for English sentences, four are particularly common and important: declaratives, imperatives, yes-no questions, and wh-questions. \ndeclarative \nSentences with declarative structure have a subject noun phrase followed by a verb phrase, like “I prefer a morning flight”. Sentences with this structure have a great number of different uses that we follow up on in Chapter 24. Here are a number of examples from the ATIS domain: \nI want a flight from Ontario to Chicago The flight should be eleven a.m. tomorrow The return flight should leave at around seven p.m. \nimperative \nSentences with imperative structure often begin with a verb phrase and have no subject. They are called imperative because they are almost always used for commands and suggestions; in the ATIS domain they are commands to the system. \nShow the lowest fare   \nGive me Sunday’s flights arriving in Las Vegas from New York City List all flights between five and seven p.m. \nWe can model this sentence structure with another rule for the expansion of $S$ : \nyes-no question \nSentences with yes-no question structure are often (though not always) used to ask questions; they begin with an auxiliary verb, followed by a subject $N P$ , followed by a $V P$ . Here are some examples. Note that the third example is not a question at all but a request; Chapter 24 discusses the uses of these question forms to perform different pragmatic functions such as asking, requesting, or suggesting. \nDo any of these flights have stops? Does American’s flight eighteen twenty five serve dinner? Can you give me the same information for United? \nHere’s the rule: \nwh-phrase wh-word \nThe most complex sentence-level structures we examine here are the various whstructures. These are so named because one of their constituents is a wh-phrase, that is, one that includes a wh-word (who, whose, when, where, what, which, how, why). These may be broadly grouped into two classes of sentence-level structures. The wh-subject-question structure is identical to the declarative structure, except that the first noun phrase contains some wh-word. \nWhat airlines fly from Burbank to Denver?   \nWhich flights depart Burbank after noon and arrive in Denver by six p.m?   \nWhose flights serve breakfast? \nHere is a rule. Exercise 12.7 discusses rules for the constituents that make up the Wh-NP. \nwh-non-subjectquestion \nIn the wh-non-subject-question structure, the wh-phrase is not the subject of the sentence, and so the sentence includes another subject. In these types of sentences the auxiliary appears before the subject $N P$ , just as in the yes-no question structures. Here is an example followed by a sample rule: \nWhat flights do you have from Burbank to Tacoma Washington? \nlong-distance dependencies \nConstructions like the wh-non-subject-question contain what are called longdistance dependencies because the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the $V P$ . In some models of parsing and understanding compatible with the grammar rule above, long-distance dependencies like the relation between flights and have are thought of as a semantic relation. In such models, the job of figuring out that flights is the argument of have is done during semantic interpretation. Other models of parsing represent the relationship between flights and have as a syntactic relation, and the grammar is modified to insert a small marker called a trace or empty category after the verb. We discuss empty-category models when we introduce the Penn Treebank on page 274. \nclause \n12.3.2 Clauses and Sentences \nBefore we move on, we should clarify the status of the S rules in the grammars we just described. S rules are intended to account for entire sentences that stand alone as fundamental units of discourse. However, $S$ can also occur on the right-hand side of grammar rules and hence can be embedded within larger sentences. Clearly then, there’s more to being an $S$ than just standing alone as a unit of discourse. \nWhat differentiates sentence constructions (i.e., the $S$ rules) from the rest of the grammar is the notion that they are in some sense complete. In this way they correspond to the notion of a clause, which traditional grammars often describe as forming a complete thought. One way of making this notion of “complete thought” more precise is to say an $S$ is a node of the parse tree below which the main verb of the $s$ has all of its arguments. We define verbal arguments later, but for now let’s just see an illustration from the tree for I prefer a morning flight in Fig. 12.4 on page 264. The verb prefer has two arguments: the subject $I$ and the object a morning flight. One of the arguments appears below the $V P$ node, but the other one, the subject NP, appears only below the S node. \n12.3.3 The Noun Phrase \nOur $mathcal { L } _ { 0 }$ grammar introduced three of the most frequent types of noun phrases that occur in English: pronouns, proper nouns and the $N P  D e t$ Nominal construction. The central focus of this section is on the last type since that is where the bulk of the syntactic complexity resides. These noun phrases consist of a head, the central noun in the noun phrase, along with various modifiers that can occur before or after the head noun. Let’s take a close look at the various parts.",
        "chapter": "Constituency Grammars",
        "section": "Some Grammar Rules for English",
        "subsection": "Sentence-Level Constructions",
        "subsubsection": "N/A"
    },
    {
        "content": "What airlines fly from Burbank to Denver?   \nWhich flights depart Burbank after noon and arrive in Denver by six p.m?   \nWhose flights serve breakfast? \nHere is a rule. Exercise 12.7 discusses rules for the constituents that make up the Wh-NP. \nwh-non-subjectquestion \nIn the wh-non-subject-question structure, the wh-phrase is not the subject of the sentence, and so the sentence includes another subject. In these types of sentences the auxiliary appears before the subject $N P$ , just as in the yes-no question structures. Here is an example followed by a sample rule: \nWhat flights do you have from Burbank to Tacoma Washington? \nlong-distance dependencies \nConstructions like the wh-non-subject-question contain what are called longdistance dependencies because the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the $V P$ . In some models of parsing and understanding compatible with the grammar rule above, long-distance dependencies like the relation between flights and have are thought of as a semantic relation. In such models, the job of figuring out that flights is the argument of have is done during semantic interpretation. Other models of parsing represent the relationship between flights and have as a syntactic relation, and the grammar is modified to insert a small marker called a trace or empty category after the verb. We discuss empty-category models when we introduce the Penn Treebank on page 274. \nclause \n12.3.2 Clauses and Sentences \nBefore we move on, we should clarify the status of the S rules in the grammars we just described. S rules are intended to account for entire sentences that stand alone as fundamental units of discourse. However, $S$ can also occur on the right-hand side of grammar rules and hence can be embedded within larger sentences. Clearly then, there’s more to being an $S$ than just standing alone as a unit of discourse. \nWhat differentiates sentence constructions (i.e., the $S$ rules) from the rest of the grammar is the notion that they are in some sense complete. In this way they correspond to the notion of a clause, which traditional grammars often describe as forming a complete thought. One way of making this notion of “complete thought” more precise is to say an $S$ is a node of the parse tree below which the main verb of the $s$ has all of its arguments. We define verbal arguments later, but for now let’s just see an illustration from the tree for I prefer a morning flight in Fig. 12.4 on page 264. The verb prefer has two arguments: the subject $I$ and the object a morning flight. One of the arguments appears below the $V P$ node, but the other one, the subject NP, appears only below the S node. \n12.3.3 The Noun Phrase \nOur $mathcal { L } _ { 0 }$ grammar introduced three of the most frequent types of noun phrases that occur in English: pronouns, proper nouns and the $N P  D e t$ Nominal construction. The central focus of this section is on the last type since that is where the bulk of the syntactic complexity resides. These noun phrases consist of a head, the central noun in the noun phrase, along with various modifiers that can occur before or after the head noun. Let’s take a close look at the various parts.",
        "chapter": "Constituency Grammars",
        "section": "Some Grammar Rules for English",
        "subsection": "Clauses and Sentences",
        "subsubsection": "N/A"
    },
    {
        "content": "What airlines fly from Burbank to Denver?   \nWhich flights depart Burbank after noon and arrive in Denver by six p.m?   \nWhose flights serve breakfast? \nHere is a rule. Exercise 12.7 discusses rules for the constituents that make up the Wh-NP. \nwh-non-subjectquestion \nIn the wh-non-subject-question structure, the wh-phrase is not the subject of the sentence, and so the sentence includes another subject. In these types of sentences the auxiliary appears before the subject $N P$ , just as in the yes-no question structures. Here is an example followed by a sample rule: \nWhat flights do you have from Burbank to Tacoma Washington? \nlong-distance dependencies \nConstructions like the wh-non-subject-question contain what are called longdistance dependencies because the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the $V P$ . In some models of parsing and understanding compatible with the grammar rule above, long-distance dependencies like the relation between flights and have are thought of as a semantic relation. In such models, the job of figuring out that flights is the argument of have is done during semantic interpretation. Other models of parsing represent the relationship between flights and have as a syntactic relation, and the grammar is modified to insert a small marker called a trace or empty category after the verb. We discuss empty-category models when we introduce the Penn Treebank on page 274. \nclause \n12.3.2 Clauses and Sentences \nBefore we move on, we should clarify the status of the S rules in the grammars we just described. S rules are intended to account for entire sentences that stand alone as fundamental units of discourse. However, $S$ can also occur on the right-hand side of grammar rules and hence can be embedded within larger sentences. Clearly then, there’s more to being an $S$ than just standing alone as a unit of discourse. \nWhat differentiates sentence constructions (i.e., the $S$ rules) from the rest of the grammar is the notion that they are in some sense complete. In this way they correspond to the notion of a clause, which traditional grammars often describe as forming a complete thought. One way of making this notion of “complete thought” more precise is to say an $S$ is a node of the parse tree below which the main verb of the $s$ has all of its arguments. We define verbal arguments later, but for now let’s just see an illustration from the tree for I prefer a morning flight in Fig. 12.4 on page 264. The verb prefer has two arguments: the subject $I$ and the object a morning flight. One of the arguments appears below the $V P$ node, but the other one, the subject NP, appears only below the S node. \n12.3.3 The Noun Phrase \nOur $mathcal { L } _ { 0 }$ grammar introduced three of the most frequent types of noun phrases that occur in English: pronouns, proper nouns and the $N P  D e t$ Nominal construction. The central focus of this section is on the last type since that is where the bulk of the syntactic complexity resides. These noun phrases consist of a head, the central noun in the noun phrase, along with various modifiers that can occur before or after the head noun. Let’s take a close look at the various parts. \nThe Determiner \nNoun phrases can begin with simple lexical determiners: \na stop the flights this flight those flights any flights some flights \nThe role of the determiner can also be filled by more complex expressions: \nUnited’s flight   \nUnited’s pilot’s union   \nDenver’s mayor’s mother’s canceled flight \nIn these examples, the role of the determiner is filled by a possessive expression consisting of a noun phrase followed by an ${ bf Phi } _ { S }$ as a possessive marker, as in the following rule. \nThe fact that this rule is recursive (since an $N P$ can start with a $D e t  r _ { i }$ ) helps us model the last two examples above, in which a sequence of possessive expressions serves as a determiner. \nUnder some circumstances determiners are optional in English. For example, determiners may be omitted if the noun they modify is plural: \n(12.2) Show me flights from San Francisco to Denver on weekdays \nAs we saw in Chapter 8, mass nouns also don’t require determination. Recall that mass nouns often (not always) involve something that is treated like a substance (including e.g., water and snow), don’t take the indefinite article “a”, and don’t tend to pluralize. Many abstract nouns are mass nouns (music, homework). Mass nouns in the ATIS domain include breakfast, lunch, and dinner: \n(12.3) Does this flight serve dinner? \nThe Nominal \nThe nominal construction follows the determiner and contains any pre- and posthead noun modifiers. As indicated in grammar ${ mathcal { L } } _ { 0 }$ , in its simplest form a nominal can consist of a single noun. \nAs we’ll see, this rule also provides the basis for the bottom of various recursive rules used to capture more complex nominal constructions. \ncardinal numbers ordinal numbers quantifiers \nBefore the Head Noun \nA number of different kinds of word classes can appear before the head noun but after the determiner (the “postdeterminers”) in a nominal. These include cardinal numbers, ordinal numbers, quantifiers, and adjectives. Examples of cardinal numbers: \ntwo friends one stop \nOrdinal numbers include first, second, third, and so on, but also words like next, last, past, other, and another: \nthe first one the next day the second leg the last flight the other American flight \nSome quantifiers (many, (a) few, several) occur only with plural count nouns: \nmany fares Adjectives occur after quantifiers but before nouns. \na first-class fare a non-stop flight the longest layover the earliest lunch flight \nadjective phrase \nAdjectives can also be grouped into a phrase called an adjective phrase or AP. APs can have an adverb before the adjective (see Chapter 8 for definitions of adjectives and adverbs): \nthe least expensive fare \nAfter the Head Noun \nA head noun can be followed by postmodifiers. Three kinds of nominal postmodifiers are common in English: \nprepositional phrases all flights from Cleveland non-finite clauses any flights arriving after eleven a.m. relative clauses a flight that serves breakfast \nThey are especially common in the ATIS corpus since they are used to mark the origin and destination of flights. \nHere are some examples of prepositional phrase postmodifiers, with brackets inserted to show the boundaries of each PP; note that two or more PPs can be strung together within a single NP: \nall flights [from Cleveland] [to Newark] arrival [in San Jose] [before seven p.m.] a reservation [on flight six oh six] [from Tampa] [to Montreal] \nHere’s a new nominal rule to account for postnominal $P P mathrm { s }$ : \nnon-finite \nThe three most common kinds of non-finite postmodifiers are the gerundive (- ing), -ed, and infinitive forms. \ngerundive \nGerundive postmodifiers are so called because they consist of a verb phrase that begins with the gerundive (-ing) form of the verb. Here are some examples: \nany of those [leaving on Thursday] any flights [arriving after eleven a.m.] flights [arriving within thirty minutes of each other] \nWe can define the Nominals with gerundive modifiers as follows, making use of a new non-terminal GerundVP: \nNominal $$ Nominal GerundVP \nWe can make rules for GerundVP constituents by duplicating all of our VP productions, substituting GerundV for $V .$ . \nGerundV can then be defined as \nThe phrases in italics below are examples of the two other common kinds of non-finite clauses, infinitives and -ed forms: \nrelative pronoun \nthe last flight to arrive in Boston I need to have dinner served Which is the aircraft used by this flight? \npredeterminers \nA postnominal relative clause (more correctly a restrictive relative clause), is a clause that often begins with a relative pronoun (that and who are the most common). The relative pronoun functions as the subject of the embedded verb in the following examples: \na flight that serves breakfast flights that leave in the morning the one that leaves at ten thirty five \nWe might add rules like the following to deal with these: \nThe relative pronoun may also function as the object of the embedded verb, as in the following example; we leave for the reader the exercise of writing grammar rules for more complex relative clauses of this kind. \nthe earliest American Airlines flight that I can get \nVarious postnominal modifiers can be combined: \na flight [from Phoenix to Detroit] [leaving Monday evening] evening flights [from Nashville to Houston] [that serve dinner] a friend [living in Denver] [that would like to visit me in DC] \nBefore the Noun Phrase \nWord classes that modify and appear before NPs are called predeterminers. Many of these have to do with number or amount; a common predeterminer is all: \nall the flights all flights all non-stop flights \nThe example noun phrase given in Fig. 12.5 illustrates some of the complexity that arises when these rules are combined. \n12.3.4 The Verb Phrase \nThe verb phrase consists of the verb and a number of other constituents. In the simple rules we have built so far, these other constituents include NPs and $P P mathrm { s }$ and combinations of the two: \nsentential complements \nVerb phrases can be significantly more complicated than this. Many other kinds of constituents, such as an entire embedded sentence, can follow the verb. These are called sentential complements: \nYou $left[ boldsymbol { V } boldsymbol { P }  right[ boldsymbol { V }$ said $[ boldsymbol { s }$ you had a two hundred sixty-six dollar fare]] [VP [V Tell] [NP me] [S how to get from the airport to downtown]] $mathrm { ~ I ~ } [ boldsymbol { nu } P left[ boldsymbol { nu } right.$ think $[ boldsymbol { s } ^ { intercal } ]$ would like to take the nine thirty flight]]",
        "chapter": "Constituency Grammars",
        "section": "Some Grammar Rules for English",
        "subsection": "The Noun Phrase",
        "subsubsection": "N/A"
    },
    {
        "content": "relative pronoun \nthe last flight to arrive in Boston I need to have dinner served Which is the aircraft used by this flight? \npredeterminers \nA postnominal relative clause (more correctly a restrictive relative clause), is a clause that often begins with a relative pronoun (that and who are the most common). The relative pronoun functions as the subject of the embedded verb in the following examples: \na flight that serves breakfast flights that leave in the morning the one that leaves at ten thirty five \nWe might add rules like the following to deal with these: \nThe relative pronoun may also function as the object of the embedded verb, as in the following example; we leave for the reader the exercise of writing grammar rules for more complex relative clauses of this kind. \nthe earliest American Airlines flight that I can get \nVarious postnominal modifiers can be combined: \na flight [from Phoenix to Detroit] [leaving Monday evening] evening flights [from Nashville to Houston] [that serve dinner] a friend [living in Denver] [that would like to visit me in DC] \nBefore the Noun Phrase \nWord classes that modify and appear before NPs are called predeterminers. Many of these have to do with number or amount; a common predeterminer is all: \nall the flights all flights all non-stop flights \nThe example noun phrase given in Fig. 12.5 illustrates some of the complexity that arises when these rules are combined. \n12.3.4 The Verb Phrase \nThe verb phrase consists of the verb and a number of other constituents. In the simple rules we have built so far, these other constituents include NPs and $P P mathrm { s }$ and combinations of the two: \nsentential complements \nVerb phrases can be significantly more complicated than this. Many other kinds of constituents, such as an entire embedded sentence, can follow the verb. These are called sentential complements: \nYou $left[ boldsymbol { V } boldsymbol { P }  right[ boldsymbol { V }$ said $[ boldsymbol { s }$ you had a two hundred sixty-six dollar fare]] [VP [V Tell] [NP me] [S how to get from the airport to downtown]] $mathrm { ~ I ~ } [ boldsymbol { nu } P left[ boldsymbol { nu } right.$ think $[ boldsymbol { s } ^ { intercal } ]$ would like to take the nine thirty flight]] \nHere’s a rule for these: \nSimilarly, another potential constituent of the $V P$ is another VP. This is often the case for verbs like want, would like, try, intend, need: \nI want $[ boldsymbol {  l } _ { V P }$ to fly from Milwaukee to Orlando] Hi, I want $big [ boldsymbol { V } P _ { } big .$ to arrange three flights] \ntransitive intransitive subcategorize subcategorizes for complements subcategorization frame \n\nWhile a verb phrase can have many possible kinds of constituents, not every verb is compatible with every verb phrase. For example, the verb want can be used either with an NP complement (I want a flight . . . ) or with an infinitive $V P$ complement (I want to fly to . . . ). By contrast, a verb like find cannot take this sort of $V P$ complement (* I found to fly to Dallas). \nThis idea that verbs are compatible with different kinds of complements is a very old one; traditional grammar distinguishes between transitive verbs like find, which take a direct object NP (I found a flight), and intransitive verbs like disappear, which do not ( $^ { * } I$ disappeared a flight). \nWhere traditional grammars subcategorize verbs into these two categories (transitive and intransitive), modern grammars distinguish as many as 100 subcategories. We say that a verb like find subcategorizes for an NP, and a verb like want subcategorizes for either an NP or a non-finite VP. We also call these constituents the complements of the verb (hence our use of the term sentential complement above). So we say that want can take a $V P$ complement. These possible sets of complements are called the subcategorization frame for the verb. Another way of talking about the relation between the verb and these other constituents is to think of the verb as a logical predicate and the constituents as logical arguments of the predicate. So we can think of such predicate-argument relations as FIND(I, A FLIGHT) or WANT(I, TO FLY). We talk more about this view of verbs and arguments in Chapter 15 when we talk about predicate calculus representations of verb semantics. Subcategorization frames for a set of example verbs are given in Fig. 12.6. \nWe can capture the association between verbs and their complements by making separate subtypes of the class Verb (e.g., Verb-with-NP-complement, Verb-with-InfVP-complement, Verb-with-S-complement, and so on): \nEach $V P$ rule could then be modified to require the appropriate verb subtype: \nA problem with this approach is the significant increase in the number of rules and the associated loss of generality. \n12.3.5 Coordination \nconjunctions coordinate \nThe major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. For example, a coordinate noun phrase can consist of two other noun phrases separated by a conjunction: \nPlease repeat [NP [NP the flights] and [NP the costs]] I need to know [NP [NP the aircraft] and [NP the flight number]] \nHere’s a rule that allows these structures: \nNote that the ability to form coordinate phrases through conjunctions is often used as a test for constituency. Consider the following examples, which differ from the ones given above in that they lack the second determiner. \nPlease repeat the $operatorname { [ } _ { N o m }  [ boldsymbol {  O } _ { N o m }$ flights] and $mathsf { I } _ { N o m }$ costs]] I need to know the $operatorname { [ } _ { N o m }  [ boldsymbol { N o m }$ aircraft] and $operatorname { Pi } [ { } _ { N o m }$ flight number]] \nThe fact that these phrases can be conjoined is evidence for the presence of the underlying Nominal constituent we have been making use of. Here’s a rule for this: \nNominal $$ Nominal and Nominal \nThe following examples illustrate conjunctions involving VPs and Ss.",
        "chapter": "Constituency Grammars",
        "section": "Some Grammar Rules for English",
        "subsection": "The Verb Phrase",
        "subsubsection": "N/A"
    },
    {
        "content": "We can capture the association between verbs and their complements by making separate subtypes of the class Verb (e.g., Verb-with-NP-complement, Verb-with-InfVP-complement, Verb-with-S-complement, and so on): \nEach $V P$ rule could then be modified to require the appropriate verb subtype: \nA problem with this approach is the significant increase in the number of rules and the associated loss of generality. \n12.3.5 Coordination \nconjunctions coordinate \nThe major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. For example, a coordinate noun phrase can consist of two other noun phrases separated by a conjunction: \nPlease repeat [NP [NP the flights] and [NP the costs]] I need to know [NP [NP the aircraft] and [NP the flight number]] \nHere’s a rule that allows these structures: \nNote that the ability to form coordinate phrases through conjunctions is often used as a test for constituency. Consider the following examples, which differ from the ones given above in that they lack the second determiner. \nPlease repeat the $operatorname { [ } _ { N o m }  [ boldsymbol {  O } _ { N o m }$ flights] and $mathsf { I } _ { N o m }$ costs]] I need to know the $operatorname { [ } _ { N o m }  [ boldsymbol { N o m }$ aircraft] and $operatorname { Pi } [ { } _ { N o m }$ flight number]] \nThe fact that these phrases can be conjoined is evidence for the presence of the underlying Nominal constituent we have been making use of. Here’s a rule for this: \nNominal $$ Nominal and Nominal \nThe following examples illustrate conjunctions involving VPs and Ss. \nWhat flights do you have [VP [VP leaving Denver] and [VP arriving in San Francisco]]   \n[S [S I’m interested in a flight from Dallas to Washington] and $[ boldsymbol { s }$ I’m also interested in going to Baltimore]] \nThe rules for $V P$ and $S$ conjunctions mirror the $N P$ one given above. \nmetarules \nSince all the major phrase types can be conjoined in this fashion, it is also possible to represent this conjunction fact more generally; a number of grammar formalisms such as GPSG (Gazdar et al., 1985) do this using metarules like: \nThis metarule states that any non-terminal can be conjoined with the same nonterminal to yield a constituent of the same type; the variable $X$ must be designated as a variable that stands for any non-terminal rather than a non-terminal itself. \n12.4 Treebanks \ntreebank \nSufficiently robust grammars consisting of context-free grammar rules can be used to assign a parse tree to any sentence. This means that it is possible to build a corpus where every sentence in the collection is paired with a corresponding parse tree. Such a syntactically annotated corpus is called a treebank. Treebanks play an important role in parsing, as we discuss in Chapter 13, as well as in linguistic investigations of syntactic phenomena. \nPenn Treebank \nA wide variety of treebanks have been created, generally through the use of parsers (of the sort described in the next few chapters) to automatically parse each sentence, followed by the use of humans (linguists) to hand-correct the parses. The Penn Treebank project (whose POS tagset we introduced in Chapter 8) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese. A number of treebanks use the dependency representation we will introduce in Chapter 14, including many that are part of the Universal Dependencies project (Nivre et al., 2016b). \n12.4.1 Example: The Penn Treebank Project \ntraces \nFigure 12.7 shows sentences from the Brown and ATIS portions of the Penn Treebank.2 Note the formatting differences for the part-of-speech tags; such small differences are common and must be dealt with in processing treebanks. The Penn Treebank part-of-speech tagset was defined in Chapter 8. The use of LISP-style parenthesized notation for trees is extremely common and resembles the bracketed notation we saw earlier in (12.1). For those who are not familiar with it we show a standard node-and-line tree representation in Fig. 12.8. \nFigure 12.9 shows a tree from the Wall Street Journal. This tree shows another feature of the Penn Treebanks: the use of traces (-NONE- nodes) to mark",
        "chapter": "Constituency Grammars",
        "section": "Some Grammar Rules for English",
        "subsection": "Coordination",
        "subsubsection": "N/A"
    },
    {
        "content": "What flights do you have [VP [VP leaving Denver] and [VP arriving in San Francisco]]   \n[S [S I’m interested in a flight from Dallas to Washington] and $[ boldsymbol { s }$ I’m also interested in going to Baltimore]] \nThe rules for $V P$ and $S$ conjunctions mirror the $N P$ one given above. \nmetarules \nSince all the major phrase types can be conjoined in this fashion, it is also possible to represent this conjunction fact more generally; a number of grammar formalisms such as GPSG (Gazdar et al., 1985) do this using metarules like: \nThis metarule states that any non-terminal can be conjoined with the same nonterminal to yield a constituent of the same type; the variable $X$ must be designated as a variable that stands for any non-terminal rather than a non-terminal itself. \n12.4 Treebanks \ntreebank \nSufficiently robust grammars consisting of context-free grammar rules can be used to assign a parse tree to any sentence. This means that it is possible to build a corpus where every sentence in the collection is paired with a corresponding parse tree. Such a syntactically annotated corpus is called a treebank. Treebanks play an important role in parsing, as we discuss in Chapter 13, as well as in linguistic investigations of syntactic phenomena. \nPenn Treebank \nA wide variety of treebanks have been created, generally through the use of parsers (of the sort described in the next few chapters) to automatically parse each sentence, followed by the use of humans (linguists) to hand-correct the parses. The Penn Treebank project (whose POS tagset we introduced in Chapter 8) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese. A number of treebanks use the dependency representation we will introduce in Chapter 14, including many that are part of the Universal Dependencies project (Nivre et al., 2016b). \n12.4.1 Example: The Penn Treebank Project \ntraces \nFigure 12.7 shows sentences from the Brown and ATIS portions of the Penn Treebank.2 Note the formatting differences for the part-of-speech tags; such small differences are common and must be dealt with in processing treebanks. The Penn Treebank part-of-speech tagset was defined in Chapter 8. The use of LISP-style parenthesized notation for trees is extremely common and resembles the bracketed notation we saw earlier in (12.1). For those who are not familiar with it we show a standard node-and-line tree representation in Fig. 12.8. \nFigure 12.9 shows a tree from the Wall Street Journal. This tree shows another feature of the Penn Treebanks: the use of traces (-NONE- nodes) to mark \nsyntactic movement \nlong-distance dependencies or syntactic movement. For example, quotations often follow a quotative verb like say. But in this example, the quotation “We would have to wait until we have collected on those assets” precedes the words he said. An empty $s$ containing only the node -NONE- marks the position after said where the quotation sentence often occurs. This empty node is marked (in Treebanks II and III) with the index 2, as is the quotation $S$ at the beginning of the sentence. Such co-indexing may make it easier for some parsers to recover the fact that this fronted or topicalized quotation is the complement of the verb said. A similar -NONE- node marks the fact that there is no syntactic subject right before the verb to wait; instead, the subject is the earlier NP We. Again, they are both co-indexed with the index 1. \nThe Penn Treebank II and Treebank III releases added further information to make it easier to recover the relationships between predicates and arguments. Certain phrases were marked with tags indicating the grammatical function of the phrase (as surface subject, logical topic, cleft, non-VP predicates) its presence in particular text categories (headlines, titles), and its semantic function (temporal phrases, lo\nFigure 12.9 A sentence from the Wall Street Journal portion of the LDC Penn Treebank.   \nNote the use of the empty -NONE- nodes. \ncations) (Marcus et al. 1994, Bies et al. 1995). Figure 12.9 shows examples of the -SBJ (surface subject) and -TMP (temporal phrase) tags. Figure 12.8 shows in addition the -PRD tag, which is used for predicates that are not VPs (the one in Fig. 12.8 is an ADJP). We’ll return to the topic of grammatical function when we consider dependency grammars and parsing in Chapter 14. \n12.4.2 Treebanks as Grammars \nThe sentences in a treebank implicitly constitute a grammar of the language represented by the corpus being annotated. For example, from the three parsed sentences in Fig. 12.7 and Fig. 12.9, we can extract each of the CFG rules in them. For simplicity, let’s strip off the rule suffixes (-SBJ and so on). The resulting grammar is shown in Fig. 12.10. \nThe grammar used to parse the Penn Treebank is relatively flat, resulting in very many and very long rules. For example, among the approximately 4,500 different rules for expanding VPs are separate rules for PP sequences of any length and every possible arrangement of verb arguments: \n$tt V P  tt V B D$ PP $tt V P  tt V B D$ PP PP $tt V P  tt V B D$ PP PP PP $tt V P  tt V B D$ PP PP PP PP $mathsf { V P }  mathsf { V B }$ ADVP PP $mathsf { V P }  mathsf { V B }$ PP ADVP VP → ADVP VB PP \nas well as even longer rules, such as which comes from the $V P$ marked in italics:",
        "chapter": "Constituency Grammars",
        "section": "Treebanks",
        "subsection": "Example: The Penn Treebank Project",
        "subsubsection": "N/A"
    },
    {
        "content": "Figure 12.9 A sentence from the Wall Street Journal portion of the LDC Penn Treebank.   \nNote the use of the empty -NONE- nodes. \ncations) (Marcus et al. 1994, Bies et al. 1995). Figure 12.9 shows examples of the -SBJ (surface subject) and -TMP (temporal phrase) tags. Figure 12.8 shows in addition the -PRD tag, which is used for predicates that are not VPs (the one in Fig. 12.8 is an ADJP). We’ll return to the topic of grammatical function when we consider dependency grammars and parsing in Chapter 14. \n12.4.2 Treebanks as Grammars \nThe sentences in a treebank implicitly constitute a grammar of the language represented by the corpus being annotated. For example, from the three parsed sentences in Fig. 12.7 and Fig. 12.9, we can extract each of the CFG rules in them. For simplicity, let’s strip off the rule suffixes (-SBJ and so on). The resulting grammar is shown in Fig. 12.10. \nThe grammar used to parse the Penn Treebank is relatively flat, resulting in very many and very long rules. For example, among the approximately 4,500 different rules for expanding VPs are separate rules for PP sequences of any length and every possible arrangement of verb arguments: \n$tt V P  tt V B D$ PP $tt V P  tt V B D$ PP PP $tt V P  tt V B D$ PP PP PP $tt V P  tt V B D$ PP PP PP PP $mathsf { V P }  mathsf { V B }$ ADVP PP $mathsf { V P }  mathsf { V B }$ PP ADVP VP → ADVP VB PP \nas well as even longer rules, such as which comes from the $V P$ marked in italics: \n\nThis mostly happens because we go from football in the fall to lifting in the winter to football again in the spring. \nSome of the many thousands of $N P$ rules include \n$tt N P  tt D T$ JJ NN   \n$mathbb { N P }  mathbb { D T }$ JJ NNS   \n$mathbb { N P }  mathbb { D T }$ JJ NN NN   \n$tt N P  tt D T$ JJ JJ NN   \nNP → DT JJ CD NNS   \n$tt N P  tt R B$ DT JJ NN NN   \n$tt N P  tt R B$ DT JJ JJ NNS   \n$tt N P  tt D T$ JJ JJ NNP NNS   \n$mathbb { N P }  mathbb { D T }$ NNP NNP NNP NNP JJ NN   \nNP → DT JJ NNP CC JJ JJ NN NNS   \n$tt N P  tt R B$ DT JJS NN NN SBAR   \n$mathbb { N P }  mathbb { D T }$ VBG JJ NNP NNP CC NNP   \nNP DT JJ NNS , NNS CC NN NNS NN   \nNP DT JJ JJ VBG NN NNP NNP FW NNP   \nNP NP JJ , JJ ‘ SBAR ’’ NNS The last two of those rules, for example, come from the following two noun phrases:   \n[DT The] [JJ state-owned] $big [ mathrm { J } mathrm { J }$ industrial] [VBG holding] [NN company] [NNP Instituto] [NNP Nacional] [FW de] [NNP Industria]   \n[NP Shearson’s] [JJ easy-to-film], $big [ mathrm { mathbf { J } } mathrm { mathbf { J } }$ black-and-white] “[SBAR Where We Stand]” [NNS commercials] \n\nViewed as a large grammar in this way, the Penn Treebank III Wall Street Journal corpus, which contains about 1 million words, also has about 1 million non-lexical rule tokens, consisting of about 17,500 distinct rule types. \nVarious facts about the treebank grammars, such as their large numbers of flat rules, pose problems for probabilistic parsing algorithms. For this reason, it is common to make various modifications to a grammar extracted from a treebank. We discuss these further in Appendix C. \n12.4.3 Heads and Head Finding \nWe suggested informally earlier that syntactic constituents could be associated with a lexical head; $N$ is the head of an NP, V is the head of a VP. This idea of a head for each constituent dates back to Bloomfield 1914, and is central to the dependency grammars and dependency parsing we’ll introduce in Chapter 14. Heads are also important in probabilistic parsing (Appendix C) and in constituent-based grammar formalisms like Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994).. \nIn one simple model of lexical heads, each context-free rule is associated with a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is grammatically the most important. Heads are passed up the parse tree; thus, each non-terminal in a parse tree is annotated with a single word, which is its lexical head. Figure 12.11 shows an example of such a tree from Collins (1999), in which each non-terminal is annotated with its head. \nFor the generation of such a tree, each CFG rule must be augmented to identify one right-side constituent to be the head child. The headword for a node is then set to the headword of its head child. Choosing these head children is simple for textbook examples (NN is the head of $N P$ ) but is complicated and indeed controversial for most phrases. (Should the complementizer to or the verb be the head of an infinite verb phrase?) Modern linguistic theories of syntax generally include a component that defines heads (see, e.g., (Pollard and Sag, 1994)). \nAn alternative approach to finding a head is used in most practical computational systems. Instead of specifying head rules in the grammar itself, heads are identified dynamically in the context of trees for specific sentences. In other words, once a sentence is parsed, the resulting tree is walked to decorate each node with the appropriate head. Most current systems rely on a simple set of handwritten rules, such as a practical one for Penn Treebank grammars given in Collins (1999) but developed originally by Magerman (1995). For example, the rule for finding the head of an $N P$ is as follows (Collins, 1999, p. 238):",
        "chapter": "Constituency Grammars",
        "section": "Treebanks",
        "subsection": "Treebanks as Grammars",
        "subsubsection": "N/A"
    },
    {
        "content": "Various facts about the treebank grammars, such as their large numbers of flat rules, pose problems for probabilistic parsing algorithms. For this reason, it is common to make various modifications to a grammar extracted from a treebank. We discuss these further in Appendix C. \n12.4.3 Heads and Head Finding \nWe suggested informally earlier that syntactic constituents could be associated with a lexical head; $N$ is the head of an NP, V is the head of a VP. This idea of a head for each constituent dates back to Bloomfield 1914, and is central to the dependency grammars and dependency parsing we’ll introduce in Chapter 14. Heads are also important in probabilistic parsing (Appendix C) and in constituent-based grammar formalisms like Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994).. \nIn one simple model of lexical heads, each context-free rule is associated with a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is grammatically the most important. Heads are passed up the parse tree; thus, each non-terminal in a parse tree is annotated with a single word, which is its lexical head. Figure 12.11 shows an example of such a tree from Collins (1999), in which each non-terminal is annotated with its head. \nFor the generation of such a tree, each CFG rule must be augmented to identify one right-side constituent to be the head child. The headword for a node is then set to the headword of its head child. Choosing these head children is simple for textbook examples (NN is the head of $N P$ ) but is complicated and indeed controversial for most phrases. (Should the complementizer to or the verb be the head of an infinite verb phrase?) Modern linguistic theories of syntax generally include a component that defines heads (see, e.g., (Pollard and Sag, 1994)). \nAn alternative approach to finding a head is used in most practical computational systems. Instead of specifying head rules in the grammar itself, heads are identified dynamically in the context of trees for specific sentences. In other words, once a sentence is parsed, the resulting tree is walked to decorate each node with the appropriate head. Most current systems rely on a simple set of handwritten rules, such as a practical one for Penn Treebank grammars given in Collins (1999) but developed originally by Magerman (1995). For example, the rule for finding the head of an $N P$ is as follows (Collins, 1999, p. 238): \n• Else search from right to left for the first child which is an NN, NNP, NNPS, NX, POS, or JJR.   \n• Else search from left to right for the first child which is an NP.   \n• Else search from right to left for the first child which is a $$ 5$ , ADJP, or PRN.   \n• Else search from right to left for the first child which is a CD.   \n• Else search from right to left for the first child which is a JJ, JJS, RB or QP.   \n• Else return the last word \nSelected other rules from this set are shown in Fig. 12.12. For example, for $V P$ rules of the form $V P  Y _ { 1 } cdots Y _ { n }$ , the algorithm would start from the left of $Y _ { 1 } cdots$ $Y _ { n }$ looking for the first $Y _ { i }$ of type TO; if no TOs are found, it would search for the first $Y _ { i }$ of type VBD; if no VBDs are found, it would search for a VBN, and so on. See Collins (1999) for more details. \n12.5 Grammar Equivalence and Normal Form \nnormal form \nChomsky normal form \nbinary branching \nA formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free grammars generate the same language. \nWe usually distinguish two kinds of grammar equivalence: weak equivalence and strong equivalence. Two grammars are strongly equivalent if they generate the same set of strings and if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols). Two grammars are weakly equivalent if they generate the same set of strings but do not assign the same phrase structure to each sentence. \nIt is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a context-free grammar is in Chomsky normal form (CNF) (Chomsky, 1963) if it is $epsilon$ -free and if in addition each production is either of the form $A  B$ $C$ or $A to a$ . That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol. Chomsky normal form grammars are binary branching, that is they have binary trees (down to the prelexical nodes). We make use of this binary branching property in the CKY parsing algorithm in Chapter 13. \nAny context-free grammar can be converted into a weakly equivalent Chomsky normal form grammar. For example, a rule of the form",
        "chapter": "Constituency Grammars",
        "section": "Treebanks",
        "subsection": "Heads and Head Finding",
        "subsubsection": "N/A"
    },
    {
        "content": "• Else search from right to left for the first child which is an NN, NNP, NNPS, NX, POS, or JJR.   \n• Else search from left to right for the first child which is an NP.   \n• Else search from right to left for the first child which is a $$ 5$ , ADJP, or PRN.   \n• Else search from right to left for the first child which is a CD.   \n• Else search from right to left for the first child which is a JJ, JJS, RB or QP.   \n• Else return the last word \nSelected other rules from this set are shown in Fig. 12.12. For example, for $V P$ rules of the form $V P  Y _ { 1 } cdots Y _ { n }$ , the algorithm would start from the left of $Y _ { 1 } cdots$ $Y _ { n }$ looking for the first $Y _ { i }$ of type TO; if no TOs are found, it would search for the first $Y _ { i }$ of type VBD; if no VBDs are found, it would search for a VBN, and so on. See Collins (1999) for more details. \n12.5 Grammar Equivalence and Normal Form \nnormal form \nChomsky normal form \nbinary branching \nA formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free grammars generate the same language. \nWe usually distinguish two kinds of grammar equivalence: weak equivalence and strong equivalence. Two grammars are strongly equivalent if they generate the same set of strings and if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols). Two grammars are weakly equivalent if they generate the same set of strings but do not assign the same phrase structure to each sentence. \nIt is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a context-free grammar is in Chomsky normal form (CNF) (Chomsky, 1963) if it is $epsilon$ -free and if in addition each production is either of the form $A  B$ $C$ or $A to a$ . That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol. Chomsky normal form grammars are binary branching, that is they have binary trees (down to the prelexical nodes). We make use of this binary branching property in the CKY parsing algorithm in Chapter 13. \nAny context-free grammar can be converted into a weakly equivalent Chomsky normal form grammar. For example, a rule of the form \ncan be converted into the following two CNF rules (Exercise 12.8 asks the reader to formulate the complete algorithm): \nSometimes using binary branching can actually produce smaller grammars. For example, the sentences that might be characterized as \nare represented in the Penn Treebank by this series of rules: \nbut could also be generated by the following two-rule grammar: \nChomskyadjunction \nThe generation of a symbol A with a potentially infinite sequence of symbols B with a rule of the form $textbf { A }  textbf { A } textbf { B }$ is known as Chomsky-adjunction. \n12.6 Lexicalized Grammars \nThe approach to grammar presented thus far emphasizes phrase-structure rules while minimizing the role of the lexicon. However, as we saw in the discussions of agreement, subcategorization, and long-distance dependencies, this approach leads to solutions that are cumbersome at best, yielding grammars that are redundant, hard to manage, and brittle. To overcome these issues, numerous alternative approaches have been developed that all share the common theme of making better use of the lexicon. Among the more computationally relevant approaches are Lexical-Functional Grammar (LFG) (Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory Categorial Grammar (CCG). These approaches differ with respect to how lexicalized they are—the degree to which they rely on the lexicon as opposed to phrase structure rules to capture facts about the language. \nThe following section provides an introduction to CCG, a heavily lexicalized approach motivated by both syntactic and semantic considerations, which we will return to in Chapter 15. Chapter 14 discusses dependency grammars, an approach that eliminates phrase-structure rules entirely. \n12.6.1 Combinatory Categorial Grammar \ncategorial grammar \ncombinatory categorial grammar \nIn this section, we provide an overview of categorial grammar (Ajdukiewicz 1935, Bar-Hillel 1953), an early lexicalized grammar model, as well as an important modern extension, combinatory categorial grammar, or CCG (Steedman 1996, Steedman 1989, Steedman 2000).",
        "chapter": "Constituency Grammars",
        "section": "Grammar Equivalence and Normal Form",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "can be converted into the following two CNF rules (Exercise 12.8 asks the reader to formulate the complete algorithm): \nSometimes using binary branching can actually produce smaller grammars. For example, the sentences that might be characterized as \nare represented in the Penn Treebank by this series of rules: \nbut could also be generated by the following two-rule grammar: \nChomskyadjunction \nThe generation of a symbol A with a potentially infinite sequence of symbols B with a rule of the form $textbf { A }  textbf { A } textbf { B }$ is known as Chomsky-adjunction. \n12.6 Lexicalized Grammars \nThe approach to grammar presented thus far emphasizes phrase-structure rules while minimizing the role of the lexicon. However, as we saw in the discussions of agreement, subcategorization, and long-distance dependencies, this approach leads to solutions that are cumbersome at best, yielding grammars that are redundant, hard to manage, and brittle. To overcome these issues, numerous alternative approaches have been developed that all share the common theme of making better use of the lexicon. Among the more computationally relevant approaches are Lexical-Functional Grammar (LFG) (Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory Categorial Grammar (CCG). These approaches differ with respect to how lexicalized they are—the degree to which they rely on the lexicon as opposed to phrase structure rules to capture facts about the language. \nThe following section provides an introduction to CCG, a heavily lexicalized approach motivated by both syntactic and semantic considerations, which we will return to in Chapter 15. Chapter 14 discusses dependency grammars, an approach that eliminates phrase-structure rules entirely. \n12.6.1 Combinatory Categorial Grammar \ncategorial grammar \ncombinatory categorial grammar \nIn this section, we provide an overview of categorial grammar (Ajdukiewicz 1935, Bar-Hillel 1953), an early lexicalized grammar model, as well as an important modern extension, combinatory categorial grammar, or CCG (Steedman 1996, Steedman 1989, Steedman 2000). \nThe categorial approach consists of three major elements: a set of categories, a lexicon that associates words with categories, and a set of rules that govern how categories combine in context. \nCategories \nCategories are either atomic elements or single-argument functions that return a category as a value when provided with a desired category as argument. More formally, we can define $mathcal { C }$ , a set of categories for a grammar as follows: \n• ${ mathcal { A } } subseteq { mathcal { C } }$ , where $mathcal { A }$ is a given set of atomic elements • $( X / Y )$ , $( X backslash Y ) in { mathcal { C } }$ , if $X , Y in { mathcal { C } }$ \nThe slash notation shown here is used to define the functions in the grammar. It specifies the type of the expected argument, the direction it is expected be found, and the type of the result. Thus, $( X / Y )$ is a function that seeks a constituent of type $Y$ to its right and returns a value of $X$ ; $( X backslash Y )$ is the same except it seeks its argument to the left. \nThe set of atomic categories is typically very small and includes familiar elements such as sentences and noun phrases. Functional categories include verb phrases and complex noun phrases among others. \nThe Lexicon \nThe lexicon in a categorial approach consists of assignments of categories to words. These assignments can either be to atomic or functional categories, and due to lexical ambiguity words can be assigned to multiple categories. Consider the following sample lexical entries. \nNouns and proper nouns like flight and Miami are assigned to atomic categories, reflecting their typical role as arguments to functions. On the other hand, a transitive verb like cancel is assigned the category $( S backslash N P ) / N P$ : a function that seeks an $N P$ on its right and returns as its value a function with the type $( S backslash N P )$ . This function can, in turn, combine with an $N P$ on the left, yielding an $s$ as the result. This captures the kind of subcategorization information discussed in Section 12.3.4, however here the information has a rich, computationally useful, internal structure. \nDitransitive verbs like give, which expect two arguments after the verb, would have the category $( ( S backslash N P ) / N P ) / N P$ : a function that combines with an $N P$ on its right to yield yet another function corresponding to the transitive verb $( S backslash N P ) / N P$ category such as the one given above for cancel. \nRules \nThe rules of a categorial grammar specify how functions and their arguments combine. The following two rule templates constitute the basis for all categorial grammars. \nThe first rule applies a function to its argument on the right, while the second looks to the left for its argument. We’ll refer to the first as forward function application, and the second as backward function application. The result of applying either of these rules is the category specified as the value of the function being applied. \nGiven these rules and a simple lexicon, let’s consider an analysis of the sentence United serves Miami. Assume that serves is a transitive verb with the category $( S backslash N P ) / N P$ and that United and Miami are both simple NPs. Using both forward and backward function application, the derivation would proceed as follows: \nCategorial grammar derivations are illustrated growing down from the words, rule applications are illustrated with a horizontal line that spans the elements involved, with the type of the operation indicated at the right end of the line. In this example, there are two function applications: one forward function application indicated by the $>$ that applies the verb serves to the $N P$ on its right, and one backward function application indicated by the $<$ that applies the result of the first to the NP United on its left. \nWith the addition of another rule, the categorial approach provides a straightforward way to implement the coordination metarule described earlier on page 273. Recall that English permits the coordination of two constituents of the same type, resulting in a new constituent of the same type. The following rule provides the mechanism to handle such examples. \nThis rule states that when two constituents of the same category are separated by a constituent of type CONJ they can be combined into a single larger constituent of the same type. The following derivation illustrates the use of this rule. \nHere the two $S backslash N P$ constituents are combined via the conjunction operator $mathbf { nabla } < Phi >$ to form a larger constituent of the same type, which can then be combined with the subject $N P$ via backward function application. \nThese examples illustrate the lexical nature of the categorial grammar approach. The grammatical facts about a language are largely encoded in the lexicon, while the rules of the grammar are boiled down to a set of three rules. Unfortunately, the basic categorial approach does not give us any more expressive power than we had with traditional CFG rules; it just moves information from the grammar to the lexicon. To move beyond these limitations CCG includes operations that operate over functions. \nThe first pair of operators permit us to compose adjacent functions. \nforward composition \nThe first rule, called forward composition, can be applied to adjacent constituents where the first is a function seeking an argument of type $Y$ to its right, and the second is a function that provides $Y$ as a result. This rule allows us to compose these two functions into a single one with the type of the first constituent and the argument of the second. Although the notation is a little awkward, the second rule, backward composition is the same, except that we’re looking to the left instead of to the right for the relevant arguments. Both kinds of composition are signalled by a $mathbf { B }$ in CCG diagrams, accompanied by $mathbf { a } < mathbf { o r } >$ to indicate the direction. \nbackward composition \ntype raising \nThe next operator is type raising. Type raising elevates simple categories to the status of functions. More specifically, type raising takes a category and converts it to function that seeks as an argument a function that takes the original category as its argument. The following schema show two versions of type raising: one for arguments to the right, and one for the left. \nThe category $T$ in these rules can correspond to any of the atomic or functional categories already present in the grammar. \nA particularly useful example of type raising transforms a simple $N P$ argument in subject position to a function that can compose with a following $V P$ . To see how this works, let’s revisit our earlier example of United serves Miami. Instead of classifying United as an $N P$ which can serve as an argument to the function attached to serve, we can use type raising to reinvent it as a function in its own right as follows. \nCombining this type-raised constituent with the forward composition rule (12.7) permits the following alternative to our previous derivation. \nBy type raising United to $S / ( S backslash N P )$ , we can compose it with the transitive verb serves to yield the $( S / N P )$ function needed to complete the derivation. \nThere are several interesting things to note about this derivation. First, it provides a left-to-right, word-by-word derivation that more closely mirrors the way humans process language. This makes CCG a particularly apt framework for psycholinguistic studies. Second, this derivation involves the use of an intermediate unit of analysis, United serves, that does not correspond to a traditional constituent in English. This ability to make use of such non-constituent elements provides CCG with the ability to handle the coordination of phrases that are not proper constituents, as in the following example. \n(12.11) We flew IcelandAir to Geneva and SwissAir to London. \nHere, the segments that are being coordinated are IcelandAir to Geneva and SwissAir to London, phrases that would not normally be considered constituents, as can be seen in the following standard derivation for the verb phrase flew IcelandAir to Geneva. \nIn this derivation, there is no single constituent that corresponds to IcelandAir to Geneva, and hence no opportunity to make use of the $mathbf { nabla } < Phi >$ operator. Note that complex CCG categories can get a little cumbersome, so we’ll use $V P$ as a shorthand for $( S backslash N P )$ in this and the following derivations. \nThe following alternative derivation provides the required element through the use of both backward type raising (12.10) and backward function composition (12.8). \nApplying the same analysis to SwissAir to London satisfies the requirements for the $llangle Phi >$ operator, yielding the following derivation for our original example (12.11). \nFinally, let’s examine how these advanced operators can be used to handle longdistance dependencies (also referred to as syntactic movement or extraction). As mentioned in Section 12.3.1, long-distance dependencies arise from many English constructions including wh-questions, relative clauses, and topicalization. What these constructions have in common is a constituent that appears somewhere distant from its usual, or expected, location. Consider the following relative clause as an example. \nthe flight that United diverted \nHere, divert is a transitive verb that expects two $N P$ arguments, a subject $N P$ to its left and a direct object $N P$ to its right; its category is therefore $( S backslash N P ) / N P$ . However, in this example the direct object the flight has been “moved” to the beginning of the clause, while the subject United remains in its normal position. What is needed is a way to incorporate the subject argument, while dealing with the fact that the flight is not in its expected location. \nThe following derivation accomplishes this, again through the combined use of type raising and function composition. \nAs we saw with our earlier examples, the first step of this derivation is type raising United to the category $S / ( S backslash N P )$ allowing it to combine with diverted via forward composition. The result of this composition is $S / N P$ which preserves the fact that we are still looking for an $N P$ to fill the missing direct object. The second critical piece is the lexical category assigned to the word that: $( N P backslash N P ) / ( S / N P )$ . This function seeks a verb phrase missing an argument to its right, and transforms it into an $N P$ seeking a missing element to its left, precisely where we find the flight. \nCCGbank \nAs with phrase-structure approaches, treebanks play an important role in CCGbased approaches to parsing. CCGbank (Hockenmaier and Steedman, 2007) is the largest and most widely used CCG treebank. It was created by automatically translating phrase-structure trees from the Penn Treebank via a rule-based approach. The method produced successful translations of over $9 9 %$ of the trees in the Penn Treebank resulting in 48,934 sentences paired with CCG derivations. It also provides a lexicon of 44,000 words with over 1200 categories. Appendix C will discuss how these resources can be used to train CCG parsers. \n12.7 Summary \nThis chapter has introduced a number of fundamental concepts in syntax through the use of context-free grammars. \n• In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars).   \n• A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar.   \n• A generative grammar is a traditional name in linguistics for a formal language that is used to model the grammar of a natural language.   \n• There are many sentence-level grammatical constructions in English; declarative, imperative, yes-no question, and wh-question are four common types; these can be modeled with context-free rules.   \n• An English noun phrase can have determiners, numbers, quantifiers, and adjective phrases preceding the head noun, which can be followed by a number of postmodifiers; gerundive and infinitive VPs are common possibilities.   \n• Subjects in English agree with the main verb in person and number.   \n• Verbs can be subcategorized by the types of complements they expect. Simple subcategories are transitive and intransitive; most grammars include many more categories than these.   \n• Treebanks of parsed sentences exist for many genres of English and for many languages. Treebanks can be searched with tree-search tools.   \n• Any context-free grammar can be converted to Chomsky normal form, in which the right-hand side of each rule has either two non-terminals or a single terminal.   \n• Lexicalized grammars place more emphasis on the structure of the lexicon, lessening the burden on pure phrase-structure rules.   \n• Combinatorial categorial grammar (CCG) is an important computationally relevant lexicalized approach.",
        "chapter": "Constituency Grammars",
        "section": "Lexicalized Grammars",
        "subsection": "Combinatory Categorial Grammar",
        "subsubsection": "N/A"
    },
    {
        "content": "As we saw with our earlier examples, the first step of this derivation is type raising United to the category $S / ( S backslash N P )$ allowing it to combine with diverted via forward composition. The result of this composition is $S / N P$ which preserves the fact that we are still looking for an $N P$ to fill the missing direct object. The second critical piece is the lexical category assigned to the word that: $( N P backslash N P ) / ( S / N P )$ . This function seeks a verb phrase missing an argument to its right, and transforms it into an $N P$ seeking a missing element to its left, precisely where we find the flight. \nCCGbank \nAs with phrase-structure approaches, treebanks play an important role in CCGbased approaches to parsing. CCGbank (Hockenmaier and Steedman, 2007) is the largest and most widely used CCG treebank. It was created by automatically translating phrase-structure trees from the Penn Treebank via a rule-based approach. The method produced successful translations of over $9 9 %$ of the trees in the Penn Treebank resulting in 48,934 sentences paired with CCG derivations. It also provides a lexicon of 44,000 words with over 1200 categories. Appendix C will discuss how these resources can be used to train CCG parsers. \n12.7 Summary \nThis chapter has introduced a number of fundamental concepts in syntax through the use of context-free grammars. \n• In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars).   \n• A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar.   \n• A generative grammar is a traditional name in linguistics for a formal language that is used to model the grammar of a natural language.   \n• There are many sentence-level grammatical constructions in English; declarative, imperative, yes-no question, and wh-question are four common types; these can be modeled with context-free rules.   \n• An English noun phrase can have determiners, numbers, quantifiers, and adjective phrases preceding the head noun, which can be followed by a number of postmodifiers; gerundive and infinitive VPs are common possibilities.   \n• Subjects in English agree with the main verb in person and number.   \n• Verbs can be subcategorized by the types of complements they expect. Simple subcategories are transitive and intransitive; most grammars include many more categories than these.   \n• Treebanks of parsed sentences exist for many genres of English and for many languages. Treebanks can be searched with tree-search tools.   \n• Any context-free grammar can be converted to Chomsky normal form, in which the right-hand side of each rule has either two non-terminals or a single terminal.   \n• Lexicalized grammars place more emphasis on the structure of the lexicon, lessening the burden on pure phrase-structure rules.   \n• Combinatorial categorial grammar (CCG) is an important computationally relevant lexicalized approach.",
        "chapter": "Constituency Grammars",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "As we saw with our earlier examples, the first step of this derivation is type raising United to the category $S / ( S backslash N P )$ allowing it to combine with diverted via forward composition. The result of this composition is $S / N P$ which preserves the fact that we are still looking for an $N P$ to fill the missing direct object. The second critical piece is the lexical category assigned to the word that: $( N P backslash N P ) / ( S / N P )$ . This function seeks a verb phrase missing an argument to its right, and transforms it into an $N P$ seeking a missing element to its left, precisely where we find the flight. \nCCGbank \nAs with phrase-structure approaches, treebanks play an important role in CCGbased approaches to parsing. CCGbank (Hockenmaier and Steedman, 2007) is the largest and most widely used CCG treebank. It was created by automatically translating phrase-structure trees from the Penn Treebank via a rule-based approach. The method produced successful translations of over $9 9 %$ of the trees in the Penn Treebank resulting in 48,934 sentences paired with CCG derivations. It also provides a lexicon of 44,000 words with over 1200 categories. Appendix C will discuss how these resources can be used to train CCG parsers. \n12.7 Summary \nThis chapter has introduced a number of fundamental concepts in syntax through the use of context-free grammars. \n• In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars).   \n• A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar.   \n• A generative grammar is a traditional name in linguistics for a formal language that is used to model the grammar of a natural language.   \n• There are many sentence-level grammatical constructions in English; declarative, imperative, yes-no question, and wh-question are four common types; these can be modeled with context-free rules.   \n• An English noun phrase can have determiners, numbers, quantifiers, and adjective phrases preceding the head noun, which can be followed by a number of postmodifiers; gerundive and infinitive VPs are common possibilities.   \n• Subjects in English agree with the main verb in person and number.   \n• Verbs can be subcategorized by the types of complements they expect. Simple subcategories are transitive and intransitive; most grammars include many more categories than these.   \n• Treebanks of parsed sentences exist for many genres of English and for many languages. Treebanks can be searched with tree-search tools.   \n• Any context-free grammar can be converted to Chomsky normal form, in which the right-hand side of each rule has either two non-terminals or a single terminal.   \n• Lexicalized grammars place more emphasis on the structure of the lexicon, lessening the burden on pure phrase-structure rules.   \n• Combinatorial categorial grammar (CCG) is an important computationally relevant lexicalized approach.",
        "chapter": "Constituency Grammars",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "As we saw with our earlier examples, the first step of this derivation is type raising United to the category $S / ( S backslash N P )$ allowing it to combine with diverted via forward composition. The result of this composition is $S / N P$ which preserves the fact that we are still looking for an $N P$ to fill the missing direct object. The second critical piece is the lexical category assigned to the word that: $( N P backslash N P ) / ( S / N P )$ . This function seeks a verb phrase missing an argument to its right, and transforms it into an $N P$ seeking a missing element to its left, precisely where we find the flight. \nCCGbank \nAs with phrase-structure approaches, treebanks play an important role in CCGbased approaches to parsing. CCGbank (Hockenmaier and Steedman, 2007) is the largest and most widely used CCG treebank. It was created by automatically translating phrase-structure trees from the Penn Treebank via a rule-based approach. The method produced successful translations of over $9 9 %$ of the trees in the Penn Treebank resulting in 48,934 sentences paired with CCG derivations. It also provides a lexicon of 44,000 words with over 1200 categories. Appendix C will discuss how these resources can be used to train CCG parsers. \n12.7 Summary \nThis chapter has introduced a number of fundamental concepts in syntax through the use of context-free grammars. \n• In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars).   \n• A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar.   \n• A generative grammar is a traditional name in linguistics for a formal language that is used to model the grammar of a natural language.   \n• There are many sentence-level grammatical constructions in English; declarative, imperative, yes-no question, and wh-question are four common types; these can be modeled with context-free rules.   \n• An English noun phrase can have determiners, numbers, quantifiers, and adjective phrases preceding the head noun, which can be followed by a number of postmodifiers; gerundive and infinitive VPs are common possibilities.   \n• Subjects in English agree with the main verb in person and number.   \n• Verbs can be subcategorized by the types of complements they expect. Simple subcategories are transitive and intransitive; most grammars include many more categories than these.   \n• Treebanks of parsed sentences exist for many genres of English and for many languages. Treebanks can be searched with tree-search tools.   \n• Any context-free grammar can be converted to Chomsky normal form, in which the right-hand side of each rule has either two non-terminals or a single terminal.   \n• Lexicalized grammars place more emphasis on the structure of the lexicon, lessening the burden on pure phrase-structure rules.   \n• Combinatorial categorial grammar (CCG) is an important computationally relevant lexicalized approach. \n\nBibliographical and Historical Notes \nAccording to Percival (1976), the idea of breaking up a sentence into a hierarchy of constituents appeared in the V¨olkerpsychologie of the groundbreaking psychologist Wilhelm Wundt (Wundt, 1900): \n...den sprachlichen Ausdruck f¨ur die willk¨urliche Gliederung einer Gesammtvorstellung in ihre in logische Beziehung zueinander gesetzten Bestandteile \n[the linguistic expression for the arbitrary division of a total idea into its constituent parts placed in logical relations to one another] \nWundt’s idea of constituency was taken up into linguistics by Leonard Bloomfield in his early book An Introduction to the Study of Language (Bloomfield, 1914). By the time of his later book, Language (Bloomfield, 1933), what was then called “immediate-constituent analysis” was a well-established method of syntactic study in the United States. By contrast, traditional European grammar, dating from the Classical period, defined relations between words rather than constituents, and European syntacticians retained this emphasis on such dependency grammars, the subject of Chapter 14. \nAmerican Structuralism saw a number of specific definitions of the immediate constituent, couched in terms of their search for a “discovery procedure”: a methodological algorithm for describing the syntax of a language. In general, these attempt to capture the intuition that “The primary criterion of the immediate constituent is the degree in which combinations behave as simple units” (Bazell, 1952/1966, p. 284). The most well known of the specific definitions is Harris’ idea of distributional similarity to individual units, with the substitutability test. Essentially, the method proceeded by breaking up a construction into constituents by attempting to substitute simple structures for possible constituents—if a substitution of a simple form, say, man, was substitutable in a construction for a more complex set (like intense young man), then the form intense young man was probably a constituent. Harris’s test was the beginning of the intuition that a constituent is a kind of equivalence class. \nThe first formalization of this idea of hierarchical constituency was the phrasestructure grammar defined in Chomsky (1956) and further expanded upon (and argued against) in Chomsky (1957) and Chomsky (1956/1975). From this time on, \nX-bar schemata \nmost generative linguistic theories were based at least in part on context-free grammars or generalizations of them (such as Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), the Minimalist Program (Chomsky, 1995), and Construction Grammar (Kay and Fillmore, 1999), inter alia); many of these theories used schematic context-free templates known as $mathbf { X }$ -bar schemata, which also relied on the notion of syntactic head. \nShortly after Chomsky’s initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming language; Backus (1996) noted that he was influenced by the productions of Emil Post and that Naur’s work was independent of his (Backus’) own. After this early work, a great number of computational models of natural language processing were based on context-free grammars because of the early development of efficient algorithms to parse these grammars (see Chapter 13). \nThre are various classes of extensions to CFGs, many designed to handle longdistance dependencies in the syntax. (Other grammars instead treat long-distancedependent items as being related semantically rather than syntactically (Kay and Fillmore 1999, Culicover and Jackendoff 2005). \nOne extended formalism is Tree Adjoining Grammar (TAG) (Joshi, 1985). The primary TAG data structure is the tree, rather than the rule. Trees come in two kinds: initial trees and auxiliary trees. Initial trees might, for example, represent simple sentential structures, and auxiliary trees add recursion into a tree. Trees are combined by two operations called substitution and adjunction. The adjunction operation handles long-distance dependencies. See Joshi (1985) for more details. Tree Adjoining Grammar is a member of the family of mildly context-sensitive languages. \nWe mentioned on page 274 another way of handling long-distance dependencies, based on the use of empty categories and co-indexing. The Penn Treebank uses this model, which draws (in various Treebank corpora) from the Extended Standard Theory and Minimalism (Radford, 1997). \nReaders interested in the grammar of English should get one of the three large reference grammars of English: Huddleston and Pullum (2002), Biber et al. (1999), and Quirk et al. (1985). \ngenerative \nThere are many good introductory textbooks on syntax from different perspectives. Sag et al. (2003) is an introduction to syntax from a generative perspective, focusing on the use of phrase-structure rules, unification, and the type hierarchy in Head-Driven Phrase Structure Grammar. Van Valin, Jr. and La Polla (1997) is an introduction from a functional perspective, focusing on cross-linguistic data and on the functional motivation for syntactic structures. \nfunctional \nExercises \n12.1 Draw tree structures for the following ATIS phrases: \n1. Dallas   \n2. from Denver   \n3. after five p.m.   \n4. arriving in Washington   \n5. early flights   \n6. all redeye flights   \n7. on Thursday   \n8. a one-way fare   \n9. any delays in Denver \n\n12.2 Draw tree structures for the following ATIS sentences: \n1. Does American Airlines have a flight between five a.m. and six a.m.?   \n2. I would like to fly on American Airlines.   \n3. Please repeat that.   \n4. Does American 487 have a first-class section?   \n5. I need to fly between Philadelphia and Atlanta.   \n6. What is the fare from Atlanta to Denver?   \n7. Is there an American Airlines flight from Philadelphia to Dallas? \n12.3 Assume a grammar that has many $V P$ rules for different subcategorizations, as expressed in Section 12.3.4, and differently subcategorized verb rules like Verb-with-NP-complement. How would the rule for postnominal relative clauses (12.4) need to be modified if we wanted to deal properly with examples like the earliest flight that you have? Recall that in such examples the pronoun that is the object of the verb get. Your rules should allow this noun phrase but should correctly rule out the ungrammatical $S ^ { mathrm {  * } } I g e t$ . \n12.4 Does your solution to the previous problem correctly model the NP the earliest flight that I can get? How about the earliest flight that I think my mother wants me to book for her? Hint: this phenomenon is called long-distance dependency. \n12.5 Write rules expressing the verbal subcategory of English auxiliaries; for example, you might have a rule verb-with-bare-stem-VP-complemen $stackrel { prime } {  } c a n$ . \npossessive genitive \n12.6 NPs like Fortune’s office or my uncle’s marks are called possessive or genitive noun phrases. We can model possessive noun phrases by treating the sub-NP like Fortune’s or my uncle’s as a determiner of the following head noun. Write grammar rules for English possessives. You may treat $mathbf { chi } _ { s }$ as if it were a separate word (i.e., as if there were always a space before ’s). \n12.7 Page 267 discussed the need for a Wh-NP constituent. The simplest Wh-NP is one of the Wh-pronouns (who, whom, whose, which). The Wh-words what and which can be determiners: which four will you have?, what credit do you have with the Duke? Write rules for the different types of Wh-NPs. \n12.8 Write an algorithm for converting an arbitrary context-free grammar into Chomsky normal form. \n13 Constituency Parsing \nOne morning I shot an elephant in my pajamas. How he got into my pajamas I don’t know. Groucho Marx, Animal Crackers, 1930 \nSyntactic parsing is the task of assigning a syntactic structure to a sentence. This chapter focuses on constituency structures, those assigned by context-free grammars of the kind described in Chapter 12. In the next chapter we’ll introduce dependency parses, an alternative kind of parse structure, \nParse trees can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for semantic analysis (as we show in Chapter 16) and thus play a role in applications like question answering. For example to answer the question \nWhich flights to Denver depart before the Seattle flight? \nwe’ll need to know that the questioner wants a list of flights going to Denver, not flights going to Seattle, and parse structure (knowing that to Denver modifies flights, and which flights to Denver is the subject of the depart) can help us. \nWe begin by discussing ambiguity and the problems it presents, and then give the Cocke-Kasami-Younger (CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic programming approach to syntactic parsing. We’ve already seen other dynamic programming algorithms like minimum edit distance (Chapter 2) and Viterbi (Chapter 8). \nThe vanilla CKY algorithm returns an efficient representation of the set of parse trees for a sentence, but doesn’t tell us which parse tree is the right one. For that, we need to augment CKY with scores for each possible constituent. We’ll see how to do this with neural span-based parsers. And we’ll introduce other methods like supertagging for parsing CCG, partial parsing methods, for use in situations in which a superficial syntactic analysis of an input may be sufficient, and the standard set of metrics for evaluating parser accuracy. \n13.1 Ambiguity \nstructural ambiguity \nAmbiguity is the most serious problem faced by syntactic parsers. Chapter 8 introduced the notions of part-of-speech ambiguity and part-of-speech disambiguation. Here, we introduce a new kind of ambiguity, called structural ambiguity, illustrated with a new toy grammar $mathcal { L } _ { 1 }$ , shown in Figure 13.1, which adds a few rules to the ${ mathcal { L } } _ { 0 }$ grammar from the last chapter. \nStructural ambiguity occurs when the grammar can assign more than one parse to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal Crackers is ambiguous because the phrase in my pajamas can be part of the NP",
        "chapter": "Constituency Grammars",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "13 Constituency Parsing \nOne morning I shot an elephant in my pajamas. How he got into my pajamas I don’t know. Groucho Marx, Animal Crackers, 1930 \nSyntactic parsing is the task of assigning a syntactic structure to a sentence. This chapter focuses on constituency structures, those assigned by context-free grammars of the kind described in Chapter 12. In the next chapter we’ll introduce dependency parses, an alternative kind of parse structure, \nParse trees can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for semantic analysis (as we show in Chapter 16) and thus play a role in applications like question answering. For example to answer the question \nWhich flights to Denver depart before the Seattle flight? \nwe’ll need to know that the questioner wants a list of flights going to Denver, not flights going to Seattle, and parse structure (knowing that to Denver modifies flights, and which flights to Denver is the subject of the depart) can help us. \nWe begin by discussing ambiguity and the problems it presents, and then give the Cocke-Kasami-Younger (CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic programming approach to syntactic parsing. We’ve already seen other dynamic programming algorithms like minimum edit distance (Chapter 2) and Viterbi (Chapter 8). \nThe vanilla CKY algorithm returns an efficient representation of the set of parse trees for a sentence, but doesn’t tell us which parse tree is the right one. For that, we need to augment CKY with scores for each possible constituent. We’ll see how to do this with neural span-based parsers. And we’ll introduce other methods like supertagging for parsing CCG, partial parsing methods, for use in situations in which a superficial syntactic analysis of an input may be sufficient, and the standard set of metrics for evaluating parser accuracy. \n13.1 Ambiguity \nstructural ambiguity \nAmbiguity is the most serious problem faced by syntactic parsers. Chapter 8 introduced the notions of part-of-speech ambiguity and part-of-speech disambiguation. Here, we introduce a new kind of ambiguity, called structural ambiguity, illustrated with a new toy grammar $mathcal { L } _ { 1 }$ , shown in Figure 13.1, which adds a few rules to the ${ mathcal { L } } _ { 0 }$ grammar from the last chapter. \nStructural ambiguity occurs when the grammar can assign more than one parse to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal Crackers is ambiguous because the phrase in my pajamas can be part of the NP \nattachment ambiguity \nheaded by elephant or a part of the verb phrase headed by shot. Figure 13.2 illustrates these two analyses of Marx’s line using rules from $mathcal { L } _ { 1 }$ . \nStructural ambiguity, appropriately enough, comes in many forms. Two common kinds of ambiguity are attachment ambiguity and coordination ambiguity. A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. The Groucho Marx sentence is an example of PP-attachment ambiguity. Various kinds of adverbial phrases are also subject to this kind of ambiguity. For instance, in the following example the gerundive-VP flying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower or it can be an adjunct modifying the VP headed by saw: \n(13.1) We saw the Eiffel Tower flying to Paris. \nIn coordination ambiguity phrases can be conjoined by a conjunction like and. \nFor example, the phrase old men and women can be bracketed as [old [men and women]], referring to old men and old women, or as [old men] and [women], in which case it is only the men who are old. These ambiguities combine in complex ways in real sentences, like the following news sentence from the Brown corpus: \n(13.2) President Kennedy today pushed aside other White House business to devote all his time and attention to working on the Berlin crisis address he will deliver tomorrow night to the American people over nationwide television and radio. \nThis sentence has a number of ambiguities, although since they are semantically unreasonable, it requires a careful reading to see them. The last noun phrase could be parsed [nationwide [television and radio]] or [[nationwide television] and radio]. The direct object of pushed aside should be other White House business but could also be the bizarre phrase [other White House business to devote all his time and attention to working] (i.e., a structure like Kennedy affirmed [his intention to propose a new budget to address the deficit]). Then the phrase on the Berlin crisis address he will deliver tomorrow night to the American people could be an adjunct modifying the verb pushed. A $P P$ like over nationwide television and radio could be attached to any of the higher VPs or NPs (e.g., it could modify people or night). \nThe fact that there are many grammatically correct but semantically unreasonable parses for naturally occurring sentences is an irksome problem that affects all parsers. Fortunately, the CKY algorithm below is designed to efficiently handle structural ambiguities. And as we’ll see in the following section, we can augment CKY with neural methods to choose a single correct parse by syntactic disambiguation. \n13.2 CKY Parsing: A Dynamic Programming Approach \nchart parsing \nDynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to sub-problems. The complete table has the solution to all the sub-problems needed to solve the problem as a whole. In the case of syntactic parsing, these sub-problems represent parse trees for all the constituents detected in the input. \nThe dynamic programming advantage arises from the context-free nature of our grammar rules—once a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-KasamiYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach, and dynamic programming methods are often referred to as chart parsing methods. \n13.2.1 Conversion to Chomsky Normal Form \nThe CKY algorithm requires grammars to first be in Chomsky Normal Form (CNF). Recall from Chapter 12 that grammars in CNF are restricted to rules of the form $A  B C$ or $A  w$ . That is, the right-hand side of each rule must expand either to two non-terminals or to a single terminal. Restricting a grammar to CNF does not lead to any loss in expressiveness, since any context-free grammar can be converted into a corresponding CNF grammar that accepts exactly the same set of strings as the original grammar.",
        "chapter": "Constituency Parsing",
        "section": "Ambiguity",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "For example, the phrase old men and women can be bracketed as [old [men and women]], referring to old men and old women, or as [old men] and [women], in which case it is only the men who are old. These ambiguities combine in complex ways in real sentences, like the following news sentence from the Brown corpus: \n(13.2) President Kennedy today pushed aside other White House business to devote all his time and attention to working on the Berlin crisis address he will deliver tomorrow night to the American people over nationwide television and radio. \nThis sentence has a number of ambiguities, although since they are semantically unreasonable, it requires a careful reading to see them. The last noun phrase could be parsed [nationwide [television and radio]] or [[nationwide television] and radio]. The direct object of pushed aside should be other White House business but could also be the bizarre phrase [other White House business to devote all his time and attention to working] (i.e., a structure like Kennedy affirmed [his intention to propose a new budget to address the deficit]). Then the phrase on the Berlin crisis address he will deliver tomorrow night to the American people could be an adjunct modifying the verb pushed. A $P P$ like over nationwide television and radio could be attached to any of the higher VPs or NPs (e.g., it could modify people or night). \nThe fact that there are many grammatically correct but semantically unreasonable parses for naturally occurring sentences is an irksome problem that affects all parsers. Fortunately, the CKY algorithm below is designed to efficiently handle structural ambiguities. And as we’ll see in the following section, we can augment CKY with neural methods to choose a single correct parse by syntactic disambiguation. \n13.2 CKY Parsing: A Dynamic Programming Approach \nchart parsing \nDynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to sub-problems. The complete table has the solution to all the sub-problems needed to solve the problem as a whole. In the case of syntactic parsing, these sub-problems represent parse trees for all the constituents detected in the input. \nThe dynamic programming advantage arises from the context-free nature of our grammar rules—once a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-KasamiYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach, and dynamic programming methods are often referred to as chart parsing methods. \n13.2.1 Conversion to Chomsky Normal Form \nThe CKY algorithm requires grammars to first be in Chomsky Normal Form (CNF). Recall from Chapter 12 that grammars in CNF are restricted to rules of the form $A  B C$ or $A  w$ . That is, the right-hand side of each rule must expand either to two non-terminals or to a single terminal. Restricting a grammar to CNF does not lead to any loss in expressiveness, since any context-free grammar can be converted into a corresponding CNF grammar that accepts exactly the same set of strings as the original grammar. \n\nLet’s start with the process of converting a generic CFG into one represented in CNF. Assuming we’re dealing with an $epsilon$ -free grammar, there are three situations we need to address in any generic grammar: rules that mix terminals with non-terminals on the right-hand side, rules that have a single non-terminal on the right-hand side, and rules in which the length of the right-hand side is greater than 2. \nThe remedy for rules that mix terminals and non-terminals is to simply introduce a new dummy non-terminal that covers only the original terminal. For example, a rule for an infinitive verb phrase such as INF- $cdot V P  t o V P$ would be replaced by the two rules $I N F { cdot } V P  T O V P$ and $T O  t o$ . \nRules with a single non-terminal on the right are called unit productions. We can eliminate unit productions by rewriting the right-hand side of the original rules with the right-hand side of all the non-unit production rules that they ultimately lead to. More formally, if $A stackrel { * } { Rightarrow } B$ by a chain of one or more unit productions and $B to gamma$ is a non-unit production in our grammar, then we add $A to gamma$ for each such rule in the grammar and discard all the intervening unit productions. As we demonstrate with our toy grammar, this can lead to a substantial flattening of the grammar and a consequent promotion of terminals to fairly high levels in the resulting trees. \nRules with right-hand sides longer than 2 are normalized through the introduction of new non-terminals that spread the longer sequences over several new rules. Formally, if we have a rule like \nwe replace the leftmost pair of non-terminals with a new non-terminal and introduce a new production, resulting in the following new rules: \nIn the case of longer right-hand sides, we simply iterate this process until the offending rule has been replaced by rules of length 2. The choice of replacing the leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results in binary rules would suffice. \nIn our current grammar, the rule $S  A u x N P V P$ would be replaced by the two rules $S  X I  V P$ and $X I  A u x N P$ . \nThe entire conversion process can be summarized as follows: \n1. Copy all conforming rules to the new grammar unchanged.   \n2. Convert terminals within rules to dummy non-terminals.   \n3. Convert unit productions.   \n4. Make all rules binary and add them to new grammar. \nFigure 13.3 shows the results of applying this entire conversion procedure to the $mathcal { L } _ { 1 }$ grammar introduced earlier on page 289. Note that this figure doesn’t show the original lexical rules; since these original lexical rules are already in CNF, they all carry over unchanged to the new grammar. Figure 13.3 does, however, show the various places where the process of eliminating unit productions has, in effect, created new lexical rules. For example, all the original verbs have been promoted to both $V P mathrm { s }$ and to $ { s } _ { mathrm { s } }$ in the converted grammar. \n13.2.2 CKY Recognition \nWith our grammar now in CNF, each non-terminal node above the part-of-speech level in a parse tree will have exactly two daughters. A two-dimensional matrix can be used to encode the structure of an entire tree. For a sentence of length $n$ , we will work with the upper-triangular portion of an $( n + 1 ) times ( n + 1 )$ matrix. Each cell $[ i , j ]$ in this matrix contains the set of non-terminals that represent all the constituents that span positions $i$ through $j$ of the input. Since our indexing scheme begins with 0, it’s natural to think of the indexes as pointing at the gaps between the input words (as in $phantom { } _ { 0 } B o o k phantom { } _ { 1 }$ that 2 flight 3). These gaps are often called fenceposts, on the metaphor of the posts between segments of fencing. It follows then that the cell that represents the entire input resides in position $[ 0 , n ]$ in the matrix. \nSince each non-terminal entry in our table has two daughters in the parse, it follows that for each constituent represented by an entry $[ i , j ]$ , there must be a position in the input, $k$ , where it can be split into two parts such that $i < k < j$ . Given such a position $k$ , the first constituent $[ i , k ]$ must lie to the left of entry $[ i , j ]$ somewhere along row $i$ , and the second entry $left[ k , j right]$ must lie beneath it, along column $j$ . \nTo make this more concrete, consider the following example with its completed parse matrix, shown in Fig. 13.4. \n(13.3) Book the flight through Houston. \nThe superdiagonal row in the matrix contains the parts of speech for each word in the input. The subsequent diagonals above that superdiagonal contain constituents that cover all the spans of increasing length in the input. \nGiven this setup, CKY recognition consists of filling the parse table in the right way. To do this, we’ll proceed in a bottom-up fashion so that at the point where we are filling any cell $[ i , j ]$ , the cells containing the parts that could contribute to this entry (i.e., the cells to the left and the cells below) have already been filled. The algorithm given in Fig. 13.5 fills the upper-triangular matrix a column at a time working from left to right, with each column filled from bottom to top, as the right side of Fig. 13.4 illustrates. This scheme guarantees that at each point in time we have all the information we need (to the left, since all the columns to the left have already been filled, and below since we’re filling bottom to top). It also mirrors online processing, since filling the columns from left to right corresponds to processing each word one at a time.",
        "chapter": "Constituency Parsing",
        "section": "CKY Parsing: A Dynamic Programming Approach",
        "subsection": "Conversion to Chomsky Normal Form",
        "subsubsection": "N/A"
    },
    {
        "content": "13.2.2 CKY Recognition \nWith our grammar now in CNF, each non-terminal node above the part-of-speech level in a parse tree will have exactly two daughters. A two-dimensional matrix can be used to encode the structure of an entire tree. For a sentence of length $n$ , we will work with the upper-triangular portion of an $( n + 1 ) times ( n + 1 )$ matrix. Each cell $[ i , j ]$ in this matrix contains the set of non-terminals that represent all the constituents that span positions $i$ through $j$ of the input. Since our indexing scheme begins with 0, it’s natural to think of the indexes as pointing at the gaps between the input words (as in $phantom { } _ { 0 } B o o k phantom { } _ { 1 }$ that 2 flight 3). These gaps are often called fenceposts, on the metaphor of the posts between segments of fencing. It follows then that the cell that represents the entire input resides in position $[ 0 , n ]$ in the matrix. \nSince each non-terminal entry in our table has two daughters in the parse, it follows that for each constituent represented by an entry $[ i , j ]$ , there must be a position in the input, $k$ , where it can be split into two parts such that $i < k < j$ . Given such a position $k$ , the first constituent $[ i , k ]$ must lie to the left of entry $[ i , j ]$ somewhere along row $i$ , and the second entry $left[ k , j right]$ must lie beneath it, along column $j$ . \nTo make this more concrete, consider the following example with its completed parse matrix, shown in Fig. 13.4. \n(13.3) Book the flight through Houston. \nThe superdiagonal row in the matrix contains the parts of speech for each word in the input. The subsequent diagonals above that superdiagonal contain constituents that cover all the spans of increasing length in the input. \nGiven this setup, CKY recognition consists of filling the parse table in the right way. To do this, we’ll proceed in a bottom-up fashion so that at the point where we are filling any cell $[ i , j ]$ , the cells containing the parts that could contribute to this entry (i.e., the cells to the left and the cells below) have already been filled. The algorithm given in Fig. 13.5 fills the upper-triangular matrix a column at a time working from left to right, with each column filled from bottom to top, as the right side of Fig. 13.4 illustrates. This scheme guarantees that at each point in time we have all the information we need (to the left, since all the columns to the left have already been filled, and below since we’re filling bottom to top). It also mirrors online processing, since filling the columns from left to right corresponds to processing each word one at a time. \n\nThe outermost loop of the algorithm given in Fig. 13.5 iterates over the columns, and the second loop iterates over the rows, from the bottom up. The purpose of the innermost loop is to range over all the places where a substring spanning $i$ to $j$ in the input might be split in two. As $k$ ranges over the places where the string can be split, the pairs of cells we consider move, in lockstep, to the right along row $i$ and down along column $j$ . Figure 13.6 illustrates the general case of filling cell $[ i , j ]$ . At each such split, the algorithm considers whether the contents of the two cells can be combined in a way that is sanctioned by a rule in the grammar. If such a rule exists, the non-terminal on its left-hand side is entered into the table. \nFigure 13.7 shows how the five cells of column 5 of the table are filled after the word Houston is read. The arrows point out the two spans that are being used to add an entry to the table. Note that the action in cell [0, 5] indicates the presence of three alternative parses for this input, one where the $P P$ modifies the flight, one where it modifies the booking, and one that captures the second argument in the original $V P  V e r b N P P P P P$ rule, now captured indirectly with the $V P  X 2  : P P$ rule. \n\n13.2.3 CKY Parsing \nThe algorithm given in Fig. 13.5 is a recognizer, not a parser. That is, it can tell us whether a valid parse exists for a given sentence based on whether or not if finds an $S$ in cell $[ 0 , n ]$ , but it can’t provide the derivation, which is the actual job for a parser. To turn it into a parser capable of returning all possible parses for a given input, we can make two simple changes to the algorithm: the first change is to augment the entries in the table so that each non-terminal is paired with pointers to the table entries from which it was derived (more or less as shown in Fig. 13.7), the second change is to permit multiple versions of the same non-terminal to be entered into the table (again as shown in Fig. 13.7). With these changes, the completed table contains all the possible parses for a given input. Returning an arbitrary single parse consists of choosing an $S$ from cell $[ 0 , n ]$ and then recursively retrieving its component constituents from the table. \nReturning every parse for a sentence may not be useful, since there may be an exponential number of parses. We’ll see in the next section how to retrieve only the best parse.",
        "chapter": "Constituency Parsing",
        "section": "CKY Parsing: A Dynamic Programming Approach",
        "subsection": "CKY Recognition",
        "subsubsection": "N/A"
    },
    {
        "content": "13.2.3 CKY Parsing \nThe algorithm given in Fig. 13.5 is a recognizer, not a parser. That is, it can tell us whether a valid parse exists for a given sentence based on whether or not if finds an $S$ in cell $[ 0 , n ]$ , but it can’t provide the derivation, which is the actual job for a parser. To turn it into a parser capable of returning all possible parses for a given input, we can make two simple changes to the algorithm: the first change is to augment the entries in the table so that each non-terminal is paired with pointers to the table entries from which it was derived (more or less as shown in Fig. 13.7), the second change is to permit multiple versions of the same non-terminal to be entered into the table (again as shown in Fig. 13.7). With these changes, the completed table contains all the possible parses for a given input. Returning an arbitrary single parse consists of choosing an $S$ from cell $[ 0 , n ]$ and then recursively retrieving its component constituents from the table. \nReturning every parse for a sentence may not be useful, since there may be an exponential number of parses. We’ll see in the next section how to retrieve only the best parse. \n13.2.4 CKY in Practice \nFinally, we should note that while the restriction to CNF does not pose a problem theoretically, it does pose some non-trivial problems in practice. Obviously, as things stand now, our parser isn’t returning trees that are consistent with the grammar given to us by our friendly syntacticians. In addition to making our grammar developers unhappy, the conversion to CNF will complicate any syntax-driven approach to semantic analysis. \nOne approach to getting around these problems is to keep enough information around to transform our trees back to the original grammar as a post-processing step of the parse. This is trivial in the case of the transformation used for rules with length greater than 2. Simply deleting the new dummy non-terminals and promoting their daughters restores the original tree. \nIn the case of unit productions, it turns out to be more convenient to alter the basic CKY algorithm to handle them directly than it is to store the information needed to recover the correct trees. Exercise 13.3 asks you to make this change. Many of the probabilistic parsers presented in Appendix C use the CKY algorithm altered in just this manner. \n13.3 Span-Based Neural Constituency Parsing \nWhile the CKY parsing algorithm we’ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn’t tell us which parse is the correct one! That is, it doesn’t disambiguate among the possible parses. To solve the disambiguation problem we’ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. \nHere we’ll describe a version of the algorithm from Kitaev et al. (2019). This parser learns to map a span of words to a constituent, and, like CKY, hierarchically combines larger and larger spans to build the parse-tree bottom-up. But unlike classic CKY, this parser doesn’t use the hand-written grammar to constrain what constituents can be combined, instead just relying on the learned neural representations of spans to encode likely combinations. \n13.3.1 Computing Scores for a Span \nspan Let’s begin by considering just the constituent (we’ll call it a span) that lies between fencepost positions $i$ and $j$ with non-terminal symbol label l. We’ll build a classifier to assign a score $s ( i , j , l )$ to this constituent span. \nFig. 13.8 sketches the architecture. The input word tokens are embedded by passing them through a pretrained language model like BERT. Because BERT operates on the level of subword (wordpiece) tokens rather than words, we’ll first need to convert the BERT outputs to word representations. One standard way of doing this is to simply use the last subword unit as the representation for the word (using the first subword unit seems to work equivalently well). The embeddings can then be passed through some postprocessing layers; Kitaev et al. (2019), for example, use 8 Transformer layers.",
        "chapter": "Constituency Parsing",
        "section": "CKY Parsing: A Dynamic Programming Approach",
        "subsection": "CKY Parsing",
        "subsubsection": "N/A"
    },
    {
        "content": "13.2.4 CKY in Practice \nFinally, we should note that while the restriction to CNF does not pose a problem theoretically, it does pose some non-trivial problems in practice. Obviously, as things stand now, our parser isn’t returning trees that are consistent with the grammar given to us by our friendly syntacticians. In addition to making our grammar developers unhappy, the conversion to CNF will complicate any syntax-driven approach to semantic analysis. \nOne approach to getting around these problems is to keep enough information around to transform our trees back to the original grammar as a post-processing step of the parse. This is trivial in the case of the transformation used for rules with length greater than 2. Simply deleting the new dummy non-terminals and promoting their daughters restores the original tree. \nIn the case of unit productions, it turns out to be more convenient to alter the basic CKY algorithm to handle them directly than it is to store the information needed to recover the correct trees. Exercise 13.3 asks you to make this change. Many of the probabilistic parsers presented in Appendix C use the CKY algorithm altered in just this manner. \n13.3 Span-Based Neural Constituency Parsing \nWhile the CKY parsing algorithm we’ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn’t tell us which parse is the correct one! That is, it doesn’t disambiguate among the possible parses. To solve the disambiguation problem we’ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. \nHere we’ll describe a version of the algorithm from Kitaev et al. (2019). This parser learns to map a span of words to a constituent, and, like CKY, hierarchically combines larger and larger spans to build the parse-tree bottom-up. But unlike classic CKY, this parser doesn’t use the hand-written grammar to constrain what constituents can be combined, instead just relying on the learned neural representations of spans to encode likely combinations. \n13.3.1 Computing Scores for a Span \nspan Let’s begin by considering just the constituent (we’ll call it a span) that lies between fencepost positions $i$ and $j$ with non-terminal symbol label l. We’ll build a classifier to assign a score $s ( i , j , l )$ to this constituent span. \nFig. 13.8 sketches the architecture. The input word tokens are embedded by passing them through a pretrained language model like BERT. Because BERT operates on the level of subword (wordpiece) tokens rather than words, we’ll first need to convert the BERT outputs to word representations. One standard way of doing this is to simply use the last subword unit as the representation for the word (using the first subword unit seems to work equivalently well). The embeddings can then be passed through some postprocessing layers; Kitaev et al. (2019), for example, use 8 Transformer layers.",
        "chapter": "Constituency Parsing",
        "section": "CKY Parsing: A Dynamic Programming Approach",
        "subsection": "CKY in Practice",
        "subsubsection": "N/A"
    },
    {
        "content": "13.2.4 CKY in Practice \nFinally, we should note that while the restriction to CNF does not pose a problem theoretically, it does pose some non-trivial problems in practice. Obviously, as things stand now, our parser isn’t returning trees that are consistent with the grammar given to us by our friendly syntacticians. In addition to making our grammar developers unhappy, the conversion to CNF will complicate any syntax-driven approach to semantic analysis. \nOne approach to getting around these problems is to keep enough information around to transform our trees back to the original grammar as a post-processing step of the parse. This is trivial in the case of the transformation used for rules with length greater than 2. Simply deleting the new dummy non-terminals and promoting their daughters restores the original tree. \nIn the case of unit productions, it turns out to be more convenient to alter the basic CKY algorithm to handle them directly than it is to store the information needed to recover the correct trees. Exercise 13.3 asks you to make this change. Many of the probabilistic parsers presented in Appendix C use the CKY algorithm altered in just this manner. \n13.3 Span-Based Neural Constituency Parsing \nWhile the CKY parsing algorithm we’ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn’t tell us which parse is the correct one! That is, it doesn’t disambiguate among the possible parses. To solve the disambiguation problem we’ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. \nHere we’ll describe a version of the algorithm from Kitaev et al. (2019). This parser learns to map a span of words to a constituent, and, like CKY, hierarchically combines larger and larger spans to build the parse-tree bottom-up. But unlike classic CKY, this parser doesn’t use the hand-written grammar to constrain what constituents can be combined, instead just relying on the learned neural representations of spans to encode likely combinations. \n13.3.1 Computing Scores for a Span \nspan Let’s begin by considering just the constituent (we’ll call it a span) that lies between fencepost positions $i$ and $j$ with non-terminal symbol label l. We’ll build a classifier to assign a score $s ( i , j , l )$ to this constituent span. \nFig. 13.8 sketches the architecture. The input word tokens are embedded by passing them through a pretrained language model like BERT. Because BERT operates on the level of subword (wordpiece) tokens rather than words, we’ll first need to convert the BERT outputs to word representations. One standard way of doing this is to simply use the last subword unit as the representation for the word (using the first subword unit seems to work equivalently well). The embeddings can then be passed through some postprocessing layers; Kitaev et al. (2019), for example, use 8 Transformer layers. \nThe resulting word encoder outputs $y _ { t }$ are then use to compute a span score. First, we must map the word encodings (indexed by word positions) to span encodings (indexed by fenceposts). We do this by representing each fencepost with two separate values; the intuition is that a span endpoint to the right of a word represents different information than a span endpoint to the left of a word. We convert each word output $y _ { t }$ into a (leftward-pointing) value for spans ending at this fencepost, $vec { boldsymbol { y } } _ { t }$ , and a (rightward-pointing) value $left. right.$ for spans beginning at this fencepost, by splitting $y _ { t }$ into two halves. Each span then stretches from one double-vector fencepost to another, as in the following representation of the flight, which is span $( 1 , 3 )$ : \nA traditional way to represent a span, developed originally for RNN-based models (Wang and Chang, 2016), but extended also to Transformers, is to take the difference between the embeddings of its start and end, i.e., representing span $( i , j )$ by subtracting the embedding of $i$ from the embedding of $j$ . Here we represent a span by concatenating the difference of each of its fencepost components: \nThe span vector $nu$ is then passed through an MLP span classifier, with two fullyconnected layers and one ReLU activation function, whose output dimensionality is the number of possible non-terminal labels: \nThe MLP then outputs a score for each possible non-terminal. \n13.3.2 Integrating Span Scores into a Parse \nNow we have a score for each labeled constituent span $s ( i , j , l )$ . But we need a score for an entire parse tree. Formally a tree $T$ is represented as a set of $| T |$ such labeled spans, with the $t ^ { mathrm { t h } }$ span starting at position $i _ { t }$ and ending at position $j _ { t }$ , with label $l _ { t }$ : \nThus once we have a score for each span, the parser can compute a score for the whole tree $s ( T )$ simply by summing over the scores of its constituent spans: \nAnd we can choose the final parse tree as the tree with the maximum score: \nThe simplest method to produce the most likely parse is to greedily choose the highest scoring label for each span. This greedy method is not guaranteed to produce a tree, since the best label for a span might not fit into a complete tree. In practice, however, the greedy method tends to find trees; in their experiments Gaddy et al. (2018) finds that $9 5 %$ of predicted bracketings form valid trees. \nNonetheless it is more common to use a variant of the CKY algorithm to find the full parse. The variant defined in Gaddy et al. (2018) works as follows. Let’s define $s _ { mathrm { b e s t } } ( i , j )$ as the score of the best subtree spanning $( i , j )$ . For spans of length one, we choose the best label: \nFor other spans $( i , j )$ , the recursion is: \nNote that the parser is using the max label for span $( i , j ) +$ the max labels for spans $( i , k )$ and $( k , j )$ without worrying about whether those decisions make sense given a grammar. The role of the grammar in classical parsing is to help constrain possible combinations of constituents (NPs like to be followed by VPs). By contrast, the neural model seems to learn these kinds of contextual constraints during its mapping from spans to non-terminals. \nFor more details on span-based parsing, including the margin-based training algorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). \n13.4 Evaluating Parsers \nThe standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or “gold standard”) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank.",
        "chapter": "Constituency Parsing",
        "section": "Span-Based Neural Constituency Parsing",
        "subsection": "Computing Scores for a Span",
        "subsubsection": "N/A"
    },
    {
        "content": "13.3.2 Integrating Span Scores into a Parse \nNow we have a score for each labeled constituent span $s ( i , j , l )$ . But we need a score for an entire parse tree. Formally a tree $T$ is represented as a set of $| T |$ such labeled spans, with the $t ^ { mathrm { t h } }$ span starting at position $i _ { t }$ and ending at position $j _ { t }$ , with label $l _ { t }$ : \nThus once we have a score for each span, the parser can compute a score for the whole tree $s ( T )$ simply by summing over the scores of its constituent spans: \nAnd we can choose the final parse tree as the tree with the maximum score: \nThe simplest method to produce the most likely parse is to greedily choose the highest scoring label for each span. This greedy method is not guaranteed to produce a tree, since the best label for a span might not fit into a complete tree. In practice, however, the greedy method tends to find trees; in their experiments Gaddy et al. (2018) finds that $9 5 %$ of predicted bracketings form valid trees. \nNonetheless it is more common to use a variant of the CKY algorithm to find the full parse. The variant defined in Gaddy et al. (2018) works as follows. Let’s define $s _ { mathrm { b e s t } } ( i , j )$ as the score of the best subtree spanning $( i , j )$ . For spans of length one, we choose the best label: \nFor other spans $( i , j )$ , the recursion is: \nNote that the parser is using the max label for span $( i , j ) +$ the max labels for spans $( i , k )$ and $( k , j )$ without worrying about whether those decisions make sense given a grammar. The role of the grammar in classical parsing is to help constrain possible combinations of constituents (NPs like to be followed by VPs). By contrast, the neural model seems to learn these kinds of contextual constraints during its mapping from spans to non-terminals. \nFor more details on span-based parsing, including the margin-based training algorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). \n13.4 Evaluating Parsers \nThe standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or “gold standard”) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank.",
        "chapter": "Constituency Parsing",
        "section": "Span-Based Neural Constituency Parsing",
        "subsection": "Integrating Span Scores into a Parse",
        "subsubsection": "N/A"
    },
    {
        "content": "13.3.2 Integrating Span Scores into a Parse \nNow we have a score for each labeled constituent span $s ( i , j , l )$ . But we need a score for an entire parse tree. Formally a tree $T$ is represented as a set of $| T |$ such labeled spans, with the $t ^ { mathrm { t h } }$ span starting at position $i _ { t }$ and ending at position $j _ { t }$ , with label $l _ { t }$ : \nThus once we have a score for each span, the parser can compute a score for the whole tree $s ( T )$ simply by summing over the scores of its constituent spans: \nAnd we can choose the final parse tree as the tree with the maximum score: \nThe simplest method to produce the most likely parse is to greedily choose the highest scoring label for each span. This greedy method is not guaranteed to produce a tree, since the best label for a span might not fit into a complete tree. In practice, however, the greedy method tends to find trees; in their experiments Gaddy et al. (2018) finds that $9 5 %$ of predicted bracketings form valid trees. \nNonetheless it is more common to use a variant of the CKY algorithm to find the full parse. The variant defined in Gaddy et al. (2018) works as follows. Let’s define $s _ { mathrm { b e s t } } ( i , j )$ as the score of the best subtree spanning $( i , j )$ . For spans of length one, we choose the best label: \nFor other spans $( i , j )$ , the recursion is: \nNote that the parser is using the max label for span $( i , j ) +$ the max labels for spans $( i , k )$ and $( k , j )$ without worrying about whether those decisions make sense given a grammar. The role of the grammar in classical parsing is to help constrain possible combinations of constituents (NPs like to be followed by VPs). By contrast, the neural model seems to learn these kinds of contextual constraints during its mapping from spans to non-terminals. \nFor more details on span-based parsing, including the margin-based training algorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). \n13.4 Evaluating Parsers \nThe standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or “gold standard”) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank. \n\nA constituent in a hypothesis parse $C _ { h }$ of a sentence $s$ is labeled correct if there is a constituent in the reference parse $C _ { r }$ with the same starting point, ending point, and non-terminal symbol. We can then measure the precision and recall just as for tasks we’ve seen already like named entity tagging: \n# of correct constituents in hypothesis parse of $s$ labeled recall: $mathbf { Sigma } = mathbf { Sigma }$ # of correct constituents in reference parse of $s$ \n$= { frac { #  : ( } { # } }$ of correct constituents in hypothesis parse of $s$ labeled precision: of total constituents in hypothesis parse of $s$ \nAs usual, we often report a combination of the two, $mathrm { F } _ { 1 }$ : \nWe additionally use a new metric, crossing brackets, for each sentence $s$ : \ncross-brackets: the number of constituents for which the reference parse has a bracketing such as ((A B) C) but the hypothesis parse has a bracketing such as (A (B C)). \nFor comparing parsers that use different grammars, the PARSEVAL metric includes a canonicalization algorithm for removing information likely to be grammarspecific (auxiliaries, pre-infinitival “to”, etc.) and for computing a simplified score (Black et al., 1991). The canonical implementation of the PARSEVAL metrics is called evalb (Sekine and Collins, 1997). \nevalb \n13.5 Partial Parsing \npartial parse shallow parse \nchunking \nMany language processing tasks do not require complex, complete parse trees for all inputs. For these tasks, a partial parse, or shallow parse, of input sentences may be sufficient. For example, information extraction systems generally do not extract all the possible information from a text: they simply identify and classify the segments in a text that are likely to contain valuable information. \nOne kind of partial parsing is known as chunking. Chunking is the process of identifying and classifying the flat, non-overlapping segments of a sentence that constitute the basic non-recursive phrases corresponding to the major content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and prepositional phrases. The task of finding all the base noun phrases in a text is particularly common. Since chunked texts lack a hierarchical structure, a simple bracketing notation is sufficient to denote the location and the type of the chunks in a given example: \n(13.12) [NP The morning flight] $[ { cal P P }$ from] [NP Denver] [VP has arrived.] \nThis bracketing notation makes clear the two fundamental tasks that are involved in chunking: segmenting (finding the non-overlapping extents of the chunks) and labeling (assigning the correct tag to the discovered chunks). Some input words may not be part of any chunk, particularly in tasks like base NP:",
        "chapter": "Constituency Parsing",
        "section": "Evaluating Parsers",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A constituent in a hypothesis parse $C _ { h }$ of a sentence $s$ is labeled correct if there is a constituent in the reference parse $C _ { r }$ with the same starting point, ending point, and non-terminal symbol. We can then measure the precision and recall just as for tasks we’ve seen already like named entity tagging: \n# of correct constituents in hypothesis parse of $s$ labeled recall: $mathbf { Sigma } = mathbf { Sigma }$ # of correct constituents in reference parse of $s$ \n$= { frac { #  : ( } { # } }$ of correct constituents in hypothesis parse of $s$ labeled precision: of total constituents in hypothesis parse of $s$ \nAs usual, we often report a combination of the two, $mathrm { F } _ { 1 }$ : \nWe additionally use a new metric, crossing brackets, for each sentence $s$ : \ncross-brackets: the number of constituents for which the reference parse has a bracketing such as ((A B) C) but the hypothesis parse has a bracketing such as (A (B C)). \nFor comparing parsers that use different grammars, the PARSEVAL metric includes a canonicalization algorithm for removing information likely to be grammarspecific (auxiliaries, pre-infinitival “to”, etc.) and for computing a simplified score (Black et al., 1991). The canonical implementation of the PARSEVAL metrics is called evalb (Sekine and Collins, 1997). \nevalb \n13.5 Partial Parsing \npartial parse shallow parse \nchunking \nMany language processing tasks do not require complex, complete parse trees for all inputs. For these tasks, a partial parse, or shallow parse, of input sentences may be sufficient. For example, information extraction systems generally do not extract all the possible information from a text: they simply identify and classify the segments in a text that are likely to contain valuable information. \nOne kind of partial parsing is known as chunking. Chunking is the process of identifying and classifying the flat, non-overlapping segments of a sentence that constitute the basic non-recursive phrases corresponding to the major content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and prepositional phrases. The task of finding all the base noun phrases in a text is particularly common. Since chunked texts lack a hierarchical structure, a simple bracketing notation is sufficient to denote the location and the type of the chunks in a given example: \n(13.12) [NP The morning flight] $[ { cal P P }$ from] [NP Denver] [VP has arrived.] \nThis bracketing notation makes clear the two fundamental tasks that are involved in chunking: segmenting (finding the non-overlapping extents of the chunks) and labeling (assigning the correct tag to the discovered chunks). Some input words may not be part of any chunk, particularly in tasks like base NP: \n(13.13) [NP The morning flight] from [NP Denver] has arrived. \nWhat constitutes a syntactic base phrase depends on the application (and whether the phrases come from a treebank). Nevertheless, some standard guidelines are followed in most systems. First and foremost, base phrases of a given type do not recursively contain any constituents of the same type. Eliminating this kind of recursion leaves us with the problem of determining the boundaries of the non-recursive phrases. In most approaches, base phrases include the headword of the phrase, along with any pre-head material within the constituent, while crucially excluding any post-head material. Eliminating post-head modifiers obviates the need to resolve attachment ambiguities. This exclusion does lead to certain oddities, such as $P P mathrm { s }$ and $V P mathrm { s }$ often consisting solely of their heads. Thus a flight from Indianapolis to Houston would be reduced to the following: \n(13.14) [NP a flight] $[ { cal P P }$ from] [NP Indianapolis][PP to][NP Houston] \nChunking Algorithms Chunking is generally done via supervised learning, training a BIO sequence labeler of the sort we saw in Chapter 8 from annotated training data. Recall that in BIO tagging, we have a tag for the beginning (B) and inside (I) of each chunk type, and one for tokens outside (O) any chunk. The following example shows the bracketing notation of (13.12) on page 299 reframed as a tagging task: \n(13.15) The morning flight from Denver has arrived B NP I NP I NP B PP B NP B VP I VP \nThe same sentence with only the base-NPs tagged illustrates the role of the O tags. \n(13.16) The morning flight from Denver has arrived. \nB NP I NP I NP O B NP O O \nSince annotation efforts are expensive and time consuming, chunkers usually rely on existing treebanks like the Penn Treebank, extracting syntactic phrases from the full parse constituents of a sentence, finding the appropriate heads and then including the material to the left of the head, ignoring the text to the right. This is somewhat error-prone since it relies on the accuracy of the head-finding rules described in Chapter 12. \nGiven a training set, any sequence model can be used to chunk: CRF, RNN, Transformer, etc. As with the evaluation of named-entity taggers, the evaluation of chunkers proceeds by comparing chunker output with gold-standard answers provided by human annotators, using precision, recall, and $mathrm { F } _ { 1 }$ . \n13.6 CCG Parsing \nLexicalized grammar frameworks such as CCG pose problems for which the phrasebased methods we’ve been discussing are not particularly well-suited. To quickly review, CCG consists of three major parts: a set of categories, a lexicon that associates words with categories, and a set of rules that govern how categories combine in context. Categories can be either atomic elements, such as $S$ and $N P$ , or functions such as $( S backslash N P ) / N P$ which specifies the transitive verb category. Rules specify how functions, their arguments, and other functions combine. For example, the following rule templates, forward and backward function application, specify the way that functions apply to their arguments.",
        "chapter": "Constituency Parsing",
        "section": "Partial Parsing",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The first rule applies a function to its argument on the right, while the second looks to the left for its argument. The result of applying either of these rules is the category specified as the value of the function being applied. For the purposes of this discussion, we’ll rely on these two rules along with the forward and backward composition rules and type-raising, as described in Chapter 12. \n13.6.1 Ambiguity in CCG \nAs is always the case in parsing, managing ambiguity is the key to successful CCG parsing. The difficulties with CCG parsing arise from the ambiguity caused by the large number of complex lexical categories combined with the very general nature of the grammatical rules. To see some of the ways that ambiguity arises in a categorial framework, consider the following example. \n(13.17) United diverted the flight to Reno. \nOur grasp of the role of the flight in this example depends on whether the prepositional phrase to Reno is taken as a modifier of the flight, as a modifier of the entire verb phrase, or as a potential second argument to the verb divert. In a context-free grammar approach, this ambiguity would manifest itself as a choice among the following rules in the grammar. \nIn a phrase-structure approach we would simply assign the word to to the category $P$ allowing it to combine with Reno to form a prepositional phrase. The subsequent choice of grammar rules would then dictate the ultimate derivation. In the categorial approach, we can associate to with distinct categories to reflect the ways in which it might interact with other elements in a sentence. The fairly abstract combinatoric rules would then sort out which derivations are possible. Therefore, the source of ambiguity arises not from the grammar but rather from the lexicon. \nLet’s see how this works by considering several possible derivations for this example. To capture the case where the prepositional phrase to Reno modifies the flight, we assign the preposition $t o$ the category $( N P backslash N P ) / N P$ , which gives rise to the following derivation. \nHere, the category assigned to to expects to find two arguments: one to the right as with a traditional preposition, and one to the left that corresponds to the $N P$ to be modified. \nAlternatively, we could assign to to the category $( S backslash S ) / N P$ , which permits the following derivation where to Reno modifies the preceding verb phrase. \nA third possibility is to view divert as a ditransitive verb by assigning it to the category $( ( S backslash N P ) / P P ) / N P$ , while treating to Reno as a simple prepositional phrase. \nWhile CCG parsers are still subject to ambiguity arising from the choice of grammar rules, including the kind of spurious ambiguity discussed in Chapter 12, it should be clear that the choice of lexical categories is the primary problem to be addressed in CCG parsing. \n13.6.2 CCG Parsing Frameworks \nSince the rules in combinatory grammars are either binary or unary, a bottom-up, tabular approach based on the CKY algorithm should be directly applicable to CCG parsing. Unfortunately, the large number of lexical categories available for each word, combined with the promiscuity of CCG’s combinatoric rules, leads to an explosion in the number of (mostly useless) constituents added to the parsing table. The key to managing this explosion of zombie constituents is to accurately assess and exploit the most likely lexical categories possible for each word—a process called supertagging. \nThe following sections describe two approaches to CCG parsing that make use of supertags. Section 13.6.4, presents an approach that structures the parsing process as a heuristic search through the use of the $mathbf { A } ^ { * }$ algorithm. The following section then briefly describes a more traditional classifier-based approach that manages the search space complexity through the use of adaptive supertagging—a process that iteratively considers more and more tags until a parse is found. \nsupertagging \n13.6.3 Supertagging \nChapter 8 introduced the task of part-of-speech tagging, the process of assigning the correct lexical category to each word in a sentence. Supertagging is the corresponding task for highly lexicalized grammar frameworks, where the assigned tags often dictate much of the derivation for a sentence (Bangalore and Joshi, 1999). \nCCG supertaggers rely on treebanks such as CCGbank to provide both the overall set of lexical categories as well as the allowable category assignments for each word in the lexicon. CCGbank includes over 1000 lexical categories, however, in practice, most supertaggers limit their tagsets to those tags that occur at least 10 times in the training corpus. This results in a total of around 425 lexical categories available for use in the lexicon. Note that even this smaller number is large in contrast to the $4 5 mathrm { P O S }$ types used by the Penn Treebank tagset.",
        "chapter": "Constituency Parsing",
        "section": "CCG Parsing",
        "subsection": "Ambiguity in CCG",
        "subsubsection": "N/A"
    },
    {
        "content": "A third possibility is to view divert as a ditransitive verb by assigning it to the category $( ( S backslash N P ) / P P ) / N P$ , while treating to Reno as a simple prepositional phrase. \nWhile CCG parsers are still subject to ambiguity arising from the choice of grammar rules, including the kind of spurious ambiguity discussed in Chapter 12, it should be clear that the choice of lexical categories is the primary problem to be addressed in CCG parsing. \n13.6.2 CCG Parsing Frameworks \nSince the rules in combinatory grammars are either binary or unary, a bottom-up, tabular approach based on the CKY algorithm should be directly applicable to CCG parsing. Unfortunately, the large number of lexical categories available for each word, combined with the promiscuity of CCG’s combinatoric rules, leads to an explosion in the number of (mostly useless) constituents added to the parsing table. The key to managing this explosion of zombie constituents is to accurately assess and exploit the most likely lexical categories possible for each word—a process called supertagging. \nThe following sections describe two approaches to CCG parsing that make use of supertags. Section 13.6.4, presents an approach that structures the parsing process as a heuristic search through the use of the $mathbf { A } ^ { * }$ algorithm. The following section then briefly describes a more traditional classifier-based approach that manages the search space complexity through the use of adaptive supertagging—a process that iteratively considers more and more tags until a parse is found. \nsupertagging \n13.6.3 Supertagging \nChapter 8 introduced the task of part-of-speech tagging, the process of assigning the correct lexical category to each word in a sentence. Supertagging is the corresponding task for highly lexicalized grammar frameworks, where the assigned tags often dictate much of the derivation for a sentence (Bangalore and Joshi, 1999). \nCCG supertaggers rely on treebanks such as CCGbank to provide both the overall set of lexical categories as well as the allowable category assignments for each word in the lexicon. CCGbank includes over 1000 lexical categories, however, in practice, most supertaggers limit their tagsets to those tags that occur at least 10 times in the training corpus. This results in a total of around 425 lexical categories available for use in the lexicon. Note that even this smaller number is large in contrast to the $4 5 mathrm { P O S }$ types used by the Penn Treebank tagset.",
        "chapter": "Constituency Parsing",
        "section": "CCG Parsing",
        "subsection": "CCG Parsing Frameworks",
        "subsubsection": "N/A"
    },
    {
        "content": "A third possibility is to view divert as a ditransitive verb by assigning it to the category $( ( S backslash N P ) / P P ) / N P$ , while treating to Reno as a simple prepositional phrase. \nWhile CCG parsers are still subject to ambiguity arising from the choice of grammar rules, including the kind of spurious ambiguity discussed in Chapter 12, it should be clear that the choice of lexical categories is the primary problem to be addressed in CCG parsing. \n13.6.2 CCG Parsing Frameworks \nSince the rules in combinatory grammars are either binary or unary, a bottom-up, tabular approach based on the CKY algorithm should be directly applicable to CCG parsing. Unfortunately, the large number of lexical categories available for each word, combined with the promiscuity of CCG’s combinatoric rules, leads to an explosion in the number of (mostly useless) constituents added to the parsing table. The key to managing this explosion of zombie constituents is to accurately assess and exploit the most likely lexical categories possible for each word—a process called supertagging. \nThe following sections describe two approaches to CCG parsing that make use of supertags. Section 13.6.4, presents an approach that structures the parsing process as a heuristic search through the use of the $mathbf { A } ^ { * }$ algorithm. The following section then briefly describes a more traditional classifier-based approach that manages the search space complexity through the use of adaptive supertagging—a process that iteratively considers more and more tags until a parse is found. \nsupertagging \n13.6.3 Supertagging \nChapter 8 introduced the task of part-of-speech tagging, the process of assigning the correct lexical category to each word in a sentence. Supertagging is the corresponding task for highly lexicalized grammar frameworks, where the assigned tags often dictate much of the derivation for a sentence (Bangalore and Joshi, 1999). \nCCG supertaggers rely on treebanks such as CCGbank to provide both the overall set of lexical categories as well as the allowable category assignments for each word in the lexicon. CCGbank includes over 1000 lexical categories, however, in practice, most supertaggers limit their tagsets to those tags that occur at least 10 times in the training corpus. This results in a total of around 425 lexical categories available for use in the lexicon. Note that even this smaller number is large in contrast to the $4 5 mathrm { P O S }$ types used by the Penn Treebank tagset. \n\nAs with traditional part-of-speech tagging, the standard approach to building a CCG supertagger is to use supervised machine learning to build a sequence labeler from hand-annotated training data. To find the most likely sequence of tags given a sentence, it is most common to use a neural sequence model, either RNN or Transformer. \nIt’s also possible, however, to use the CRF tagging model described in Chapter 8, using similar features; the current word $w _ { i }$ , its surrounding words within $l$ words, local POS tags and character suffixes, and the supertag from the prior timestep, training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm as described in Chapter 8. \nUnfortunately the large number of possible supertags combined with high perword ambiguity leads the naive CRF algorithm to error rates that are too high for practical use in a parser. The single best tag sequence $hat { T }$ will typically contain too many incorrect tags for effective parsing to take place. To overcome this, we instead return a probability distribution over the possible supertags for each word in the input. The following table illustrates an example distribution for a simple sentence, in which each column represents the probability of each supertag for a given word in the context of the input sentence. The “...” represent all the remaining supertags possible for each word. \nTo get the probability of each possible word/tag pair, we’ll need to sum the probabilities of all the supertag sequences that contain that tag at that location. This can be done with the forward-backward algorithm that is also used to train the CRF, described in Appendix A. \n13.6.4 CCG Parsing using the $mathbf { A } ^ { * }$ Algorithm \nThe $mathbf { A } ^ { * }$ algorithm is a heuristic search method that employs an agenda to find an optimal solution. Search states representing partial solutions are added to an agenda based on a cost function, with the least-cost option being selected for further exploration at each iteration. When a state representing a complete solution is first selected from the agenda, it is guaranteed to be optimal and the search terminates. \nThe $mathbf { A } ^ { * }$ cost function, $f ( n )$ , is used to efficiently guide the search to a solution. The $f$ -cost has two components: $g ( n )$ , the exact cost of the partial solution represented by the state $n$ , and $h ( n )$ a heuristic approximation of the cost of a solution that makes use of $n$ . When $h ( n )$ satisfies the criteria of not overestimating the actual cost, $mathbf { A } ^ { * }$ will find an optimal solution. Not surprisingly, the closer the heuristic can get to the actual cost, the more effective $mathbf { A } ^ { * }$ is at finding a solution without having to explore a significant portion of the solution space. \nWhen applied to parsing, search states correspond to edges representing completed constituents. Each edge specifies a constituent’s start and end positions, its grammatical category, and its $f$ -cost. Here, the $g$ component represents the current cost of an edge and the $h$ component represents an estimate of the cost to complete a derivation that makes use of that edge. The use of $mathbf { A } ^ { * }$ for phrase structure parsing originated with Klein and Manning (2003), while the CCG approach presented here is based on the work of Lewis and Steedman (2014).",
        "chapter": "Constituency Parsing",
        "section": "CCG Parsing",
        "subsection": "Supertagging",
        "subsubsection": "N/A"
    },
    {
        "content": "As with traditional part-of-speech tagging, the standard approach to building a CCG supertagger is to use supervised machine learning to build a sequence labeler from hand-annotated training data. To find the most likely sequence of tags given a sentence, it is most common to use a neural sequence model, either RNN or Transformer. \nIt’s also possible, however, to use the CRF tagging model described in Chapter 8, using similar features; the current word $w _ { i }$ , its surrounding words within $l$ words, local POS tags and character suffixes, and the supertag from the prior timestep, training by maximizing log-likelihood of the training corpus and decoding via the Viterbi algorithm as described in Chapter 8. \nUnfortunately the large number of possible supertags combined with high perword ambiguity leads the naive CRF algorithm to error rates that are too high for practical use in a parser. The single best tag sequence $hat { T }$ will typically contain too many incorrect tags for effective parsing to take place. To overcome this, we instead return a probability distribution over the possible supertags for each word in the input. The following table illustrates an example distribution for a simple sentence, in which each column represents the probability of each supertag for a given word in the context of the input sentence. The “...” represent all the remaining supertags possible for each word. \nTo get the probability of each possible word/tag pair, we’ll need to sum the probabilities of all the supertag sequences that contain that tag at that location. This can be done with the forward-backward algorithm that is also used to train the CRF, described in Appendix A. \n13.6.4 CCG Parsing using the $mathbf { A } ^ { * }$ Algorithm \nThe $mathbf { A } ^ { * }$ algorithm is a heuristic search method that employs an agenda to find an optimal solution. Search states representing partial solutions are added to an agenda based on a cost function, with the least-cost option being selected for further exploration at each iteration. When a state representing a complete solution is first selected from the agenda, it is guaranteed to be optimal and the search terminates. \nThe $mathbf { A } ^ { * }$ cost function, $f ( n )$ , is used to efficiently guide the search to a solution. The $f$ -cost has two components: $g ( n )$ , the exact cost of the partial solution represented by the state $n$ , and $h ( n )$ a heuristic approximation of the cost of a solution that makes use of $n$ . When $h ( n )$ satisfies the criteria of not overestimating the actual cost, $mathbf { A } ^ { * }$ will find an optimal solution. Not surprisingly, the closer the heuristic can get to the actual cost, the more effective $mathbf { A } ^ { * }$ is at finding a solution without having to explore a significant portion of the solution space. \nWhen applied to parsing, search states correspond to edges representing completed constituents. Each edge specifies a constituent’s start and end positions, its grammatical category, and its $f$ -cost. Here, the $g$ component represents the current cost of an edge and the $h$ component represents an estimate of the cost to complete a derivation that makes use of that edge. The use of $mathbf { A } ^ { * }$ for phrase structure parsing originated with Klein and Manning (2003), while the CCG approach presented here is based on the work of Lewis and Steedman (2014). \n\nUsing information from a supertagger, an agenda and a parse table are initialized with states representing all the possible lexical categories for each word in the input, along with their $f$ -costs. The main loop removes the lowest cost edge from the agenda and tests to see if it is a complete derivation. If it reflects a complete derivation it is selected as the best solution and the loop terminates. Otherwise, new states based on the applicable CCG rules are generated, assigned costs, and entered into the agenda to await further processing. The loop continues until a complete derivation is discovered, or the agenda is exhausted, indicating a failed parse. The algorithm is given in Fig. 13.9. \nHeuristic Functions \nBefore we can define a heuristic function for our $mathbf { A } ^ { * }$ search, we need to decide how to assess the quality of CCG derivations. We’ll make the simplifying assumption that the probability of a CCG derivation is just the product of the probability of the supertags assigned to the words in the derivation, ignoring the rules used in the derivation. More formally, given a sentence $S$ and derivation $D$ that contains supertag sequence $T$ , we have: \nTo better fit with the traditional $mathbf { A } ^ { * }$ approach, we’d prefer to have states scored by a cost function where lower is better (i.e., we’re trying to minimize the cost of a \nderivation). To achieve this, we’ll use negative log probabilities to score derivations; this results in the following equation, which we’ll use to score completed CCG derivations. \nGiven this model, we can define our $f$ -cost as follows. The $f$ -cost of an edge is the sum of two components: $g ( n )$ , the cost of the span represented by the edge, and $h ( n )$ , the estimate of the cost to complete a derivation containing that edge (these are often referred to as the inside and outside costs). We’ll define $g ( n )$ for an edge using Equation 13.21. That is, it is just the sum of the costs of the supertags that comprise the span. \nFor $h ( n )$ , we need a score that approximates but never overestimates the actual cost of the final derivation. A simple heuristic that meets this requirement assumes that each of the words in the outside span will be assigned its most probable supertag. If these are the tags used in the final derivation, then its score will equal the heuristic. If any other tags are used in the final derivation the $f$ -cost will be higher since the new tags must have higher costs, thus guaranteeing that we will not overestimate. \nPutting this all together, we arrive at the following definition of a suitable $f$ -cost for an edge. \nAs an example, consider an edge representing the word serves with the supertag $N$ in the following example. \n(13.23) United serves Denver. \nThe $g$ -cost for this edge is just the negative log probability of this tag, $- l o g _ { 1 0 } ( 0 . 1 )$ , or 1. The outside $h$ -cost consists of the most optimistic supertag assignments for United and Denver, which are $N / N$ and $N P$ respectively. The resulting $f$ -cost for this edge is therefore 1.443. \nAn Example \nFig. 13.10 shows the initial agenda and the progress of a complete parse for this example. After initializing the agenda and the parse table with information from the supertagger, it selects the best edge from the agenda—the entry for United with the tag $N / N$ and $f$ -cost 0.591. This edge does not constitute a complete parse and is therefore used to generate new states by applying all the relevant grammar rules. In this case, applying forward application to United: N/N and serves: $N$ results in the creation of the edge United serves: N[0,2], 1.795 to the agenda. \nSkipping ahead, at the third iteration an edge representing the complete derivation United serves Denver, S[0,3], .716 is added to the agenda. However, the algorithm does not terminate at this point since the cost of this edge (.716) does not place it at the top of the agenda. Instead, the edge representing Denver with the category NP is popped. This leads to the addition of another edge to the agenda (type-raising Denver). Only after this edge is popped and dealt with does the earlier state representing a complete derivation rise to the top of the agenda where it is popped, goal tested, and returned as a solution. \n\nThe effectiveness of the $mathbf { A } ^ { * }$ approach is reflected in the coloring of the states in Fig. 13.10 as well as the final parsing table. The edges shown in blue (including all the initial lexical category assignments not explicitly shown) reflect states in the search space that never made it to the top of the agenda and, therefore, never contributed any edges to the final table. This is in contrast to the PCKY approach where the parser systematically fills the parse table with all possible constituents for all possible spans in the input, filling the table with myriad constituents that do not contribute to the final analysis. \n\n13.7 Summary \nThis chapter introduced constituency parsing. Here’s a summary of the main points: \n• Structural ambiguity is a significant problem for parsers. Common sources of structural ambiguity include PP-attachment, coordination ambiguity, and noun-phrase bracketing ambiguity.   \n• Dynamic programming parsing algorithms, such as CKY, use a table of partial parses to efficiently parse ambiguous sentences.   \n• CKY restricts the form of the grammar to Chomsky normal form (CNF).   \n• The basic CKY algorithm compactly represents all possible parses of the sentence but doesn’t choose a single best parse.   \n• Choosing a single parse from all possible parses (disambiguation) can be done by neural constituency parsers.   \n• Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree.   \n• Much of the difficulty in CCG parsing is disambiguating the highly rich lexical entries, and so CCG parsers are generally based on supertagging. Supertagging is the equivalent of part-of-speech tagging in highly lexicalized grammar frameworks. The tags are very grammatically rich and dictate much of the derivation for a sentence.   \n• Parsers are evaluated with three metrics: labeled recall, labeled precision, and cross-brackets.   \n• Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on syntactically-annotated data. \nBibliographical and Historical Notes \nWriting about the history of compilers, Knuth notes: \nIn this field there has been an unusual amount of parallel discovery of the same technique by people working independently. \nWell, perhaps not unusual, since multiple discovery is the norm in science (see page ??). But there has certainly been enough parallel publication that this history errs on the side of succinctness in giving only a characteristic early mention of each algorithm; the interested reader should see Aho and Ullman (1972). \nBottom-up parsing seems to have been first described by Yngve (1955), who gave a breadth-first, bottom-up parsing algorithm as part of an illustration of a machine translation procedure. Top-down approaches to parsing and translation were described (presumably independently) by at least Glennie (1960), Irons (1961), and",
        "chapter": "Constituency Parsing",
        "section": "CCG Parsing",
        "subsection": "CCG Parsing using the A* Algorithm",
        "subsubsection": "N/A"
    },
    {
        "content": "13.7 Summary \nThis chapter introduced constituency parsing. Here’s a summary of the main points: \n• Structural ambiguity is a significant problem for parsers. Common sources of structural ambiguity include PP-attachment, coordination ambiguity, and noun-phrase bracketing ambiguity.   \n• Dynamic programming parsing algorithms, such as CKY, use a table of partial parses to efficiently parse ambiguous sentences.   \n• CKY restricts the form of the grammar to Chomsky normal form (CNF).   \n• The basic CKY algorithm compactly represents all possible parses of the sentence but doesn’t choose a single best parse.   \n• Choosing a single parse from all possible parses (disambiguation) can be done by neural constituency parsers.   \n• Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree.   \n• Much of the difficulty in CCG parsing is disambiguating the highly rich lexical entries, and so CCG parsers are generally based on supertagging. Supertagging is the equivalent of part-of-speech tagging in highly lexicalized grammar frameworks. The tags are very grammatically rich and dictate much of the derivation for a sentence.   \n• Parsers are evaluated with three metrics: labeled recall, labeled precision, and cross-brackets.   \n• Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on syntactically-annotated data. \nBibliographical and Historical Notes \nWriting about the history of compilers, Knuth notes: \nIn this field there has been an unusual amount of parallel discovery of the same technique by people working independently. \nWell, perhaps not unusual, since multiple discovery is the norm in science (see page ??). But there has certainly been enough parallel publication that this history errs on the side of succinctness in giving only a characteristic early mention of each algorithm; the interested reader should see Aho and Ullman (1972). \nBottom-up parsing seems to have been first described by Yngve (1955), who gave a breadth-first, bottom-up parsing algorithm as part of an illustration of a machine translation procedure. Top-down approaches to parsing and translation were described (presumably independently) by at least Glennie (1960), Irons (1961), and",
        "chapter": "Constituency Parsing",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "13.7 Summary \nThis chapter introduced constituency parsing. Here’s a summary of the main points: \n• Structural ambiguity is a significant problem for parsers. Common sources of structural ambiguity include PP-attachment, coordination ambiguity, and noun-phrase bracketing ambiguity.   \n• Dynamic programming parsing algorithms, such as CKY, use a table of partial parses to efficiently parse ambiguous sentences.   \n• CKY restricts the form of the grammar to Chomsky normal form (CNF).   \n• The basic CKY algorithm compactly represents all possible parses of the sentence but doesn’t choose a single best parse.   \n• Choosing a single parse from all possible parses (disambiguation) can be done by neural constituency parsers.   \n• Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree.   \n• Much of the difficulty in CCG parsing is disambiguating the highly rich lexical entries, and so CCG parsers are generally based on supertagging. Supertagging is the equivalent of part-of-speech tagging in highly lexicalized grammar frameworks. The tags are very grammatically rich and dictate much of the derivation for a sentence.   \n• Parsers are evaluated with three metrics: labeled recall, labeled precision, and cross-brackets.   \n• Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on syntactically-annotated data. \nBibliographical and Historical Notes \nWriting about the history of compilers, Knuth notes: \nIn this field there has been an unusual amount of parallel discovery of the same technique by people working independently. \nWell, perhaps not unusual, since multiple discovery is the norm in science (see page ??). But there has certainly been enough parallel publication that this history errs on the side of succinctness in giving only a characteristic early mention of each algorithm; the interested reader should see Aho and Ullman (1972). \nBottom-up parsing seems to have been first described by Yngve (1955), who gave a breadth-first, bottom-up parsing algorithm as part of an illustration of a machine translation procedure. Top-down approaches to parsing and translation were described (presumably independently) by at least Glennie (1960), Irons (1961), and",
        "chapter": "Constituency Parsing",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "13.7 Summary \nThis chapter introduced constituency parsing. Here’s a summary of the main points: \n• Structural ambiguity is a significant problem for parsers. Common sources of structural ambiguity include PP-attachment, coordination ambiguity, and noun-phrase bracketing ambiguity.   \n• Dynamic programming parsing algorithms, such as CKY, use a table of partial parses to efficiently parse ambiguous sentences.   \n• CKY restricts the form of the grammar to Chomsky normal form (CNF).   \n• The basic CKY algorithm compactly represents all possible parses of the sentence but doesn’t choose a single best parse.   \n• Choosing a single parse from all possible parses (disambiguation) can be done by neural constituency parsers.   \n• Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree.   \n• Much of the difficulty in CCG parsing is disambiguating the highly rich lexical entries, and so CCG parsers are generally based on supertagging. Supertagging is the equivalent of part-of-speech tagging in highly lexicalized grammar frameworks. The tags are very grammatically rich and dictate much of the derivation for a sentence.   \n• Parsers are evaluated with three metrics: labeled recall, labeled precision, and cross-brackets.   \n• Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on syntactically-annotated data. \nBibliographical and Historical Notes \nWriting about the history of compilers, Knuth notes: \nIn this field there has been an unusual amount of parallel discovery of the same technique by people working independently. \nWell, perhaps not unusual, since multiple discovery is the norm in science (see page ??). But there has certainly been enough parallel publication that this history errs on the side of succinctness in giving only a characteristic early mention of each algorithm; the interested reader should see Aho and Ullman (1972). \nBottom-up parsing seems to have been first described by Yngve (1955), who gave a breadth-first, bottom-up parsing algorithm as part of an illustration of a machine translation procedure. Top-down approaches to parsing and translation were described (presumably independently) by at least Glennie (1960), Irons (1961), and \nKuno and Oettinger (1963). While parsing via cascades of finite-state automata had been common in the early history of parsing (Harris, 1962), the focus shifted to full CFG parsing quite soon afterward. \nDynamic programming parsing, once again, has a history of independent discovery. According to Martin Kay (personal communication), a dynamic programming parser containing the roots of the CKY algorithm was first implemented by John Cocke in 1960. Later work extended and formalized the algorithm, as well as proving its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related well-formed substring table (WFST) seems to have been independently proposed by Kuno (1965) as a data structure that stores the results of all previous computations in the course of the parse. Based on a generalization of Cocke’s work, a similar data structure had been independently described in Kay (1967) (and Kay 1973). The top-down application of dynamic programming to parsing was described in Earley’s Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence of the WFST and the Earley algorithm. Norvig (1991) shows that the efficiency offered by dynamic programming can be captured in any language with a memoization function (such as in LISP) simply by wrapping the memoization operation around a simple top-down parser. \nThe earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969). Baker (1979) proposed the inside-outside algorithm for unsupervised training of PCFG probabilities, and used a CKY-style parsing algorithm to compute inside probabilities. Jelinek and Lafferty (1991) extended the CKY algorithm to compute probabilities for prefixes. A number of researchers starting in the early 1990s worked on adding lexical dependencies to PCFGs and on making PCFG rule probabilities more sensitive to surrounding syntactic structure. See the Statistical Constituency chapter for more history. \nNeural methods were applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 14 for the parallel history of neural dependency parsing. \nThe classic reference for parsing algorithms is Aho and Ullman (1972); although the focus of that book is on computer languages, most of the algorithms have been applied to natural language. \nExercises \n13.1 Implement the algorithm to convert arbitrary context-free grammars to CNF. Apply your program to the $mathcal { L } _ { 1 }$ grammar.   \n13.2 Implement the CKY algorithm and test it with your converted $mathcal { L } _ { 1 }$ grammar.   \n13.3 Rewrite the CKY algorithm given in Fig. 13.5 on page 293 so that it can accept grammars that contain unit productions.   \n13.4 Discuss the relative advantages and disadvantages of partial versus full parsing.   \n13.5 Discuss how to augment a parser to deal with input that may be incorrect, for example, containing spelling errors or mistakes arising from automatic speech recognition.   \n13.6 Implement the PARSEVAL metrics described in Section 13.4. Next, use a parser and a treebank, compare your metrics against a standard implementation. Analyze the errors in your approach. \n\nCHAPTER 14 Dependency Parsing \ndependency grammars \nThe focus of the two previous chapters has been on context-free grammars and constituent-based representations. Here we present another important family of grammar formalisms called dependency grammars. In dependency formalisms, phrasal constituents and phrase-structure rules do not play a direct role. Instead, the syntactic structure of a sentence is described solely in terms of directed binary grammatical relations between the words, as in the following dependency parse: \ntyped dependency \nRelations among the words are illustrated above the sentence with directed, labeled arcs from heads to dependents. We call this a typed dependency structure because the labels are drawn from a fixed inventory of grammatical relations. A root node explicitly marks the root of the tree, the head of the entire structure. \nfree word order \nFigure 14.1 shows the same dependency analysis as a tree alongside its corresponding phrase-structure analysis of the kind given in Chapter 12. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between lexical items in the sentence. These head-dependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. \nA major advantage of dependency grammars is their ability to deal with languages that are morphologically rich and have a relatively free word order. For example, word order in Czech can be much more flexible than in English; a grammatical object might occur before or after a location adverbial. A phrase-structure grammar would need a separate rule for each possible place in the parse tree where such an adverbial phrase could occur. A dependency-based approach would just have one link type representing this particular adverbial relation. Thus, a dependency grammar approach abstracts away from word order information, representing only the information that is necessary for the parse.",
        "chapter": "Constituency Parsing",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "An additional practical motivation for a dependency-based approach is that the head-dependent relations provide an approximation to the semantic relationship between predicates and their arguments that makes them directly useful for many applications such as coreference resolution, question answering and information extraction. Constituent-based approaches to parsing provide similar information, but it often has to be distilled from the trees via techniques such as the head-finding rules discussed in Chapter 12. \nIn the following sections, we’ll discuss in more detail the inventory of relations used in dependency parsing, as well as the formal basis for these dependency structures. We’ll then move on to discuss the dominant families of algorithms that are used to automatically produce these structures. Finally, we’ll discuss how to evaluate dependency parsers and point to some of the ways they are used in language processing applications. \n14.1 Dependency Relations \ngrammatical relation \nhead dependent \nThe traditional linguistic notion of grammatical relation provides the basis for the binary relations that comprise these dependency structures. The arguments to these relations consist of a head and a dependent. We’ve already discussed the notion of heads in Chapter 12 in the context of constituent structures. There, the head word of a constituent was the central organizing word of a larger constituent (e.g, the primary noun in a noun phrase, or verb in a verb phrase). The remaining words in the constituent are either direct, or indirect, dependents of their head. In dependencybased approaches, the head-dependent relationship is made explicit by directly linking heads to the words that are immediately dependent on them, bypassing the need for constituent structures. \nIn addition to specifying the head-dependent pairs, dependency grammars allow \ngrammatical function \nus to classify the kinds of grammatical relations, or grammatical function that the dependent plays with respect to its head. These include familiar notions such as subject, direct object and indirect object. In English these notions strongly correlate with, but by no means determine, both position in a sentence and constituent type and are therefore somewhat redundant with the kind of information found in phrase-structure trees. However, in languages with more flexible word order, the information encoded directly in these grammatical relations is critical since phrasebased constituent syntax provides little help. \nLinguists have developed taxonomies of relations that go well beyond the familiar notions of subject and object. While there is considerable variation from theory to theory, there is enough commonality that cross-linguistic standards have been developed. The Universal Dependencies (UD) project (Nivre et al., 2016b) provides an inventory of dependency relations that are linguistically motivated, computationally useful, and cross-linguistically applicable. Fig. 14.2 shows a subset of the UD relations. Fig. 14.3 provides some example sentences illustrating selected relations. \nThe motivation for all of the relations in the Universal Dependency scheme is beyond the scope of this chapter, but the core set of frequently used relations can be broken into two sets: clausal relations that describe syntactic roles with respect to a predicate (often a verb), and modifier relations that categorize the ways that words can modify their heads. \nConsider, for example, the following sentence: \nHere the clausal relations NSUBJ and DOBJ identify the subject and direct object of the predicate cancel, while the NMOD, DET, and CASE relations denote modifiers of the nouns flights and Houston.",
        "chapter": "Dependency Parsing",
        "section": "Dependency Relations",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "14.2 Dependency Formalisms \ndependency tree \nA dependency structure can be represented as a directed graph $G = left( V , A right)$ , consisting of a set of vertices $V$ , and a set of ordered pairs of vertices $A$ , which we’ll call arcs. \nFor the most part we will assume that the set of vertices, $V$ , corresponds exactly to the set of words in a given sentence. However, they might also correspond to punctuation, or when dealing with morphologically complex languages the set of vertices might consist of stems and affixes. The set of arcs, $A$ , captures the headdependent and grammatical function relationships between the elements in $V$ . \nDifferent grammatical theories or formalisms may place further constraints on these dependency structures. Among the more frequent restrictions are that the structures must be connected, have a designated root node, and be acyclic or planar. Of most relevance to the parsing approaches discussed in this chapter is the common, computationally-motivated, restriction to rooted trees. That is, a dependency tree is a directed graph that satisfies the following constraints: \n1. There is a single designated root node that has no incoming arcs.   \n2. With the exception of the root node, each vertex has exactly one incoming arc.   \n3. There is a unique path from the root node to each vertex in $V$ . \nTaken together, these constraints ensure that each word has a single head, that the dependency structure is connected, and that there is a single root node from which one can follow a unique directed path to each of the words in the sentence. \nprojective \n14.2.1 Projectivity \nThe notion of projectivity imposes an additional constraint that is derived from the order of the words in the input. An arc from a head to a dependent is said to be projective if there is a path from the head to every word that lies between the head and the dependent in the sentence. A dependency tree is then said to be projective if all the arcs that make it up are projective. All the dependency trees we’ve seen thus far have been projective. There are, however, many valid constructions which lead to non-projective trees, particularly in languages with relatively flexible word order. \nConsider the following example. \nIn this example, the arc from flight to its modifier was is non-projective since there is no path from flight to the intervening words this and morning. As we can see from this diagram, projectivity (and non-projectivity) can be detected in the way we’ve been drawing our trees. A dependency tree is projective if it can be drawn with no crossing edges. Here there is no way to link flight to its dependent was without crossing the arc that links morning to its head. \nOur concern with projectivity arises from two related issues. First, the most widely used English dependency treebanks were automatically derived from phrasestructure treebanks through the use of head-finding rules (Chapter 12). The trees generated in such a fashion will always be projective, and hence will be incorrect when non-projective examples like this one are encountered. \nSecond, there are computational limitations to the most widely used families of parsing algorithms. The transition-based approaches discussed in Section 14.4 can only produce projective trees, hence any sentences with non-projective structures will necessarily contain some errors. This limitation is one of the motivations for the more flexible graph-based parsing approach described in Section 14.5. \n14.3 Dependency Treebanks \nAs with constituent-based methods, treebanks play a critical role in the development and evaluation of dependency parsers. Dependency treebanks have been created using similar approaches to those discussed in Chapter 12—having human annotators directly generate dependency structures for a given corpus, or using automatic parsers to provide an initial parse and then having annotators hand correct the parses. We can also use a deterministic process to translate existing constituent-based treebanks into dependency trees through the use of head rules. \nFor the most part, directly annotated dependency treebanks have been created for morphologically rich languages such as Czech, Hindi and Finnish that lend themselves to dependency grammar approaches, with the Prague Dependency Treebank (Bejcˇek et al., 2013) for Czech being the most well-known effort. The major English dependency treebanks have largely been extracted from existing resources such as the Wall Street Journal sections of the Penn Treebank (Marcus et al., 1993). The more recent OntoNotes project (Hovy et al. 2006, Weischedel et al. 2011) extends this approach beyond news text to include conversational telephone speech, blogs, newsgroups, broadcasts, and talk shows in English, Chinese and Arabic. \nThe translation process from constituent to dependency structures has two subtasks: identifying all the head-dependent relations in the structure and identifying the correct dependency relations for these relations. The first task relies heavily on the use of head rules discussed in Chapter 12 first developed for use in lexicalized probabilistic parsers (Magerman 1994, Collins 1999, Collins 2003). Here’s a simple and effective algorithm from Xia and Palmer (2001):",
        "chapter": "Dependency Parsing",
        "section": "Dependency Formalisms",
        "subsection": "Projectivity",
        "subsubsection": "N/A"
    },
    {
        "content": "Consider the following example. \nIn this example, the arc from flight to its modifier was is non-projective since there is no path from flight to the intervening words this and morning. As we can see from this diagram, projectivity (and non-projectivity) can be detected in the way we’ve been drawing our trees. A dependency tree is projective if it can be drawn with no crossing edges. Here there is no way to link flight to its dependent was without crossing the arc that links morning to its head. \nOur concern with projectivity arises from two related issues. First, the most widely used English dependency treebanks were automatically derived from phrasestructure treebanks through the use of head-finding rules (Chapter 12). The trees generated in such a fashion will always be projective, and hence will be incorrect when non-projective examples like this one are encountered. \nSecond, there are computational limitations to the most widely used families of parsing algorithms. The transition-based approaches discussed in Section 14.4 can only produce projective trees, hence any sentences with non-projective structures will necessarily contain some errors. This limitation is one of the motivations for the more flexible graph-based parsing approach described in Section 14.5. \n14.3 Dependency Treebanks \nAs with constituent-based methods, treebanks play a critical role in the development and evaluation of dependency parsers. Dependency treebanks have been created using similar approaches to those discussed in Chapter 12—having human annotators directly generate dependency structures for a given corpus, or using automatic parsers to provide an initial parse and then having annotators hand correct the parses. We can also use a deterministic process to translate existing constituent-based treebanks into dependency trees through the use of head rules. \nFor the most part, directly annotated dependency treebanks have been created for morphologically rich languages such as Czech, Hindi and Finnish that lend themselves to dependency grammar approaches, with the Prague Dependency Treebank (Bejcˇek et al., 2013) for Czech being the most well-known effort. The major English dependency treebanks have largely been extracted from existing resources such as the Wall Street Journal sections of the Penn Treebank (Marcus et al., 1993). The more recent OntoNotes project (Hovy et al. 2006, Weischedel et al. 2011) extends this approach beyond news text to include conversational telephone speech, blogs, newsgroups, broadcasts, and talk shows in English, Chinese and Arabic. \nThe translation process from constituent to dependency structures has two subtasks: identifying all the head-dependent relations in the structure and identifying the correct dependency relations for these relations. The first task relies heavily on the use of head rules discussed in Chapter 12 first developed for use in lexicalized probabilistic parsers (Magerman 1994, Collins 1999, Collins 2003). Here’s a simple and effective algorithm from Xia and Palmer (2001): \n1. Mark the head child of each node in a phrase structure, using the appropriate head rules.   \n2. In the dependency structure, make the head of each non-head child depend on the head of the head-child. \nWhen a phrase-structure parse is annotated with grammatical relations and function tags, as in the case of the Penn Treebank, these tags can be used to label the edges in the resulting tree. When applied to the parse tree in Fig. 14.4, this algorithm would produce the dependency structure in example 14.4. \nThe primary shortcoming of these extraction methods is that they are limited by the information present in the original constituent trees. Among the most important issues are the failure to integrate morphological information with the phrasestructure trees, the inability to easily represent non-projective structures, and the lack of internal structure to most noun-phrases, as reflected in the generally flat rules used in most treebank grammars. For these reasons, outside of English, most dependency treebanks are developed directly using human annotators. \n14.4 Transition-Based Dependency Parsing \ntransition-based \nOur first approach to dependency parsing is called transition-based parsing. This architecture draws on shift-reduce parsing, a paradigm originally developed for analyzing programming languages (Aho and Ullman, 1972). In transition-based parsing we’ll have a stack on which we build the parse, a buffer of tokens to be parsed, and a parser which takes actions on the parse via a predictor called an oracle, as illustrated in Fig. 14.5. \nThe parser walks through the sentence left-to-right, successively shifting items from the buffer onto the stack. At each time point we examine the top two elements on the stack, and the oracle makes a decision about what transition to apply to build the parse. The possible transitions correspond to the intuitive actions one might take in creating a dependency tree by examining the words in a single pass over the input from left to right (Covington, 2001): \n• Assign the current word as the head of some previously seen word, • Assign some previously seen word as the head of the current word, • Postpone dealing with the current word, storing it for later processing.",
        "chapter": "Dependency Parsing",
        "section": "Dependency Treebanks",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Second, we are assuming that the oracle always provides the correct operator at each point in the parse—an assumption that is unlikely to be true in practice. As a result, given the greedy nature of this algorithm, incorrect choices will lead to incorrect parses since the parser has no opportunity to go back and pursue alternative choices. Section 14.4.4 will introduce several techniques that allow transition-based approaches to explore the search space more fully. \nFinally, for simplicity, we have illustrated this example without the labels on the dependency relations. To produce labeled trees, we can parameterize the LEFTARC and RIGHTARC operators with dependency labels, as in LEFTARC(NSUBJ) or RIGHTARC(DOBJ). This is equivalent to expanding the set of transition operators from our original set of three to a set that includes LEFTARC and RIGHTARC operators for each relation in the set of dependency relations being used, plus an additional one for the SHIFT operator. This, of course, makes the job of the oracle more difficult since it now has a much larger set of operators from which to choose. \n14.4.1 Creating an Oracle \nThe oracle for greedily selecting the appropriate transition is trained by supervised machine learning. As with all supervised machine learning methods, we will need training data: configurations annotated with the correct transition to take. We can draw these from dependency trees. And we need to extract features of the configuration. We’ll introduce neural classifiers that represent the configuration via embeddings, as well as classic systems that use hand-designed features. \nGenerating Training Data \nThe oracle from the algorithm in Fig. 14.6 takes as input a configuration and returns a transition operator. Therefore, to train a classifier, we will need configurations paired with transition operators (i.e., LEFTARC, RIGHTARC, or SHIFT). Unfortunately, treebanks pair entire sentences with their corresponding trees, not configurations with transitions. \nTo generate the required training data, we employ the oracle-based parsing algorithm in a clever way. We supply our oracle with the training sentences to be parsed along with their corresponding reference parses from the treebank. To produce training instances, we then simulate the operation of the parser by running the algorithm and relying on a new training oracle to give us correct transition operators for each successive configuration. \nTo see how this works, let’s first review the operation of our parser. It begins with a default initial configuration where the stack contains the ROOT, the input list is just the list of words, and the set of relations is empty. The LEFTARC and RIGHTARC operators each add relations between the words at the top of the stack to the set of relations being accumulated for a given sentence. Since we have a gold-standard reference parse for each training sentence, we know which dependency relations are valid for a given sentence. Therefore, we can use the reference parse to guide the selection of operators as the parser steps through a sequence of configurations. \n\nTo be more precise, given a reference parse and a configuration, the training oracle proceeds as follows: \n• Choose LEFTARC if it produces a correct head-dependent relation given the reference parse and the current configuration,   \n• Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent relation given the reference parse and (2) all of the dependents of the word at the top of the stack have already been assigned,   \n• Otherwise, choose SHIFT. \nThe restriction on selecting the RIGHTARC operator is needed to ensure that a word is not popped from the stack, and thus lost to further processing, before all its dependents have been assigned to it. \nMore formally, during training the oracle has access to the following: \n• A current configuration with a stack $S$ and a set of dependency relations $R _ { c }$ • A reference parse consisting of a set of vertices $V$ and a set of dependency relations $R _ { p }$ \nGiven this information, the oracle chooses transitions as follows: \nLEFTARC(r): if $( S _ { 1 }  r S _ { 2 } ) in R _ { p }$   \nRIGHTARC(r): if $left( S _ { 2 } r S _ { 1 } right) in R _ { p }$ and $forall r ^ { prime } , w  s . t . ( S _ { 1 }  r ^ { prime }  w ) in R _ { p }$ then $( S _ { 1 } r ^ { prime } w ) in R _ { c }$   \nSHIFT: otherwise \nLet’s walk through the processing of the following example as shown in Fig. 14.8. \nAt Step 1, LEFTARC is not applicable in the initial configuration since it asserts a relation, $mathrm { bf ~ r o o t }  mathrm { bf ~ b o o k }$ ), not in the reference answer; RIGHTARC does assert a relation contained in the final answer $( mathrm { r o o t } to mathrm { b o o k }$ ), however book has not been attached to any of its dependents yet, so we have to defer, leaving SHIFT as the only possible action. The same conditions hold in the next two steps. In step 3, LEFTARC is selected to link the to its head. \nNow consider the situation in Step 4. \nHere, we might be tempted to add a dependency relation between book and flight, which is present in the reference parse. But doing so now would prevent the later attachment of Houston since flight would have been removed from the stack. Fortunately, the precondition on choosing RIGHTARC prevents this choice and we’re again left with SHIFT as the only viable option. The remaining choices complete the set of operators needed for this example. \nTo recap, we derive appropriate training instances consisting of configurationtransition pairs from a treebank by simulating the operation of a parser in the context of a reference dependency tree. We can deterministically record correct parser actions at each step as we progress through each training example, thereby creating the training set we require. \n14.4.2 A feature-based classifier \nWe’ll now introduce two classifiers for choosing transitions, here a classic featurebased algorithm and in the next section a neural classifier using embedding features. \nFeatured-based classifiers generally use the same features we’ve seen with partof-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the head, and the dependency relation to the head. Other features may be relevant for some languages, for example morphosyntactic features like case marking on subjects or objects. The features are extracted from the training configurations, which consist of the stack, the buffer and the current set of relations. Most useful are features referencing the top levels of the stack, the words near the front of the buffer, and the dependency relations already associated with any of those elements. \nWe can use the feature template that we introduced for sentiment analysis and part-of-speech tagging. Feature templates allow us to automatically generate large numbers of specific features from a training set. For example, consider the following feature templates that are based on single positions in a configuration. \nHere features are denoted as location.property, where $s =$ stack, $b =$ the word buffer, $w =$ word forms, $l =$ lemmas, $t =$ part-of-speech, and $o p =$ operator. Thus the feature for the word form at the top of the stack would be $s _ { 1 } . boldsymbol { w }$ , the part of speech tag at the front of the buffer $b _ { 1 } . t$ , and the concatenated feature $s _ { 1 } . w t$ represents the word form concatenated with the part of speech of the word at the top of the stack. Consider applying these templates to the following intermediate configuration derived from a training oracle for Exercise 14.14.2.",
        "chapter": "Dependency Parsing",
        "section": "Transition-Based Dependency Parsing",
        "subsection": "Creating an Oracle",
        "subsubsection": "N/A"
    },
    {
        "content": "At Step 1, LEFTARC is not applicable in the initial configuration since it asserts a relation, $mathrm { bf ~ r o o t }  mathrm { bf ~ b o o k }$ ), not in the reference answer; RIGHTARC does assert a relation contained in the final answer $( mathrm { r o o t } to mathrm { b o o k }$ ), however book has not been attached to any of its dependents yet, so we have to defer, leaving SHIFT as the only possible action. The same conditions hold in the next two steps. In step 3, LEFTARC is selected to link the to its head. \nNow consider the situation in Step 4. \nHere, we might be tempted to add a dependency relation between book and flight, which is present in the reference parse. But doing so now would prevent the later attachment of Houston since flight would have been removed from the stack. Fortunately, the precondition on choosing RIGHTARC prevents this choice and we’re again left with SHIFT as the only viable option. The remaining choices complete the set of operators needed for this example. \nTo recap, we derive appropriate training instances consisting of configurationtransition pairs from a treebank by simulating the operation of a parser in the context of a reference dependency tree. We can deterministically record correct parser actions at each step as we progress through each training example, thereby creating the training set we require. \n14.4.2 A feature-based classifier \nWe’ll now introduce two classifiers for choosing transitions, here a classic featurebased algorithm and in the next section a neural classifier using embedding features. \nFeatured-based classifiers generally use the same features we’ve seen with partof-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the head, and the dependency relation to the head. Other features may be relevant for some languages, for example morphosyntactic features like case marking on subjects or objects. The features are extracted from the training configurations, which consist of the stack, the buffer and the current set of relations. Most useful are features referencing the top levels of the stack, the words near the front of the buffer, and the dependency relations already associated with any of those elements. \nWe can use the feature template that we introduced for sentiment analysis and part-of-speech tagging. Feature templates allow us to automatically generate large numbers of specific features from a training set. For example, consider the following feature templates that are based on single positions in a configuration. \nHere features are denoted as location.property, where $s =$ stack, $b =$ the word buffer, $w =$ word forms, $l =$ lemmas, $t =$ part-of-speech, and $o p =$ operator. Thus the feature for the word form at the top of the stack would be $s _ { 1 } . boldsymbol { w }$ , the part of speech tag at the front of the buffer $b _ { 1 } . t$ , and the concatenated feature $s _ { 1 } . w t$ represents the word form concatenated with the part of speech of the word at the top of the stack. Consider applying these templates to the following intermediate configuration derived from a training oracle for Exercise 14.14.2. \nThe correct transition here is SHIFT (you should convince yourself of this before proceeding). The application of our set of feature templates to this configuration would result in the following set of instantiated features. \nGiven that the left and right arc transitions operate on the top two elements of the stack, features that combine properties from these positions are even more useful. For example, a feature like $s _ { 1 } . t circ s _ { 2 } . t$ concatenates the part of speech tag of the word at the top of the stack with the tag of the word beneath it. \nGiven the training data and features, any classifier, like multinomial logistic regression or support vector machines, can be used. \n14.4.3 A neural classifier \nThe oracle can also be implemented by a neural classifier. A standard architecture is simply to pass the sentence through an encoder, then take the presentation of the top 2 words on the stack and the first word of the buffer, concatenate them, and present to a feedforward network that predicts the transition to take (Kiperwasser and Goldberg, 2016; Kulmizev et al., 2019). Fig. 14.9 sketches this model. Learning can be done with cross-entropy loss. \n14.4.4 Advanced Methods in Transition-Based Parsing \nThe basic transition-based approach can be elaborated in a number of ways to improve performance by addressing some of the most obvious flaws in the approach. \nAlternative Transition Systems \nThe arc-standard transition system described above is only one of many possible systems. A frequently used alternative is the arc eager transition system. The arc eager approach gets its name from its ability to assert rightward relations much sooner than in the arc standard approach. To see this, let’s revisit the arc standard trace of Example 14.7, repeated here.",
        "chapter": "Dependency Parsing",
        "section": "Transition-Based Dependency Parsing",
        "subsection": "A feature-based classifier",
        "subsubsection": "N/A"
    },
    {
        "content": "The correct transition here is SHIFT (you should convince yourself of this before proceeding). The application of our set of feature templates to this configuration would result in the following set of instantiated features. \nGiven that the left and right arc transitions operate on the top two elements of the stack, features that combine properties from these positions are even more useful. For example, a feature like $s _ { 1 } . t circ s _ { 2 } . t$ concatenates the part of speech tag of the word at the top of the stack with the tag of the word beneath it. \nGiven the training data and features, any classifier, like multinomial logistic regression or support vector machines, can be used. \n14.4.3 A neural classifier \nThe oracle can also be implemented by a neural classifier. A standard architecture is simply to pass the sentence through an encoder, then take the presentation of the top 2 words on the stack and the first word of the buffer, concatenate them, and present to a feedforward network that predicts the transition to take (Kiperwasser and Goldberg, 2016; Kulmizev et al., 2019). Fig. 14.9 sketches this model. Learning can be done with cross-entropy loss. \n14.4.4 Advanced Methods in Transition-Based Parsing \nThe basic transition-based approach can be elaborated in a number of ways to improve performance by addressing some of the most obvious flaws in the approach. \nAlternative Transition Systems \nThe arc-standard transition system described above is only one of many possible systems. A frequently used alternative is the arc eager transition system. The arc eager approach gets its name from its ability to assert rightward relations much sooner than in the arc standard approach. To see this, let’s revisit the arc standard trace of Example 14.7, repeated here.",
        "chapter": "Dependency Parsing",
        "section": "Transition-Based Dependency Parsing",
        "subsection": "A neural classifier",
        "subsubsection": "N/A"
    },
    {
        "content": "The correct transition here is SHIFT (you should convince yourself of this before proceeding). The application of our set of feature templates to this configuration would result in the following set of instantiated features. \nGiven that the left and right arc transitions operate on the top two elements of the stack, features that combine properties from these positions are even more useful. For example, a feature like $s _ { 1 } . t circ s _ { 2 } . t$ concatenates the part of speech tag of the word at the top of the stack with the tag of the word beneath it. \nGiven the training data and features, any classifier, like multinomial logistic regression or support vector machines, can be used. \n14.4.3 A neural classifier \nThe oracle can also be implemented by a neural classifier. A standard architecture is simply to pass the sentence through an encoder, then take the presentation of the top 2 words on the stack and the first word of the buffer, concatenate them, and present to a feedforward network that predicts the transition to take (Kiperwasser and Goldberg, 2016; Kulmizev et al., 2019). Fig. 14.9 sketches this model. Learning can be done with cross-entropy loss. \n14.4.4 Advanced Methods in Transition-Based Parsing \nThe basic transition-based approach can be elaborated in a number of ways to improve performance by addressing some of the most obvious flaws in the approach. \nAlternative Transition Systems \nThe arc-standard transition system described above is only one of many possible systems. A frequently used alternative is the arc eager transition system. The arc eager approach gets its name from its ability to assert rightward relations much sooner than in the arc standard approach. To see this, let’s revisit the arc standard trace of Example 14.7, repeated here. \nConsider the dependency relation between book and flight in this analysis. As is shown in Fig. 14.8, an arc-standard approach would assert this relation at Step 8, despite the fact that book and flight first come together on the stack much earlier at Step 4. The reason this relation can’t be captured at this point is due to the presence of the postnominal modifier through Houston. In an arc-standard approach, dependents are removed from the stack as soon as they are assigned their heads. If flight had been assigned book as its head in Step 4, it would no longer be available to serve as the head of Houston. \nWhile this delay doesn’t cause any issues in this example, in general the longer a word has to wait to get assigned its head the more opportunities there are for something to go awry. The arc-eager system addresses this issue by allowing words to be attached to their heads as early as possible, before all the subsequent words dependent on them have been seen. This is accomplished through minor changes to the LEFTARC and RIGHTARC operators and the addition of a new REDUCE operator. \n• LEFTARC: Assert a head-dependent relation between the word at the front of the input buffer and the word at the top of the stack; pop the stack.   \n• RIGHTARC: Assert a head-dependent relation between the word on the top of the stack and the word at front of the input buffer; shift the word at the front of the input buffer to the stack.   \n• SHIFT: Remove the word from the front of the input buffer and push it onto the stack.   \n• REDUCE: Pop the stack. \nThe LEFTARC and RIGHTARC operators are applied to the top of the stack and the front of the input buffer, instead of the top two elements of the stack as in the arc-standard approach. The RIGHTARC operator now moves the dependent to the stack from the buffer rather than removing it, thus making it available to serve as the head of following words. The new REDUCE operator removes the top element from the stack. Together these changes permit a word to be eagerly assigned its head and still allow it to serve as the head for later dependents. The trace shown in Fig. 14.10 illustrates the new decision sequence for this example. \nIn addition to demonstrating the arc-eager transition system, this example demonstrates the power and flexibility of the overall transition-based approach. We were \nbeam search \nbeam width \nable to swap in a new transition system without having to make any changes to the underlying parsing algorithm. This flexibility has led to the development of a diverse set of transition systems that address different aspects of syntax and semantics including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the generation of non-projective dependency structures (Nivre, 2009), assigning semantic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages (Bhat et al., 2017). \nBeam Search \nThe computational efficiency of the transition-based approach discussed earlier derives from the fact that it makes a single pass through the sentence, greedily making decisions without considering alternatives. Of course, this is also a weakness – once a decision has been made it can not be undone, even in the face of overwhelming evidence arriving later in a sentence. We can use beam search to explore alternative decision sequences. Recall from Chapter 10 that beam search uses a breadth-first search strategy with a heuristic filter that prunes the search frontier to stay within a fixed-size beam width. \nIn applying beam search to transition-based parsing, we’ll elaborate on the algorithm given in Fig. 14.6. Instead of choosing the single best transition operator at each iteration, we’ll apply all applicable operators to each state on an agenda and then score the resulting configurations. We then add each of these new configurations to the frontier, subject to the constraint that there has to be room within the beam. As long as the size of the agenda is within the specified beam width, we can add new configurations to the agenda. Once the agenda reaches the limit, we only add new configurations that are better than the worst configuration on the agenda (removing the worst element so that we stay within the limit). Finally, to insure that we retrieve the best possible state on the agenda, the while loop continues as long as there are non-final states on the agenda. \nThe beam search approach requires a more elaborate notion of scoring than we used with the greedy algorithm. There, we assumed that the oracle would be a supervised classifier that chose the best transition operator based on features of the current configuration. This choice can be viewed as assigning a score to all the possible transitions and picking the best one. \nWith beam search we are now searching through the space of decision sequences, so it makes sense to base the score for a configuration on its entire history. So we \ncan define the score for a new configuration as the score of its predecessor plus the score of the operator used to produce it. \nThis score is used both in filtering the agenda and in selecting the final answer. The new beam search version of transition-based parsing is given in Fig. 14.11. \nfunction DEPENDENCYBEAMPARSE(words, width) returns dependency tree state [root], [words], [], 0.0 ;initial configuration agenda $left| gets left. s t a t e right. right.$ ;initial agenda while agenda contains non-final states newagenda $ langle rangle$ for each state $in$ agenda do for all ${ t mid t in$ VALIDOPERATORS(state)} do child $$ APPLY(t, state) newagenda $$ ADDTOBEAM(child, newagenda, width) agenda $$ newagenda return BESTOF(agenda)   \nfunction ADDTOBEAM(state, agenda, width) returns updated agenda if LENGTH(agenda) $<$ width then agenda $$ INSERT(state, agenda) else if SCORE(state) $>$ SCORE(WORSTOF(agenda)) agenda $$ REMOVE(WORSTOF(agenda)) agenda $$ INSERT(state, agenda) return agenda \n14.5 Graph-Based Dependency Parsing \nGraph-based methods are the second important family of dependency parsing algorithms. Graph-based parsers are more accurate than transition-based parsers, especially on long sentences; transition-based methods have trouble when the heads are very far from the dependents (McDonald and Nivre, 2011). Graph-based methods avoid this difficulty by scoring entire trees, rather than relying on greedy local decisions. Furthermore, unlike transition-based approaches, graph-based parsers can produce non-projective trees. Although projectivity is not a significant issue for English, it is definitely a problem for many of the world’s languages. \nGraph-based dependency parsers search through the space of possible trees for a given sentence for a tree (or trees) that maximize some score. These methods encode the search space as directed graphs and employ methods drawn from graph theory to search the space for optimal solutions. More formally, given a sentence $s$ we’re looking for the best dependency tree in $mathcal { G } _ { s }$ , the space of all possible trees for that",
        "chapter": "Dependency Parsing",
        "section": "Transition-Based Dependency Parsing",
        "subsection": "Advanced Methods in Transition-Based Parsing",
        "subsubsection": "N/A"
    },
    {
        "content": "sentence, that maximizes some score. \nedge-factored \nWe’ll make the simplifying assumption that this score can be edge-factored, meaning that the overall score for a tree is the sum of the scores of each of the scores of the edges that comprise the tree. \nGraph-based algorithms have to solve two problems: (1) assigning a score to each edge, and (2) finding the best parse tree given the scores of all potential edges. In the next few sections we’ll introduce solutions to these two problems, beginning with the second problem of finding trees, and then giving a feature-based and a neural algorithm for solving the first problem of assigning scores. \n14.5.1 Parsing via finding the maximum spanning tree \nIn graph-based parsing, given a sentence $S$ we start by creating a graph $G$ which is a fully-connected, weighted, directed graph where the vertices are the input words and the directed edges represent all possible head-dependent assignments. We’ll include an additional ROOT node with outgoing edges directed at all of the other vertices. The weights of each edge in $G$ reflect the score for each possible head-dependent relation assigned by some scoring algorithm. \nIt turns out that finding the best dependency parse for $S$ is equivalent to finding the maximum spanning tree over $G$ . A spanning tree over a graph $G$ is a subset of $G$ that is a tree and covers all the vertices in G; a spanning tree over $G$ that starts from the ROOT is a valid parse of $S$ . A maximum spanning tree is the spanning tree with the highest score. Thus a maximum spanning tree of $G$ emanating from the ROOT is the optimal dependency parse for the sentence. \nA directed graph for the example Book that flight is shown in Fig. 14.12, with the maximum spanning tree corresponding to the desired parse shown in blue. For ease of exposition, we’ll describe here the algorithm for unlabeled dependency parsing. \nBefore describing the algorithm it’s useful to consider two intuitions about directed graphs and their spanning trees. The first intuition begins with the fact that every vertex in a spanning tree has exactly one incoming edge. It follows from this that every connected component of a spanning tree (i.e., every set of vertices that are linked to each other by paths over edges) will also have one incoming edge. \nThe second intuition is that the absolute values of the edge scores are not critical to determining its maximum spanning tree. Instead, it is the relative weights of the edges entering each vertex that matters. If we were to subtract a constant amount from each edge entering a given vertex it would have no impact on the choice of the maximum spanning tree since every possible spanning tree would decrease by exactly the same amount. \nThe first step of the algorithm itself is quite straightforward. For each vertex in the graph, an incoming edge (representing a possible head assignment) with the highest score is chosen. If the resulting set of edges produces a spanning tree then we’re done. More formally, given the original fully-connected graph $G = ( V , E )$ , a subgraph $T = left( V , F right)$ is a spanning tree if it has no cycles and each vertex (other than the root) has exactly one edge entering it. If the greedy selection process produces such a tree then it is the best possible one. \nUnfortunately, this approach doesn’t always lead to a tree since the set of edges selected may contain cycles. Fortunately, in yet another case of multiple discovery, there is a straightforward way to eliminate cycles generated during the greedy selection phase. Chu and Liu (1965) and Edmonds (1967) independently developed an approach that begins with greedy selection and follows with an elegant recursive cleanup phase that eliminates cycles. \nThe cleanup phase begins by adjusting all the weights in the graph by subtracting the score of the maximum edge entering each vertex from the score of all the edges entering that vertex. This is where the intuitions mentioned earlier come into play. We have scaled the values of the edges so that the weights of the edges in the cycle have no bearing on the weight of any of the possible spanning trees. Subtracting the value of the edge with maximum weight from each edge entering a vertex results in a weight of zero for all of the edges selected during the greedy selection phase, including all of the edges involved in the cycle. \nHaving adjusted the weights, the algorithm creates a new graph by selecting a cycle and collapsing it into a single new node. Edges that enter or leave the cycle are altered so that they now enter or leave the newly collapsed node. Edges that do not touch the cycle are included and edges within the cycle are dropped. \nNow, if we knew the maximum spanning tree of this new graph, we would have what we need to eliminate the cycle. The edge of the maximum spanning tree directed towards the vertex representing the collapsed cycle tells us which edge to delete to eliminate the cycle. How do we find the maximum spanning tree of this new graph? We recursively apply the algorithm to the new graph. This will either result in a spanning tree or a graph with a cycle. The recursions can continue as long as cycles are encountered. When each recursion completes we expand the collapsed vertex, restoring all the vertices and edges from the cycle with the exception of the single edge to be deleted. \nPutting all this together, the maximum spanning tree algorithm consists of greedy edge selection, re-scoring of edge costs and a recursive cleanup phase when needed. The full algorithm is shown in Fig. 14.13. \nFig. 14.14 steps through the algorithm with our Book that flight example. The first row of the figure illustrates greedy edge selection with the edges chosen shown in blue (corresponding to the set $F$ in the algorithm). This results in a cycle between that and flight. The scaled weights using the maximum value entering each node are shown in the graph to the right. \nCollapsing the cycle between that and flight to a single node (labelled $t f )$ and recursing with the newly scaled costs is shown in the second row. The greedy selection step in this recursion yields a spanning tree that links root to book, as well as an edge that links book to the contracted node. Expanding the contracted node, we can see that this edge corresponds to the edge from book to flight in the original graph. This in turn tells us which edge to drop to eliminate the cycle. \n\nOn arbitrary directed graphs, this version of the CLE algorithm runs in $O ( m n )$ time, where $m$ is the number of edges and $n$ is the number of nodes. Since this particular application of the algorithm begins by constructing a fully connected graph $m = n ^ { 2 }$ yielding a running time of $O ( n ^ { 3 } )$ . Gabow et al. (1986) present a more efficient implementation with a running time of $O ( m + n l o g n )$ . \n14.5.2 A feature-based algorithm for assigning scores \nRecall that given a sentence, $S$ , and a candidate tree, $T$ , edge-factored parsing models make the simplification that the score for the tree is the sum of the scores of the edges that comprise the tree: \nIn a feature-based algorithm we compute the edge score as a weighted sum of features extracted from it: \nOr more succinctly.",
        "chapter": "Dependency Parsing",
        "section": "Graph-Based Dependency Parsing",
        "subsection": "Parsing via finding the maximum spanning tree",
        "subsubsection": "N/A"
    },
    {
        "content": "On arbitrary directed graphs, this version of the CLE algorithm runs in $O ( m n )$ time, where $m$ is the number of edges and $n$ is the number of nodes. Since this particular application of the algorithm begins by constructing a fully connected graph $m = n ^ { 2 }$ yielding a running time of $O ( n ^ { 3 } )$ . Gabow et al. (1986) present a more efficient implementation with a running time of $O ( m + n l o g n )$ . \n14.5.2 A feature-based algorithm for assigning scores \nRecall that given a sentence, $S$ , and a candidate tree, $T$ , edge-factored parsing models make the simplification that the score for the tree is the sum of the scores of the edges that comprise the tree: \nIn a feature-based algorithm we compute the edge score as a weighted sum of features extracted from it: \nOr more succinctly. \nGiven this formulation, we need to identify relevant features and train the weights. The features used to train edge-factored models mirror those used in training transition-based parsers. To summarize this earlier discussion, commonly used features include: \n• Wordforms, lemmas, and parts of speech of the headword and its dependent.   \n• Corresponding features from the contexts before, after and between the words.   \n• Word embeddings.   \n• The dependency relation itself.   \n• The direction of the relation (to the right or left).   \n• The distance from the head to the dependent. \nAs with transition-based approaches, pre-selected combinations of these features are often used as well. \nGiven a set of features, our next problem is to learn a set of weights corresponding to each. Unlike many of the learning problems discussed in earlier chapters, here we are not training a model to associate training items with class labels, or parser actions. Instead, we seek to train a model that assigns higher scores to correct trees than to incorrect ones. An effective framework for problems like this is to use inference-based learning combined with the perceptron learning rule. In this framework, we parse a sentence (i.e, perform inference) from the training set using some initially random set of initial weights. If the resulting parse matches the corresponding tree in the training data, we do nothing to the weights. Otherwise, we find those features in the incorrect parse that are not present in the reference parse and we lower their weights by a small amount based on the learning rate. We do this incrementally for each sentence in our training data until the weights converge. \n\n14.5.3 A neural algorithm for assigning scores \nState-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words $w _ { i }$ and $w _ { j }$ , these parsers run the sentence through an encoder, and then pass the encoded representation of the two words $w _ { i }$ and $w _ { j }$ through a network that estimates a score for the edge $i  j$ . \nHere we’ll sketch the biaffine algorithm of Dozat and Manning (2017) and Dozat et al. (2017) shown in Fig. 14.15, drawing on the work of Gru¨newald et al. (2021) who tested many versions of the algorithm via their STEPS system. The algorithm first runs the sentence $X = x _ { 1 } , . . . , x _ { n }$ through an encoder to produce a contextual embedding representation for each token $R = r _ { 1 } , . . . , r _ { n }$ . The embedding for each token is now passed through two separate feedforward networks, one to produce a representation of this token as a head, and one to produce a representation of this token as a dependent: \nNow to assign a score to the directed edge $i  j$ , ( $cdot w _ { i }$ is the head and $j$ is the dependent), we feed the head representation of $i .$ , $mathbf { h } _ { i } ^ { h e a d }$ , and the dependent representation",
        "chapter": "Dependency Parsing",
        "section": "Graph-Based Dependency Parsing",
        "subsection": "A feature-based algorithm for assigning scores",
        "subsubsection": "N/A"
    },
    {
        "content": "14.5.3 A neural algorithm for assigning scores \nState-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words $w _ { i }$ and $w _ { j }$ , these parsers run the sentence through an encoder, and then pass the encoded representation of the two words $w _ { i }$ and $w _ { j }$ through a network that estimates a score for the edge $i  j$ . \nHere we’ll sketch the biaffine algorithm of Dozat and Manning (2017) and Dozat et al. (2017) shown in Fig. 14.15, drawing on the work of Gru¨newald et al. (2021) who tested many versions of the algorithm via their STEPS system. The algorithm first runs the sentence $X = x _ { 1 } , . . . , x _ { n }$ through an encoder to produce a contextual embedding representation for each token $R = r _ { 1 } , . . . , r _ { n }$ . The embedding for each token is now passed through two separate feedforward networks, one to produce a representation of this token as a head, and one to produce a representation of this token as a dependent: \nNow to assign a score to the directed edge $i  j$ , ( $cdot w _ { i }$ is the head and $j$ is the dependent), we feed the head representation of $i .$ , $mathbf { h } _ { i } ^ { h e a d }$ , and the dependent representation \nof $j , boldsymbol { mathsf { h } } _ { j } ^ { d e p }$ , into a biaffine scoring function: \nwhere $mathbf { Delta } mathbf { u } , mathbf { w }$ , and $b$ are weights learned by the model. The idea of using a biaffine function is allow the system to learn multiplicative interactions between the vectors $pmb { x }$ and $pmb {  y }$ . \nIf we pass Score $( i  j )$ through a softmax, we end up with a probability distribution, for each token $j$ , over potential heads $i$ (all other tokens in the sentence): \nThis probability can then be passed to the maximum spanning tree algorithm of Section 14.5.1 to find the best tree. \nThis $p ( i  j )$ classifier is trained by optimizing the cross-entropy loss. \nNote that the algorithm as we’ve described it is unlabeled. To make this into a labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two classifiers. The first classifier, the edge-scorer, the one we described above, assigns a probability $p ( i  j )$ to each word $w _ { i }$ and $w _ { j }$ . Then the Maximum Spanning Tree algorithm is run to get a single best dependency parse tree for the second. We then apply a second classifier, the label-scorer, whose job is to find the maximum probability label for each edge in this parse. This second classifier has the same form as (14.13-14.15), but instead of being trained to predict with binary softmax the probability of an edge existing between two words, it is trained with a softmax over dependency labels to predict the dependency label between the words. \n14.6 Evaluation \nAs with phrase structure-based parsing, the evaluation of dependency parsers proceeds by measuring how well they work on a test set. An obvious metric would be exact match (EM)—how many sentences are parsed correctly. This metric is quite pessimistic, with most sentences being marked wrong. Such measures are not finegrained enough to guide the development process. Our metrics need to be sensitive enough to tell if actual improvements are being made. \nFor these reasons, the most common method for evaluating dependency parsers are labeled and unlabeled attachment accuracy. Labeled attachment refers to the proper assignment of a word to its head along with the correct dependency relation. Unlabeled attachment simply looks at the correctness of the assigned head, ignoring the dependency relation. Given a system output and a corresponding reference parse, accuracy is simply the percentage of words in an input that are assigned the correct head with the correct relation. These metrics are usually referred to as the labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we can make use of a label accuracy score (LS), the percentage of tokens with correct labels, ignoring where the relations are coming from. \nAs an example, consider the reference parse and system parse for the following example shown in Fig. 14.16. \n(14.16) Book me the flight through Houston.",
        "chapter": "Dependency Parsing",
        "section": "Graph-Based Dependency Parsing",
        "subsection": "A neural algorithm for assigning scores",
        "subsubsection": "N/A"
    },
    {
        "content": "of $j , boldsymbol { mathsf { h } } _ { j } ^ { d e p }$ , into a biaffine scoring function: \nwhere $mathbf { Delta } mathbf { u } , mathbf { w }$ , and $b$ are weights learned by the model. The idea of using a biaffine function is allow the system to learn multiplicative interactions between the vectors $pmb { x }$ and $pmb {  y }$ . \nIf we pass Score $( i  j )$ through a softmax, we end up with a probability distribution, for each token $j$ , over potential heads $i$ (all other tokens in the sentence): \nThis probability can then be passed to the maximum spanning tree algorithm of Section 14.5.1 to find the best tree. \nThis $p ( i  j )$ classifier is trained by optimizing the cross-entropy loss. \nNote that the algorithm as we’ve described it is unlabeled. To make this into a labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two classifiers. The first classifier, the edge-scorer, the one we described above, assigns a probability $p ( i  j )$ to each word $w _ { i }$ and $w _ { j }$ . Then the Maximum Spanning Tree algorithm is run to get a single best dependency parse tree for the second. We then apply a second classifier, the label-scorer, whose job is to find the maximum probability label for each edge in this parse. This second classifier has the same form as (14.13-14.15), but instead of being trained to predict with binary softmax the probability of an edge existing between two words, it is trained with a softmax over dependency labels to predict the dependency label between the words. \n14.6 Evaluation \nAs with phrase structure-based parsing, the evaluation of dependency parsers proceeds by measuring how well they work on a test set. An obvious metric would be exact match (EM)—how many sentences are parsed correctly. This metric is quite pessimistic, with most sentences being marked wrong. Such measures are not finegrained enough to guide the development process. Our metrics need to be sensitive enough to tell if actual improvements are being made. \nFor these reasons, the most common method for evaluating dependency parsers are labeled and unlabeled attachment accuracy. Labeled attachment refers to the proper assignment of a word to its head along with the correct dependency relation. Unlabeled attachment simply looks at the correctness of the assigned head, ignoring the dependency relation. Given a system output and a corresponding reference parse, accuracy is simply the percentage of words in an input that are assigned the correct head with the correct relation. These metrics are usually referred to as the labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we can make use of a label accuracy score (LS), the percentage of tokens with correct labels, ignoring where the relations are coming from. \nAs an example, consider the reference parse and system parse for the following example shown in Fig. 14.16. \n(14.16) Book me the flight through Houston. \nThe system correctly finds 4 of the 6 dependency relations present in the reference parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found by the system holds between book and flight, which are in a head-dependent relation in the reference parse; the system therefore achieves a UAS of $5 / 6$ . \nBeyond attachment scores, we may also be interested in how well a system is performing on a particular kind of dependency relation, for example NSUBJ, across a development corpus. Here we can make use of the notions of precision and recall introduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by the system that were correct (precision), and the percentage of the NSUBJ relations present in the development set that were in fact discovered by the system (recall). We can employ a confusion matrix to keep track of how often each dependency type was confused for another. \n14.7 Summary \nThis chapter has introduced the concept of dependency grammars and dependency parsing. Here’s a summary of the main points that we covered: \n• In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses.   \n• The relations in a dependency structure capture the head-dependent relationship among the words in a sentence.   \n• Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering.   \n• Transition-based parsing systems employ a greedy stack-based algorithm to create dependency structures.   \n• Graph-based methods for creating dependency structures are based on the use of maximum spanning tree methods from graph theory.   \n• Both transition-based and graph-based approaches are developed using supervised machine learning techniques.   \n• Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks.   \n• Evaluation of dependency parsers is based on labeled and unlabeled accuracy scores as measured against withheld development and test corpora.",
        "chapter": "Dependency Parsing",
        "section": "Evaluation",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The system correctly finds 4 of the 6 dependency relations present in the reference parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found by the system holds between book and flight, which are in a head-dependent relation in the reference parse; the system therefore achieves a UAS of $5 / 6$ . \nBeyond attachment scores, we may also be interested in how well a system is performing on a particular kind of dependency relation, for example NSUBJ, across a development corpus. Here we can make use of the notions of precision and recall introduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by the system that were correct (precision), and the percentage of the NSUBJ relations present in the development set that were in fact discovered by the system (recall). We can employ a confusion matrix to keep track of how often each dependency type was confused for another. \n14.7 Summary \nThis chapter has introduced the concept of dependency grammars and dependency parsing. Here’s a summary of the main points that we covered: \n• In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses.   \n• The relations in a dependency structure capture the head-dependent relationship among the words in a sentence.   \n• Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering.   \n• Transition-based parsing systems employ a greedy stack-based algorithm to create dependency structures.   \n• Graph-based methods for creating dependency structures are based on the use of maximum spanning tree methods from graph theory.   \n• Both transition-based and graph-based approaches are developed using supervised machine learning techniques.   \n• Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks.   \n• Evaluation of dependency parsers is based on labeled and unlabeled accuracy scores as measured against withheld development and test corpora.",
        "chapter": "Dependency Parsing",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The system correctly finds 4 of the 6 dependency relations present in the reference parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found by the system holds between book and flight, which are in a head-dependent relation in the reference parse; the system therefore achieves a UAS of $5 / 6$ . \nBeyond attachment scores, we may also be interested in how well a system is performing on a particular kind of dependency relation, for example NSUBJ, across a development corpus. Here we can make use of the notions of precision and recall introduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by the system that were correct (precision), and the percentage of the NSUBJ relations present in the development set that were in fact discovered by the system (recall). We can employ a confusion matrix to keep track of how often each dependency type was confused for another. \n14.7 Summary \nThis chapter has introduced the concept of dependency grammars and dependency parsing. Here’s a summary of the main points that we covered: \n• In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses.   \n• The relations in a dependency structure capture the head-dependent relationship among the words in a sentence.   \n• Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering.   \n• Transition-based parsing systems employ a greedy stack-based algorithm to create dependency structures.   \n• Graph-based methods for creating dependency structures are based on the use of maximum spanning tree methods from graph theory.   \n• Both transition-based and graph-based approaches are developed using supervised machine learning techniques.   \n• Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks.   \n• Evaluation of dependency parsers is based on labeled and unlabeled accuracy scores as measured against withheld development and test corpora.",
        "chapter": "Dependency Parsing",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The system correctly finds 4 of the 6 dependency relations present in the reference parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found by the system holds between book and flight, which are in a head-dependent relation in the reference parse; the system therefore achieves a UAS of $5 / 6$ . \nBeyond attachment scores, we may also be interested in how well a system is performing on a particular kind of dependency relation, for example NSUBJ, across a development corpus. Here we can make use of the notions of precision and recall introduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by the system that were correct (precision), and the percentage of the NSUBJ relations present in the development set that were in fact discovered by the system (recall). We can employ a confusion matrix to keep track of how often each dependency type was confused for another. \n14.7 Summary \nThis chapter has introduced the concept of dependency grammars and dependency parsing. Here’s a summary of the main points that we covered: \n• In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses.   \n• The relations in a dependency structure capture the head-dependent relationship among the words in a sentence.   \n• Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering.   \n• Transition-based parsing systems employ a greedy stack-based algorithm to create dependency structures.   \n• Graph-based methods for creating dependency structures are based on the use of maximum spanning tree methods from graph theory.   \n• Both transition-based and graph-based approaches are developed using supervised machine learning techniques.   \n• Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks.   \n• Evaluation of dependency parsers is based on labeled and unlabeled accuracy scores as measured against withheld development and test corpora. \nBibliographical and Historical Notes \nThe dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars. (Recall from Chapter 12 that the notion of constituency was proposed by Wundt (1900), and popularized by Bloomfield (1914) and (Wells, 1947), and formalized by Chomsky (1956)). By contrast dependency grammar dates back to the Indian grammarian Pa¯n. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the work of Tesnie`re (1959). The most influential dependency grammar frameworks include MeaningText Theory (MTT) (Mel’c˘uk, 1988), Word Grammar (Hudson, 1984), Functional Generative Description (FDG) (Sgall et al., 1986). These frameworks differ along a number of dimensions including the degree and manner in which they deal with morphological, syntactic, semantic and pragmatic factors, their use of multiple layers of representation, and the set of relations used to categorize dependency relations. \nAutomatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). \nDependency parsing saw a major resurgence in the late 1990’s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. \nTransition-based parsing is based on the shift-reduce parsing algorithm originally developed for analyzing programming languages (Aho and Ullman, 1972). Shift-reduce parsing also makes use of a context-free grammar. Input tokens are successively shifted onto the stack and the top two elements of the stack are matched against the right-hand side of the rules in the grammar; when a match is found the matched elements are replaced on the stack (reduced) by the non-terminal from the left-hand side of the rule being matched. In transition-based dependency parsing we skip the grammar, and alter the reduce operation to add a dependency relation between a word and its head. \nNivre (2003) defined the modern, deterministic, transition-based approach to dependency parsing. Subsequent work by Nivre and his colleagues formalized and analyzed the performance of numerous transition systems, training methods, and methods for dealing with non-projective language Nivre and Scholz 2004, Nivre 2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007. The neural approach was pioneered by Chen and Manning (2014) and extended by Kiperwasser and Goldberg (2016); Kulmizev et al. (2019). \nThe graph-based maximum spanning tree approach to dependency parsing was introduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classifier was introduced by (Kiperwasser and Goldberg, 2016). \n\nThe earliest source of data for training and evaluating dependency English parsers came from the WSJ Penn Treebank (Marcus et al., 1993) described in Chapter 12. The use of head-finding rules developed for use with probabilistic parsing facilitated the automatic extraction of dependency parses from phrase-based ones (Xia and Palmer, 2001). \nThe long-running Prague Dependency Treebank project (Hajicˇ, 1998) is the most significant effort to directly annotate a corpus with multiple layers of morphological, syntactic and semantic information. The current PDT 3.0 now contains over $1 . 5 { bf M }$ tokens (Bejcˇek et al., 2013). \nUniversal Dependencies (UD) (Nivre et al., 2016b) is a project directed at creating a consistent framework for dependency treebank annotation across languages with the goal of advancing parser development across the world’s languages. The UD annotation scheme evolved out of several distinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marneffe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). Under the auspices of this effort, treebanks for over 90 languages have been annotated and made available in a single consistent format (Nivre et al., 2016b). \nThe Conference on Natural Language Learning (CoNLL) has conducted an influential series of shared tasks related to dependency parsing over the years (Buchholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajicˇ et al. 2009). More recent evaluations have focused on parser robustness with respect to morphologically rich languages (Seddah et al., 2013), and non-canonical language forms such as social media, texts, and spoken language (Petrov and McDonald, 2012). Choi et al. (2015) presents a performance analysis of 10 dependency parsers across a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool. \nExercises \n15 Logical Representations of Sentence Meaning \nISHMAEL: Surely all this is not without meaning. Herman Melville, Moby Dick \nmeaning representations \nsemantic parsing \ncomputational semantics \nIn this chapter we introduce the idea that the meaning of linguistic expressions can be captured in formal structures called meaning representations. Consider tasks that require some form of semantic processing, like learning to use a new piece of software by reading the manual, deciding what to order at a restaurant by reading a menu, or following a recipe. Accomplishing these tasks requires representations that link the linguistic elements to the necessary non-linguistic knowledge of the world. Reading a menu and deciding what to order, giving advice about where to go to dinner, following a recipe, and generating new recipes all require knowledge about food and its preparation, what people like to eat, and what restaurants are like. Learning to use a piece of software by reading a manual, or giving advice on using software, requires knowledge about the software and similar apps, computers, and users in general. \nIn this chapter, we assume that linguistic expressions have meaning representations that are made up of the same kind of stuff that is used to represent this kind of everyday common-sense knowledge of the world. The process whereby such representations are created and assigned to linguistic inputs is called semantic parsing or semantic analysis, and the entire enterprise of designing meaning representations and associated semantic parsers is referred to as computational semantics. \nConsider Fig. 15.1, which shows example meaning representations for the sentence I have a car using four commonly used meaning representation languages. The top row illustrates a sentence in First-Order Logic, covered in detail in Section 15.3; the directed graph and its corresponding textual form is an example of an Abstract Meaning Representation (AMR) form (Banarescu et al., 2013), and on the right is a frame-based or slot-filler representation, discussed in Section 15.5 and again in Chapter 17.",
        "chapter": "Dependency Parsing",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "While there are non-trivial differences among these approaches, they all share the notion that a meaning representation consists of structures composed from a set of symbols, or representational vocabulary. When appropriately arranged, these symbol structures are taken to correspond to objects, properties of objects, and relations among objects in some state of affairs being represented or reasoned about. In this case, all four representations make use of symbols corresponding to the speaker, a car, and a relation denoting the possession of one by the other. \nImportantly, these representations can be viewed from at least two distinct perspectives in all of these approaches: as representations of the meaning of the particular linguistic input I have a car, and as representations of the state of affairs in some world. It is this dual perspective that allows these representations to be used to link linguistic inputs to the world and to our knowledge of it. \nIn the next sections we give some background: our desiderata for a meaning representation language and some guarantees that these representations will actually do what we need them to do—provide a correspondence to the state of affairs being represented. In Section 15.3 we introduce First-Order Logic, historically the primary technique for investigating natural language semantics, and see in Section 15.4 how it can be used to capture the semantics of events and states in English. Chapter 16 then introduces techniques for semantic parsing: generating these formal meaning representations given linguistic inputs. \n15.1 Computational Desiderata for Representations \nverifiability \nLet’s consider why meaning representations are needed and what they should do for us. To focus this discussion, let’s consider a system that gives restaurant advice to tourists based on a knowledge base. \nVerifiability \nConsider the following simple question: \n(15.1) Does Maharani serve vegetarian food? \nTo answer this question, we have to know what it’s asking, and know whether what it’s asking is true of Maharini or not. verifiability is a system’s ability to compare the state of affairs described by a representation to the state of affairs in some world as modeled in a knowledge base. For example, we’ll need some sort of representation like Serves(Maharani,VegetarianFood), which a system can match against its knowledge base of facts about particular restaurants, and if it finds a representation matching this proposition, it can answer yes. Otherwise, it must either say No if its knowledge of local restaurants is complete, or say that it doesn’t know if it knows its knowledge is incomplete. \nUnambiguous Representations \nSemantics, like all the other domains we have studied, is subject to ambiguity. Words and sentences have different meaning representations in different contexts. Consider the following example: \n(15.2) I wanna eat someplace that’s close to ICSI. \nThis sentence can either mean that the speaker wants to eat at some nearby location, or under a Godzilla-as-speaker interpretation, the speaker may want to devour some nearby location. The sentence is ambiguous; a single linguistic expression can have one of two meanings. But our meaning representations itself cannot be ambiguous. The representation of an input’s meaning should be free from any ambiguity, so that the the system can reason over a representation that means either one thing or the other in order to decide how to answer. \n\nA concept closely related to ambiguity is vagueness: in which a meaning representation leaves some parts of the meaning underspecified. Vagueness does not give rise to multiple representations. Consider the following request: \n(15.3) I want to eat Italian food. \nWhile Italian food may provide enough information to provide recommendations, it is nevertheless vague as to what the user really wants to eat. A vague representation of the meaning of this phrase may be appropriate for some purposes, while a more specific representation may be needed for other purposes. \nCanonical Form \ncanonical form \nThe doctrine of canonical form says that distinct inputs that mean the same thing should have the same meaning representation. This approach greatly simplifies reasoning, since systems need only deal with a single meaning representation for a potentially wide range of expressions. \nConsider the following alternative ways of expressing (15.1): \n(15.4) Does Maharani have vegetarian dishes? (15.5) Do they have vegetarian food at Maharani? (15.6) Are vegetarian dishes served at Maharani? (15.7) Does Maharani serve vegetarian fare? \nDespite the fact these alternatives use different words and syntax, we want them to map to a single canonical meaning representations. If they were all different, assuming the system’s knowledge base contains only a single representation of this fact, most of the representations wouldn’t match. We could, of course, store all possible alternative representations of the same fact in the knowledge base, but doing so would lead to enormous difficulty in keeping the knowledge base consistent. \nCanonical form does complicate the task of semantic parsing. Our system must conclude that vegetarian fare, vegetarian dishes, and vegetarian food refer to the same thing, that having and serving are equivalent here, and that all these parse structures still lead to the same meaning representation. Or consider this pair of examples: \n(15.8) Maharani serves vegetarian dishes.   \n(15.9) Vegetarian dishes are served by Maharani. \nDespite the different placement of the arguments to serve, a system must still assign Maharani and vegetarian dishes to the same roles in the two examples by drawing on grammatical knowledge, such as the relationship between active and passive sentence constructions. \nInference and Variables \nWhat about more complex requests such as: \n(15.10) Can vegetarians eat at Maharani? \nThis request results in the same answer as the others not because they mean the same thing, but because there is a common-sense connection between what vegetarians eat \ninference \nand what vegetarian restaurants serve. This is a fact about the world. We’ll need to connect the meaning representation of this request with this fact about the world in a knowledge base. A system must be able to use inference—to draw valid conclusions based on the meaning representation of inputs and its background knowledge. It must be possible for the system to draw conclusions about the truth of propositions that are not explicitly represented in the knowledge base but that are nevertheless logically derivable from the propositions that are present. \nNow consider the following somewhat more complex request: \n(15.11) I’d like to find a restaurant where I can get vegetarian food \nvariables \nThis request does not make reference to any particular restaurant; the user wants information about an unknown restaurant that serves vegetarian food. Since no restaurants are named, simple matching is not going to work. Answering this request requires the use of variables, using some representation like the following: \nMatching succeeds only if the variable $x$ can be replaced by some object in the knowledge base in such a way that the entire proposition will then match. The concept that is substituted for the variable can then be used to fulfill the user’s request. It is critical for any meaning representation language to be able to handle these kinds of indefinite references. \nExpressiveness \nFinally, a meaning representation scheme must be expressive enough to handle a wide range of subject matter, ideally any sensible natural language utterance. Although this is probably too much to expect from any single representational system, First-Order Logic, as described in Section 15.3, is expressive enough to handle quite a lot of what needs to be represented. \n15.2 Model-Theoretic Semantics \nmodel \nWhat is it about meaning representation languages that allows them to fulfill these desiderata, bridging the gap from formal representations to representations that tell us something about some state of affairs in the world? \nThe answer is a model. A model is a formal construct that stands for the particular state of affairs in the world. Expressions in a meaning representation language can be mapped to elements of the model, like objects, properties of objects, and relations among objects. If the model accurately captures the facts we’re interested in, then a consistent mapping between the meaning representation and the model provides the bridge between meaning representation and world. Models provide a surprisingly simple and powerful way to ground the expressions in meaning representation languages. \nnon-logical vocabulary \nlogical vocabulary \nFirst, some terminology. The vocabulary of a meaning representation consists of two parts: the non-logical vocabulary and the logical vocabulary. The non-logical vocabulary consists of the open-ended set of names for the objects, properties, and relations that make up the world we’re trying to represent. These appear in various schemes as predicates, nodes, labels on links, or labels in slots in frames, The logical vocabulary consists of the closed set of symbols, operators, quantifiers, links,",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Computational Desiderata for Representations",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "inference \nand what vegetarian restaurants serve. This is a fact about the world. We’ll need to connect the meaning representation of this request with this fact about the world in a knowledge base. A system must be able to use inference—to draw valid conclusions based on the meaning representation of inputs and its background knowledge. It must be possible for the system to draw conclusions about the truth of propositions that are not explicitly represented in the knowledge base but that are nevertheless logically derivable from the propositions that are present. \nNow consider the following somewhat more complex request: \n(15.11) I’d like to find a restaurant where I can get vegetarian food \nvariables \nThis request does not make reference to any particular restaurant; the user wants information about an unknown restaurant that serves vegetarian food. Since no restaurants are named, simple matching is not going to work. Answering this request requires the use of variables, using some representation like the following: \nMatching succeeds only if the variable $x$ can be replaced by some object in the knowledge base in such a way that the entire proposition will then match. The concept that is substituted for the variable can then be used to fulfill the user’s request. It is critical for any meaning representation language to be able to handle these kinds of indefinite references. \nExpressiveness \nFinally, a meaning representation scheme must be expressive enough to handle a wide range of subject matter, ideally any sensible natural language utterance. Although this is probably too much to expect from any single representational system, First-Order Logic, as described in Section 15.3, is expressive enough to handle quite a lot of what needs to be represented. \n15.2 Model-Theoretic Semantics \nmodel \nWhat is it about meaning representation languages that allows them to fulfill these desiderata, bridging the gap from formal representations to representations that tell us something about some state of affairs in the world? \nThe answer is a model. A model is a formal construct that stands for the particular state of affairs in the world. Expressions in a meaning representation language can be mapped to elements of the model, like objects, properties of objects, and relations among objects. If the model accurately captures the facts we’re interested in, then a consistent mapping between the meaning representation and the model provides the bridge between meaning representation and world. Models provide a surprisingly simple and powerful way to ground the expressions in meaning representation languages. \nnon-logical vocabulary \nlogical vocabulary \nFirst, some terminology. The vocabulary of a meaning representation consists of two parts: the non-logical vocabulary and the logical vocabulary. The non-logical vocabulary consists of the open-ended set of names for the objects, properties, and relations that make up the world we’re trying to represent. These appear in various schemes as predicates, nodes, labels on links, or labels in slots in frames, The logical vocabulary consists of the closed set of symbols, operators, quantifiers, links, \ndenotation \netc., that provide the formal means for composing expressions in a given meaning representation language. \ndomain \nEach element of the non-logical vocabulary must have a denotation in the model, meaning that every element corresponds to a fixed, well-defined part of the model. Let’s start with objects. The domain of a model is the set of objects that are being represented. Each distinct concept, category, or individual denotes a unique element in the domain. \nextensional \nWe represent properties of objects in a model by denoting the domain elements that have the property; that is, properties denote sets. The denotation of the property red is the set of things we think are red. Similarly, a relation among object denotes a set of ordered lists, or tuples, of domain elements that take part in the relation: the denotation of the relation Married is set of pairs of domain objects that are married. This approach to properties and relations is called extensional, because we define concepts by their extension, their denotations. To summarize: \n• Objects denote elements of the domain • Properties denote sets of elements of the domain • Relations denote sets of tuples of elements of the domain \ninterpretation \nWe now need a mapping that gets us from our meaning representation to the corresponding denotations: a function that maps from the non-logical vocabulary of our meaning representation to the proper denotations in the model. We’ll call such a mapping an interpretation. \nLet’s return to our restaurant advice application, and let its domain consist of sets of restaurants, patrons, facts about the likes and dislikes of the patrons, and facts about the restaurants such as their cuisine, typical cost, and noise level. To begin populating our domain, $mathrm { textmathcal { D } }$ , let’s assume that we’re dealing with four patrons designated by the non-logical symbols Matthew, Franco, Katie, and Caroline. denoting four unique domain elements. We’ll use the constants ${ } _ { a , b , c }$ and, $d$ to stand for these domain elements. We’re deliberately using meaningless, non-mnemonic names for our domain elements to emphasize the fact that whatever it is that we know about these entities has to come from the formal properties of the model and not from the names of the symbols. Continuing, let’s assume that our application includes three restaurants, designated as Frasca, Med, and Rio in our meaning representation, that denote the domain elements $_ { e , f }$ , and $g$ . Finally, let’s assume that we’re dealing with the three cuisines Italian, Mexican, and Eclectic, denoted by $h , i$ , and $j$ in our model. \nProperties like Noisy denote the subset of restaurants from our domain that are known to be noisy. Two-place relational notions, such as which restaurants individual patrons Like, denote ordered pairs, or tuples, of the objects from the domain. And, since we decided to represent cuisines as objects in our model, we can capture which restaurants Serve which cuisines as a set of tuples. One possible state of affairs using this scheme is given in Fig. 15.2. \nGiven this simple scheme, we can ground our meaning representations by consulting the appropriate denotations in the corresponding model. For example, we can evaluate a representation claiming that Matthew likes the Rio, or that The Med serves Italian by mapping the objects in the meaning representations to their corresponding domain elements and mapping any links, predicates, or slots in the meaning representation to the appropriate relations in the model. More concretely, we can verify a representation asserting that Matthew likes Frasca by first using our interpretation function to map the symbol Matthew to its denotation a, Frasca to $e$ , and the Likes relation to the appropriate set of tuples. We then check that set of tuples for the presence of the tuple $langle a , e rangle$ . If, as it is in this case, the tuple is present in the model, then we can conclude that Matthew likes Frasca is true; if it isn’t then we can’t. \n\nThis is all pretty straightforward—we’re using sets and operations on sets to ground the expressions in our meaning representations. Of course, the more interesting part comes when we consider more complex examples such as the following: \n(15.13) Katie likes the Rio and Matthew likes the Med.   \n(15.14) Katie and Caroline like the same restaurants.   \n(15.15) Franco likes noisy, expensive restaurants.   \n(15.16) Not everybody likes Frasca. \nOur simple scheme for grounding the meaning of representations is not adequate for examples such as these. Plausible meaning representations for these examples will not map directly to individual entities, properties, or relations. Instead, they involve complications such as conjunctions, equality, quantified variables, and negations. To assess whether these statements are consistent with our model, we’ll have to tear them apart, assess the parts, and then determine the meaning of the whole from the meaning of the parts. \nConsider the first example above. A meaning representation for this example will include two distinct propositions expressing the individual patron’s preferences, conjoined with some kind of implicit or explicit conjunction operator. Our model doesn’t have a relation that encodes pairwise preferences for all of the patrons and restaurants in our model, nor does it need to. We know from our model that Matthew likes the Med and separately that Katie likes the Rio (that is, the tuples $left. a , f right.$ and $langle c , g rangle$ are members of the set denoted by the Likes relation). All we really need to know is how to deal with the semantics of the conjunction operator. If we assume the simplest possible semantics for the English word and, the whole statement is true if it is the case that each of the components is true in our model. In this case, both components are true since the appropriate tuples are present and therefore the sentence as a whole is true. \nWhat we’ve done with this example is provide a truth-conditional semantics for the assumed conjunction operator in some meaning representation. That is, we’ve provided a method for determining the truth of a complex expression from the meanings of the parts (by consulting a model) and the meaning of an operator by consulting a truth table. Meaning representation languages are truth-conditional to the extent that they give a formal specification as to how we can determine the meaning of complex sentences from the meaning of their parts. In particular, we need to know the semantics of the entire logical vocabulary of the meaning representation scheme being used. \n\nNote that although the details of how this happens depend on details of the particular meaning representation being used, it should be clear that assessing the truth conditions of examples like these involves nothing beyond the simple set operations we’ve been discussing. We return to these issues in the next section in the context of the semantics of First-Order Logic. \n15.3 First-Order Logic \nFirst-Order Logic (FOL) is a flexible, well-understood, and computationally tractable meaning representation language that satisfies many of the desiderata given in Section 15.1. It provides a sound computational basis for the verifiability, inference, and expressiveness requirements, as well as a sound model-theoretic semantics. \nAn additional attractive feature of FOL is that it makes few specific commitments as to how things ought to be represented, and those it does are shared by many of the schemes mentioned earlier: the represented world consists of objects, properties of objects, and relations among objects. \nThe remainder of this section introduces the basic syntax and semantics of FOL and then describes the application of FOL to the representation of events. \n15.3.1 Basic Elements of First-Order Logic \nLet’s explore FOL by first examining its various atomic elements and then showing how they can be composed to create larger meaning representations. Figure 15.3,",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Model-Theoretic Semantics",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Note that although the details of how this happens depend on details of the particular meaning representation being used, it should be clear that assessing the truth conditions of examples like these involves nothing beyond the simple set operations we’ve been discussing. We return to these issues in the next section in the context of the semantics of First-Order Logic. \n15.3 First-Order Logic \nFirst-Order Logic (FOL) is a flexible, well-understood, and computationally tractable meaning representation language that satisfies many of the desiderata given in Section 15.1. It provides a sound computational basis for the verifiability, inference, and expressiveness requirements, as well as a sound model-theoretic semantics. \nAn additional attractive feature of FOL is that it makes few specific commitments as to how things ought to be represented, and those it does are shared by many of the schemes mentioned earlier: the represented world consists of objects, properties of objects, and relations among objects. \nThe remainder of this section introduces the basic syntax and semantics of FOL and then describes the application of FOL to the representation of events. \n15.3.1 Basic Elements of First-Order Logic \nLet’s explore FOL by first examining its various atomic elements and then showing how they can be composed to create larger meaning representations. Figure 15.3, \nwhich provides a complete context-free grammar for the particular syntax of FOL that we will use, is our roadmap for this section. \nterm \nLet’s begin by examining the notion of a term, the FOL device for representing objects. As can be seen from Fig. 15.3, FOL provides three ways to represent these basic building blocks: constants, functions, and variables. Each of these devices can be thought of as designating an object in the world under consideration. \nconstant \nConstants in FOL refer to specific objects in the world being described. Such constants are conventionally depicted as either single capitalized letters such as $A$ and $B$ or single capitalized words that are often reminiscent of proper nouns such as Maharani and Harry. Like programming language constants, FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them. \nfunction \nFunctions in FOL correspond to concepts that are often expressed in English as genitives such as Frasca’s location. A FOL translation of such an expression might look like the following. \nvariable \nFOL functions are syntactically the same as single argument predicates. It is important to remember, however, that while they have the appearance of predicates, they are in fact terms in that they refer to unique objects. Functions provide a convenient way to refer to specific objects without having to associate a named constant with them. This is particularly convenient in cases in which many named objects, like restaurants, have a unique concept such as a location associated with them. \nVariables are our final FOL mechanism for referring to objects. Variables, depicted as single lower-case letters, let us make assertions and draw inferences about objects without having to make reference to any particular named object. This ability to make statements about anonymous objects comes in two flavors: making statements about a particular unknown object and making statements about all the objects in some arbitrary world of objects. We return to the topic of variables after we have presented quantifiers, the elements of FOL that make variables useful. \nNow that we have the means to refer to objects, we can move on to the FOL mechanisms that are used to state relations that hold among objects. Predicates are symbols that refer to, or name, the relations that hold among some fixed number of objects in a given domain. Returning to the example introduced informally in Section 15.1, a reasonable FOL representation for Maharani serves vegetarian food might look like the following formula: \nThis FOL sentence asserts that Serves, a two-place predicate, holds between the objects denoted by the constants Maharani and VegetarianFood. \nA somewhat different use of predicates is illustrated by the following fairly typical representation for a sentence like Maharani is a restaurant: \nRestaurant(Maharani)\nThis is an example of a one-place predicate that is used, not to relate multiple objects, but rather to assert a property of a single object. In this case, it encodes the category membership of Maharani. \nWith the ability to refer to objects, to assert facts about objects, and to relate objects to one another, we can create rudimentary composite representations. These representations correspond to the atomic formula level in Fig. 15.3. This ability to compose complex representations is, however, not limited to the use of single predicates. Larger composite representations can also be put together through the use of logical connectives. As can be seen from Fig. 15.3, logical connectives let us create larger representations by conjoining logical formulas using one of three operators. Consider, for example, the following BERP sentence and one possible representation for it: \n\n(15.20) I only have five dollars and I don’t have a lot of time. \nThe semantic representation for this example is built up in a straightforward way from the semantics of the individual clauses through the use of the $wedge$ and $lnot$ operators. Note that the recursive nature of the grammar in Fig. 15.3 allows an infinite number of logical formulas to be created through the use of these connectives. Thus, as with syntax, we can use a finite device to create an infinite number of representations. \n15.3.2 Variables and Quantifiers \nquantifiers \nWe now have all the machinery necessary to return to our earlier discussion of variables. As noted above, variables are used in two ways in FOL: to refer to particular anonymous objects and to refer generically to all objects in a collection. These two uses are made possible through the use of operators known as quantifiers. The two operators that are basic to FOL are the existential quantifier, which is denoted and is pronounced as “there exists”, and the universal quantifier, which is denoted $forall$ and is pronounced as “for all”. \nThe need for an existentially quantified variable is often signaled by the presence of an indefinite noun phrase in English. Consider the following example: \n(15.22) a restaurant that serves Mexican food near ICSI. \nHere, reference is being made to an anonymous object of a specified category with particular properties. The following would be a reasonable representation of the meaning of such a phrase: \nThe existential quantifier at the head of this sentence instructs us on how to interpret the variable $x$ in the context of this sentence. Informally, it says that for this sentence to be true there must be at least one object such that if we were to substitute it for the variable $x$ , the resulting sentence would be true. For example, if AyCaramba is a Mexican restaurant near ICSI, then substituting AyCaramba for $x$ results in the following logical formula: \nBased on the semantics of the $wedge$ operator, this sentence will be true if all of its three component atomic formulas are true. These in turn will be true if they are either present in the system’s knowledge base or can be inferred from other facts in the knowledge base. \nThe use of the universal quantifier also has an interpretation based on substitution of known objects for variables. The substitution semantics for the universal quantifier takes the expression for all quite literally; the $forall$ operator states that for the logical formula in question to be true, the substitution of any object in the knowledge base for the universally quantified variable should result in a true formula. This is in marked contrast to the ∃ operator, which only insists on a single valid substitution for the sentence to be true.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "First-Order Logic",
        "subsection": "Basic Elements of First-Order Logic",
        "subsubsection": "N/A"
    },
    {
        "content": "(15.20) I only have five dollars and I don’t have a lot of time. \nThe semantic representation for this example is built up in a straightforward way from the semantics of the individual clauses through the use of the $wedge$ and $lnot$ operators. Note that the recursive nature of the grammar in Fig. 15.3 allows an infinite number of logical formulas to be created through the use of these connectives. Thus, as with syntax, we can use a finite device to create an infinite number of representations. \n15.3.2 Variables and Quantifiers \nquantifiers \nWe now have all the machinery necessary to return to our earlier discussion of variables. As noted above, variables are used in two ways in FOL: to refer to particular anonymous objects and to refer generically to all objects in a collection. These two uses are made possible through the use of operators known as quantifiers. The two operators that are basic to FOL are the existential quantifier, which is denoted and is pronounced as “there exists”, and the universal quantifier, which is denoted $forall$ and is pronounced as “for all”. \nThe need for an existentially quantified variable is often signaled by the presence of an indefinite noun phrase in English. Consider the following example: \n(15.22) a restaurant that serves Mexican food near ICSI. \nHere, reference is being made to an anonymous object of a specified category with particular properties. The following would be a reasonable representation of the meaning of such a phrase: \nThe existential quantifier at the head of this sentence instructs us on how to interpret the variable $x$ in the context of this sentence. Informally, it says that for this sentence to be true there must be at least one object such that if we were to substitute it for the variable $x$ , the resulting sentence would be true. For example, if AyCaramba is a Mexican restaurant near ICSI, then substituting AyCaramba for $x$ results in the following logical formula: \nBased on the semantics of the $wedge$ operator, this sentence will be true if all of its three component atomic formulas are true. These in turn will be true if they are either present in the system’s knowledge base or can be inferred from other facts in the knowledge base. \nThe use of the universal quantifier also has an interpretation based on substitution of known objects for variables. The substitution semantics for the universal quantifier takes the expression for all quite literally; the $forall$ operator states that for the logical formula in question to be true, the substitution of any object in the knowledge base for the universally quantified variable should result in a true formula. This is in marked contrast to the ∃ operator, which only insists on a single valid substitution for the sentence to be true. \n\nConsider the following example: \n(15.25) All vegetarian restaurants serve vegetarian food. \nA reasonable representation for this sentence would be something like the following: \nFor this sentence to be true, every substitution of a known object for $x$ must result in a sentence that is true. We can divide the set of all possible substitutions into the set of objects consisting of vegetarian restaurants and the set consisting of everything else. Let us first consider the case in which the substituted object actually is a vegetarian restaurant; one such substitution would result in the following sentence: \nIf we assume that we know that the consequent clause \nis true, then this sentence as a whole must be true. Both the antecedent and the consequent have the value True and, therefore, according to the first two rows of Fig. 15.4 on page 346 the sentence itself can have the value True. This result will be the same for all possible substitutions of Terms representing vegetarian restaurants for $x$ . \nRemember, however, that for this sentence to be true, it must be true for all possible substitutions. What happens when we consider a substitution from the set of objects that are not vegetarian restaurants? Consider the substitution of a nonvegetarian restaurant such as AyCaramba for the variable $x$ : \nSince the antecedent of the implication is False, we can determine from Fig. 15.4 that the sentence is always True, again satisfying the $forall$ constraint. \nNote that it may still be the case that AyCaramba serves vegetarian food without actually being a vegetarian restaurant. Note also that, despite our choice of examples, there are no implied categorical restrictions on the objects that can be substituted for $x$ by this kind of reasoning. In other words, there is no restriction of $x$ to restaurants or concepts related to them. Consider the following substitution: \nHere the antecedent is still false so the rule remains true under this kind of irrelevant substitution. \nTo review, variables in logical formulas must be either existentially $textcircled{7}$ or universally $left( forall right)$ quantified. To satisfy an existentially quantified variable, at least one substitution must result in a true sentence. To satisfy a universally quantified variable, all substitutions must result in true sentences. \n15.3.3 Lambda Notation \nlambda notation \nThe final element we need to complete our discussion of FOL is called the lambda notation (Church, 1940). This notation provides a way to abstract from fully specified FOL formulas in a way that will be particularly useful for semantic analysis. The lambda notation extends the syntax of FOL to include expressions of the following form: \nSuch expressions consist of the Greek symbol $lambda$ , followed by one or more variables, followed by a FOL formula that makes use of those variables. \nThe usefulness of these $lambda$ -expressions is based on the ability to apply them to logical terms to yield new FOL expressions where the formal parameter variables are bound to the specified terms. This process is known as $lambda$ -reduction, and consists of a simple textual replacement of the $lambda$ variables and the removal of the $lambda$ . The following expressions illustrate the application of a $lambda$ -expression to the constant $A$ , followed by the result of performing a $lambda$ -reduction on this expression: \nAn important and useful variation of this technique is the use of one $lambda$ -expression as the body of another as in the following expression: \nThis fairly abstract expression can be glossed as the state of something being near something else. The following expressions illustrate a single $lambda$ -application and subsequent reduction with this kind of embedded $lambda$ -expression: \nThe important point here is that the resulting expression is still a $lambda$ -expression; the first reduction bound the variable $x$ and removed the outer $lambda$ , thus revealing the inner expression. As might be expected, this resulting $lambda$ -expression can, in turn, be applied to another term to arrive at a fully specified logical formula, as in the following: \nThis general technique, called currying1 (Scho¨nfinkel, 1924) is a way of converting a predicate with multiple arguments into a sequence of single-argument predicates. \nAs we show in Chapter 16, the $lambda$ -notation provides a way to incrementally gather arguments to a predicate when they do not all appear together as daughters of the predicate in a parse tree. \n15.3.4 The Semantics of First-Order Logic \nThe various objects, properties, and relations represented in a FOL knowledge base acquire their meanings by virtue of their correspondence to objects, properties, and relations out in the external world being modeled. We can accomplish this by employing the model-theoretic approach introduced in Section 15.2. Recall that this approach employs simple set-theoretic notions to provide a truth-conditional mapping from the expressions in a meaning representation to the state of affairs being modeled. We can apply this approach to FOL by going through all the elements in Fig. 15.3 on page 341 and specifying how each should be accounted for.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "First-Order Logic",
        "subsection": "Variables and Quantifiers",
        "subsubsection": "N/A"
    },
    {
        "content": "15.3.3 Lambda Notation \nlambda notation \nThe final element we need to complete our discussion of FOL is called the lambda notation (Church, 1940). This notation provides a way to abstract from fully specified FOL formulas in a way that will be particularly useful for semantic analysis. The lambda notation extends the syntax of FOL to include expressions of the following form: \nSuch expressions consist of the Greek symbol $lambda$ , followed by one or more variables, followed by a FOL formula that makes use of those variables. \nThe usefulness of these $lambda$ -expressions is based on the ability to apply them to logical terms to yield new FOL expressions where the formal parameter variables are bound to the specified terms. This process is known as $lambda$ -reduction, and consists of a simple textual replacement of the $lambda$ variables and the removal of the $lambda$ . The following expressions illustrate the application of a $lambda$ -expression to the constant $A$ , followed by the result of performing a $lambda$ -reduction on this expression: \nAn important and useful variation of this technique is the use of one $lambda$ -expression as the body of another as in the following expression: \nThis fairly abstract expression can be glossed as the state of something being near something else. The following expressions illustrate a single $lambda$ -application and subsequent reduction with this kind of embedded $lambda$ -expression: \nThe important point here is that the resulting expression is still a $lambda$ -expression; the first reduction bound the variable $x$ and removed the outer $lambda$ , thus revealing the inner expression. As might be expected, this resulting $lambda$ -expression can, in turn, be applied to another term to arrive at a fully specified logical formula, as in the following: \nThis general technique, called currying1 (Scho¨nfinkel, 1924) is a way of converting a predicate with multiple arguments into a sequence of single-argument predicates. \nAs we show in Chapter 16, the $lambda$ -notation provides a way to incrementally gather arguments to a predicate when they do not all appear together as daughters of the predicate in a parse tree. \n15.3.4 The Semantics of First-Order Logic \nThe various objects, properties, and relations represented in a FOL knowledge base acquire their meanings by virtue of their correspondence to objects, properties, and relations out in the external world being modeled. We can accomplish this by employing the model-theoretic approach introduced in Section 15.2. Recall that this approach employs simple set-theoretic notions to provide a truth-conditional mapping from the expressions in a meaning representation to the state of affairs being modeled. We can apply this approach to FOL by going through all the elements in Fig. 15.3 on page 341 and specifying how each should be accounted for.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "First-Order Logic",
        "subsection": "Lambda Notation",
        "subsubsection": "N/A"
    },
    {
        "content": "15.3.3 Lambda Notation \nlambda notation \nThe final element we need to complete our discussion of FOL is called the lambda notation (Church, 1940). This notation provides a way to abstract from fully specified FOL formulas in a way that will be particularly useful for semantic analysis. The lambda notation extends the syntax of FOL to include expressions of the following form: \nSuch expressions consist of the Greek symbol $lambda$ , followed by one or more variables, followed by a FOL formula that makes use of those variables. \nThe usefulness of these $lambda$ -expressions is based on the ability to apply them to logical terms to yield new FOL expressions where the formal parameter variables are bound to the specified terms. This process is known as $lambda$ -reduction, and consists of a simple textual replacement of the $lambda$ variables and the removal of the $lambda$ . The following expressions illustrate the application of a $lambda$ -expression to the constant $A$ , followed by the result of performing a $lambda$ -reduction on this expression: \nAn important and useful variation of this technique is the use of one $lambda$ -expression as the body of another as in the following expression: \nThis fairly abstract expression can be glossed as the state of something being near something else. The following expressions illustrate a single $lambda$ -application and subsequent reduction with this kind of embedded $lambda$ -expression: \nThe important point here is that the resulting expression is still a $lambda$ -expression; the first reduction bound the variable $x$ and removed the outer $lambda$ , thus revealing the inner expression. As might be expected, this resulting $lambda$ -expression can, in turn, be applied to another term to arrive at a fully specified logical formula, as in the following: \nThis general technique, called currying1 (Scho¨nfinkel, 1924) is a way of converting a predicate with multiple arguments into a sequence of single-argument predicates. \nAs we show in Chapter 16, the $lambda$ -notation provides a way to incrementally gather arguments to a predicate when they do not all appear together as daughters of the predicate in a parse tree. \n15.3.4 The Semantics of First-Order Logic \nThe various objects, properties, and relations represented in a FOL knowledge base acquire their meanings by virtue of their correspondence to objects, properties, and relations out in the external world being modeled. We can accomplish this by employing the model-theoretic approach introduced in Section 15.2. Recall that this approach employs simple set-theoretic notions to provide a truth-conditional mapping from the expressions in a meaning representation to the state of affairs being modeled. We can apply this approach to FOL by going through all the elements in Fig. 15.3 on page 341 and specifying how each should be accounted for. \n\nWe can start by asserting that the objects in our world, FOL terms, denote elements in a domain, and asserting that atomic formulas are captured either as sets of domain elements for properties, or as sets of tuples of elements for relations. As an example, consider the following: \n(15.34) Centro is near Bacaro. \nCapturing the meaning of this example in FOL involves identifying the Terms and Predicates that correspond to the various grammatical elements in the sentence and creating logical formulas that capture the relations implied by the words and syntax of the sentence. For this example, such an effort might yield something like the following: \nThe meaning of this logical formula is based on whether the domain elements denoted by the terms Centro and Bacaro are contained among the tuples denoted by the relation denoted by the predicate Near in the current model. \nThe interpretation of formulas involving logical connectives is based on the meanings of the components in the formulas combined with the meanings of the connectives they contain. Figure 15.4 gives interpretations for each of the logical operators shown in Fig. 15.3. \nThe semantics of the $wedge$ (and) and $lnot$ (not) operators are fairly straightforward, and are correlated with at least some of the senses of the corresponding English terms. However, it is worth pointing out that the (or) operator is not disjunctive in the same way that the corresponding English word is, and that the $implies$ (implies) operator is only loosely based on any common-sense notions of implication or causation. \nThe final bit we need to address involves variables and quantifiers. Recall that there are no variables in our set-based models, only elements of the domain and relations that hold among them. We can provide a model-based account for formulas with variables by employing the notion of a substitution introduced earlier on page 343. Formulas involving are true if a substitution of terms for variables results in a formula that is true in the model. Formulas involving $forall$ must be true under all possible substitutions. \n15.3.5 Inference \nA meaning representation language must support inference to add valid new propositions to a knowledge base or to determine the truth of propositions not explicitly",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "First-Order Logic",
        "subsection": "The Semantics of First-Order Logic",
        "subsubsection": "N/A"
    },
    {
        "content": "We can start by asserting that the objects in our world, FOL terms, denote elements in a domain, and asserting that atomic formulas are captured either as sets of domain elements for properties, or as sets of tuples of elements for relations. As an example, consider the following: \n(15.34) Centro is near Bacaro. \nCapturing the meaning of this example in FOL involves identifying the Terms and Predicates that correspond to the various grammatical elements in the sentence and creating logical formulas that capture the relations implied by the words and syntax of the sentence. For this example, such an effort might yield something like the following: \nThe meaning of this logical formula is based on whether the domain elements denoted by the terms Centro and Bacaro are contained among the tuples denoted by the relation denoted by the predicate Near in the current model. \nThe interpretation of formulas involving logical connectives is based on the meanings of the components in the formulas combined with the meanings of the connectives they contain. Figure 15.4 gives interpretations for each of the logical operators shown in Fig. 15.3. \nThe semantics of the $wedge$ (and) and $lnot$ (not) operators are fairly straightforward, and are correlated with at least some of the senses of the corresponding English terms. However, it is worth pointing out that the (or) operator is not disjunctive in the same way that the corresponding English word is, and that the $implies$ (implies) operator is only loosely based on any common-sense notions of implication or causation. \nThe final bit we need to address involves variables and quantifiers. Recall that there are no variables in our set-based models, only elements of the domain and relations that hold among them. We can provide a model-based account for formulas with variables by employing the notion of a substitution introduced earlier on page 343. Formulas involving are true if a substitution of terms for variables results in a formula that is true in the model. Formulas involving $forall$ must be true under all possible substitutions. \n15.3.5 Inference \nA meaning representation language must support inference to add valid new propositions to a knowledge base or to determine the truth of propositions not explicitly \ncontained within a knowledge base (Section 15.1). This section briefly discusses modus ponens, the most widely implemented inference method provided by FOL. \nModus ponens is a form of inference that corresponds to what is informally known as $i f$ -then reasoning. We can abstractly define modus ponens as follows, where $alpha$ and $beta$ should be taken as FOL formulas: \nforward chaining \nA schema like this indicates that the formula below the line can be inferred from the formulas above the line by some form of inference. Modus ponens states that if the left-hand side of an implication rule is true, then the right-hand side of the rule can be inferred. In the following discussions, we will refer to the left-hand side of an implication as the antecedent and the right-hand side as the consequent. \nFor a typical use of modus ponens, consider the following example, which uses a rule from the last section: \nHere, the formula VegetarianRestaurant $( L e a f )$ matches the antecedent of the rule, thus allowing us to use modus ponens to conclude $S e r nu e s ( L e a f , V e g e t a r i a n F o o d )$ . \nModus ponens can be put to practical use in one of two ways: forward chaining and backward chaining. In forward chaining systems, modus ponens is used in precisely the manner just described. As individual facts are added to the knowledge base, modus ponens is used to fire all applicable implication rules. In this kind of arrangement, as soon as a new fact is added to the knowledge base, all applicable implication rules are found and applied, each resulting in the addition of new facts to the knowledge base. These new propositions in turn can be used to fire implication rules applicable to them. The process continues until no further facts can be deduced. \nThe forward chaining approach has the advantage that facts will be present in the knowledge base when needed, because, in a sense all inference is performed in advance. This can substantially reduce the time needed to answer subsequent queries since they should all amount to simple lookups. The disadvantage of this approach is that facts that will never be needed may be inferred and stored. \nIn backward chaining, modus ponens is run in reverse to prove specific propositions called queries. The first step is to see if the query formula is true by determining if it is present in the knowledge base. If it is not, then the next step is to search for applicable implication rules present in the knowledge base. An applicable rule is one whereby the consequent of the rule matches the query formula. If there are any such rules, then the query can be proved if the antecedent of any one them can be shown to be true. This can be performed recursively by backward chaining on the antecedent as a new query. The Prolog programming language is a backward chaining system that implements this strategy. \nTo see how this works, let’s assume that we have been asked to verify the truth of the proposition $S e r nu e s ( L e a f , V e g e t a r i a n F o o d )$ , assuming the facts given above the line in (15.37). Since this proposition is not present in the knowledge base, a search for an applicable rule is initiated resulting in the rule given above. After substituting the constant Leaf for the variable $x$ , our next task is to prove the antecedent of the rule, VegetarianRestaurant $( L e a f )$ , which, of course, is one of the facts we are given. \n\nNote that it is critical to distinguish between reasoning by backward chaining from queries to known facts and reasoning backwards from known consequents to unknown antecedents. To be specific, by reasoning backwards we mean that if the consequent of a rule is known to be true, we assume that the antecedent will be as well. For example, let’s assume that we know that $S e r nu e s ( L e a f , V e g e t a r i a n F o o d )$ is true. Since this fact matches the consequent of our rule, we might reason backwards to the conclusion that VegetarianRestaurant(Leaf ). \nabduction \nWhile backward chaining is a sound method of reasoning, reasoning backwards is an invalid, though frequently useful, form of plausible reasoning. Plausible reasoning from consequents to antecedents is known as abduction, and as we show in Chapter 22, is often useful in accounting for many of the inferences people make while analyzing extended discourses. \ncomplete \nresolution \nWhile forward and backward reasoning are sound, neither is complete. This means that there are valid inferences that cannot be found by systems using these methods alone. Fortunately, there is an alternative inference technique called resolution that is sound and complete. Unfortunately, inference systems based on resolution are far more computationally expensive than forward or backward chaining systems. In practice, therefore, most systems use some form of chaining and place a burden on knowledge base developers to encode the knowledge in a fashion that permits the necessary inferences to be drawn. \n15.4 Event and State Representations \nMuch of the semantics that we wish to capture consists of representations of states and events. States are conditions, or properties, that remain unchanged over an extended period of time, and events denote changes in some state of affairs. The representation of both states and events may involve a host of participants, props, times and locations. \nThe representations for events and states that we have used thus far have consisted of single predicates with as many arguments as are needed to incorporate all the roles associated with a given example. For example, the representation for Leaf serves vegetarian fare consists of a single predicate with arguments for the entity doing the serving and the thing served. \nThis approach assumes that the predicate used to represent an event verb has the same number of arguments as are present in the verb’s syntactic subcategorization frame. Unfortunately, this is clearly not always the case. Consider the following examples of the verb eat: \n(15.39) I ate.   \n(15.40) I ate a turkey sandwich.   \n(15.41) I ate a turkey sandwich at my desk.   \n(15.42) I ate at my desk.   \n(15.43) I ate lunch.   \n(15.44) I ate a turkey sandwich for lunch.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "First-Order Logic",
        "subsection": "Inference",
        "subsubsection": "N/A"
    },
    {
        "content": "(15.45) I ate a turkey sandwich for lunch at my desk. \nevent variable \nClearly, choosing the correct number of arguments for the predicate representing the meaning of eat is a tricky problem. These examples introduce five distinct arguments, or roles, in an array of different syntactic forms, locations, and combinations. Unfortunately, predicates in FOL have fixed arity – they take a fixed number of arguments. \nTo address this problem, we introduce the notion of an event variable to allow us to make assertions about particular events. To do this, we can refactor our event predicates to have an existentially quantified variable as their first, and only, argument. Using this event variable, we can introduce additional predicates to represent the other information we have about the event. These predicates take an event variable as their first argument and related FOL terms as their second argument. The following formula illustrates this scheme with the meaning representation of 15.40 from our earlier discussion. \nneoDavidsonian \nHere, the quantified variable $e$ stands for the eating event and is used to bind the event predicate with the core information provided via the named roles Eater and Eaten. To handle the more complex examples, we simply add additional relations to capture the provided information, as in the following for 15.45. \nEvent representations of this sort are referred to as neo-Davidsonian event representations (Davidson 1967, Parsons 1990) after the philosopher Donald Davidson who introduced the notion of an event variable (Davidson, 1967). To summarize, in the neo-Davidsonian approach to event representations: \n• Events are captured with predicates that take a single event variable as an argument.   \n• There is no need to specify a fixed number of arguments for a given FOL predicate; rather, as many roles and fillers can be glued on as are provided in the input.   \n• No more roles are postulated than are mentioned in the input.   \n• The logical connections among closely related inputs that share the same predicate are satisfied without the need for additional inference. \nThis approach still leaves us with the problem of determining the set of predicates needed to represent roles associated with specific events like Eater and Eaten, as well as more general concepts like Location and Time. We’ll return to this problem in more detail in Chapter 10. \n15.4.1 Representing Time \ntemporal logic \nIn our discussion of events, we did not seriously address the issue of capturing the time when the represented events are supposed to have occurred. The representation of such information in a useful form is the domain of temporal logic. This discussion introduces the most basic concerns of temporal logic and briefly discusses the means by which human languages convey temporal information, which, among other things, includes tense logic, the ways that verb tenses convey temporal infor\ntense logic \nmation. A more detailed discussion of robust approaches to the representation and analysis of temporal expressions is presented in Chapter 17. \nThe most straightforward theory of time holds that it flows inexorably forward and that events are associated with either points or intervals in time, as on a timeline. We can order distinct events by situating them on the timeline; one event precedes another if the flow of time leads from the first event to the second. Accompanying these notions in most theories is the idea of the current moment in time. Combining this notion with the idea of a temporal ordering relationship yields the familiar notions of past, present, and future. \nMany schemes can represent this kind of temporal information. The one presented here is a fairly simple one that stays within the FOL framework of reified events that we have been pursuing. Consider the following examples: \n(15.47) I arrived in New York.   \n(15.48) I am arriving in New York.   \n(15.49) I will arrive in New York. \nThese sentences all refer to the same kind of event and differ solely in the tense of the verb. In our current scheme for representing events, all three would share the following kind of representation, which lacks any temporal information: \nThe temporal information provided by the tense of the verbs can be exploited by predicating additional information about the event variable $e$ . Specifically, we can add temporal variables representing the interval corresponding to the event, the end point of the event, and temporal predicates relating this end point to the current time as indicated by the tense of the verb. Such an approach yields the following representations for our arriving examples: \nThis representation introduces a variable to stand for the interval of time associated with the event and a variable that stands for the end of that interval. The two-place predicate Precedes represents the notion that the first time-point argument precedes the second in time; the constant Now refers to the current time. For past events, the end point of the interval must precede the current time. Similarly, for future events the current time must precede the end of the event. For events happening in the present, the current time is contained within the event interval. \nUnfortunately, the relation between simple verb tenses and points in time is by no means straightforward. Consider the following examples: (15.51) Ok, we fly from San Francisco to Boston at 10. (15.52) Flight 1390 will be at the gate an hour now. In the first example, the present tense of the verb fly is used to refer to a future event, while in the second the future tense is used to refer to a past event. \nMore complications occur when we consider some of the other verb tenses. Consider the following examples: \n(15.53) Flight 1902 arrived late.   \n(15.54) Flight 1902 had arrived late. \nreference point \nAlthough both refer to events in the past, representing them in the same way seems wrong. The second example seems to have another unnamed event lurking in the background (e.g., Flight 1902 had already arrived late when something else happened). To account for this phenomena, Reichenbach (1947) introduced the notion of a reference point. In our simple temporal scheme, the current moment in time is equated with the time of the utterance and is used as a reference point for when the event occurred (before, at, or after). In Reichenbach’s approach, the notion of the reference point is separated from the utterance time and the event time. The following examples illustrate the basics of this approach: \n(15.55) When Mary’s flight departed, I ate lunch.   \n(15.56) When Mary’s flight departed, I had eaten lunch. \nIn both of these examples, the eating event has happened in the past, that is, prior to the utterance. However, the verb tense in the first example indicates that the eating event began when the flight departed, while the second example indicates that the eating was accomplished prior to the flight’s departure. Therefore, in Reichenbach’s terms the departure event specifies the reference point. These facts can be accommodated by additional constraints relating the eating and departure events. In the first example, the reference point precedes the eating event, and in the second example, the eating precedes the reference point. Figure 15.5 illustrates Reichenbach’s approach with the primary English tenses. Exercise 15.6 asks you to represent these examples in FOL. \nThis discussion has focused narrowly on the broad notions of past, present, and future and how they are signaled by various English verb tenses. Of course, languages have many other ways to convey temporal information, including temporal expressions: \n(15.57) I’d like to go at 6:45 in the morning.   \n(15.58) Somewhere around noon, please. \nAs we show in Chapter 17, grammars for such temporal expressions are of considerable practical importance to information extraction and question-answering applications. \nFinally, we should note that a systematic conceptual organization is reflected in examples like these. In particular, temporal expressions in English are frequently expressed in spatial terms, as is illustrated by the various uses of at, in, somewhere, and near in these examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations such as these, in which one domain is systematically expressed in terms of another, are very common in languages of the world. \naspect \n15.4.2 Aspect \nIn the last section, we discussed ways to represent the time of an event with respect to the time of an utterance describing it. Here we introduce a related notion, called aspect, that describes how to categorize events by their internal temporal structure or temporal contour. By this we mean whether events are ongoing or have ended, or whether they is conceptualized as happening at a point in time or over some interval. Such notions of temporal contour have been used to divide event expressions into classes since Aristotle, although the set of four classes we’ll introduce here is due to Vendler (1967). \nevents states stative \nThe most basic aspectual distinction is between events (which involve change) and states (which do not involve change). Stative expressions represent the notion of an event participant being in a state, or having a particular property, at a given point in time. Stative expressions capture aspects of the world at a single point in time, and conceptualize the participant as unchanging and continuous. Consider the following ATIS examples. \nactivity \ntelic \n(15.59) I like Flight 840.   \n(15.60) I need the cheapest fare.   \n(15.61) I want to go first class. \nIn examples like these, the event participant denoted by the subject can be seen as experiencing something at a specific point in time, and don’t involve any kind of internal change over time (the liking or needing is conceptualized as continuous and unchanging). \nNon-states (which we’ll refer to as events) are divided into subclasses; we’ll introduce three here. Activity expressions describe events undertaken by a participant that occur over a span of time (rather than being conceptualized as a single point in time like stative expressions), and have no particular end point. Of course in practice all things end, but the meaning of the expression doesn’t represent this fact. Consider the following examples: \n(15.62) She drove a Mazda.   \n(15.63) I live in Brooklyn. \nThese examples both specify that the subject is engaged in, or has engaged in, the activity specified by the verb for some period of time, but doesn’t specify when the driving or living might have stopped. \nTwo more classes of expressions, achievement expressions and accomplishment expressions, describe events that take place over time, but also conceptualize the event as having a particular kind of endpoint or goal. The Greek word telos means ‘end’ or ’goal’ and so the events described by these kinds of expressions are often called telic events.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Event and State Representations",
        "subsection": "Representing Time",
        "subsubsection": "N/A"
    },
    {
        "content": "As we show in Chapter 17, grammars for such temporal expressions are of considerable practical importance to information extraction and question-answering applications. \nFinally, we should note that a systematic conceptual organization is reflected in examples like these. In particular, temporal expressions in English are frequently expressed in spatial terms, as is illustrated by the various uses of at, in, somewhere, and near in these examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations such as these, in which one domain is systematically expressed in terms of another, are very common in languages of the world. \naspect \n15.4.2 Aspect \nIn the last section, we discussed ways to represent the time of an event with respect to the time of an utterance describing it. Here we introduce a related notion, called aspect, that describes how to categorize events by their internal temporal structure or temporal contour. By this we mean whether events are ongoing or have ended, or whether they is conceptualized as happening at a point in time or over some interval. Such notions of temporal contour have been used to divide event expressions into classes since Aristotle, although the set of four classes we’ll introduce here is due to Vendler (1967). \nevents states stative \nThe most basic aspectual distinction is between events (which involve change) and states (which do not involve change). Stative expressions represent the notion of an event participant being in a state, or having a particular property, at a given point in time. Stative expressions capture aspects of the world at a single point in time, and conceptualize the participant as unchanging and continuous. Consider the following ATIS examples. \nactivity \ntelic \n(15.59) I like Flight 840.   \n(15.60) I need the cheapest fare.   \n(15.61) I want to go first class. \nIn examples like these, the event participant denoted by the subject can be seen as experiencing something at a specific point in time, and don’t involve any kind of internal change over time (the liking or needing is conceptualized as continuous and unchanging). \nNon-states (which we’ll refer to as events) are divided into subclasses; we’ll introduce three here. Activity expressions describe events undertaken by a participant that occur over a span of time (rather than being conceptualized as a single point in time like stative expressions), and have no particular end point. Of course in practice all things end, but the meaning of the expression doesn’t represent this fact. Consider the following examples: \n(15.62) She drove a Mazda.   \n(15.63) I live in Brooklyn. \nThese examples both specify that the subject is engaged in, or has engaged in, the activity specified by the verb for some period of time, but doesn’t specify when the driving or living might have stopped. \nTwo more classes of expressions, achievement expressions and accomplishment expressions, describe events that take place over time, but also conceptualize the event as having a particular kind of endpoint or goal. The Greek word telos means ‘end’ or ’goal’ and so the events described by these kinds of expressions are often called telic events. \nAccomplishment expressions describe events that have a natural end point and result in a particular state. Consider the following examples: \n(15.64) He booked me a reservation.   \n(15.65) United flew me to New York. \nachievement expressions \nIn these examples, an event is seen as occurring over some period of time that ends when the intended state is accomplished (i.e., the state of me having a reservation, or me being in New York). \nThe final aspectual class, achievement expressions, is only subtly different than accomplishments. Consider the following: \n(15.66) She found her gate.   \n(15.67) I reached New York. \nLike accomplishment expressions, achievement expressions result in a state. But unlike accomplishments, achievement events are ‘punctual’: they are thought of as happening in an instant and the verb doesn’t conceptualize the process or activity leading up the state. Thus the events in these examples may in fact have been preceded by extended searching or traveling events, but the verb doesn’t conceptualize these preceding processes, but rather conceptualizes the events corresponding to finding and reaching as points, not intervals. \nIn summary, a standard way of categorizing event expressions by their temporal contours is via these four general classes: \nStative: I know my departure gate. \nActivity: John is flying. \nAccomplishment: Sally booked her flight. \nAchievement: She found her gate. \nBefore moving on, note that event expressions can easily be shifted from one class to another. Consider the following examples: \n(15.68) I flew.   \n(15.69) I flew to New York. \nThe first example is a simple activity; it has no natural end point. The second example is clearly an accomplishment event since it has an end point, and results in a particular state. Clearly, the classification of an event is not solely governed by the verb, but by the semantics of the entire expression in context. \n15.5 Description Logics \nAs noted at the beginning of this chapter, a fair number of representational schemes have been invented to capture the meaning of linguistic utterances. It is now widely accepted that meanings represented in these various approaches can, in principle, be translated into equivalent statements in FOL with relative ease. The difficulty is that in many of these approaches the semantics of a statement are defined procedurally. That is, the meaning arises from whatever the system that interprets it does with it. \nDescription logics are an effort to better specify the semantics of these earlier structured network representations and to provide a conceptual framework that is especially well suited to certain kinds of domain modeling. Formally, the term Description Logics refers to a family of logical approaches that correspond to varying subsets of FOL. The restrictions placed on the expressiveness of Description Logics serve to guarantee the tractability of various critical kinds of inference. Our focus here, however, will be on the modeling aspects of DLs rather than on computational complexity issues.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Event and State Representations",
        "subsection": "Aspect",
        "subsubsection": "N/A"
    },
    {
        "content": "Accomplishment expressions describe events that have a natural end point and result in a particular state. Consider the following examples: \n(15.64) He booked me a reservation.   \n(15.65) United flew me to New York. \nachievement expressions \nIn these examples, an event is seen as occurring over some period of time that ends when the intended state is accomplished (i.e., the state of me having a reservation, or me being in New York). \nThe final aspectual class, achievement expressions, is only subtly different than accomplishments. Consider the following: \n(15.66) She found her gate.   \n(15.67) I reached New York. \nLike accomplishment expressions, achievement expressions result in a state. But unlike accomplishments, achievement events are ‘punctual’: they are thought of as happening in an instant and the verb doesn’t conceptualize the process or activity leading up the state. Thus the events in these examples may in fact have been preceded by extended searching or traveling events, but the verb doesn’t conceptualize these preceding processes, but rather conceptualizes the events corresponding to finding and reaching as points, not intervals. \nIn summary, a standard way of categorizing event expressions by their temporal contours is via these four general classes: \nStative: I know my departure gate. \nActivity: John is flying. \nAccomplishment: Sally booked her flight. \nAchievement: She found her gate. \nBefore moving on, note that event expressions can easily be shifted from one class to another. Consider the following examples: \n(15.68) I flew.   \n(15.69) I flew to New York. \nThe first example is a simple activity; it has no natural end point. The second example is clearly an accomplishment event since it has an end point, and results in a particular state. Clearly, the classification of an event is not solely governed by the verb, but by the semantics of the entire expression in context. \n15.5 Description Logics \nAs noted at the beginning of this chapter, a fair number of representational schemes have been invented to capture the meaning of linguistic utterances. It is now widely accepted that meanings represented in these various approaches can, in principle, be translated into equivalent statements in FOL with relative ease. The difficulty is that in many of these approaches the semantics of a statement are defined procedurally. That is, the meaning arises from whatever the system that interprets it does with it. \nDescription logics are an effort to better specify the semantics of these earlier structured network representations and to provide a conceptual framework that is especially well suited to certain kinds of domain modeling. Formally, the term Description Logics refers to a family of logical approaches that correspond to varying subsets of FOL. The restrictions placed on the expressiveness of Description Logics serve to guarantee the tractability of various critical kinds of inference. Our focus here, however, will be on the modeling aspects of DLs rather than on computational complexity issues. \n\nterminology TBox ABox ontology \nWhen using Description Logics to model an application domain, the emphasis is on the representation of knowledge about categories, individuals that belong to those categories, and the relationships that can hold among these individuals. The set of categories, or concepts, that make up a particular application domain is called its terminology. The portion of a knowledge base that contains the terminology is traditionally called the TBox; this is in contrast to the ABox that contains facts about individuals. The terminology is typically arranged into a hierarchical organization called an ontology that captures the subset/superset relations among the categories. \nReturning to our earlier culinary domain, we represented domain concepts using unary predicates such as Restaurant $( x )$ ; the DL equivalent omits the variable, so the restaurant category is simply written as Restaurant.2 To capture the fact that a particular domain element, such as Frasca, is a restaurant, we assert Restaurant(Frasca) in much the same way we would in FOL. The semantics of these categories are specified in precisely the same way that was introduced earlier in Section 15.2: a category like Restaurant simply denotes the set of domain elements that are restaurants. \nOnce we’ve specified the categories of interest in a particular domain, the next step is to arrange them into a hierarchical structure. There are two ways to capture the hierarchical relationships present in a terminology: we can directly assert relations between categories that are related hierarchically, or we can provide complete definitions for our concepts and then rely on inference to provide hierarchical relationships. The choice between these methods hinges on the use to which the resulting categories will be put and the feasibility of formulating precise definitions for many naturally occurring categories. We’ll discuss the first option here and return to the notion of definitions later in this section. \nsubsumption \nTo directly specify a hierarchical structure, we can assert subsumption relations between the appropriate concepts in a terminology. The subsumption relation is conventionally written as $C subseteq D$ and is read as $C$ is subsumed by $D$ ; that is, all members of the category $C$ are also members of the category $D$ . Not surprisingly, the formal semantics of this relation are provided by a simple set relation; any domain element that is in the set denoted by $C$ is also in the set denoted by $D$ . \nAdding the following statements to the TBox asserts that all restaurants are commercial establishments and, moreover, that there are various subtypes of restaurants. \nOntologies such as this are conventionally illustrated with diagrams such as the one shown in Fig. 15.6, where subsumption relations are denoted by links between the nodes representing the categories. \nNote, that it was precisely the vague nature of semantic network diagrams like this that motivated the development of Description Logics. For example, from this diagram we can’t tell whether the given set of categories is exhaustive or disjoint. That is, we can’t tell if these are all the kinds of restaurants that we’ll be dealing with in our domain or whether there might be others. We also can’t tell if an individual restaurant must fall into only one of these categories, or if it is possible, for example, for a restaurant to be both Italian and Chinese. The DL statements given above are more transparent in their meaning; they simply assert a set of subsumption relations between categories and make no claims about coverage or mutual exclusion. \n\nIf an application requires coverage and disjointness information, then such information must be made explicitly. The simplest ways to capture this kind of information is through the use of negation and disjunction operators. For example, the following assertion would tell us that Chinese restaurants can’t also be Italian restaurants. \nChineseRestaurant $sqsubseteq$ not ItalianRestaurant \nSpecifying that a set of subconcepts covers a category can be achieved with disjunction, as in the following: \nRestaurant (or ItalianRestaurant ChineseRestaurant MexicanRestaurant) \nHaving a hierarchy such as the one given in Fig. 15.6 tells us next to nothing about the concepts in it. We certainly don’t know anything about what makes a restaurant a restaurant, much less Italian, Chinese, or expensive. What is needed are additional assertions about what it means to be a member of any of these categories. In Description Logics such statements come in the form of relations between the concepts being described and other concepts in the domain. In keeping with its origins in structured network representations, relations in Description Logics are typically binary and are often referred to as roles, or role-relations. \nTo see how such relations work, let’s consider some of the facts about restaurants discussed earlier in the chapter. We’ll use the hasCuisine relation to capture information as to what kinds of food restaurants serve and the hasPriceRange relation to capture how pricey particular restaurants tend to be. We can use these relations to say something more concrete about our various classes of restaurants. Let’s start with our ItalianRestaurant concept. As a first approximation, we might say something uncontroversial like Italian restaurants serve Italian cuisine. To capture these notions, let’s first add some new concepts to our terminology to represent various kinds of cuisine. \nMexicanCuisine 口 Cuisine ExpensiveRestaurant $sqsubseteq$ Restaurant ItalianCuisine $sqsubseteq$ Cuisine ModerateRestaurant $sqsubseteq$ Restaurant ChineseCuisine Cuisine CheapRestaurant $sqsubseteq$ Restaurant VegetarianCuisine $sqsubseteq$ Cuisine \nNext, let’s revise our earlier version of ItalianRestaurant to capture cuisine information. \nItalianRestaurant Restaurant hasCuisine.ItalianCuisine \nThe correct way to read this expression is that individuals in the category ItalianRestaurant are subsumed both by the category Restaurant and by an unnamed class defined by the existential clause—the set of entities that serve Italian cuisine. An equivalent statement in FOL would be \nThis FOL translation should make it clear what the DL assertions given above do and do not entail. In particular, they don’t say that domain entities classified as Italian restaurants can’t engage in other relations like being expensive or even serving Chinese cuisine. And critically, they don’t say much about domain entities that we know do serve Italian cuisine. In fact, inspection of the FOL translation makes it clear that we cannot infer that any new entities belong to this category based on their characteristics. The best we can do is infer new facts about restaurants that we’re explicitly told are members of this category. \nOf course, inferring the category membership of individuals given certain characteristics is a common and critical reasoning task that we need to support. This brings us back to the alternative approach to creating hierarchical structures in a terminology: actually providing a definition of the categories we’re creating in the form of necessary and sufficient conditions for category membership. In this case, we might explicitly provide a definition for ItalianRestaurant as being those restaurants that serve Italian cuisine, and ModerateRestaurant as being those whose price range is moderate. \nItalianRestaurant $equiv$ Restaurant hasCuisine.ItalianCuisine (15.78   \nModerateRestaurant $equiv$ Restaurant hasPriceRange.ModeratePrices (15.79 \nWhile our earlier statements provided necessary conditions for membership in these categories, these statements provide both necessary and sufficient conditions. \nFinally, let’s now consider the superficially similar case of vegetarian restaurants. Clearly, vegetarian restaurants are those that serve vegetarian cuisine. But they don’t merely serve vegetarian fare, that’s all they serve. We can accommodate this kind of constraint by adding an additional restriction in the form of a universal quantifier to our earlier description of VegetarianRestaurants, as follows: \nVegetarianRestaurant $equiv$ Restaurant hasCuisine.VegetarianCuisine hasCuisine.VegetarianCuisine \nInference \nParalleling the focus of Description Logics on categories, relations, and individuals is a processing focus on a restricted subset of logical inference. Rather than employing the full range of reasoning permitted by FOL, DL reasoning systems emphasize the closely coupled problems of subsumption and instance checking. \nSubsumption, as a form of inference, is the task of determining, based on the facts asserted in a terminology, whether a superset/subset relationship exists between two concepts. Correspondingly, instance checking asks if an individual can be a member of a particular category given the facts we know about both the individual and the terminology. The inference mechanisms underlying subsumption and instance checking go beyond simply checking for explicitly stated subsumption relations in a terminology. They must explicitly reason using the relational information asserted about the terminology to infer appropriate subsumption and membership relations. \nReturning to our restaurant domain, let’s add a new kind of restaurant using the following statement: \nIlFornaio ModerateRestaurant hasCuisine.ItalianCuisine \nGiven this assertion, we might ask whether the IlFornaio chain of restaurants might be classified as an Italian restaurant or a vegetarian restaurant. More precisely, we can pose the following questions to our reasoning system: \nIlFornaio VegetarianRestaurant \nThe answer to the first question is positive since IlFornaio meets the criteria we specified for the category ItalianRestaurant: it’s a Restaurant since we explicitly classified it as a ModerateRestaurant, which is a subtype of Restaurant, and it meets the has.Cuisine class restriction since we’ve asserted that directly. \nThe answer to the second question is negative. Recall, that our criteria for vegetarian restaurants contains two requirements: it has to serve vegetarian fare, and that’s all it can serve. Our current definition for IlFornaio fails on both counts since we have not asserted any relations that state that IlFornaio serves vegetarian fare, and the relation we have asserted, hasCuisine.ItalianCuisine, contradicts the second criteria. \nA related reasoning task, based on the basic subsumption inference, is to derive the implied hierarchy for a terminology given facts about the categories in the terminology. This task roughly corresponds to a repeated application of the subsumption operator to pairs of concepts in the terminology. Given our current collection of statements, the expanded hierarchy shown in Fig. 15.7 can be inferred. You should convince yourself that this diagram contains all and only the subsumption links that should be present given our current knowledge. \nInstance checking is the task of determining whether a particular individual can be classified as a member of a particular category. This process takes what is known about a given individual, in the form of relations and explicit categorical statements, and then compares that information with what is known about the current terminology. It then returns a list of the most specific categories to which the individual can belong. \nAs an example of a categorization problem, consider an establishment that we’re told is a restaurant and serves Italian cuisine. \n\nRestaurant(Gondolier) hasCuisine(Gondolier, ItalianCuisine) \nHere, we’re being told that the entity denoted by the term Gondolier is a restaurant and serves Italian food. Given this new information and the contents of our current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is a vegetarian restaurant, or if it has moderate prices. \nAssuming the definitional statements given earlier, we can indeed categorize the Gondolier as an Italian restaurant. That is, the information we’ve been given about it meets the necessary and sufficient conditions required for membership in this category. And as with the IlFornaio category, this individual fails to match the stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also turn out to be a moderately priced restaurant, but we can’t tell at this point since we don’t know anything about its prices. What this means is that given our current knowledge the answer to the query ModerateRestaurant(Gondolier) would be false since it lacks the required hasPriceRange relation. \nThe implementation of subsumption, instance checking, as well as other kinds of inferences needed for practical applications, varies according to the expressivity of the Description Logic being used. However, for a Description Logic of even modest power, the primary implementation techniques are based on satisfiability methods that in turn rely on the underlying model-based semantics introduced earlier in this chapter. \nOWL and the Semantic Web \nThe highest-profile role for Description Logics, to date, has been as a part of the development of the Semantic Web. The Semantic Web is an ongoing effort to provide a way to formally specify the semantics of the contents of the Web (Fensel et al., 2003). A key component of this effort involves the creation and deployment of ontologies for various application areas of interest. The meaning representation language used to represent this knowledge is the Web Ontology Language (OWL) (McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that corresponds roughly to the one we’ve been describing here.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Description Logics",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Restaurant(Gondolier) hasCuisine(Gondolier, ItalianCuisine) \nHere, we’re being told that the entity denoted by the term Gondolier is a restaurant and serves Italian food. Given this new information and the contents of our current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is a vegetarian restaurant, or if it has moderate prices. \nAssuming the definitional statements given earlier, we can indeed categorize the Gondolier as an Italian restaurant. That is, the information we’ve been given about it meets the necessary and sufficient conditions required for membership in this category. And as with the IlFornaio category, this individual fails to match the stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also turn out to be a moderately priced restaurant, but we can’t tell at this point since we don’t know anything about its prices. What this means is that given our current knowledge the answer to the query ModerateRestaurant(Gondolier) would be false since it lacks the required hasPriceRange relation. \nThe implementation of subsumption, instance checking, as well as other kinds of inferences needed for practical applications, varies according to the expressivity of the Description Logic being used. However, for a Description Logic of even modest power, the primary implementation techniques are based on satisfiability methods that in turn rely on the underlying model-based semantics introduced earlier in this chapter. \nOWL and the Semantic Web \nThe highest-profile role for Description Logics, to date, has been as a part of the development of the Semantic Web. The Semantic Web is an ongoing effort to provide a way to formally specify the semantics of the contents of the Web (Fensel et al., 2003). A key component of this effort involves the creation and deployment of ontologies for various application areas of interest. The meaning representation language used to represent this knowledge is the Web Ontology Language (OWL) (McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that corresponds roughly to the one we’ve been describing here.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Restaurant(Gondolier) hasCuisine(Gondolier, ItalianCuisine) \nHere, we’re being told that the entity denoted by the term Gondolier is a restaurant and serves Italian food. Given this new information and the contents of our current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is a vegetarian restaurant, or if it has moderate prices. \nAssuming the definitional statements given earlier, we can indeed categorize the Gondolier as an Italian restaurant. That is, the information we’ve been given about it meets the necessary and sufficient conditions required for membership in this category. And as with the IlFornaio category, this individual fails to match the stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also turn out to be a moderately priced restaurant, but we can’t tell at this point since we don’t know anything about its prices. What this means is that given our current knowledge the answer to the query ModerateRestaurant(Gondolier) would be false since it lacks the required hasPriceRange relation. \nThe implementation of subsumption, instance checking, as well as other kinds of inferences needed for practical applications, varies according to the expressivity of the Description Logic being used. However, for a Description Logic of even modest power, the primary implementation techniques are based on satisfiability methods that in turn rely on the underlying model-based semantics introduced earlier in this chapter. \nOWL and the Semantic Web \nThe highest-profile role for Description Logics, to date, has been as a part of the development of the Semantic Web. The Semantic Web is an ongoing effort to provide a way to formally specify the semantics of the contents of the Web (Fensel et al., 2003). A key component of this effort involves the creation and deployment of ontologies for various application areas of interest. The meaning representation language used to represent this knowledge is the Web Ontology Language (OWL) (McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that corresponds roughly to the one we’ve been describing here.",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Restaurant(Gondolier) hasCuisine(Gondolier, ItalianCuisine) \nHere, we’re being told that the entity denoted by the term Gondolier is a restaurant and serves Italian food. Given this new information and the contents of our current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is a vegetarian restaurant, or if it has moderate prices. \nAssuming the definitional statements given earlier, we can indeed categorize the Gondolier as an Italian restaurant. That is, the information we’ve been given about it meets the necessary and sufficient conditions required for membership in this category. And as with the IlFornaio category, this individual fails to match the stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also turn out to be a moderately priced restaurant, but we can’t tell at this point since we don’t know anything about its prices. What this means is that given our current knowledge the answer to the query ModerateRestaurant(Gondolier) would be false since it lacks the required hasPriceRange relation. \nThe implementation of subsumption, instance checking, as well as other kinds of inferences needed for practical applications, varies according to the expressivity of the Description Logic being used. However, for a Description Logic of even modest power, the primary implementation techniques are based on satisfiability methods that in turn rely on the underlying model-based semantics introduced earlier in this chapter. \nOWL and the Semantic Web \nThe highest-profile role for Description Logics, to date, has been as a part of the development of the Semantic Web. The Semantic Web is an ongoing effort to provide a way to formally specify the semantics of the contents of the Web (Fensel et al., 2003). A key component of this effort involves the creation and deployment of ontologies for various application areas of interest. The meaning representation language used to represent this knowledge is the Web Ontology Language (OWL) (McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that corresponds roughly to the one we’ve been describing here. \n15.6 Summary \nThis chapter has introduced the representational approach to meaning. The following are some of the highlights of this chapter: \n• A major approach to meaning in computational linguistics involves the creation of formal meaning representations that capture the meaning-related content of linguistic inputs. These representations are intended to bridge the gap from language to common-sense knowledge of the world.   \n• The frameworks that specify the syntax and semantics of these representations are called meaning representation languages. A wide variety of such languages are used in natural language processing and artificial intelligence.   \n• Such representations need to be able to support the practical computational requirements of semantic processing. Among these are the need to determine the truth of propositions, to support unambiguous representations, to represent variables, to support inference, and to be sufficiently expressive.   \n• Human languages have a wide variety of features that are used to convey meaning. Among the most important of these is the ability to convey a predicateargument structure.   \n• First-Order Logic is a well-understood, computationally tractable meaning representation language that offers much of what is needed in a meaning representation language.   \n• Important elements of semantic representation including states and events can be captured in FOL.   \n• Semantic networks and frames can be captured within the FOL framework.   \n• Modern Description Logics consist of useful and computationally tractable subsets of full First-Order Logic. The most prominent use of a description logic is the Web Ontology Language (OWL), used in the specification of the Semantic Web. \nBibliographical and Historical Notes \nThe earliest computational use of declarative meaning representations in natural language processing was in the context of question-answering systems (Green et al. 1961, Raphael 1968, Lindsey 1963). These systems employed ad hoc representations for the facts needed to answer questions. Questions were then translated into a form that could be matched against facts in the knowledge base. Simmons (1965) provides an overview of these early efforts. \nWoods (1967) investigated the use of FOL-like representations in question answering as a replacement for the ad hoc representations in use at the time. Woods (1973) further developed and extended these ideas in the landmark Lunar system. Interestingly, the representations used in Lunar had both truth-conditional and procedural semantics. Winograd (1972) employed a similar representation based on the Micro-Planner language in his SHRDLU system. \nDuring this same period, researchers interested in the cognitive modeling of language and memory had been working with various forms of associative network representations. Masterman (1957) was the first to make computational use of a semantic network-like knowledge representation, although semantic networks are generally credited to Quillian (1968). A considerable amount of work in the semantic network framework was carried out during this era (Norman and Rumelhart 1975, Schank 1972, Wilks 1975c, Wilks 1975b, Kintsch 1974). It was during this period that a number of researchers began to incorporate Fillmore’s notion of case roles (Fillmore, 1968) into their representations. Simmons (1973) was the earliest adopter of case roles as part of representations for natural language processing. \n\nDetailed analyses by Woods (1975) and Brachman (1979) aimed at figuring out what semantic networks actually mean led to the development of a number of more sophisticated network-like languages including KRL (Bobrow and Winograd, 1977) and KL-ONE (Brachman and Schmolze, 1985). As these frameworks became more sophisticated and well defined, it became clear that they were restricted variants of FOL coupled with specialized indexing inference procedures. A useful collection of papers covering much of this work can be found in Brachman and Levesque (1985). Russell and Norvig (2002) describe a modern perspective on these representational efforts. \nLinguistic efforts to assign semantic structures to natural language sentences in the generative era began with the work of Katz and Fodor (1963). The limitations of their simple feature-based representations and the natural fit of logic to many of the linguistic problems of the day quickly led to the adoption of a variety of predicate-argument structures as preferred semantic representations (Lakoff 1972a, McCawley 1968). The subsequent introduction by Montague (1973) of the truthconditional model-theoretic framework into linguistic theory led to a much tighter integration between theories of formal syntax and a wide range of formal semantic frameworks. Good introductions to Montague semantics and its role in linguistic theory can be found in Dowty et al. (1981) and Partee (1976). \nThe representation of events as reified objects is due to Davidson (1967). The approach presented here, which explicitly reifies event participants, is due to Parsons (1990). \nMost current computational approaches to temporal reasoning are based on Allen’s notion of temporal intervals (Allen, 1984); see Chapter 17. ter Meulen (1995) provides a modern treatment of tense and aspect. Davis (1990) describes the use of FOL to represent knowledge across a wide range of common-sense domains including quantities, space, time, and beliefs. \nA recent comprehensive treatment of logic and language can be found in van Benthem and ter Meulen (1997). A classic semantics text is Lyons (1977). McCawley (1993) is an indispensable textbook covering a wide range of topics concerning logic and language. Chierchia and McConnell-Ginet (1991) also broadly covers semantic issues from a linguistic perspective. Heim and Kratzer (1998) is a more recent text written from the perspective of current generative theory. \nExercises \n15.1 Peruse your daily newspaper for three examples of ambiguous sentences or headlines. Describe the various sources of the ambiguities.   \n15.2 Consider a domain in which the word coffee can refer to the following concepts in a knowledge-based system: a caffeinated or decaffeinated beverage, ground coffee used to make either kind of beverage, and the beans themselves. Give arguments as to which of the following uses of coffee are ambiguous and which are vague. \n1. I’ve had my coffee for today.   \n2. Buy some coffee on your way home.   \n3. Please grind some more coffee. \n15.3 The following rule, which we gave as a translation for Example 15.25, is not a reasonable definition of what it means to be a vegetarian restaurant. \nGive a FOL rule that better defines vegetarian restaurants in terms of what they serve. \n15.4 Give FOL translations for the following sentences: \n1. Vegetarians do not eat meat.   \n2. Not all vegetarians eat eggs. \n15.5 Give a set of facts and inferences necessary to prove the following assertions: \n1. McDonald’s is not a vegetarian restaurant.   \n2. Some vegetarians can eat at McDonald’s. \nDon’t just place these facts in your knowledge base. Show that they can be inferred from some more general facts about vegetarians and McDonald’s. \n15.6 For the following sentences, give FOL translations that capture the temporal relationships between the events. 1. When Mary’s flight departed, I ate lunch. 2. When Mary’s flight departed, I had eaten lunch. \n15.7 On page 346, we gave the representation $N e a r ( C e n t r o , B a c a r o )$ as a translation for the sentence Centro is near Bacaro. In a truth-conditional semantics, this formula is either true or false given some model. Critique this truthconditional approach with respect to the meaning of words like near. \nCHAPTER 16 Computational Semantics and Semantic Parsing Placeholder",
        "chapter": "Logical Representations of Sentence Meaning",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "CHAPTER 16 Computational Semantics and Semantic Parsing Placeholder \nCHAPTER 17 Information Extraction \nI am the very model of a modern Major-General, I’ve information vegetable, animal, and mineral, I know the kings of England, and I quote the fights historical From Marathon to Waterloo, in order categorical... Gilbert and Sullivan, Pirates of Penzance \nImagine that you are an analyst with an investment firm that tracks airline stocks. You’re given the task of determining the relationship (if any) between airline announcements of fare increases and the behavior of their stocks the next day. Historical data about stock prices is easy to come by, but what about the airline announcements? You will need to know at least the name of the airline, the nature of the proposed fare hike, the dates of the announcement, and possibly the response of other airlines. Fortunately, these can be all found in news articles like this one: \nCiting high fuel prices, United Airlines said Friday it has increased fares by $$ 6$ per round trip on flights to some cities also served by lowercost carriers. American Airlines, a unit of AMR Corp., immediately matched the move, spokesman Tim Wagner said. United, a unit of UAL Corp., said the increase took effect Thursday and applies to most routes where it competes against discount carriers, such as Chicago to Dallas and Denver to San Francisco. \ninformation extraction \nThis chapter presents techniques for extracting limited kinds of semantic content from text. This process of information extraction (IE) turns the unstructured information embedded in texts into structured data, for example for populating a relational database to enable further processing. \nrelation extraction \nknowledge graphs \nevent extraction \nWe begin with the task of relation extraction: finding and classifying semantic relations among entities mentioned in a text, like child-of (X is the child-of Y), or part-whole or geospatial relations. Relation extraction has close links to populating a relational database, and knowledge graphs, datasets of structured relational knowledge, are a useful way for search engines to present information to users. \ntemporal expression \nNext, we discuss three tasks related to events. Event extraction is finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Event coreference (Chapter 22) is needed to figure out which event mentions in a text refer to the same event; the two instances of increase and the phrase the move all refer to the same event. To figure out when the events in a text happened we extract temporal expressions like days of the week (Friday and Thursday) or two days from now and times such as $3 { : } 3 0 P . M .$ , and normalize them onto specific calendar dates or times. We’ll need to link Friday to the time of United’s announcement, Thursday to the previous day’s fare increase, and produce a timeline in which United’s announcement follows the fare increase and American’s announcement follows both of those events.",
        "chapter": "Computational Semantics and Semantic Parsing",
        "section": "N/A",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "template filling \nFinally, many texts describe recurring stereotypical events or situations. The task of template filling is to find such situations in documents and fill in the template slots. These slot-fillers may consist of text segments extracted directly from the text, or concepts like times, amounts, or ontology entities that have been inferred from text elements through additional processing. \nOur airline text is an example of this kind of stereotypical situation since airlines often raise fares and then wait to see if competitors follow along. In this situation, we can identify United as a lead airline that initially raised its fares, $$ 6$ as the amount, Thursday as the increase date, and American as an airline that followed along, leading to a filled template like the following. \n17.1 Relation Extraction \nLet’s assume that we have detected the named entities in our sample text (perhaps using the techniques of Chapter 8), and would like to discern the relationships that exist among the detected entities: \nCiting high fuel prices, [ORG United Airlines] said $mathrm { Delta I T I M E }$ Friday] it has increased fares by [MONEY $$ 6]$ per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect [TIME Thursday] and applies to most routes where it competes against discount carriers, such as $mathrm { Delta I L O C }$ Chicago] to [LOC Dallas] and [LOC Denver] to [LOC San Francisco]. \nThe text tells us, for example, that Tim Wagner is a spokesman for American Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR. These binary relations are instances of more generic relations such as part-of or employs that are fairly frequent in news-style texts. Figure 17.1 lists the 17 relations used in the ACE relation extraction evaluations and Fig. 17.2 shows some sample relations. We might also extract more domain-specific relation such as the notion of an airline route. For example from this text we can conclude that United has routes to Chicago, Dallas, Denver, and San Francisco. \nThese relations correspond nicely to the model-theoretic notions we introduced in Chapter 15 to ground the meanings of the logical forms. That is, a relation consists of a set of ordered tuples over elements of a domain. In most standard informationextraction applications, the domain elements correspond to the named entities that occur in the text, to the underlying entities that result from coreference resolution, or to entities selected from a domain ontology. Figure 17.3 shows a model-based view of the set of entities and relations that can be extracted from our running example. Notice how this model-theoretic view subsumes the NER task as well; named entity recognition corresponds to the identification of a class of unary relations. \nSets of relations have been defined for many other domains as well. For example UMLS, the Unified Medical Language System from the US National Library of \nMedicine has a network that defines 134 broad subject categories, entity types, and 54 relations between the entities, such as the following: \nGiven a medical sentence like this one: \n(17.1) Doppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type 2 diabetes \nWe could thus extract the UMLS relation: \nEchocardiography, Doppler Diagnoses Acquired stenosis \ninfoboxes \nWikipedia also offers a large supply of relations, drawn from infoboxes, structured tables associated with certain Wikipedia articles. For example, the Wikipedia infobox for Stanford includes structured facts like state $mathbf { Sigma } = mathbf { Sigma }$ \"California\" or president $mathbf { tau } = mathbf { tau }$ \"Marc Tessier-Lavigne\". These facts can be turned into relations like president-of or located-in. or into relations in a metalanguage called RDF (Resource Description Framework). An RDF triple is a tuple of entity-relationentity, called a subject-predicate-object expression. Here’s a sample RDF triple: \nsubject predicate object Golden Gate Park location San Francisco \nFreebase \nFor example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology derived from Wikipedia containing over 2 billion RDF triples. Another dataset from Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrandecˇic´ and Kro¨tzsch, 2014), has relations between people and their nationality, or locations, and other locations they are contained in. \nis-a hypernym \nWordNet or other ontologies offer useful ontological relations that express hierarchical relations between words or concepts. For example WordNet has the is-a or hypernym relation between classes, \nGiraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ... \nWordNet also has Instance-of relation between individuals and classes, so that for example San Francisco is in the Instance-of relation with city. Extracting these relations is an important step in extending or building ontologies. \nFinally, there are large datasets that contain sentences hand-labeled with their relations, designed for training and testing relation extractors. The TACRED dataset (Zhang et al., 2017) contains 106,264 examples of relation triples about particular people or organizations, labeled in sentences from news and web text drawn from the annual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains 41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse), plus a no relation tag; examples are shown in Fig. 17.4. About $80 %$ of all examples are annotated as no relation; having sufficient negative data is important for training supervised classifiers. \nA standard dataset was also produced for the SemEval 2010 Task 8, detecting relations between nominals (Hendrickx et al., 2009). The dataset has 10,717 examples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed relations like product-producer ( a factory manufactures suits) or component-whole (my apartment has a large kitchen). \n17.2 Relation Extraction Algorithms \nThere are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We’ll introduce each of these in the next sections. \nHearst patterns \n17.2.1 Using Patterns to Extract Relations \nThe earliest and still common algorithm for relation extraction is lexico-syntactic patterns, first developed by Hearst (1992a), and therefore often called Hearst patterns. Consider the following sentence: \nAgar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use. \nHearst points out that most human readers will not know what Gelidium is, but that they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is. She suggests that the following lexico-syntactic pattern \nimplies the following semantics \nallowing us to infer \nFigure 17.5 shows five patterns Hearst (1992a, 1998) suggested for inferring the hyponym relation; we’ve shown ${ mathrm { N P } } _ { mathrm { H } }$ as the parent/hyponym. Modern versions of the pattern-based approach extend it by adding named entity constraints. For example if our goal is to answer questions about “Who holds what office in which organization?”, we can use patterns like the following: \nPER, POSITION of ORG: George Marshall, Secretary of State of the United States \nPER (named appointed chose etc.) PER Prep? POSITION Truman appointed Marshall Secretary of State \nPER [be]? (named|appointed|etc.) Prep? ORG POSITION George Marshall was named US Secretary of State \nHand-built patterns have the advantage of high-precision and they can be tailored to specific domains. On the other hand, they are often low-recall, and it’s a lot of work to create them for all possible patterns.",
        "chapter": "Information Extraction",
        "section": "Relation Extraction",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "17.2 Relation Extraction Algorithms \nThere are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We’ll introduce each of these in the next sections. \nHearst patterns \n17.2.1 Using Patterns to Extract Relations \nThe earliest and still common algorithm for relation extraction is lexico-syntactic patterns, first developed by Hearst (1992a), and therefore often called Hearst patterns. Consider the following sentence: \nAgar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use. \nHearst points out that most human readers will not know what Gelidium is, but that they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is. She suggests that the following lexico-syntactic pattern \nimplies the following semantics \nallowing us to infer \nFigure 17.5 shows five patterns Hearst (1992a, 1998) suggested for inferring the hyponym relation; we’ve shown ${ mathrm { N P } } _ { mathrm { H } }$ as the parent/hyponym. Modern versions of the pattern-based approach extend it by adding named entity constraints. For example if our goal is to answer questions about “Who holds what office in which organization?”, we can use patterns like the following: \nPER, POSITION of ORG: George Marshall, Secretary of State of the United States \nPER (named appointed chose etc.) PER Prep? POSITION Truman appointed Marshall Secretary of State \nPER [be]? (named|appointed|etc.) Prep? ORG POSITION George Marshall was named US Secretary of State \nHand-built patterns have the advantage of high-precision and they can be tailored to specific domains. On the other hand, they are often low-recall, and it’s a lot of work to create them for all possible patterns. \n17.2.2 Relation Extraction via Supervised Learning \nSupervised machine learning approaches to relation extraction follow a scheme that should be familiar by now. A fixed set of relations and entities is chosen, a training corpus is hand-annotated with the relations and entities, and the annotated texts are then used to train classifiers to annotate an unseen test set. \nThe most straightforward approach, illustrated in Fig. 17.6 is: (1) Find pairs of named entities (usually in the same sentence). (2): Apply a relation-classification on each pair. The classifier can use any supervised technique (logistic regression, RNN, Transformer, random forest, etc.). \nAn optional intermediate filtering classifier can be used to speed up the processing by making a binary decision on whether a given pair of named entities are related (by any relation). It’s trained on positive examples extracted directly from all relations in the annotated corpus, and negative examples generated from within-sentence entity pairs that are not annotated with a relation. \nFeature-based supervised relation classifiers. Let’s consider sample features for a feature-based classifier (like logistic regression or random forests), classifying the relationship between American Airlines (Mention 1, or M1) and Tim Wagner (Mention 2, M2) from this sentence: \n(17.5) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said \nThese include word features (as embeddings, or 1-hot, stemmed or not): \n• The headwords of M1 and M2 and their concatenation Airlines Wagner Airlines-Wagner   \n• Bag-of-words and bigrams in M1 and M2 American, Airlines, Tim, Wagner, American Airlines, Tim Wagner   \n• Words or bigrams in particular positions M2: -1 spokesman M2: $+ 1$ said   \n• Bag of words or bigrams between M1 and M2: a, AMR, of, immediately, matched, move, spokesman, the, unit \nNamed entity features: \n• Named-entity types and their concatenation (M1: ORG, M2: PER, M1M2: ORG-PER)   \n• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN) M1: NAME [it or he would be PRONOUN] M2: NAME [the company would be NOMINAL]",
        "chapter": "Information Extraction",
        "section": "Relation Extraction Algorithms",
        "subsection": "Using Patterns to Extract Relations",
        "subsubsection": "N/A"
    },
    {
        "content": "17.2.2 Relation Extraction via Supervised Learning \nSupervised machine learning approaches to relation extraction follow a scheme that should be familiar by now. A fixed set of relations and entities is chosen, a training corpus is hand-annotated with the relations and entities, and the annotated texts are then used to train classifiers to annotate an unseen test set. \nThe most straightforward approach, illustrated in Fig. 17.6 is: (1) Find pairs of named entities (usually in the same sentence). (2): Apply a relation-classification on each pair. The classifier can use any supervised technique (logistic regression, RNN, Transformer, random forest, etc.). \nAn optional intermediate filtering classifier can be used to speed up the processing by making a binary decision on whether a given pair of named entities are related (by any relation). It’s trained on positive examples extracted directly from all relations in the annotated corpus, and negative examples generated from within-sentence entity pairs that are not annotated with a relation. \nFeature-based supervised relation classifiers. Let’s consider sample features for a feature-based classifier (like logistic regression or random forests), classifying the relationship between American Airlines (Mention 1, or M1) and Tim Wagner (Mention 2, M2) from this sentence: \n(17.5) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said \nThese include word features (as embeddings, or 1-hot, stemmed or not): \n• The headwords of M1 and M2 and their concatenation Airlines Wagner Airlines-Wagner   \n• Bag-of-words and bigrams in M1 and M2 American, Airlines, Tim, Wagner, American Airlines, Tim Wagner   \n• Words or bigrams in particular positions M2: -1 spokesman M2: $+ 1$ said   \n• Bag of words or bigrams between M1 and M2: a, AMR, of, immediately, matched, move, spokesman, the, unit \nNamed entity features: \n• Named-entity types and their concatenation (M1: ORG, M2: PER, M1M2: ORG-PER)   \n• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN) M1: NAME [it or he would be PRONOUN] M2: NAME [the company would be NOMINAL] \n• Number of entities between the arguments (in this case 1, for AMR) \nSyntactic structure is a useful signal, often represented as the dependency or constituency syntactic path traversed through the tree between the entities. \n• Constituent paths between M1 and M2 $N P uparrow N P uparrow S uparrow S downarrow N P$   \n• Dependency-tree paths Airlines $ { } _ { s u b j }$ matched ${  } _ { c o m p }$ said →sub j Wagner \nNeural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let’s consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In TACRED we are given a sentence and two spans within it: a subject, which is a person or organization, and an object, which is any other entity. The task is to assign a relation from the 42 TAC relations, or no relation. \nA typical Transformer-encoder algorithm, showin in Fig. 17.7, simply takes a pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is partially de-lexified; the subject and object entities are replaced in the input by their NER tags. This helps keep the system from overfitting to the individual lexical items (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it helps to use versions of BERT like RoBERTa (Liu et al., 2019) or SPANbert (Joshi et al., 2020) that don’t have two sequences separated by a [SEP] token, but instead form the input from a single long sequence of sentences. \nIn general, if the test set is similar enough to the training set, and if there is enough hand-labeled data, supervised relation extraction systems can get high accuracies. But labeling a large training set is extremely expensive and supervised models are brittle: they don’t generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next. \n17.2.3 Semisupervised Relation Extraction via Bootstrapping \nseed patterns seed tuples bootstrapping \nSupervised machine learning assumes that we have lots of labeled data. Unfortunately, this is expensive. But suppose we just have a few high-precision seed patterns, like those in Section 17.2.1, or perhaps a few seed tuples. That’s enough to bootstrap a classifier! Bootstrapping proceeds by taking the entities in the seed pair, and then finding sentences (on the web, or whatever dataset we are using) that contain both entities. From all such sentences, we extract and generalize the context around the entities to learn new patterns. Fig. 17.8 sketches a basic algorithm.",
        "chapter": "Information Extraction",
        "section": "Relation Extraction Algorithms",
        "subsection": "Relation Extraction via Supervised Learning",
        "subsubsection": "N/A"
    },
    {
        "content": "• Number of entities between the arguments (in this case 1, for AMR) \nSyntactic structure is a useful signal, often represented as the dependency or constituency syntactic path traversed through the tree between the entities. \n• Constituent paths between M1 and M2 $N P uparrow N P uparrow S uparrow S downarrow N P$   \n• Dependency-tree paths Airlines $ { } _ { s u b j }$ matched ${  } _ { c o m p }$ said →sub j Wagner \nNeural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let’s consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In TACRED we are given a sentence and two spans within it: a subject, which is a person or organization, and an object, which is any other entity. The task is to assign a relation from the 42 TAC relations, or no relation. \nA typical Transformer-encoder algorithm, showin in Fig. 17.7, simply takes a pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is partially de-lexified; the subject and object entities are replaced in the input by their NER tags. This helps keep the system from overfitting to the individual lexical items (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it helps to use versions of BERT like RoBERTa (Liu et al., 2019) or SPANbert (Joshi et al., 2020) that don’t have two sequences separated by a [SEP] token, but instead form the input from a single long sequence of sentences. \nIn general, if the test set is similar enough to the training set, and if there is enough hand-labeled data, supervised relation extraction systems can get high accuracies. But labeling a large training set is extremely expensive and supervised models are brittle: they don’t generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next. \n17.2.3 Semisupervised Relation Extraction via Bootstrapping \nseed patterns seed tuples bootstrapping \nSupervised machine learning assumes that we have lots of labeled data. Unfortunately, this is expensive. But suppose we just have a few high-precision seed patterns, like those in Section 17.2.1, or perhaps a few seed tuples. That’s enough to bootstrap a classifier! Bootstrapping proceeds by taking the entities in the seed pair, and then finding sentences (on the web, or whatever dataset we are using) that contain both entities. From all such sentences, we extract and generalize the context around the entities to learn new patterns. Fig. 17.8 sketches a basic algorithm. \n\nSuppose, for example, that we need to create a list of airline/hub pairs, and we know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover new patterns by finding other mentions of this relation in our corpus. We search for the terms Ryanair, Charleroi and hub in some proximity. Perhaps we find the following set of sentences: \nconfidence values semantic drift \n(17.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all weekend flights out of the airport.   \n(17.7) All flights in and out of Ryanair’s hub at Charleroi airport were grounded on Friday...   \n(17.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000 passengers had already been affected. \nFrom these results, we can use the context of words between the entity mentions, the words before mention one, the word after mention two, and the named entity types of the two mentions, and perhaps other features, to extract general patterns such as the following: \n/ [ORG], which uses [LOC] as a hub / / [ORG]’s hub at [LOC] / / [LOC], a main hub for [ORG] / hese new patterns can then be used to search for additional tuples. \nBootstrapping systems also assign confidence values to new tuples to avoid semantic drift. In semantic drift, an erroneous pattern leads to the introduction of erroneous tuples, which, in turn, lead to the creation of problematic patterns and the meaning of the extracted relations ‘drifts’. Consider the following example: \n(17.9) Sydney has a ferry hub at Circular Quay. \nIf accepted as a positive example, this expression could lead to the incorrect introduction of the tuple $langle S y d n e y , C i r c u l a r Q u a y rangle$ . Patterns based on this tuple could propagate further errors into the database. \nConfidence values for patterns are based on balancing two factors: the pattern’s performance with respect to the current set of tuples and the pattern’s productivity in terms of the number of matches it produces in the document collection. More formally, given a document collection $mathcal { D }$ , a current set of tuples $T$ , and a proposed pattern $p$ , we need to track two factors: \n• $h i t s ( p )$ : the set of tuples in $T$ that $p$ matches while looking in $mathrm { textmathcal { D } }$ • $quad  f n d s ( p )$ : The total set of tuples that $p$ finds in $mathrm { textmathcal { D } }$ \n\nThe following equation balances these considerations (Riloff and Jones, 1999). \nThis metric is generally normalized to produce a probability. \nnoisy-or \nWe can assess the confidence in a proposed new tuple by combining the evidence supporting it from all the patterns $P ^ { prime }$ that match that tuple in $mathrm { textmathcal { D } }$ (Agichtein and Gravano, 2000). One way to combine such evidence is the noisy-or technique. Assume that a given tuple is supported by a subset of the patterns in $P$ , each with its own confidence assessed as above. In the noisy-or model, we make two basic assumptions. First, that for a proposed tuple to be false, all of its supporting patterns must have been in error, and second, that the sources of their individual failures are all independent. If we loosely treat our confidence measures as probabilities, then the probability of any individual pattern $p$ failing is $1 - C o n f ( p )$ ; the probability of all of the supporting patterns for a tuple being wrong is the product of their individual failure probabilities, leaving us with the following equation for our confidence in a new tuple. \nSetting conservative confidence thresholds for the acceptance of new patterns and tuples during the bootstrapping process helps prevent the system from drifting away from the targeted relation. \n17.2.4 Distant Supervision for Relation Extraction \nAlthough hand-labeling text with relation labels is expensive to produce, there are ways to find indirect sources of training data. The distant supervision method (Mintz et al., 2009) combines the advantages of bootstrapping with supervised learning. Instead of just a handful of seeds, distant supervision uses a large database to acquire a huge number of seed examples, creates lots of noisy pattern features from all these examples and then combines them in a supervised classifier. \nFor example suppose we are trying to learn the place-of-birth relationship between people and their birth cities. In the seed-based approach, we might have only 5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase have tens of thousands of examples of many relations; including over 100,000 examples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein, Ulm>, etc.,). The next step is to run named entity taggers on large amounts of text— Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences that have two named entities that match the tuple, like the following: \n...Hubble was born in Marshfield...   \n...Einstein, born (1879), Ulm...   \n...Hubble’s birthplace in Marshfield... \nTraining instances can now be extracted from this data, one training instance for each identical tuple <relation, entity1, entity2>. Thus there will be one training instance for each of: \n<born-in, Edwin Hubble, Marshfield> <born-in, Albert Einstein, Ulm> <born-year, Albert Einstein, 1879>",
        "chapter": "Information Extraction",
        "section": "Relation Extraction Algorithms",
        "subsection": "Semisupervised Relation Extraction via Bootstrapping",
        "subsubsection": "N/A"
    },
    {
        "content": "The following equation balances these considerations (Riloff and Jones, 1999). \nThis metric is generally normalized to produce a probability. \nnoisy-or \nWe can assess the confidence in a proposed new tuple by combining the evidence supporting it from all the patterns $P ^ { prime }$ that match that tuple in $mathrm { textmathcal { D } }$ (Agichtein and Gravano, 2000). One way to combine such evidence is the noisy-or technique. Assume that a given tuple is supported by a subset of the patterns in $P$ , each with its own confidence assessed as above. In the noisy-or model, we make two basic assumptions. First, that for a proposed tuple to be false, all of its supporting patterns must have been in error, and second, that the sources of their individual failures are all independent. If we loosely treat our confidence measures as probabilities, then the probability of any individual pattern $p$ failing is $1 - C o n f ( p )$ ; the probability of all of the supporting patterns for a tuple being wrong is the product of their individual failure probabilities, leaving us with the following equation for our confidence in a new tuple. \nSetting conservative confidence thresholds for the acceptance of new patterns and tuples during the bootstrapping process helps prevent the system from drifting away from the targeted relation. \n17.2.4 Distant Supervision for Relation Extraction \nAlthough hand-labeling text with relation labels is expensive to produce, there are ways to find indirect sources of training data. The distant supervision method (Mintz et al., 2009) combines the advantages of bootstrapping with supervised learning. Instead of just a handful of seeds, distant supervision uses a large database to acquire a huge number of seed examples, creates lots of noisy pattern features from all these examples and then combines them in a supervised classifier. \nFor example suppose we are trying to learn the place-of-birth relationship between people and their birth cities. In the seed-based approach, we might have only 5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase have tens of thousands of examples of many relations; including over 100,000 examples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein, Ulm>, etc.,). The next step is to run named entity taggers on large amounts of text— Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences that have two named entities that match the tuple, like the following: \n...Hubble was born in Marshfield...   \n...Einstein, born (1879), Ulm...   \n...Hubble’s birthplace in Marshfield... \nTraining instances can now be extracted from this data, one training instance for each identical tuple <relation, entity1, entity2>. Thus there will be one training instance for each of: \n<born-in, Edwin Hubble, Marshfield> <born-in, Albert Einstein, Ulm> <born-year, Albert Einstein, 1879> \nand so on. \nWe can then apply feature-based or neural classification. For feature-based classification, standard supervised relation extraction features like the named entity labels of the two mentions, the words and dependency paths in between the mentions, and neighboring words. Each tuple will have features collected from many training instances; the feature vector for a single training instance like (<born-in,Albert Einstein, Ulm> will have lexical and syntactic features from many different sentences that mention Einstein and Ulm. \nBecause distant supervision has very large training sets, it is also able to use very rich features that are conjunctions of these individual features. So we will extract thousands of patterns that conjoin the entity types with the intervening words or dependency paths like these: \nPER was born in LOC PER, born (XXXX), LOC PER’s birthplace in LOC \nTo return to our running example, for this sentence: \n(17.12) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said \nwe would learn rich conjunction features like this one: \nThe result is a supervised classifier that has a huge rich set of features to use in detecting relations. Since not every test sentence will have one of the training relations, the classifier will also need to be able to label an example as no-relation. This label is trained by randomly selecting entity pairs that do not appear in any Freebase relation, extracting features for them, and building a feature vector for each such tuple. The final algorithm is sketched in Fig. 17.9. \nDistant supervision shares advantages with each of the methods we’ve examined. Like supervised classification, distant supervision uses a classifier with lots of features, and supervised by detailed hand-created knowledge. Like pattern-based classifiers, it can make use of high-precision evidence for the relation between entities. Indeed, distance supervision systems learn patterns just like the hand-built patterns of early relation extractors. For example the is-a or hypernym extraction system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as distant supervision, and then learned new patterns from large amounts of text. Their system induced exactly the original 5 template patterns of Hearst (1992a), but also 70,000 additional patterns including these four: \n\n$mathrm { N P } _ { H }$ like NP Many hormones like leptin... $mathrm { N P } _ { H }$ called NP ...using a markup language called XHTML NP is a $mathrm { N P } _ { H }$ Ruby is a programming language... NP, a $mathrm { N P } _ { H }$ IBM, a company with a long... \nThis ability to use a large number of features simultaneously means that, unlike the iterative expansion of patterns in seed-based systems, there’s no semantic drift. Like unsupervised classification, it doesn’t use a labeled training corpus of texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very large amounts of unlabeled data. Distant supervision also has the advantage that it can create training tuples to be used with neural classifiers, where features are not required. \nThe main problem with distant supervision is that it tends to produce low-precision results, and so current research focuses on ways to improve precision. Furthermore, distant supervision can only help in extracting relations for which a large enough database already exists. To extract new relations without datasets, or relations for new domains, purely unsupervised methods must be used. \n17.2.5 Unsupervised Relation Extraction \nThe goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called open information extraction or Open IE. In Open IE, the relations are simply strings of words (usually beginning with a verb). \nFor example, the ReVerb system (Fader et al., 2011) extracts a relation from a sentence $s$ in 4 steps: \n1. Run a part-of-speech tagger and entity chunker over $s$   \n2. For each verb in $s$ , find the longest sequence of words $w$ that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.   \n3. For each phrase $w$ , find the nearest noun phrase $x$ to the left which is not a relative pronoun, wh-word or existential “there”. Find the nearest noun phrase $y$ to the right.   \n4. Assign confidence $c$ to the relation ${ r } = ( x , w , y )$ using a confidence classifier and return it. \nA relation is only accepted if it meets syntactic and lexical constraints. The syntactic constraints ensure that it is a verb-initial sequence that might also include nouns (relations that begin with light verbs like make, have, or do often express the core of the relation with a noun, like have a hub in): \nThe lexical constraints are based on a dictionary $D$ that is used to prune very rare, long relation strings. The intuition is to eliminate candidate relations that don’t occur with sufficient number of distinct argument types and so are likely to be bad examples. The system first runs the above relation extraction algorithm offline on",
        "chapter": "Information Extraction",
        "section": "Relation Extraction Algorithms",
        "subsection": "Distant Supervision for Relation Extraction",
        "subsubsection": "N/A"
    },
    {
        "content": "$mathrm { N P } _ { H }$ like NP Many hormones like leptin... $mathrm { N P } _ { H }$ called NP ...using a markup language called XHTML NP is a $mathrm { N P } _ { H }$ Ruby is a programming language... NP, a $mathrm { N P } _ { H }$ IBM, a company with a long... \nThis ability to use a large number of features simultaneously means that, unlike the iterative expansion of patterns in seed-based systems, there’s no semantic drift. Like unsupervised classification, it doesn’t use a labeled training corpus of texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very large amounts of unlabeled data. Distant supervision also has the advantage that it can create training tuples to be used with neural classifiers, where features are not required. \nThe main problem with distant supervision is that it tends to produce low-precision results, and so current research focuses on ways to improve precision. Furthermore, distant supervision can only help in extracting relations for which a large enough database already exists. To extract new relations without datasets, or relations for new domains, purely unsupervised methods must be used. \n17.2.5 Unsupervised Relation Extraction \nThe goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called open information extraction or Open IE. In Open IE, the relations are simply strings of words (usually beginning with a verb). \nFor example, the ReVerb system (Fader et al., 2011) extracts a relation from a sentence $s$ in 4 steps: \n1. Run a part-of-speech tagger and entity chunker over $s$   \n2. For each verb in $s$ , find the longest sequence of words $w$ that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.   \n3. For each phrase $w$ , find the nearest noun phrase $x$ to the left which is not a relative pronoun, wh-word or existential “there”. Find the nearest noun phrase $y$ to the right.   \n4. Assign confidence $c$ to the relation ${ r } = ( x , w , y )$ using a confidence classifier and return it. \nA relation is only accepted if it meets syntactic and lexical constraints. The syntactic constraints ensure that it is a verb-initial sequence that might also include nouns (relations that begin with light verbs like make, have, or do often express the core of the relation with a noun, like have a hub in): \nThe lexical constraints are based on a dictionary $D$ that is used to prune very rare, long relation strings. The intuition is to eliminate candidate relations that don’t occur with sufficient number of distinct argument types and so are likely to be bad examples. The system first runs the above relation extraction algorithm offline on \n500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each relation $r$ is added to the dictionary if it occurs with at least 20 different arguments. Fader et al. (2011) used a dictionary of 1.7 million normalized relations. \nFinally, a confidence value is computed for each relation using a logistic regression classifier. The classifier is trained by taking 1000 random web sentences, running the extractor, and hand labeling each extracted relation as correct or incorrect. A confidence classifier is then trained on this hand-labeled data, using features of the relation and the surrounding words. Fig. 17.10 shows some sample features used in the classification. \nFor example the following sentence: \n(17.13) United has a hub in Chicago, which is the headquarters of United Continental Holdings. \nhas the relation phrases has a hub in and is the headquarters of (it also has has and is, but longer phrases are preferred). Step 3 finds United to the left and Chicago to the right of has a hub in, and skips over which to find Chicago to the left of is the headquarters of. The final output is: \nr1: <United, has a hub in, Chicago> r2: <Chicago, is the headquarters of, United Continental Holdings> \nThe great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map these large sets of strings into some canonical form for adding to databases or other knowledge sources. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally. \n17.2.6 Evaluation of Relation Extraction \nSupervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure. Labeled precision and recall require the system to classify the relation correctly, whereas unlabeled methods simply measure a system’s ability to detect entities that are related. \nSemi-supervised and unsupervised methods are much more difficult to evaluate, since they extract totally new relations from the web or a large text. Because these methods use very large amounts of text, it is generally not possible to run them solely on a small labeled test set, and as a result it’s not possible to pre-annotate a gold set of correct instances of relations.",
        "chapter": "Information Extraction",
        "section": "Relation Extraction Algorithms",
        "subsection": "Unsupervised Relation Extraction",
        "subsubsection": "N/A"
    },
    {
        "content": "500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each relation $r$ is added to the dictionary if it occurs with at least 20 different arguments. Fader et al. (2011) used a dictionary of 1.7 million normalized relations. \nFinally, a confidence value is computed for each relation using a logistic regression classifier. The classifier is trained by taking 1000 random web sentences, running the extractor, and hand labeling each extracted relation as correct or incorrect. A confidence classifier is then trained on this hand-labeled data, using features of the relation and the surrounding words. Fig. 17.10 shows some sample features used in the classification. \nFor example the following sentence: \n(17.13) United has a hub in Chicago, which is the headquarters of United Continental Holdings. \nhas the relation phrases has a hub in and is the headquarters of (it also has has and is, but longer phrases are preferred). Step 3 finds United to the left and Chicago to the right of has a hub in, and skips over which to find Chicago to the left of is the headquarters of. The final output is: \nr1: <United, has a hub in, Chicago> r2: <Chicago, is the headquarters of, United Continental Holdings> \nThe great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map these large sets of strings into some canonical form for adding to databases or other knowledge sources. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally. \n17.2.6 Evaluation of Relation Extraction \nSupervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure. Labeled precision and recall require the system to classify the relation correctly, whereas unlabeled methods simply measure a system’s ability to detect entities that are related. \nSemi-supervised and unsupervised methods are much more difficult to evaluate, since they extract totally new relations from the web or a large text. Because these methods use very large amounts of text, it is generally not possible to run them solely on a small labeled test set, and as a result it’s not possible to pre-annotate a gold set of correct instances of relations. \nFor these methods it’s possible to approximate (only) precision by drawing a random sample of relations from the output, and having a human check the accuracy of each of these relations. Usually this approach focuses on the tuples to be extracted from a body of text rather than on the relation mentions; systems need not detect every mention of a relation to be scored correctly. Instead, the evaluation is based on the set of tuples occupying the database when the system is finished. That is, we want to know if the system can discover that Ryanair has a hub at Charleroi; we don’t really care how many times it discovers it. The estimated precision $hat { P }$ is then \nAnother approach that gives us a little bit of information about recall is to compute precision at different levels of recall. Assuming that our system is able to rank the relations it produces (by probability, or confidence) we can separately compute precision for the top 1000 new relations, the top 10,000 new relations, the top 100,000, and so on. In each case we take a random sample of that set. This will show us how the precision curve behaves as we extract more and more tuples. But there is no way to directly evaluate recall. \n17.3 Extracting Times \nTimes and dates are a particularly important kind of named entity that play a role in question answering, in calendar and personal assistant applications. In order to reason about times and dates, after we extract these temporal expressions they must be normalized—converted to a standard format so we can reason about them. In this section we consider both the extraction and normalization of temporal expressions. \n17.3.1 Temporal Expression Extraction \nabsolute relative \nduration \nTemporal expressions are those that refer to absolute points in time, relative times, durations, and sets of these. Absolute temporal expressions are those that can be mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 17.11 lists some sample temporal expressions in each of these categories. \nlexical triggers \nTemporal expressions are grammatical constructions that have temporal lexical triggers as their heads. Lexical triggers might be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist of their phrasal projections: noun phrases, adjective phrases, and adverbial phrases. Figure 17.12 provides examples.",
        "chapter": "Information Extraction",
        "section": "Relation Extraction Algorithms",
        "subsection": "Evaluation of Relation Extraction",
        "subsubsection": "N/A"
    },
    {
        "content": "For these methods it’s possible to approximate (only) precision by drawing a random sample of relations from the output, and having a human check the accuracy of each of these relations. Usually this approach focuses on the tuples to be extracted from a body of text rather than on the relation mentions; systems need not detect every mention of a relation to be scored correctly. Instead, the evaluation is based on the set of tuples occupying the database when the system is finished. That is, we want to know if the system can discover that Ryanair has a hub at Charleroi; we don’t really care how many times it discovers it. The estimated precision $hat { P }$ is then \nAnother approach that gives us a little bit of information about recall is to compute precision at different levels of recall. Assuming that our system is able to rank the relations it produces (by probability, or confidence) we can separately compute precision for the top 1000 new relations, the top 10,000 new relations, the top 100,000, and so on. In each case we take a random sample of that set. This will show us how the precision curve behaves as we extract more and more tuples. But there is no way to directly evaluate recall. \n17.3 Extracting Times \nTimes and dates are a particularly important kind of named entity that play a role in question answering, in calendar and personal assistant applications. In order to reason about times and dates, after we extract these temporal expressions they must be normalized—converted to a standard format so we can reason about them. In this section we consider both the extraction and normalization of temporal expressions. \n17.3.1 Temporal Expression Extraction \nabsolute relative \nduration \nTemporal expressions are those that refer to absolute points in time, relative times, durations, and sets of these. Absolute temporal expressions are those that can be mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 17.11 lists some sample temporal expressions in each of these categories. \nlexical triggers \nTemporal expressions are grammatical constructions that have temporal lexical triggers as their heads. Lexical triggers might be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist of their phrasal projections: noun phrases, adjective phrases, and adverbial phrases. Figure 17.12 provides examples. \nLet’s look at the TimeML annotation scheme, in which temporal expressions are annotated with an XML tag, TIMEX3, and various attributes to that tag (Pustejovsky et al. 2005, Ferro et al. 2005). The following example illustrates the basic use of this scheme (we defer discussion of the attributes until Section 17.3.2). \nA fare increase initiated ${ bf mathrm { < T I M E X 3 > } } quad .$ last week ${ < } / mathrm { T I M E X } 3 { > }$ by UAL Corp’s United Airlines was matched by competitors over $<$ <TIMEX3 $>$ the weekend ${ < } / mathrm { T I M E X } 3 { > }$ , marking the second successful fare increase in <TIMEX3 $>$ two weeks</TIMEX3>. \nThe temporal expression recognition task consists of finding the start and end of all of the text spans that correspond to such temporal expressions. Rule-based approaches to temporal expression recognition use cascades of automata to recognize patterns at increasing levels of complexity. Tokens are first part-of-speech tagged, and then larger and larger chunks are recognized from the results from previous stages, based on patterns containing trigger words (e.g., February) or classes (e.g., MONTH). Figure 17.13 gives a fragment from a rule-based system. \nSequence-labeling approaches follow the same IOB scheme used for namedentity tags, marking words that are either inside, outside or at the beginning of a TIMEX3-delimited temporal expression with the I, O, and B tags as follows: \nA fare increase initiated last week by UAL Corp’s.. \nO O O O B I O O O \nFeatures are extracted from the token and its context, and a statistical sequence labeler is trained (any sequence model can be used). Figure 17.14 lists standard features used in temporal tagging. \nTemporal expression recognizers are evaluated with the usual recall, precision, and $F$ -measures. A major difficulty for all of these very lexicalized approaches is avoiding expressions that trigger false positives: \n(17.15) 1984 tells the story of Winston Smith... (17.16) ...U2’s classic Sunday Bloody Sunday \n17.3.2 Temporal Normalization \ntemporal normalization \nTemporal normalization is the process of mapping a temporal expression to either a specific point in time or to a duration. Points in time correspond to calendar dates, to times of day, or both. Durations primarily consist of lengths of time but may also include information about start and end points. Normalized times are represented with the VALUE attribute from the ISO 8601 standard for encoding temporal values (ISO8601, 2004). Fig. 17.15 reproduces our earlier example with the value attributes added in. \n<TIMEX3 i d = ’ ’ t 1 ’ ’ t y p e $ c =$ ”DATE” v a l u e =” 2007 −07 −02 ” f u n c t i o n I n D o c u m e n $mathit { Psi } : = mathit { Psi }$ CREATION TIME” $>$ J u l y 2 , 2007 ${ < } I$ TIMEX3> A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d $ c =$ ” t 2 ” t y p e $ c =$ ”DATE” v a l u e =” 2007−W26” anchorTimeID $ c =$ ” t 1 ”> l a s t week</ TIMEX3> by U n i t e d A i r l i n e s was matched by c o m p e t i t o r s o v e r <TIMEX3 i d $ c =$ ” t 3 ” t y p e $ c =$ ”DURATION” v a l u e $ c =$ ”P1WE” anchorTimeID $begin{array} { r l r } { mathbf { sigma } ) mathbf { Sigma } } & { { } = } & { ~ mathbf { sigma } } end{array}$ t h e weekend $mathrm { < / T I M E X 3 > }$ , m a r k i n g t h e s e c o n d s u c c e s s f u l f a r e i n c r e a s e i n <TIMEX3 i d =” t 4 ” t y p e $ c =$ ”DURATION” v a l u e $ c =$ ”P2W” anchorTimeID $ c =$ ” t 1 ”> two weeks </ TIMEX3>. \nThe dateline, or document date, for this text was July 2, 2007. The ISO representation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02. The encodings for the temporal expressions in our sample text all follow from this date, and are shown here as values for the VALUE attribute. \nThe first temporal expression in the text proper refers to a particular week of the year. In the ISO standard, weeks are numbered from 01 to 53, with the first week of the year being the one that has the first Thursday of the year. These weeks are represented with the template YYYY-Wnn. The ISO week for our document date is week 27; thus the value for last week is represented as “2007-W26”. \nThe next temporal expression is the weekend. ISO weeks begin on Monday; thus, weekends occur at the end of a week and are fully contained within a single week. Weekends are treated as durations, so the value of the VALUE attribute has to be a length. Durations are represented according to the pattern $ { mathrm { P } } n x$ , where $n$ is an integer denoting the length and $x$ represents the unit, as in P3Y for three years or P2D for two days. In this example, one weekend is captured as P1WE. In this case, there is also sufficient information to anchor this particular weekend as part of a particular week. Such information is encoded in the ANCHORTIMEID attribute. Finally, the phrase two weeks also denotes a duration captured as P2W. There is a lot more to the various temporal annotation standards—far too much to cover here. Figure 17.16 describes some of the basic ways that other times and durations are represented. Consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005) for more details. \nMost current approaches to temporal normalization are rule-based (Chang and Manning 2012, Stro¨tgen and Gertz 2013). Patterns that match temporal expressions are associated with semantic analysis procedures. As in the compositional",
        "chapter": "Information Extraction",
        "section": "Extracting Times",
        "subsection": "Temporal Expression Extraction",
        "subsubsection": "N/A"
    },
    {
        "content": "17.3.2 Temporal Normalization \ntemporal normalization \nTemporal normalization is the process of mapping a temporal expression to either a specific point in time or to a duration. Points in time correspond to calendar dates, to times of day, or both. Durations primarily consist of lengths of time but may also include information about start and end points. Normalized times are represented with the VALUE attribute from the ISO 8601 standard for encoding temporal values (ISO8601, 2004). Fig. 17.15 reproduces our earlier example with the value attributes added in. \n<TIMEX3 i d = ’ ’ t 1 ’ ’ t y p e $ c =$ ”DATE” v a l u e =” 2007 −07 −02 ” f u n c t i o n I n D o c u m e n $mathit { Psi } : = mathit { Psi }$ CREATION TIME” $>$ J u l y 2 , 2007 ${ < } I$ TIMEX3> A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d $ c =$ ” t 2 ” t y p e $ c =$ ”DATE” v a l u e =” 2007−W26” anchorTimeID $ c =$ ” t 1 ”> l a s t week</ TIMEX3> by U n i t e d A i r l i n e s was matched by c o m p e t i t o r s o v e r <TIMEX3 i d $ c =$ ” t 3 ” t y p e $ c =$ ”DURATION” v a l u e $ c =$ ”P1WE” anchorTimeID $begin{array} { r l r } { mathbf { sigma } ) mathbf { Sigma } } & { { } = } & { ~ mathbf { sigma } } end{array}$ t h e weekend $mathrm { < / T I M E X 3 > }$ , m a r k i n g t h e s e c o n d s u c c e s s f u l f a r e i n c r e a s e i n <TIMEX3 i d =” t 4 ” t y p e $ c =$ ”DURATION” v a l u e $ c =$ ”P2W” anchorTimeID $ c =$ ” t 1 ”> two weeks </ TIMEX3>. \nThe dateline, or document date, for this text was July 2, 2007. The ISO representation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02. The encodings for the temporal expressions in our sample text all follow from this date, and are shown here as values for the VALUE attribute. \nThe first temporal expression in the text proper refers to a particular week of the year. In the ISO standard, weeks are numbered from 01 to 53, with the first week of the year being the one that has the first Thursday of the year. These weeks are represented with the template YYYY-Wnn. The ISO week for our document date is week 27; thus the value for last week is represented as “2007-W26”. \nThe next temporal expression is the weekend. ISO weeks begin on Monday; thus, weekends occur at the end of a week and are fully contained within a single week. Weekends are treated as durations, so the value of the VALUE attribute has to be a length. Durations are represented according to the pattern $ { mathrm { P } } n x$ , where $n$ is an integer denoting the length and $x$ represents the unit, as in P3Y for three years or P2D for two days. In this example, one weekend is captured as P1WE. In this case, there is also sufficient information to anchor this particular weekend as part of a particular week. Such information is encoded in the ANCHORTIMEID attribute. Finally, the phrase two weeks also denotes a duration captured as P2W. There is a lot more to the various temporal annotation standards—far too much to cover here. Figure 17.16 describes some of the basic ways that other times and durations are represented. Consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005) for more details. \nMost current approaches to temporal normalization are rule-based (Chang and Manning 2012, Stro¨tgen and Gertz 2013). Patterns that match temporal expressions are associated with semantic analysis procedures. As in the compositional \nfully qualified \nrule-to-rule approach introduced in Chapter 16, the meaning of a constituent is computed from the meaning of its parts using a method specific to the constituent, although here the semantic composition rules involve temporal arithmetic rather than $lambda$ -calculus attachments. \nFully qualified date expressions contain a year, month, and day in some conventional form. The units in the expression must be detected and then placed in the correct place in the corresponding ISO pattern. The following pattern normalizes expressions like April 24, 1916. \ntemporal anchor \nThe non-terminals Month, Date, and Year represent constituents that have already been recognized and assigned semantic values, accessed through the *.val notation. The value of this $F Q E$ constituent can, in turn, be accessed as FQTE.val during further processing. \nFully qualified temporal expressions are fairly rare in real texts. Most temporal expressions in news articles are incomplete and are only implicitly anchored, often with respect to the dateline of the article, which we refer to as the document’s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. \nUnfortunately, even simple expressions such as the weekend or Wednesday introduce a fair amount of complexity. In our current example, the weekend clearly refers to the weekend of the week that immediately precedes the document date. But this won’t always be the case, as is illustrated in the following example. \n(17.17) Random security checks that began yesterday at Sky Harbor will continue at least through the weekend. \nIn this case, the expression the weekend refers to the weekend of the week that the anchoring date is part of (i.e., the coming weekend). The information that signals this meaning comes from the tense of continue, the verb governing the weekend. \nRelative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. \n\n17.4 Extracting Events and their Times \nevent extraction \nThe task of event extraction is to identify mentions of events in texts. For the purposes of this task, an event mention is any expression denoting an event or state that can be assigned to a particular point, or interval, in time. The following markup of the sample text on page 376 shows all the events in this text. \nreporting events \n[EVENT Citing] high fuel prices, United Airlines [EVENT said] Friday it has [EVENT increased] fares by $$ 6$ per round trip on flights to some cities also served by lower-cost carriers. American Airlines, a unit of AMR Corp., immediately [EVENT matched] [EVENT the move], spokesman Tim Wagner [EVENT said]. United, a unit of UAL Corp., [EVENT said] [EVENT the increase] took effect Thursday and [EVENT applies] to most routes where it [EVENT competes] against discount carriers, such as Chicago to Dallas and Denver to San Francisco. \nIn English, most event mentions correspond to verbs, and most verbs introduce events. However, as we can see from our example, this is not always the case. Events can be introduced by noun phrases, as in the move and the increase, and some verbs fail to introduce events, as in the phrasal verb took effect, which refers to when the event began rather than to the event itself. Similarly, light verbs such as make, take, and have often fail to denote events; for light verbs the event is often expressed by the nominal direct object (took a flight), and these light verbs just provide a syntactic structure for the noun’s arguments. \nVarious versions of the event extraction task exist, depending on the goal. For example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract events and aspects like their aspectual and temporal properties. Events are to be classified as actions, states, reporting events (say, report, tell, explain), perception events, and so on. The aspect, tense, and modality of each event also needs to be extracted. Thus for example the various said events in the sample text would be annotated as (clas $mathrm {  { ~ sigma ~ } } _ { mathrm { { s } } } =$ REPORTING, tense $approxeq$ PAST, aspect=PERFECTIVE). \nEvent extraction is generally modeled via supervised learning, detecting events via sequence models with IOB tagging, and assigning event classes and attributes with multi-class classifiers. Feature-based models use surface information like parts of speech, lexical items, and verb tense information; see Fig. 17.17.",
        "chapter": "Information Extraction",
        "section": "Extracting Times",
        "subsection": "Temporal Normalization",
        "subsubsection": "N/A"
    },
    {
        "content": "17.4.1 Temporal Ordering of Events \nAllen relations \nWith both the events and the temporal expressions in a text having been detected, the next logical task is to use this information to fit the events into a complete timeline. Such a timeline would be useful for applications such as question answering and summarization. This ambitious task is the subject of considerable current research but is beyond the capabilities of current systems. \nA somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task similar to those described earlier in Section 17.1. The temporal relation between events is classified into one of the standard set of Allen relations shown in Fig. 17.18 (Allen, 1984), using featurebased classifiers as in Section 17.1, trained on the TimeBank corpus with features like words/embeddings, parse paths, tense and aspect. \nTimeBank \nThe TimeBank corpus consists of text annotated with much of the information we’ve been discussing throughout this section (Pustejovsky et al., 2003b). TimeBank 1.2 consists of 183 news articles selected from a variety of sources, including the Penn TreeBank and PropBank collections. \n<TIMEX3 tid=\"t57\" type=\"DATE\" value $ O =$ \"1989-10-26\" functionInDocument $ c =$ \"CREATION_TIME\"> 10/26/89 </TIMEX3>   \nDelta Air Lines earnings <EVENT eid=\"e1\" class $ c =$ \"OCCURRENCE\"> soared $< / tt E V E N T >$ $3 3 %$ to a record in <TIMEX3 tid=\"t58\" type $mathbf { Phi } = mathbf { dot { Phi } }$ \"DATE\" value $ c =$ \"1989-Q1\" anchorTimeID $vDash$ \"t57\"> the fiscal first quarter </TIMEX3>, <EVENT eid=\"e3\" class $ c =$ \"OCCURRENCE\">bucking</EVENT> the industry trend toward <EVENT eid=\"e4\" class $mathbf { Phi } = mathbf {  ' }$ \"OCCURRENCE\">declining</EVENT> profits. \nFigure 17.19 Example from the TimeBank corpus. \nEach article in the TimeBank corpus has had the temporal expressions and event mentions in them explicitly annotated in the TimeML annotation (Pustejovsky et al., 2003a). In addition to temporal expressions and events, the TimeML annotation provides temporal links between events and temporal expressions that specify the nature of the relation between them. Consider the following sample sentence and its corresponding markup shown in Fig. 17.19, selected from one of the TimeBank documents. \n(17.18) Delta Air Lines earnings soared $3 3 %$ to a record in the fiscal first quarter, bucking the industry trend toward declining profits. \nAs annotated, this text includes three events and two temporal expressions. The events are all in the occurrence class and are given unique identifiers for use in further annotations. The temporal expressions include the creation time of the article, which serves as the document time, and a single temporal expression within the text. \nIn addition to these annotations, TimeBank provides four links that capture the temporal relations between the events and times in the text, using the Allen relations from Fig. 17.18. The following are the within-sentence temporal relations annotated for this example. \n• $operatorname { S o a r i n g } _ { e 1 }$ is included in the fiscal first quartert58 • Soaringe1 is before $1 9 8 9 - 1 0 - 2 6 _ { t 5 7 }$ • $operatorname { S o a r i n g } _ { e 1 }$ is simultaneous with the buckinge3 • Declining $e 4$ includes soaringe1 \n17.5 Template Filling \nscripts \ntemplates \ntemplate filling \nMany texts contain reports of events, and possibly sequences of events, that often correspond to fairly common, stereotypical situations in the world. These abstract situations or stories, related to what have been called scripts (Schank and Abelson, 1977), consist of prototypical sequences of sub-events, participants, and their roles. The strong expectations provided by these scripts can facilitate the proper classification of entities, the assignment of entities into roles and relations, and most critically, the drawing of inferences that fill in things that have been left unsaid. In their simplest form, such scripts can be represented as templates consisting of fixed sets of slots that take as values slot-fillers belonging to particular classes. The task of template filling is to find documents that invoke particular scripts and then fill the slots in the associated templates with fillers extracted from the text. These slot-fillers may consist of text segments extracted directly from the text, or they may consist of concepts that have been inferred from text elements through some additional processing. \nA filled template from our original airline story might look like the following.",
        "chapter": "Information Extraction",
        "section": "Extracting Events and their Times",
        "subsection": "Temporal Ordering of Events",
        "subsubsection": "N/A"
    },
    {
        "content": "This template has four slots (LEAD AIRLINE, AMOUNT, EFFECTIVE DATE, FOLLOWER). The next section describes a standard sequence-labeling approach to filling slots. Section 17.5.2 then describes an older system based on the use of cascades of finite-state transducers and designed to address a more complex template-filling task that current learning-based systems don’t yet address. \ntemplate recognition \n17.5.1 Machine Learning Approaches to Template Filling \nIn the standard paradigm for template filling, we are given training documents with text spans annotated with predefined templates and their slot fillers. Our goal is to create one template for each event in the input, filling in the slots with text spans. \nThe task is generally modeled by training two separate supervised systems. The first system decides whether the template is present in a particular sentence. This task is called template recognition or sometimes, in a perhaps confusing bit of terminology, event recognition. Template recognition can be treated as a text classification task, with features extracted from every sequence of words that was labeled in training documents as filling any slot from the template being detected. The usual set of features can be used: tokens, embeddings, word shapes, part-of-speech tags, syntactic chunk tags, and named entity tags. \nrole-filler extraction \nThe second system has the job of role-filler extraction. A separate classifier is trained to detect each role (LEAD-AIRLINE, AMOUNT, and so on). This can be a binary classifier that is run on every noun-phrase in the parsed input sentence, or a sequence model run over sequences of words. Each role classifier is trained on the labeled data in the training set. Again, the usual set of features can be used, but now trained only on an individual noun phrase or the fillers of a single slot. \nMultiple non-identical text segments might be labeled with the same slot label. For example in our sample text, the strings United or United Airlines might be labeled as the LEAD AIRLINE. These are not incompatible choices and the coreference resolution techniques introduced in Chapter 22 can provide a path to a solution. \nA variety of annotated collections have been used to evaluate this style of approach to template filling, including sets of job announcements, conference calls for papers, restaurant guides, and biological texts. Recent work focuses on extracting templates in cases where there is no training data or even predefined templates, by inducing templates as sets of linked events (Chambers and Jurafsky, 2011). \n17.5.2 Earlier Finite-State Template-Filling Systems \nThe templates above are relatively simple. But consider the task of producing a template that contained all the information in a text like this one (Grishman and Sundheim, 1995): \nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local concern and a Japanese trading house to produce golf clubs to be shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capitalized at 20 million new Taiwan dollars, will start production in January 1990 with production of 20,000 iron and “metal wood” clubs a month.",
        "chapter": "Information Extraction",
        "section": "Template Filling",
        "subsection": "Machine Learning Approaches to Template Filling",
        "subsubsection": "N/A"
    },
    {
        "content": "This template has four slots (LEAD AIRLINE, AMOUNT, EFFECTIVE DATE, FOLLOWER). The next section describes a standard sequence-labeling approach to filling slots. Section 17.5.2 then describes an older system based on the use of cascades of finite-state transducers and designed to address a more complex template-filling task that current learning-based systems don’t yet address. \ntemplate recognition \n17.5.1 Machine Learning Approaches to Template Filling \nIn the standard paradigm for template filling, we are given training documents with text spans annotated with predefined templates and their slot fillers. Our goal is to create one template for each event in the input, filling in the slots with text spans. \nThe task is generally modeled by training two separate supervised systems. The first system decides whether the template is present in a particular sentence. This task is called template recognition or sometimes, in a perhaps confusing bit of terminology, event recognition. Template recognition can be treated as a text classification task, with features extracted from every sequence of words that was labeled in training documents as filling any slot from the template being detected. The usual set of features can be used: tokens, embeddings, word shapes, part-of-speech tags, syntactic chunk tags, and named entity tags. \nrole-filler extraction \nThe second system has the job of role-filler extraction. A separate classifier is trained to detect each role (LEAD-AIRLINE, AMOUNT, and so on). This can be a binary classifier that is run on every noun-phrase in the parsed input sentence, or a sequence model run over sequences of words. Each role classifier is trained on the labeled data in the training set. Again, the usual set of features can be used, but now trained only on an individual noun phrase or the fillers of a single slot. \nMultiple non-identical text segments might be labeled with the same slot label. For example in our sample text, the strings United or United Airlines might be labeled as the LEAD AIRLINE. These are not incompatible choices and the coreference resolution techniques introduced in Chapter 22 can provide a path to a solution. \nA variety of annotated collections have been used to evaluate this style of approach to template filling, including sets of job announcements, conference calls for papers, restaurant guides, and biological texts. Recent work focuses on extracting templates in cases where there is no training data or even predefined templates, by inducing templates as sets of linked events (Chambers and Jurafsky, 2011). \n17.5.2 Earlier Finite-State Template-Filling Systems \nThe templates above are relatively simple. But consider the task of producing a template that contained all the information in a text like this one (Grishman and Sundheim, 1995): \nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local concern and a Japanese trading house to produce golf clubs to be shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capitalized at 20 million new Taiwan dollars, will start production in January 1990 with production of 20,000 iron and “metal wood” clubs a month. \nThe MUC-5 ‘joint venture’ task (the Message Understanding Conferences were a series of U.S. government-organized information-extraction evaluations) was to produce hierarchically linked templates describing joint ventures. Figure 17.20 shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how the filler of the ACTIVITY slot of the TIE-UP template is itself a template with slots. \nEarly systems for dealing with these complex templates were based on cascades of transducers based on handwritten rules, as sketched in Fig. 17.21. \nThe first four stages use handwritten regular expression and grammar rules to do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and events with a FST-based recognizer and inserts the recognized objects into the appropriate slots in templates. This FST recognizer is based on hand-built regular expressions like the following (NG indicates Noun-Group and VG Verb-Group), which matches the first sentence of the news story above. \nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies) VG(Produce) NG(Product) \nThe result of processing these two sentences is the five draft templates (Fig. 17.22) that must then be merged into the single hierarchical structure shown in Fig. 17.20. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. \n17.6 Summary \nThis chapter has explored techniques for extracting limited forms of semantic content from texts. \n• Relations among entities can be extracted by pattern-based approaches, supervised learning methods when annotated training data is available, lightly",
        "chapter": "Information Extraction",
        "section": "Template Filling",
        "subsection": "Earlier Finite-State Template-Filling Systems",
        "subsubsection": "N/A"
    },
    {
        "content": "The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were a series of U.S. government-organized information-extraction evaluations) was to produce hierarchically linked templates describing joint ventures. Figure 17.20 shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how the filler of the ACTIVITY slot of the TIE-UP template is itself a template with slots. \nEarly systems for dealing with these complex templates were based on cascades of transducers based on handwritten rules, as sketched in Fig. 17.21. \nThe first four stages use handwritten regular expression and grammar rules to do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and events with a FST-based recognizer and inserts the recognized objects into the appropriate slots in templates. This FST recognizer is based on hand-built regular expressions like the following (NG indicates Noun-Group and VG Verb-Group), which matches the first sentence of the news story above. \nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies) VG(Produce) NG(Product) \nThe result of processing these two sentences is the five draft templates (Fig. 17.22) that must then be merged into the single hierarchical structure shown in Fig. 17.20. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. \n17.6 Summary \nThis chapter has explored techniques for extracting limited forms of semantic content from texts. \n• Relations among entities can be extracted by pattern-based approaches, supervised learning methods when annotated training data is available, lightly",
        "chapter": "Information Extraction",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were a series of U.S. government-organized information-extraction evaluations) was to produce hierarchically linked templates describing joint ventures. Figure 17.20 shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how the filler of the ACTIVITY slot of the TIE-UP template is itself a template with slots. \nEarly systems for dealing with these complex templates were based on cascades of transducers based on handwritten rules, as sketched in Fig. 17.21. \nThe first four stages use handwritten regular expression and grammar rules to do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and events with a FST-based recognizer and inserts the recognized objects into the appropriate slots in templates. This FST recognizer is based on hand-built regular expressions like the following (NG indicates Noun-Group and VG Verb-Group), which matches the first sentence of the news story above. \nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies) VG(Produce) NG(Product) \nThe result of processing these two sentences is the five draft templates (Fig. 17.22) that must then be merged into the single hierarchical structure shown in Fig. 17.20. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. \n17.6 Summary \nThis chapter has explored techniques for extracting limited forms of semantic content from texts. \n• Relations among entities can be extracted by pattern-based approaches, supervised learning methods when annotated training data is available, lightly",
        "chapter": "Information Extraction",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were a series of U.S. government-organized information-extraction evaluations) was to produce hierarchically linked templates describing joint ventures. Figure 17.20 shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how the filler of the ACTIVITY slot of the TIE-UP template is itself a template with slots. \nEarly systems for dealing with these complex templates were based on cascades of transducers based on handwritten rules, as sketched in Fig. 17.21. \nThe first four stages use handwritten regular expression and grammar rules to do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and events with a FST-based recognizer and inserts the recognized objects into the appropriate slots in templates. This FST recognizer is based on hand-built regular expressions like the following (NG indicates Noun-Group and VG Verb-Group), which matches the first sentence of the news story above. \nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies) VG(Produce) NG(Product) \nThe result of processing these two sentences is the five draft templates (Fig. 17.22) that must then be merged into the single hierarchical structure shown in Fig. 17.20. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. \n17.6 Summary \nThis chapter has explored techniques for extracting limited forms of semantic content from texts. \n• Relations among entities can be extracted by pattern-based approaches, supervised learning methods when annotated training data is available, lightly \nsupervised bootstrapping methods when small numbers of seed tuples or seed patterns are available, distant supervision when a database of relations is available, and unsupervised or Open IE methods.   \n• Reasoning about time can be facilitated by detection and normalization of temporal expressions through a combination of statistical learning and rulebased methods.   \n• Events can be detected and ordered in time using sequence models and classifiers trained on temporally- and event-labeled data like the TimeBank corpus.   \n• Template-filling applications can recognize stereotypical situations in texts and assign elements from the text to roles represented as fixed sets of slots. \nBibliographical and Historical Notes \nThe earliest work on information extraction addressed the template-filling task in the context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S. government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sundheim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert et al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite influential and inspired later systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the MUC evaluation techniques. \nDue to the difficulty of porting systems from one domain to another, attention shifted to machine learning approaches. Early supervised learning approaches to IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acquisition process, mainly for finite-state rule-based systems. Their success, and the earlier success of HMM-based speech recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. \nProgress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (‘slots’) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). \n\nSemisupervised relation extraction was first proposed by Hearst (1992b), and extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOWBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant supervision algorithm we describe was drawn from Mintz et al. (2009), who coined the term ‘distant supervision’, but similar ideas had occurred in earlier systems like Craven and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al., 2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines the advantages of distant supervision and Open IE. \nHeidelTime (Stro¨tgen and Gertz, 2013) and SUTime (Chang and Manning, 2012) are downloadable temporal extraction and normalization systems. The 2013 TempEval challenge is described in UzZaman et al. (2013); Chambers (2013) and Bethard (2013) give typical approaches. \nExercises \n17.1 Acronym expansion, the process of associating a phrase with an acronym, can be accomplished by a simple form of relational analysis. Develop a system based on the relation analysis approaches described in this chapter to populate a database of acronym expansions. If you focus on English Three Letter Acronyms (TLAs) you can evaluate your system’s performance by comparing it to Wikipedia’s TLA page. \n17.2 A useful functionality in newer email and calendar applications is the ability to associate temporal expressions connected with events in email (doctor’s appointments, meeting planning, party invitations, etc.) with specific calendar entries. Collect a corpus of email containing temporal expressions related to event planning. How do these expressions compare to the kinds of expressions commonly found in news text that we’ve been discussing in this chapter? \n17.3 Acquire the CMU seminar corpus and develop a template-filling system by using any of the techniques mentioned in Section 17.5. Analyze how well your system performs as compared with state-of-the-art results on this corpus. \n18 Word Senses and WordNet \nLady Bracknell. Are your parents living?   \nJack. I have lost both my parents.   \nLady Bracknell. To lose one parent, Mr. Worthing, may be regarded as a   \nmisfortune; to lose both looks like carelessness. Oscar Wilde, The Importance of Being Earnest \nambiguous \nWords are ambiguous: the same word can be used to mean different things. In Chapter 6 we saw that the word “mouse” has (at least) two meanings: (1) a small rodent, or (2) a hand-operated device to control a cursor. The word “bank” can mean: (1) a financial institution or (2) a sloping mound. In the quote above from his play The Importance of Being Earnest, Oscar Wilde plays with two meanings of “lose” (to misplace an object, and to suffer the death of a close person). \nword sense \nWordNet \nWe say that the words ‘mouse’ or ‘bank’ are polysemous (from Greek ‘having many senses’, poly- ‘many’ $^ +$ sema, ‘sign, mark’).1 A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. In this chapter we discuss word senses in more detail and introduce WordNet, a large online thesaurus —a database that represents word senses—with versions in many languages. WordNet also represents relations between senses. For example, there is an IS-A relation between dog and mammal (a dog is a kind of mammal) and a part-whole relation between engine and car (an engine is a part of a car). \nword sense disambiguation \nKnowing the relation between two senses can play an important role in tasks involving meaning. Consider the antonymy relation. Two words are antonyms if they have opposite meanings, like long and short, or up and down. Distinguishing these is quite important; if a user asks a dialogue agent to turn up the music, it would be unfortunate to instead turn it down. But in fact in embedding models like word2vec, antonyms are easily confused with each other, because often one of the closest words in embedding space to a word (e.g., up) is its antonym (e.g., down). Thesauruses that represent this relationship can help! \nWe also introduce word sense disambiguation (WSD), the task of determining which sense of a word is being used in a particular context. We’ll give supervised and unsupervised algorithms for deciding which sense was intended in a particular context. This task has a very long history in computational linguistics and many applications. In question answering, we can be more helpful to a user who asks about “bat care” if we know which sense of bat is relevant. (Is the user is a vampire? or just wants to play baseball.) And the different senses of a word often have different translations; in Spanish the animal bat is a murci´elago while the baseball bat is a bate, and indeed word sense algorithms may help improve MT (Pu et al., 2018). Finally, WSD has long been used as a tool for evaluating language processing models, and understanding how models represent different word senses is an important",
        "chapter": "Information Extraction",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "analytic direction. \n18.1 Word Senses \nword sense \nA sense (or word sense) is a discrete representation of one aspect of the meaning of a word. Loosely following lexicographic tradition, we represent each sense with a superscript: bank1 and bank2, mouse1 and mouse2. In context, it’s easy to see the different meanings: \nmouse1 : .... a mouse controlling a computer system in 1968. mouse2 : .... a quiet animal like a mouse bank1 : ...a bank can hold the investments in a custodial account ... bank2 : ...as agriculture burgeons on the east bank, the river ... \n18.1.1 Defining Word Senses \nHow can we define the meaning of a word sense? We introduced in Chapter 6 the standard computational approach of representing a word as an embedding, a point in semantic space. The intuition of embedding models like word2vec or GloVe is that the meaning of a word can be defined by its co-occurrences, the counts of words that often occur nearby. But that doesn’t tell us how to define the meaning of a word sense. As we saw in Chapter 11, contextual embeddings like BERT go further by offering an embedding that represents the meaning of a word in its textual context, and we’ll see that contextual embeddings lie at the heart of modern algorithms for word sense disambiguation. \ngloss \nBut first, we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. Here are the glosses for two senses of bank: \n1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water) \nGlosses are not a formal meaning representation; they are just written for people. Consider the following fragments from the definitions of right, left, red, and blood from the American Heritage Dictionary (Morris, 1985). \nright adj. located nearer the right hand esp. being on the right when facing the same direction as the observer. left adj. located nearer to this side of the body than the right. red $n$ . the color of blood or a ruby.   \nblood $n$ . the red liquid that circulates in the heart, arteries and veins of animals. \nNote the circularity in these definitions. The definition of right makes two direct references to itself, and the entry for left contains an implicit self-reference in the phrase this side of the body, which presumably means the left side. The entries for red and blood reference each other in their definitions. For humans, such entries are useful since the user of the dictionary has sufficient grasp of these other terms. \nYet despite their circularity and lack of formal representation, glosses can still be useful for computational modeling of senses. This is because a gloss is just a sentence, and from sentences we can compute sentence embeddings that tell us something about the meaning of the sense. Dictionaries often give example sentences along with glosses, and these can again be used to help build a sense representation. \nThe second way that thesauruses offer for defining a sense is—like the dictionary definitions—defining a sense through its relationship with other senses. For example, the above definitions make it clear that right and left are similar kinds of lemmas that stand in some kind of alternation, or opposition, to one another. Similarly, we can glean that red is a color and that blood is a liquid. Sense relations of this sort (IS-A, or antonymy) are explicitly listed in on-line databases like WordNet. Given a sufficiently large database of such relations, many applications are quite capable of performing sophisticated semantic tasks about word senses (even if they do not really know their right from their left). \n18.1.2 How many senses do words have? \nDictionaries and thesauruses give discrete lists of senses. By contrast, embeddings (whether static or contextual) offer a continuous high-dimensional model of meaning that doesn’t divide up into discrete senses. \nTherefore creating a thesaurus depends on criteria for deciding when the differing uses of a word should be represented with discrete senses. We might consider two senses discrete if they have independent truth conditions, different syntactic behavior, and independent sense relations, or if they exhibit antagonistic meanings. \nConsider the following uses of the verb serve from the WSJ corpus: \n(18.1) They rarely serve red meat, preferring to prepare seafood.   \n(18.2) He served as U.S. ambassador to Norway in 1976 and 1977.   \n(18.3) He might have served his time, come out and led an upstanding life. \nThe serve of serving red meat and that of serving time clearly have different truth conditions and presuppositions; the serve of serve as ambassador has the distinct subcategorization structure serve as NP. These heuristics suggest that these are probably three distinct senses of serve. One practical technique for determining if two senses are distinct is to conjoin two uses of a word in a single sentence; this kind of conjunction of antagonistic readings is called zeugma. Consider the following examples: \n(18.4) Which of those flights serve breakfast? (18.5) Does Air France serve Philadelphia? (18.6) ?Does Air France serve breakfast and Philadelphia? \nWe use (?) to mark those examples that are semantically ill-formed. The oddness of the invented third example (a case of zeugma) indicates there is no sensible way to make a single sense of serve work for both breakfast and Philadelphia. We can use this as evidence that serve has two different senses in this case. \nDictionaries tend to use many fine-grained senses so as to capture subtle meaning differences, a reasonable approach given that the traditional role of dictionaries is aiding word learners. For computational purposes, we often don’t need these fine distinctions, so we often group or cluster the senses; we have already done this for some of the examples in this chapter. Indeed, clustering examples into senses, or senses into broader-grained categories, is an important computational task that we’ll discuss in Section 18.7.",
        "chapter": "Word Senses and WordNet",
        "section": "Word Senses",
        "subsection": "Defining Word Senses",
        "subsubsection": "N/A"
    },
    {
        "content": "Yet despite their circularity and lack of formal representation, glosses can still be useful for computational modeling of senses. This is because a gloss is just a sentence, and from sentences we can compute sentence embeddings that tell us something about the meaning of the sense. Dictionaries often give example sentences along with glosses, and these can again be used to help build a sense representation. \nThe second way that thesauruses offer for defining a sense is—like the dictionary definitions—defining a sense through its relationship with other senses. For example, the above definitions make it clear that right and left are similar kinds of lemmas that stand in some kind of alternation, or opposition, to one another. Similarly, we can glean that red is a color and that blood is a liquid. Sense relations of this sort (IS-A, or antonymy) are explicitly listed in on-line databases like WordNet. Given a sufficiently large database of such relations, many applications are quite capable of performing sophisticated semantic tasks about word senses (even if they do not really know their right from their left). \n18.1.2 How many senses do words have? \nDictionaries and thesauruses give discrete lists of senses. By contrast, embeddings (whether static or contextual) offer a continuous high-dimensional model of meaning that doesn’t divide up into discrete senses. \nTherefore creating a thesaurus depends on criteria for deciding when the differing uses of a word should be represented with discrete senses. We might consider two senses discrete if they have independent truth conditions, different syntactic behavior, and independent sense relations, or if they exhibit antagonistic meanings. \nConsider the following uses of the verb serve from the WSJ corpus: \n(18.1) They rarely serve red meat, preferring to prepare seafood.   \n(18.2) He served as U.S. ambassador to Norway in 1976 and 1977.   \n(18.3) He might have served his time, come out and led an upstanding life. \nThe serve of serving red meat and that of serving time clearly have different truth conditions and presuppositions; the serve of serve as ambassador has the distinct subcategorization structure serve as NP. These heuristics suggest that these are probably three distinct senses of serve. One practical technique for determining if two senses are distinct is to conjoin two uses of a word in a single sentence; this kind of conjunction of antagonistic readings is called zeugma. Consider the following examples: \n(18.4) Which of those flights serve breakfast? (18.5) Does Air France serve Philadelphia? (18.6) ?Does Air France serve breakfast and Philadelphia? \nWe use (?) to mark those examples that are semantically ill-formed. The oddness of the invented third example (a case of zeugma) indicates there is no sensible way to make a single sense of serve work for both breakfast and Philadelphia. We can use this as evidence that serve has two different senses in this case. \nDictionaries tend to use many fine-grained senses so as to capture subtle meaning differences, a reasonable approach given that the traditional role of dictionaries is aiding word learners. For computational purposes, we often don’t need these fine distinctions, so we often group or cluster the senses; we have already done this for some of the examples in this chapter. Indeed, clustering examples into senses, or senses into broader-grained categories, is an important computational task that we’ll discuss in Section 18.7. \n18.2 Relations Between Senses \nThis section explores the relations between word senses, especially those that have received significant computational investigation like synonymy, antonymy, and hypernymy. \nsynonym \nSynonymy \nWe introduced in Chapter 6 the idea that when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms. Synonyms include such pairs as \ncouch/sofa vomit/throw up filbert/hazelnut car/automobile \nAnd we mentioned that in practice, the word synonym is commonly used to describe a relationship of approximate or rough synonymy. But furthermore, synonymy is actually a relationship between senses rather than words. Considering the words big and large. These may seem to be synonyms in the following sentences, since we could swap big and large in either sentence and retain the same meaning: \n(18.7) How big is that plane?   \n(18.8) Would I be flying on a large or small plane?   \nBut note the following sentence in which we cannot substitute large for big: (18.9) Miss Nelson, for instance, became a kind of big sister to Benjamin.   \n(18.10) ?Miss Nelson, for instance, became a kind of large sister to Benjamin. This is because the word big has a sense that means being older or grown up, while large lacks this sense. Thus, we say that some senses of big and large are (nearly) synonymous while other ones are not. \nantonym \nAntonymy \nWhereas synonyms are words with identical or similar meanings, antonyms are words with an opposite meaning, like: \nlong/short big/little fast/slow cold/hot dark/light rise/fall up/down in/out \nreversives \nTwo senses can be antonyms if they define a binary opposition or are at opposite ends of some scale. This is the case for long/short, fast/slow, or big/little, which are at opposite ends of the length or size scale. Another group of antonyms, reversives, describe change or movement in opposite directions, such as rise/fall or up/down. \nAntonyms thus differ completely with respect to one aspect of their meaning— their position on a scale or their direction—but are otherwise very similar, sharing almost all other aspects of meaning. Thus, automatically distinguishing synonyms from antonyms can be difficult. \nTaxonomic Relations \nhyponym \nhypernym \nAnother way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym",
        "chapter": "Word Senses and WordNet",
        "section": "Word Senses",
        "subsection": "How many senses do words have?",
        "subsubsection": "N/A"
    },
    {
        "content": "18.2 Relations Between Senses \nThis section explores the relations between word senses, especially those that have received significant computational investigation like synonymy, antonymy, and hypernymy. \nsynonym \nSynonymy \nWe introduced in Chapter 6 the idea that when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms. Synonyms include such pairs as \ncouch/sofa vomit/throw up filbert/hazelnut car/automobile \nAnd we mentioned that in practice, the word synonym is commonly used to describe a relationship of approximate or rough synonymy. But furthermore, synonymy is actually a relationship between senses rather than words. Considering the words big and large. These may seem to be synonyms in the following sentences, since we could swap big and large in either sentence and retain the same meaning: \n(18.7) How big is that plane?   \n(18.8) Would I be flying on a large or small plane?   \nBut note the following sentence in which we cannot substitute large for big: (18.9) Miss Nelson, for instance, became a kind of big sister to Benjamin.   \n(18.10) ?Miss Nelson, for instance, became a kind of large sister to Benjamin. This is because the word big has a sense that means being older or grown up, while large lacks this sense. Thus, we say that some senses of big and large are (nearly) synonymous while other ones are not. \nantonym \nAntonymy \nWhereas synonyms are words with identical or similar meanings, antonyms are words with an opposite meaning, like: \nlong/short big/little fast/slow cold/hot dark/light rise/fall up/down in/out \nreversives \nTwo senses can be antonyms if they define a binary opposition or are at opposite ends of some scale. This is the case for long/short, fast/slow, or big/little, which are at opposite ends of the length or size scale. Another group of antonyms, reversives, describe change or movement in opposite directions, such as rise/fall or up/down. \nAntonyms thus differ completely with respect to one aspect of their meaning— their position on a scale or their direction—but are otherwise very similar, sharing almost all other aspects of meaning. Thus, automatically distinguishing synonyms from antonyms can be difficult. \nTaxonomic Relations \nhyponym \nhypernym \nAnother way word senses can be related is taxonomically. A word (or sense) is a hyponym of another word or sense if the first is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym \nsuperordinate \nand hyponym) are very similar and hence easily confused; for this reason, the word superordinate is often used instead of hypernym. \nWe can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym. Thus, the class of animals includes as members all dogs, and the class of moving actions includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense $A$ is a hyponym of a sense $B$ if everything that is $A$ is also $B$ , and hence being an $A$ entails being a $B$ , or $forall x A ( x ) Rightarrow B ( x )$ . Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A. \nIS-A \nHypernymy is useful for tasks like textual entailment or question answering; knowing that leukemia is a type of cancer, for example, would certainly be useful in answering questions about leukemia. \npart-whole \nMeronymy \nAnother common relation is meronymy, the part-whole relation. A leg is part of a chair; a wheel is part of a car. We say that wheel is a meronym of car, and car is a holonym of wheel. \nStructured Polysemy \nstructured polysemy \nThe senses of a word can also be related semantically, in which case we call the relationship between them structured polysemy.Consider this sense bank: \n(18.11) The bank is on the corner of Nassau and Witherspoon. \nThis sense, perhaps bank4, means something like “the building belonging to a financial institution”. These two kinds of senses (an organization and the building associated with an organization ) occur together for many other words as well (school, university, hospital, etc.). Thus, there is a systematic relationship between senses that we might represent as \nmetonymy \nThis particular subtype of polysemy relation is called metonymy. Metonymy is the use of one aspect of a concept or entity to refer to other aspects of the entity or to the entity itself. We are performing metonymy when we use the phrase the White House to refer to the administration whose office is in the White House. Other common examples of metonymy include the relation between the following pairings of senses:",
        "chapter": "Word Senses and WordNet",
        "section": "Relations Between Senses",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "supersense \nas 15 for verbs (plus 2 for adjectives and 1 for adverbs). These categories are often called supersenses, because they act as coarse semantic categories or groupings of senses which can be useful when word senses are too fine-grained (Ciaramita and Johnson 2003, Ciaramita and Altun 2006). Supersenses have also been defined for adjectives (Tsvetkov et al., 2014) and prepositions (Schneider et al., 2018). \n18.3.1 Sense Relations in WordNet \nWordNet represents all the kinds of sense relations discussed in the previous section, as illustrated in Fig. 18.3 and Fig. 18.4. \nFor example WordNet represents hyponymy (page 389) by relating each synset to its immediately more general and more specific synsets through direct hypernym and hyponym relations. These relations can be followed to produce longer chains of more general or more specific synsets. Figure 18.5 shows hypernym chains for bass3 and bass7; more general synsets are shown on successively indented lines. \nWordNet has two kinds of taxonomic entities: classes and instances. An instance is an individual, a proper noun that is a unique entity. San Francisco is an instance of city, for example. But city is a class, a hyponym of municipality and eventually \nbass3, basso (an adult male singer with the lowest voice)   \n$Rightarrow$ singer, vocalist, vocalizer, vocaliser $Rightarrow$ musician, instrumentalist, player $Rightarrow$ performer, performing artist $Rightarrow$ entertainer $Rightarrow$ person, individual, someone. $Rightarrow$ organism, being $Rightarrow$ living thing, animate thing, $Rightarrow$ whole, unit $Rightarrow$ object, physical object $Rightarrow$ physical entity $Rightarrow$ entity   \nbass7 (member with the lowest range of a family of instruments)   \n$Rightarrow$ musical instrument, instrument $Rightarrow$ device $Rightarrow$ instrumentality, instrumentation $Rightarrow$ artifact, artefact $Rightarrow$ whole, unit $Rightarrow$ object, physical object $Rightarrow$ physical entity $Rightarrow$ entity \nof location. Fig. 18.6 shows a subgraph of WordNet demonstrating many of the relations. \n{wheeledvhicle] -has-part-→{brake} antony fasty has- ? is-& 3 a -Part- {wheel} {slow) antonyn 1 art {wagon, {self-propelled vehicle} {splasher} trib. {quick, speedy} waggon} 7 个 1 {speed,swiftness,fastness} 8 L a → 5 {locomotive,engine, 1 {motor vehicle) {tractor} locomotive engine, railway locomotive} 福   \n{golf cart, {car,auto,automobile, - bas-part →{car window}   \ngolfcart} machine,motorcar} T {accelerate,speed,speed up} has Part{convertible} √ {accelerator, {air bag} acceleratorpedal, gas pedal, throttle}",
        "chapter": "Word Senses and WordNet",
        "section": "WordNet: A Database of Lexical Relations",
        "subsection": "Sense Relations in WordNet",
        "subsubsection": "N/A"
    },
    {
        "content": "18.4 Word Sense Disambiguation \nword sense disambiguation WSD \nThe task of selecting the correct sense for a word is called word sense disambiguation, or WSD. WSD algorithms take as input a word in context and a fixed inventory of potential word senses and outputs the correct word sense in context. \n18.4.1 WSD: The Task and Datasets \nIn this section we introduce the task setup for WSD, and then turn to algorithms. The inventory of sense tags depends on the task. For sense tagging in the context of translation from English to Spanish, the sense tag inventory for an English word might be the set of different Spanish translations. For automatic indexing of medical articles, the sense-tag inventory might be the set of MeSH (Medical Subject Headings) thesaurus entries. Or we can use the set of senses from a resource like WordNet, or supersenses if we want a coarser-grain set. Figure 18.4.1 shows some such examples for the word bass. \nlexical sample \nIn some situations, we just need to disambiguate a small number of words. In such lexical sample tasks, we have a small pre-selected set of target words and an inventory of senses for each word from some lexicon. Since the set of words and the set of senses are small, simple supervised classification approaches work very well. \nall-words \nMore commonly, however, we have a harder problem in which we have to disambiguate all the words in some text. In this all-words task, the system is given an entire texts and a lexicon with an inventory of senses for each entry and we have to disambiguate every word in the text (or sometimes just every content word). The all-words task is similar to part-of-speech tagging, except with a much larger set of tags since each lemma has its own set. A consequence of this larger set of tags is data sparseness. \nsemantic concordance \nSupervised all-word disambiguation tasks are generally trained from a semantic concordance, a corpus in which each open-class word in each sentence is labeled with its word sense from a specific dictionary or thesaurus, most often WordNet. The SemCor corpus is a subset of the Brown Corpus consisting of over 226,036 words that were manually tagged with WordNet senses (Miller et al. 1993, Landes et al. 1998). Other sense-tagged corpora have been built for the SENSEVAL and SemEval WSD tasks, such as the SENSEVAL-3 Task 1 English all-words test data with 2282 annotations (Snyder and Palmer, 2004) or the SemEval-13 Task 12 datasets. Large semantic concordances are also available in other languages including Dutch (Vossen et al., 2011) and German (Henrich et al., 2012). \nHere’s an example from the SemCor corpus showing the WordNet sense numbers of the tagged words; we’ve used the standard WSD notation in which a subscript marks the part of speech (Navigli, 2009): (18.12) You will findv9 that avocado1n isv1 unlike $vdots _ { j } ^ { 1 }$ othe $mathbf { chi } _ { j } ^ { 1 }$ frui $mathfrak { t } _ { n } ^ { 1 }$ you have everr1 tastedv2 Given each noun, verb, adjective, or adverb word in the hand-labeled test set (say fruit), the SemCor-based WSD task is to choose the correct sense from the possible senses in WordNet. For fruit this would mean choosing between the correct answer fruit1n (the ripened reproductive body of a seed plant), and the other two senses fruit2n (yield; an amount of a product) and frui $mathfrak { t } _ { n } ^ { 3 }$ (the consequence of some effort or action). Fig. 18.8 sketches the task. \n\nWSD systems are typically evaluated intrinsically, by computing F1 against hand-labeled sense tags in a held-out set, such as the SemCor corpus or SemEval corpora discussed above. \nA surprisingly strong baseline is simply to choose the most frequent sense for each word from the senses in a labeled corpus (Gale et al., 1992a). For WordNet, this corresponds to the first sense, since senses in WordNet are generally ordered from most frequent to least frequent based on their counts in the SemCor sense-tagged corpus. The most frequent sense baseline can be quite accurate, and is therefore often used as a default, to supply a word sense when a supervised algorithm has insufficient training data. \nA second heuristic, called one sense per discourse is based on the work of Gale et al. (1992b), who noticed that a word appearing multiple times in a text or discourse often appears with the same sense. This heuristic seems to hold better for coarse-grained senses and particularly when a word’s senses are unrelated, so isn’t generally used as a baseline. Nonetheless various kinds of disambiguation tasks often include some such bias toward resolving an ambiguity the same way inside a discourse segment. \n18.4.2 The WSD Algorithm: Contextual Embeddings \nThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithm using contextual word embeddings, due to Melamud et al. (2016) and Peters et al. (2018). At training time we pass each sentence in the SemCore labeled dataset through any contextual embedding (e.g., BERT) resulting in a contextual embedding for each labeled token in SemCore. (There are various ways to compute this contextual embedding $nu _ { i }$ for a token $i$ ; for BERT it is common to pool multiple layers by summing the vector representations of $i$ from the last four BERT layers). Then for each sense $s$ of any word in the corpus, for each of the $n$ tokens of that sense, we average their $n$ contextual representations $nu _ { i }$ to produce a contextual sense embedding ${ pmb v } _ { s }$ for $s$ :",
        "chapter": "Word Senses and WordNet",
        "section": "Word Sense Disambiguation",
        "subsection": "WSD: The Task and Datasets",
        "subsubsection": "N/A"
    },
    {
        "content": "WSD systems are typically evaluated intrinsically, by computing F1 against hand-labeled sense tags in a held-out set, such as the SemCor corpus or SemEval corpora discussed above. \nA surprisingly strong baseline is simply to choose the most frequent sense for each word from the senses in a labeled corpus (Gale et al., 1992a). For WordNet, this corresponds to the first sense, since senses in WordNet are generally ordered from most frequent to least frequent based on their counts in the SemCor sense-tagged corpus. The most frequent sense baseline can be quite accurate, and is therefore often used as a default, to supply a word sense when a supervised algorithm has insufficient training data. \nA second heuristic, called one sense per discourse is based on the work of Gale et al. (1992b), who noticed that a word appearing multiple times in a text or discourse often appears with the same sense. This heuristic seems to hold better for coarse-grained senses and particularly when a word’s senses are unrelated, so isn’t generally used as a baseline. Nonetheless various kinds of disambiguation tasks often include some such bias toward resolving an ambiguity the same way inside a discourse segment. \n18.4.2 The WSD Algorithm: Contextual Embeddings \nThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithm using contextual word embeddings, due to Melamud et al. (2016) and Peters et al. (2018). At training time we pass each sentence in the SemCore labeled dataset through any contextual embedding (e.g., BERT) resulting in a contextual embedding for each labeled token in SemCore. (There are various ways to compute this contextual embedding $nu _ { i }$ for a token $i$ ; for BERT it is common to pool multiple layers by summing the vector representations of $i$ from the last four BERT layers). Then for each sense $s$ of any word in the corpus, for each of the $n$ tokens of that sense, we average their $n$ contextual representations $nu _ { i }$ to produce a contextual sense embedding ${ pmb v } _ { s }$ for $s$ : \n\nAt test time, given a token of a target word $t$ in context, we compute its contextual embedding $mathbf { t }$ and choose its nearest neighbor sense from the training set, i.e., the sense whose sense embedding has the highest cosine with t: \nWhat do we do for words we haven’t seen in the sense-labeled training data? After all, the number of senses that appear in SemCor is only a small fraction of the words in WordNet. The simplest algorithm is to fall back to the Most Frequent Sense baseline, i.e. taking the first sense in WordNet. But that’s not very satisfactory. \nA more powerful approach, due to Loureiro and Jorge (2019), is to impute the missing sense embeddings, bottom-up, by using the WordNet taxonomy and supersenses. We get a sense embedding for any higher-level node in the WordNet taxonomy by averaging the embeddings of its children, thus computing the embedding for each synset as the average of its sense embeddings, the embedding for a hypernym as the average of its synset embeddings, and the lexicographic category (supersense) embedding as the average of the large set of synset embeddings with that category. More formally, for each missing sense in WordNet ${ hat { s } } in W$ , let the sense embeddings for the other members of its synset be $S _ { hat { s } }$ , the hypernym-specific synset embeddings be $H _ { hat { s } }$ , and the lexicographic (supersense-specific) synset embeddings be $L _ { hat { s } }$ . We can then compute the sense embedding for $hat { s }$ as follows: \nSince all of the supersenses have some labeled data in SemCor, the algorithm is guaranteed to have some representation for all possible senses by the time the algorithm backs off to the most general (supersense) information, although of course with a very coarse model. \n18.5 Alternate WSD algorithms and Tasks \ncollocation \n18.5.1 Feature-Based WSD \nFeature-based algorithms for WSD are extremely simple and function almost as well as contextual language model algorithms. The best performing IMS algorithm (Zhong and Ng, 2010), augmented by embeddings (Iacobacci et al. 2016, Raganato et al. 2017b), uses an SVM classifier to choose the sense for each input word with the following simple features of the surrounding words: \n• part-of-speech tags (for a window of 3 words on each side, stopping at sentence boundaries)   \n• collocation features of words or n-grams of lengths 1, 2, 3 at a particular location in a window of 3 words on each side (i.e., exactly one word to the right, or the two words starting 3 words to the left, and so on).   \n• weighted average of embeddings (of all words in a window of 10 words on each side, weighted exponentially by distance) \nConsider the ambiguous word bass in the following WSJ sentence: \n(18.18) An electric guitar and bass player stand off to one side, \nIf we used a small 2-word window, a standard feature vector might include parts-ofspeech, unigram and bigram collocation features, and a weighted sum $g$ of embeddings, that is: \nwould yield the following vector: \n[guitar, NN, and, CC, player, NN, stand, VB, guitar and, player stand, g(E(guitar),E(and),E(player),E(stand))] \nknowledgebased \n18.5.2 The Lesk Algorithm as WSD Baseline \nGenerating sense labeled corpora like SemCor is quite difficult and expensive. An alternative class of WSD algorithms, knowledge-based algorithms, rely solely on WordNet or other such resources and don’t require labeled data. While supervised algorithms generally work better, knowledge-based methods can be used in languages or domains where thesauruses or dictionaries but not sense labeled corpora are available. \nLesk algorithm \nThe Lesk algorithm is the oldest and most powerful knowledge-based WSD method, and is a useful baseline. Lesk is really a family of algorithms that choose the sense whose dictionary gloss or definition shares the most words with the target word’s neighborhood. Figure 18.10 shows the simplest version of the algorithm, often called the Simplified Lesk algorithm (Kilgarriff and Rosenzweig, 2000). \nSimplified Lesk",
        "chapter": "Word Senses and WordNet",
        "section": "Word Sense Disambiguation",
        "subsection": "The WSD Algorithm: Contextual Embeddings",
        "subsubsection": "N/A"
    },
    {
        "content": "Since all of the supersenses have some labeled data in SemCor, the algorithm is guaranteed to have some representation for all possible senses by the time the algorithm backs off to the most general (supersense) information, although of course with a very coarse model. \n18.5 Alternate WSD algorithms and Tasks \ncollocation \n18.5.1 Feature-Based WSD \nFeature-based algorithms for WSD are extremely simple and function almost as well as contextual language model algorithms. The best performing IMS algorithm (Zhong and Ng, 2010), augmented by embeddings (Iacobacci et al. 2016, Raganato et al. 2017b), uses an SVM classifier to choose the sense for each input word with the following simple features of the surrounding words: \n• part-of-speech tags (for a window of 3 words on each side, stopping at sentence boundaries)   \n• collocation features of words or n-grams of lengths 1, 2, 3 at a particular location in a window of 3 words on each side (i.e., exactly one word to the right, or the two words starting 3 words to the left, and so on).   \n• weighted average of embeddings (of all words in a window of 10 words on each side, weighted exponentially by distance) \nConsider the ambiguous word bass in the following WSJ sentence: \n(18.18) An electric guitar and bass player stand off to one side, \nIf we used a small 2-word window, a standard feature vector might include parts-ofspeech, unigram and bigram collocation features, and a weighted sum $g$ of embeddings, that is: \nwould yield the following vector: \n[guitar, NN, and, CC, player, NN, stand, VB, guitar and, player stand, g(E(guitar),E(and),E(player),E(stand))] \nknowledgebased \n18.5.2 The Lesk Algorithm as WSD Baseline \nGenerating sense labeled corpora like SemCor is quite difficult and expensive. An alternative class of WSD algorithms, knowledge-based algorithms, rely solely on WordNet or other such resources and don’t require labeled data. While supervised algorithms generally work better, knowledge-based methods can be used in languages or domains where thesauruses or dictionaries but not sense labeled corpora are available. \nLesk algorithm \nThe Lesk algorithm is the oldest and most powerful knowledge-based WSD method, and is a useful baseline. Lesk is really a family of algorithms that choose the sense whose dictionary gloss or definition shares the most words with the target word’s neighborhood. Figure 18.10 shows the simplest version of the algorithm, often called the Simplified Lesk algorithm (Kilgarriff and Rosenzweig, 2000). \nSimplified Lesk",
        "chapter": "Word Senses and WordNet",
        "section": "Alternate WSD algorithms and Tasks",
        "subsection": "Feature-Based WSD",
        "subsubsection": "N/A"
    },
    {
        "content": "Since all of the supersenses have some labeled data in SemCor, the algorithm is guaranteed to have some representation for all possible senses by the time the algorithm backs off to the most general (supersense) information, although of course with a very coarse model. \n18.5 Alternate WSD algorithms and Tasks \ncollocation \n18.5.1 Feature-Based WSD \nFeature-based algorithms for WSD are extremely simple and function almost as well as contextual language model algorithms. The best performing IMS algorithm (Zhong and Ng, 2010), augmented by embeddings (Iacobacci et al. 2016, Raganato et al. 2017b), uses an SVM classifier to choose the sense for each input word with the following simple features of the surrounding words: \n• part-of-speech tags (for a window of 3 words on each side, stopping at sentence boundaries)   \n• collocation features of words or n-grams of lengths 1, 2, 3 at a particular location in a window of 3 words on each side (i.e., exactly one word to the right, or the two words starting 3 words to the left, and so on).   \n• weighted average of embeddings (of all words in a window of 10 words on each side, weighted exponentially by distance) \nConsider the ambiguous word bass in the following WSJ sentence: \n(18.18) An electric guitar and bass player stand off to one side, \nIf we used a small 2-word window, a standard feature vector might include parts-ofspeech, unigram and bigram collocation features, and a weighted sum $g$ of embeddings, that is: \nwould yield the following vector: \n[guitar, NN, and, CC, player, NN, stand, VB, guitar and, player stand, g(E(guitar),E(and),E(player),E(stand))] \nknowledgebased \n18.5.2 The Lesk Algorithm as WSD Baseline \nGenerating sense labeled corpora like SemCor is quite difficult and expensive. An alternative class of WSD algorithms, knowledge-based algorithms, rely solely on WordNet or other such resources and don’t require labeled data. While supervised algorithms generally work better, knowledge-based methods can be used in languages or domains where thesauruses or dictionaries but not sense labeled corpora are available. \nLesk algorithm \nThe Lesk algorithm is the oldest and most powerful knowledge-based WSD method, and is a useful baseline. Lesk is really a family of algorithms that choose the sense whose dictionary gloss or definition shares the most words with the target word’s neighborhood. Figure 18.10 shows the simplest version of the algorithm, often called the Simplified Lesk algorithm (Kilgarriff and Rosenzweig, 2000). \nSimplified Lesk \nFigure 18.10 The Simplified Lesk algorithm. The COMPUTEOVERLAP function returns the number of words in common between two sets, ignoring function words or other words on a stop list. The original Lesk algorithm defines the context in a more complex way. \nAs an example of the Lesk algorithm at work, consider disambiguating the word bank in the following context: \n(18.20) The bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities. \ngiven the following two WordNet senses: \nSense bank1 has two non-stopwords overlapping with the context in (18.20): deposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen. \nThere are many obvious extensions to Simplified Lesk, such as weighing the overlapping words by IDF (inverse document frequency) Chapter 6 to downweight frequent words like function words; best performing is to use word embedding cosine instead of word overlap to compute the similarity between the definition and the context (Basile et al., 2014). Modern neural extensions of Lesk use the definitions to compute sense embeddings that can be directly used instead of SemCor-training embeddings (Kumar et al. 2019, Luo et al. 2018a, Luo et al. 2018b). \n18.5.3 Word-in-Context Evaluation \nWord Sense Disambiguation is a much more fine-grained evaluation of word meaning than the context-free word similarity tasks we described in Chapter 6. Recall that tasks like LexSim-999 require systems to match human judgments on the contextfree similarity between two words (how similar is cup to mug?). We can think of WSD as a kind of contextualized similarity task, since our goal is to be able to distinguish the meaning of a word like bass in one context (playing music) from another context (fishing). \nSomewhere in between lies the word-in-context task. Here the system is given two sentences, each with the same target word but in a different sentential context. The system must decide whether the target words are used in the same sense in the two sentences or in a different sense. Fig. 18.11 shows sample pairs from the WiC dataset of Pilehvar and Camacho-Collados (2019).",
        "chapter": "Word Senses and WordNet",
        "section": "Alternate WSD algorithms and Tasks",
        "subsection": "The Lesk Algorithm as WSD Baseline",
        "subsubsection": "N/A"
    },
    {
        "content": "Figure 18.10 The Simplified Lesk algorithm. The COMPUTEOVERLAP function returns the number of words in common between two sets, ignoring function words or other words on a stop list. The original Lesk algorithm defines the context in a more complex way. \nAs an example of the Lesk algorithm at work, consider disambiguating the word bank in the following context: \n(18.20) The bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities. \ngiven the following two WordNet senses: \nSense bank1 has two non-stopwords overlapping with the context in (18.20): deposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen. \nThere are many obvious extensions to Simplified Lesk, such as weighing the overlapping words by IDF (inverse document frequency) Chapter 6 to downweight frequent words like function words; best performing is to use word embedding cosine instead of word overlap to compute the similarity between the definition and the context (Basile et al., 2014). Modern neural extensions of Lesk use the definitions to compute sense embeddings that can be directly used instead of SemCor-training embeddings (Kumar et al. 2019, Luo et al. 2018a, Luo et al. 2018b). \n18.5.3 Word-in-Context Evaluation \nWord Sense Disambiguation is a much more fine-grained evaluation of word meaning than the context-free word similarity tasks we described in Chapter 6. Recall that tasks like LexSim-999 require systems to match human judgments on the contextfree similarity between two words (how similar is cup to mug?). We can think of WSD as a kind of contextualized similarity task, since our goal is to be able to distinguish the meaning of a word like bass in one context (playing music) from another context (fishing). \nSomewhere in between lies the word-in-context task. Here the system is given two sentences, each with the same target word but in a different sentential context. The system must decide whether the target words are used in the same sense in the two sentences or in a different sense. Fig. 18.11 shows sample pairs from the WiC dataset of Pilehvar and Camacho-Collados (2019). \n\nThe WiC sentences are mainly taken from the example usages for senses in WordNet. But WordNet senses are very fine-grained. For this reason tasks like word-in-context first cluster the word senses into coarser clusters, so that the two sentential contexts for the target word are marked as T if the two senses are in the same cluster. WiC clusters all pairs of senses if they are first degree connections in the WordNet semantic graph, including sister senses, or if they belong to the same supersense; we point to other sense clustering algorithms at the end of the chapter. \nThe baseline algorithm to solve the WiC task uses contextual embeddings like BERT with a simple thresholded cosine. We first compute the contextual embeddings for the target word in each of the two sentences, and then compute the cosine between them. If it’s above a threshold tuned on a devset we respond true (the two senses are the same) else we respond false. \n18.5.4 Wikipedia as a source of training data \nDatasets other than SemCor have been used for all-words WSD. One important direction is to use Wikipedia as a source of sense-labeled data. When a concept is mentioned in a Wikipedia article, the article text may contain an explicit link to the concept’s Wikipedia page, which is named by a unique identifier. This link can be used as a sense annotation. For example, the ambiguous word bar is linked to a different Wikipedia article depending on its meaning in context, including the page BAR (LAW), the page BAR (MUSIC), and so on, as in the following Wikipedia examples (Mihalcea, 2007). \nIn 1834, Sumner was admitted to the [[bar (law) bar]] at the age of twenty-three, and entered private practice in Boston. \nIt is danced in $3 / 4$ time (like most waltzes), with the couple turning approx. 180 degrees every [[bar (music)|bar]]. \nJenga is a popular beer in the [[bar (establishment) bar]]s of Thailand. \nThese sentences can then be added to the training data for a supervised system. In order to use Wikipedia in this way, however, it is necessary to map from Wikipedia concepts to whatever inventory of senses is relevant for the WSD application. Automatic algorithms that map from Wikipedia to WordNet, for example, involve finding the WordNet sense that has the greatest lexical overlap with the Wikipedia sense, by comparing the vector of words in the WordNet synset, gloss, and related senses with the vector of words in the Wikipedia page title, outgoing links, and page category (Ponzetto and Navigli, 2010). The resulting mapping has been used to create BabelNet, a large sense-annotated resource (Navigli and Ponzetto, 2012).",
        "chapter": "Word Senses and WordNet",
        "section": "Alternate WSD algorithms and Tasks",
        "subsection": "Word-in-Context Evaluation",
        "subsubsection": "N/A"
    },
    {
        "content": "The WiC sentences are mainly taken from the example usages for senses in WordNet. But WordNet senses are very fine-grained. For this reason tasks like word-in-context first cluster the word senses into coarser clusters, so that the two sentential contexts for the target word are marked as T if the two senses are in the same cluster. WiC clusters all pairs of senses if they are first degree connections in the WordNet semantic graph, including sister senses, or if they belong to the same supersense; we point to other sense clustering algorithms at the end of the chapter. \nThe baseline algorithm to solve the WiC task uses contextual embeddings like BERT with a simple thresholded cosine. We first compute the contextual embeddings for the target word in each of the two sentences, and then compute the cosine between them. If it’s above a threshold tuned on a devset we respond true (the two senses are the same) else we respond false. \n18.5.4 Wikipedia as a source of training data \nDatasets other than SemCor have been used for all-words WSD. One important direction is to use Wikipedia as a source of sense-labeled data. When a concept is mentioned in a Wikipedia article, the article text may contain an explicit link to the concept’s Wikipedia page, which is named by a unique identifier. This link can be used as a sense annotation. For example, the ambiguous word bar is linked to a different Wikipedia article depending on its meaning in context, including the page BAR (LAW), the page BAR (MUSIC), and so on, as in the following Wikipedia examples (Mihalcea, 2007). \nIn 1834, Sumner was admitted to the [[bar (law) bar]] at the age of twenty-three, and entered private practice in Boston. \nIt is danced in $3 / 4$ time (like most waltzes), with the couple turning approx. 180 degrees every [[bar (music)|bar]]. \nJenga is a popular beer in the [[bar (establishment) bar]]s of Thailand. \nThese sentences can then be added to the training data for a supervised system. In order to use Wikipedia in this way, however, it is necessary to map from Wikipedia concepts to whatever inventory of senses is relevant for the WSD application. Automatic algorithms that map from Wikipedia to WordNet, for example, involve finding the WordNet sense that has the greatest lexical overlap with the Wikipedia sense, by comparing the vector of words in the WordNet synset, gloss, and related senses with the vector of words in the Wikipedia page title, outgoing links, and page category (Ponzetto and Navigli, 2010). The resulting mapping has been used to create BabelNet, a large sense-annotated resource (Navigli and Ponzetto, 2012). \n\n18.6 Using Thesauruses to Improve Embeddings \nThesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms. A word like expensive is often very similar in embedding cosine to its antonym like cheap. Antonymy information from thesauruses can help solve this problem; Fig. 18.12 shows nearest neighbors to some target words in GloVe, and the improvement after one such method. \nretrofitting \nThere are two families of solutions. The first requires retraining: we modify the embedding training to incorporate thesaurus relations like synonymy, antonym, or supersenses. This can be done by modifying the static embedding loss function for word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual embedding training (Levine et al. 2020, Lauscher et al. 2019). \nThe second, for static embeddings, is more light-weight; after the embeddings have been trained we learn a second mapping based on a thesaurus that shifts the embeddings of words in such a way that synonyms (according to the thesaurus) are pushed closer and antonyms further apart. Such methods are called retrofitting (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (Mrksˇi´c et al., 2016). \n18.7 Word Sense Induction \nword sense induction \nIt is expensive and difficult to build large corpora in which each word is labeled for its word sense. For this reason, an unsupervised approach to sense disambiguation, often called word sense induction or WSI, is an important direction. In unsupervised approaches, we don’t use human-defined word senses. Instead, the set of “senses” of each word is created automatically from the instances of each word in the training set. \nMost algorithms for word sense induction follow the early work of Schu¨tze (Schu¨tze 1992b, Schu¨tze 1998) in using some sort of clustering over word embeddings. In training, we use three steps: \n1. For each token $w _ { i }$ of word $w$ in a corpus, compute a context vector c. 2. Use a clustering algorithm to cluster these word-token context vectors c into a predefined number of groups or clusters. Each cluster defines a sense of $w$ . 3. Compute the vector centroid of each cluster. Each vector centroid $mathbf { s _ { j } }$ is a sense vector representing that sense of $w$ .",
        "chapter": "Word Senses and WordNet",
        "section": "Alternate WSD algorithms and Tasks",
        "subsection": "Wikipedia as a source of training data",
        "subsubsection": "N/A"
    },
    {
        "content": "18.6 Using Thesauruses to Improve Embeddings \nThesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms. A word like expensive is often very similar in embedding cosine to its antonym like cheap. Antonymy information from thesauruses can help solve this problem; Fig. 18.12 shows nearest neighbors to some target words in GloVe, and the improvement after one such method. \nretrofitting \nThere are two families of solutions. The first requires retraining: we modify the embedding training to incorporate thesaurus relations like synonymy, antonym, or supersenses. This can be done by modifying the static embedding loss function for word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual embedding training (Levine et al. 2020, Lauscher et al. 2019). \nThe second, for static embeddings, is more light-weight; after the embeddings have been trained we learn a second mapping based on a thesaurus that shifts the embeddings of words in such a way that synonyms (according to the thesaurus) are pushed closer and antonyms further apart. Such methods are called retrofitting (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (Mrksˇi´c et al., 2016). \n18.7 Word Sense Induction \nword sense induction \nIt is expensive and difficult to build large corpora in which each word is labeled for its word sense. For this reason, an unsupervised approach to sense disambiguation, often called word sense induction or WSI, is an important direction. In unsupervised approaches, we don’t use human-defined word senses. Instead, the set of “senses” of each word is created automatically from the instances of each word in the training set. \nMost algorithms for word sense induction follow the early work of Schu¨tze (Schu¨tze 1992b, Schu¨tze 1998) in using some sort of clustering over word embeddings. In training, we use three steps: \n1. For each token $w _ { i }$ of word $w$ in a corpus, compute a context vector c. 2. Use a clustering algorithm to cluster these word-token context vectors c into a predefined number of groups or clusters. Each cluster defines a sense of $w$ . 3. Compute the vector centroid of each cluster. Each vector centroid $mathbf { s _ { j } }$ is a sense vector representing that sense of $w$ .",
        "chapter": "Word Senses and WordNet",
        "section": "Using Thesauruses to Improve Embeddings",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "18.6 Using Thesauruses to Improve Embeddings \nThesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms. A word like expensive is often very similar in embedding cosine to its antonym like cheap. Antonymy information from thesauruses can help solve this problem; Fig. 18.12 shows nearest neighbors to some target words in GloVe, and the improvement after one such method. \nretrofitting \nThere are two families of solutions. The first requires retraining: we modify the embedding training to incorporate thesaurus relations like synonymy, antonym, or supersenses. This can be done by modifying the static embedding loss function for word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual embedding training (Levine et al. 2020, Lauscher et al. 2019). \nThe second, for static embeddings, is more light-weight; after the embeddings have been trained we learn a second mapping based on a thesaurus that shifts the embeddings of words in such a way that synonyms (according to the thesaurus) are pushed closer and antonyms further apart. Such methods are called retrofitting (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (Mrksˇi´c et al., 2016). \n18.7 Word Sense Induction \nword sense induction \nIt is expensive and difficult to build large corpora in which each word is labeled for its word sense. For this reason, an unsupervised approach to sense disambiguation, often called word sense induction or WSI, is an important direction. In unsupervised approaches, we don’t use human-defined word senses. Instead, the set of “senses” of each word is created automatically from the instances of each word in the training set. \nMost algorithms for word sense induction follow the early work of Schu¨tze (Schu¨tze 1992b, Schu¨tze 1998) in using some sort of clustering over word embeddings. In training, we use three steps: \n1. For each token $w _ { i }$ of word $w$ in a corpus, compute a context vector c. 2. Use a clustering algorithm to cluster these word-token context vectors c into a predefined number of groups or clusters. Each cluster defines a sense of $w$ . 3. Compute the vector centroid of each cluster. Each vector centroid $mathbf { s _ { j } }$ is a sense vector representing that sense of $w$ . \nSince this is an unsupervised algorithm, we don’t have names for each of these “senses” of $w$ ; we just refer to the $j$ th sense of $w$ . \nTo disambiguate a particular token $t$ of $w$ we again have three steps: \n1. Compute a context vector c for $t$ .   \n2. Retrieve all sense vectors $s _ { j }$ for $w$ .   \n3. Assign $t$ to the sense represented by the sense vector $s _ { j }$ that is closest to $t$ . \nAll we need is a clustering algorithm and a distance metric between vectors. Clustering is a well-studied problem with a wide number of standard algorithms that can be applied to inputs structured as vectors of numerical values (Duda and Hart, 1973). A frequently used technique in language applications is known as agglomerative clustering. In this technique, each of the $N$ training instances is initially assigned to its own cluster. New clusters are then formed in a bottom-up fashion by the successive merging of the two clusters that are most similar. This process continues until either a specified number of clusters is reached, or some global goodness measure among the clusters is achieved. In cases in which the number of training instances makes this method too expensive, random sampling can be used on the original training set to achieve similar results. \nHow can we evaluate unsupervised sense disambiguation approaches? As usual, the best way is to do extrinsic evaluation embedded in some end-to-end system; one example used in a SemEval bakeoff is to improve search result clustering and diversification (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to map the automatically derived sense classes into a hand-labeled gold-standard set so that we can compare a hand-labeled test set with a set labeled by our unsupervised classifier. Various such metrics have been tested, for example in the SemEval tasks (Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013), including cluster overlap metrics, or methods that map each sense cluster to a predefined sense by choosing the sense that (in some training set) has the most overlap with the cluster. However it is fair to say that no evaluation metric for this task has yet become standard. \n18.8 Summary \nThis chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights: \n• A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.   \n• Many words are polysemous, having many senses.   \n• Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.   \n• WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.   \n• Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.   \n• The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.   \n• Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.   \n• An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.   \n• Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target word’s neighborhood.   \n• Word sense induction is the task of learning word senses unsupervised.",
        "chapter": "Word Senses and WordNet",
        "section": "Word Sense Induction",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Since this is an unsupervised algorithm, we don’t have names for each of these “senses” of $w$ ; we just refer to the $j$ th sense of $w$ . \nTo disambiguate a particular token $t$ of $w$ we again have three steps: \n1. Compute a context vector c for $t$ .   \n2. Retrieve all sense vectors $s _ { j }$ for $w$ .   \n3. Assign $t$ to the sense represented by the sense vector $s _ { j }$ that is closest to $t$ . \nAll we need is a clustering algorithm and a distance metric between vectors. Clustering is a well-studied problem with a wide number of standard algorithms that can be applied to inputs structured as vectors of numerical values (Duda and Hart, 1973). A frequently used technique in language applications is known as agglomerative clustering. In this technique, each of the $N$ training instances is initially assigned to its own cluster. New clusters are then formed in a bottom-up fashion by the successive merging of the two clusters that are most similar. This process continues until either a specified number of clusters is reached, or some global goodness measure among the clusters is achieved. In cases in which the number of training instances makes this method too expensive, random sampling can be used on the original training set to achieve similar results. \nHow can we evaluate unsupervised sense disambiguation approaches? As usual, the best way is to do extrinsic evaluation embedded in some end-to-end system; one example used in a SemEval bakeoff is to improve search result clustering and diversification (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to map the automatically derived sense classes into a hand-labeled gold-standard set so that we can compare a hand-labeled test set with a set labeled by our unsupervised classifier. Various such metrics have been tested, for example in the SemEval tasks (Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013), including cluster overlap metrics, or methods that map each sense cluster to a predefined sense by choosing the sense that (in some training set) has the most overlap with the cluster. However it is fair to say that no evaluation metric for this task has yet become standard. \n18.8 Summary \nThis chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights: \n• A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.   \n• Many words are polysemous, having many senses.   \n• Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.   \n• WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.   \n• Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.   \n• The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.   \n• Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.   \n• An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.   \n• Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target word’s neighborhood.   \n• Word sense induction is the task of learning word senses unsupervised.",
        "chapter": "Word Senses and WordNet",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Since this is an unsupervised algorithm, we don’t have names for each of these “senses” of $w$ ; we just refer to the $j$ th sense of $w$ . \nTo disambiguate a particular token $t$ of $w$ we again have three steps: \n1. Compute a context vector c for $t$ .   \n2. Retrieve all sense vectors $s _ { j }$ for $w$ .   \n3. Assign $t$ to the sense represented by the sense vector $s _ { j }$ that is closest to $t$ . \nAll we need is a clustering algorithm and a distance metric between vectors. Clustering is a well-studied problem with a wide number of standard algorithms that can be applied to inputs structured as vectors of numerical values (Duda and Hart, 1973). A frequently used technique in language applications is known as agglomerative clustering. In this technique, each of the $N$ training instances is initially assigned to its own cluster. New clusters are then formed in a bottom-up fashion by the successive merging of the two clusters that are most similar. This process continues until either a specified number of clusters is reached, or some global goodness measure among the clusters is achieved. In cases in which the number of training instances makes this method too expensive, random sampling can be used on the original training set to achieve similar results. \nHow can we evaluate unsupervised sense disambiguation approaches? As usual, the best way is to do extrinsic evaluation embedded in some end-to-end system; one example used in a SemEval bakeoff is to improve search result clustering and diversification (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to map the automatically derived sense classes into a hand-labeled gold-standard set so that we can compare a hand-labeled test set with a set labeled by our unsupervised classifier. Various such metrics have been tested, for example in the SemEval tasks (Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013), including cluster overlap metrics, or methods that map each sense cluster to a predefined sense by choosing the sense that (in some training set) has the most overlap with the cluster. However it is fair to say that no evaluation metric for this task has yet become standard. \n18.8 Summary \nThis chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights: \n• A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.   \n• Many words are polysemous, having many senses.   \n• Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.   \n• WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.   \n• Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.   \n• The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.   \n• Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.   \n• An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.   \n• Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target word’s neighborhood.   \n• Word sense induction is the task of learning word senses unsupervised.",
        "chapter": "Word Senses and WordNet",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Since this is an unsupervised algorithm, we don’t have names for each of these “senses” of $w$ ; we just refer to the $j$ th sense of $w$ . \nTo disambiguate a particular token $t$ of $w$ we again have three steps: \n1. Compute a context vector c for $t$ .   \n2. Retrieve all sense vectors $s _ { j }$ for $w$ .   \n3. Assign $t$ to the sense represented by the sense vector $s _ { j }$ that is closest to $t$ . \nAll we need is a clustering algorithm and a distance metric between vectors. Clustering is a well-studied problem with a wide number of standard algorithms that can be applied to inputs structured as vectors of numerical values (Duda and Hart, 1973). A frequently used technique in language applications is known as agglomerative clustering. In this technique, each of the $N$ training instances is initially assigned to its own cluster. New clusters are then formed in a bottom-up fashion by the successive merging of the two clusters that are most similar. This process continues until either a specified number of clusters is reached, or some global goodness measure among the clusters is achieved. In cases in which the number of training instances makes this method too expensive, random sampling can be used on the original training set to achieve similar results. \nHow can we evaluate unsupervised sense disambiguation approaches? As usual, the best way is to do extrinsic evaluation embedded in some end-to-end system; one example used in a SemEval bakeoff is to improve search result clustering and diversification (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to map the automatically derived sense classes into a hand-labeled gold-standard set so that we can compare a hand-labeled test set with a set labeled by our unsupervised classifier. Various such metrics have been tested, for example in the SemEval tasks (Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013), including cluster overlap metrics, or methods that map each sense cluster to a predefined sense by choosing the sense that (in some training set) has the most overlap with the cluster. However it is fair to say that no evaluation metric for this task has yet become standard. \n18.8 Summary \nThis chapter has covered a wide range of issues concerning the meanings associated with lexical items. The following are among the highlights: \n• A word sense is the locus of word meaning; definitions and meaning relations are defined at the level of the word sense rather than wordforms.   \n• Many words are polysemous, having many senses.   \n• Relations between senses include synonymy, antonymy, meronymy, and taxonomic relations hyponymy and hypernymy.   \n• WordNet is a large database of lexical relations for English, and WordNets exist for a variety of languages.   \n• Word-sense disambiguation (WSD) is the task of determining the correct sense of a word in context. Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. SemCor is the largest corpus with WordNet-labeled senses.   \n• The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.   \n• Feature-based algorithms using parts of speech and embeddings of words in the context of the target word also work well.   \n• An important baseline for WSD is the most frequent sense, equivalent, in WordNet, to take the first sense.   \n• Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most words with the target word’s neighborhood.   \n• Word sense induction is the task of learning word senses unsupervised. \n\nBibliographical and Historical Notes \nWord sense disambiguation traces its roots to some of the earliest applications of digital computers. The insight that underlies modern algorithms for word sense disambiguation was first articulated by Weaver (1949/1955) in the context of machine translation: \nIf one examines the words in a book, one at a time as through an opaque mask with a hole in it one word wide, then it is obviously impossible to determine, one at a time, the meaning of the words. [. . . ] But if one lengthens the slit in the opaque mask, until one can see not only the central word in question but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word. [. . . ] The practical question is : “What minimum value of N will, at least in a tolerable fraction of cases, lead to the correct choice of meaning for the central word?” \nOther notions first proposed in this early period include the use of a thesaurus for disambiguation (Masterman, 1957), supervised training of Bayesian models for disambiguation (Madhu and Lytel, 1965), and the use of clustering in word sense analysis (Sparck Jones, 1986). \nMuch disambiguation work was conducted within the context of early AI-oriented natural language processing systems. Quillian (1968) and Quillian (1969) proposed a graph-based approach to language processing, in which the definition of a word was represented by a network of word nodes connected by syntactic and semantic relations, and sense disambiguation by finding the shortest path between senses in the graph. Simmons (1973) is another influential early semantic network approach. Wilks proposed one of the earliest non-discrete models with his Preference Semantics (Wilks 1975c, Wilks 1975b, Wilks 1975a), and Small and Rieger (1982) and Riesbeck (1975) proposed understanding systems based on modeling rich procedural information for each word. Hirst’s ABSITY system (Hirst and Charniak 1982, Hirst 1987, Hirst 1988), which used a technique called marker passing based on semantic networks, represents the most advanced system of this type. As with these largely symbolic approaches, early neural network (at the time called ‘connectionist’) approaches to word sense disambiguation relied on small lexicons with handcoded representations (Cottrell 1985, Kawamoto 1988). \nThe earliest implementation of a robust empirical approach to sense disambiguation is due to Kelly and Stone (1975), who directed a team that hand-crafted a set of disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the first to use a machine-readable dictionary for word sense disambiguation. Fellbaum (1998) collects early work on WordNet. Early work using dictionaries as lexical resources include Amsler’s 1981 use of the Merriam Webster dictionary and Longman’s Dictionary of Contemporary English (Boguraev and Briscoe, 1989). \n\nSupervised approaches to disambiguation began with the use of decision trees by Black (1988). In addition to the IMS and contextual-embedding based methods for supervised WSD, recent supervised algorithms includes encoder-decoder models (Raganato et al., 2017a). \nThe need for large amounts of annotated text in supervised methods led early on to investigations into the use of bootstrapping methods (Hearst 1991, Yarowsky 1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is based on aligned parallel corpora in two languages. For example, the fact that the French word catastrophe might be translated as English disaster in one instance and tragedy in another instance can be used to disambiguate the senses of the two English words (i.e., to choose senses of disaster and tragedy that are similar). \ncoarse senses \nThe earliest use of clustering in the study of word senses was by Sparck Jones (1986); Pedersen and Bruce (1997), Sch¨utze (1997b), and Schu¨tze (1998) applied distributional methods. Clustering word senses into coarse senses has also been used to address the problem of dictionary senses being too fine-grained (Section 18.5.3) (Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and de Lacalle 2003, Palmer et al. 2004, Navigli 2006, Snow et al. 2007, Pilehvar et al. 2013). Corpora with clustered word senses for training supervised clustering algorithms include Palmer et al. (2006) and OntoNotes (Hovy et al., 2006). \nOntoNotes \ngenerative lexicon qualia structure \nSee Pustejovsky (1995), Pustejovsky and Boguraev (1996), Martin (1986), and Copestake and Briscoe (1995), inter alia, for computational approaches to the representation of polysemy. Pustejovsky’s theory of the generative lexicon, and in particular his theory of the qualia structure of words, is a way of accounting for the dynamic systematic polysemy of words in context. \nHistorical overviews of WSD include Agirre and Edmonds (2006) and Navigli (2009). \nExercises \n18.1 Collect a small corpus of example sentences of varying lengths from any newspaper or magazine. Using WordNet or any standard dictionary, determine how many senses there are for each of the open-class words in each sentence. How many distinct combinations of senses are there for each sentence? How does this number seem to vary with sentence length?   \n18.2 Using WordNet or a standard reference dictionary, tag each open-class word in your corpus with its correct tag. Was choosing the correct sense always a straightforward task? Report on any difficulties you encountered.   \n18.3 Using your favorite dictionary, simulate the original Lesk word overlap disambiguation algorithm described on page 398 on the phrase Time flies like an arrow. Assume that the words are to be disambiguated one at a time, from left to right, and that the results from earlier decisions are used later in the process. \n18.4 Build an implementation of your solution to the previous exercise. Using WordNet, implement the original Lesk word overlap disambiguation algorithm described on page 398 on the phrase Time flies like an arrow. \nCHAPTER 19 Semantic Role Labeling \n“Who, What, Where, When, With what, Why, How” The seven circumstances, associated with Hermagoras and Aristotle (Sloan, 2010) \nSometime between the 7th and 4th centuries BCE, the Indian grammarian Pa¯n. ini1 wrote a famous treatise on Sanskrit grammar, the A.st.a¯dhya¯y¯ı (‘8 books’), a treatise that has been called “one of the greatest monuments of human intelligence” (Bloomfield, 1933, 11). The work describes the linguistics of the Sanskrit language in the form of 3959 sutras, each very efficiently (since it had to be memorized!) expressing part of a formal rule system that brilliantly prefigured modern mechanisms of formal language theory (Penn and Kiparsky, 2012). One set of rules describes the ka¯rakas, semantic relationships between a verb and noun arguments, roles like agent, instrument, or destination. Pa¯n. ini’s work was the earliest we know of that modeled the linguistic realization of events and their \n\n  \nH   \n  \n  \n  \n  \nH   \n  \n  \nHenmyr   \n快   \n  \n  \nea   \n  \noeaamuen   \n:42220330   \nu \nparticipants. This task of understanding how participants relate to events—being able to answer the question “Who did what to whom” (and perhaps also “when and where”)—is a central question of natural language processing. \nLet’s move forward 2.5 millennia to the present and consider the very mundane goal of understanding text about a purchase of stock by XYZ Corporation. This purchasing event and its participants can be described by a wide variety of surface forms. The event can be described by a verb (sold, bought) or a noun (purchase), and XYZ Corp can be the syntactic subject (of bought), the indirect object (of sold), or in a genitive or noun compound relation (with the noun purchase) despite having notionally the same role in all of them: \n• XYZ corporation bought the stock.   \n• They sold the stock to XYZ corporation.   \n• The stock was bought by XYZ corporation.   \n• The purchase of the stock by XYZ corporation...   \n• The stock purchase by XYZ corporation... \nIn this chapter we introduce a level of representation that captures the commonality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codified in databases like PropBank and FrameNet. We’ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selectional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.",
        "chapter": "Word Senses and WordNet",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "19.1 Semantic Roles \nConsider how in Chapter 15 we represented the meaning of arguments for sentences like these: \n(19.1) Sasha broke the window.   \n(19.2) Pat opened the door. \nA neo-Davidsonian event representation of these two sentences would be \ndeep roles \nIn this representation, the roles of the subjects of the verbs break and open are Breaker and Opener respectively. These deep roles are specific to each event; Breaking events have Breakers, Opening events have Openers, and so on. \nIf we are going to be able to answer questions, perform inferences, or do any further kinds of semantic processing of these events, we’ll need to know a little more about the semantics of these arguments. Breakers and Openers have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. \nthematic roles agents \nThematic roles are a way to capture this semantic commonality between Breakers and Openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. \ntheme \nAlthough thematic roles are one of the oldest linguistic models, as we saw above, their modern formulation is due to Fillmore (1968) and Gruber (1965). Although there is no universally agreed-upon set of roles, Figs. 19.1 and 19.2 list some thematic roles that have been used in various computational papers, together with rough definitions and examples. Most thematic role sets have about a dozen roles, but we’ll see sets with smaller numbers of roles with even more abstract meanings, and sets with very large numbers of roles that are specific to situations. We’ll use the general term semantic roles for all sets of roles, whether small or large. \nsemantic roles \n19.2 Diathesis Alternations \nThe main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren’t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company $B$ , we’d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. \nSemantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. Consider these possible realizations of the thematic arguments of the verb break: \n(19.3) John broke the window. AGENT THEME   \n(19.4) John broke the window with a rock. AGENT THEME INSTRUMENT   \n(19.5) The rock broke the window. INSTRUMENT THEME   \n(19.6) The window broke. THEME   \n(19.7) The window was broken by John. THEME AGENT \nthematic grid case frame \nThese examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is often called the thematic grid, $theta$ -grid, or case frame. We can see that there are (among others) the following possibilities for the realization of these arguments of break: \nAGENT/Subject, THEME/Object   \nAGENT/Subject, THEME/Object, INSTRUMENT/PPwith   \nINSTRUMENT/Subject, THEME/Object   \nTHEME/Subject \nIt turns out that many verbs allow their thematic roles to be realized in various syntactic positions. For example, verbs like give can realize the THEME and GOAL arguments in two different ways:",
        "chapter": "Semantic Role Labeling",
        "section": "Semantic Roles",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "19.2 Diathesis Alternations \nThe main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren’t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company $B$ , we’d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. \nSemantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. Consider these possible realizations of the thematic arguments of the verb break: \n(19.3) John broke the window. AGENT THEME   \n(19.4) John broke the window with a rock. AGENT THEME INSTRUMENT   \n(19.5) The rock broke the window. INSTRUMENT THEME   \n(19.6) The window broke. THEME   \n(19.7) The window was broken by John. THEME AGENT \nthematic grid case frame \nThese examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is often called the thematic grid, $theta$ -grid, or case frame. We can see that there are (among others) the following possibilities for the realization of these arguments of break: \nAGENT/Subject, THEME/Object   \nAGENT/Subject, THEME/Object, INSTRUMENT/PPwith   \nINSTRUMENT/Subject, THEME/Object   \nTHEME/Subject \nIt turns out that many verbs allow their thematic roles to be realized in various syntactic positions. For example, verbs like give can realize the THEME and GOAL arguments in two different ways: \nverb   \nalternation dative   \nalternation   \na. Doris gave the book to Cary. AGENT THEME GOAL   \nb. Doris gave Cary the book. AGENT GOAL THEME \n\nThese multiple argument structure realizations (the fact that break can take AGENT, INSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in either order) are called verb alternations or diathesis alternations. The alternation we showed above for give, the dative alternation, seems to occur with particular semantic classes of verbs, including “verbs of future having” (advance, allocate, offer, owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw), and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which they belong (47 high-level classes, divided into 193 more specific classes) and the various alternations in which they participate. These lists of verb classes have been incorporated into the online resource VerbNet (Kipper et al., 2000), which links each verb to both WordNet and FrameNet entries. \n19.3 Semantic Roles: Problems with Thematic Roles \nRepresenting meaning at the thematic role level seems like it should be useful in dealing with complications like diathesis alternations. Yet it has proved quite difficult to come up with a standard set of roles, and equally difficult to produce a formal definition of roles like AGENT, THEME, or INSTRUMENT. \nFor example, researchers attempting to define role sets often find they need to fragment a role like AGENT or THEME into many specific roles. Levin and Rappaport Hovav (2005) summarize a number of such cases, such as the fact there seem to be at least two kinds of INSTRUMENTS, intermediary instruments that can appear as subjects and enabling instruments that cannot: \na. The cook opened the jar with the new gadget.   \nb. The new gadget opened the jar.   \na. Shelly ate the sliced banana with a fork.   \nb. *The fork ate the sliced banana. \nsemantic role \nproto-agent proto-patient \nIn addition to the fragmentation problem, there are cases in which we’d like to reason about and generalize across semantic roles, but the finite discrete lists of roles don’t let us do this. \nFinally, it has proved difficult to formally define the thematic roles. Consider the AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any individual noun phrase might not exhibit all of these properties. \nThese problems have led to alternative semantic role models that use either many fewer or many more roles. \nThe first of these options is to define generalized semantic roles that abstract over the specific thematic roles. For example, PROTO-AGENT and PROTO-PATIENT are generalized roles that express roughly agent-like and roughly patient-like meanings. These roles are defined, not by necessary and sufficient conditions, but rather by a set of heuristic features that accompany more agent-like or more patient-like meanings. Thus, the more an argument displays agent-like properties (being volitionally involved in the event, causing an event or a change of state in another participant, being sentient or intentionally involved, moving) the greater the likelihood that the argument can be labeled a PROTO-AGENT. The more patient-like the properties (undergoing change of state, causally affected by another participant, stationary relative to other participants, etc.), the greater the likelihood that the argument can be labeled a PROTO-PATIENT.",
        "chapter": "Semantic Role Labeling",
        "section": "Diathesis Alternations",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "verb   \nalternation dative   \nalternation   \na. Doris gave the book to Cary. AGENT THEME GOAL   \nb. Doris gave Cary the book. AGENT GOAL THEME \n\nThese multiple argument structure realizations (the fact that break can take AGENT, INSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in either order) are called verb alternations or diathesis alternations. The alternation we showed above for give, the dative alternation, seems to occur with particular semantic classes of verbs, including “verbs of future having” (advance, allocate, offer, owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw), and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which they belong (47 high-level classes, divided into 193 more specific classes) and the various alternations in which they participate. These lists of verb classes have been incorporated into the online resource VerbNet (Kipper et al., 2000), which links each verb to both WordNet and FrameNet entries. \n19.3 Semantic Roles: Problems with Thematic Roles \nRepresenting meaning at the thematic role level seems like it should be useful in dealing with complications like diathesis alternations. Yet it has proved quite difficult to come up with a standard set of roles, and equally difficult to produce a formal definition of roles like AGENT, THEME, or INSTRUMENT. \nFor example, researchers attempting to define role sets often find they need to fragment a role like AGENT or THEME into many specific roles. Levin and Rappaport Hovav (2005) summarize a number of such cases, such as the fact there seem to be at least two kinds of INSTRUMENTS, intermediary instruments that can appear as subjects and enabling instruments that cannot: \na. The cook opened the jar with the new gadget.   \nb. The new gadget opened the jar.   \na. Shelly ate the sliced banana with a fork.   \nb. *The fork ate the sliced banana. \nsemantic role \nproto-agent proto-patient \nIn addition to the fragmentation problem, there are cases in which we’d like to reason about and generalize across semantic roles, but the finite discrete lists of roles don’t let us do this. \nFinally, it has proved difficult to formally define the thematic roles. Consider the AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any individual noun phrase might not exhibit all of these properties. \nThese problems have led to alternative semantic role models that use either many fewer or many more roles. \nThe first of these options is to define generalized semantic roles that abstract over the specific thematic roles. For example, PROTO-AGENT and PROTO-PATIENT are generalized roles that express roughly agent-like and roughly patient-like meanings. These roles are defined, not by necessary and sufficient conditions, but rather by a set of heuristic features that accompany more agent-like or more patient-like meanings. Thus, the more an argument displays agent-like properties (being volitionally involved in the event, causing an event or a change of state in another participant, being sentient or intentionally involved, moving) the greater the likelihood that the argument can be labeled a PROTO-AGENT. The more patient-like the properties (undergoing change of state, causally affected by another participant, stationary relative to other participants, etc.), the greater the likelihood that the argument can be labeled a PROTO-PATIENT. \n\nThe second direction is instead to define semantic roles that are specific to a particular verb or a particular group of semantically related verbs or nouns. \nIn the next two sections we describe two commonly used lexical resources that make use of these alternative versions of semantic roles. PropBank uses both protoroles and verb-specific semantic roles. FrameNet uses semantic roles that are specific to a general semantic idea called a frame. \n19.4 The Proposition Bank \nPropBank \nThe Proposition Bank, generally referred to as PropBank, is a resource of sentences annotated with semantic roles. The English PropBank labels all the sentences in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese TreeBank. Because of the difficulty of defining a universal set of thematic roles, the semantic roles in PropBank are defined with respect to an individual verb sense. Each sense of each verb thus has a specific set of roles, which are given only numbers rather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the PROTO-AGENT, and Arg1, the PROTO-PATIENT. The semantics of the other roles are less consistent, often being defined specifically for each verb. Nonetheless there are some generalization; the Arg2 is often the benefactive, instrument, attribute, or end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4 the end point. \nHere are some slightly simplified PropBank entries for one sense each of the verbs agree and fall. Such PropBank entries are called frame files; note that the definitions in the frame file for each role (“Other entity agreeing”, “Extent, amount fallen”) are informal glosses intended to be read by humans, rather than being formal definitions. \n(19.11) agree.01 Arg0: Agreer Arg1: Proposition Arg2: Other entity agreeing Ex1: $mathrm { Delta [ _ { A r g 0 } }$ The group] agreed $mathrm { Delta I _ { A r g 1 } }$ it wouldn’t make an offer]. $mathtt { E x 2 }$ : [ArgM-TMP Usually] $mathrm { Delta [ _ { A r g 0 } }$ John] agrees $operatorname { I } _ { mathrm { A r g } 2 }$ with Mary] $mathrm { Delta [ _ { A r g 1 } }$ on everything]. \n(19.12) fall.01 \nArg1: Logical subject, patient, thing falling   \nArg2: Extent, amount fallen   \nArg3: start point   \nArg4: end point, end state of arg1   \nEx1: $mathrm { Delta I _ { A r g 1 } }$ Sales] fell $mathrm { Delta [ _ { A r g 4 } }$ to $$ 25$ million] $mathrm { I } _ { mathrm { A r g 3 } }$ from $$ 27$ million]. $mathtt { E x 2 }$ : $mathrm { Delta [ _ { A r g 1 } }$ The average junk bond] fell $operatorname { I } _ { mathrm { A r g } 2 }$ by $4 . 2 % ]$ . \nNote that there is no Arg0 role for fall, because the normal subject of fall is a PROTO-PATIENT.",
        "chapter": "Semantic Role Labeling",
        "section": "Semantic Roles: Problems with Thematic Roles",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The second direction is instead to define semantic roles that are specific to a particular verb or a particular group of semantically related verbs or nouns. \nIn the next two sections we describe two commonly used lexical resources that make use of these alternative versions of semantic roles. PropBank uses both protoroles and verb-specific semantic roles. FrameNet uses semantic roles that are specific to a general semantic idea called a frame. \n19.4 The Proposition Bank \nPropBank \nThe Proposition Bank, generally referred to as PropBank, is a resource of sentences annotated with semantic roles. The English PropBank labels all the sentences in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese TreeBank. Because of the difficulty of defining a universal set of thematic roles, the semantic roles in PropBank are defined with respect to an individual verb sense. Each sense of each verb thus has a specific set of roles, which are given only numbers rather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the PROTO-AGENT, and Arg1, the PROTO-PATIENT. The semantics of the other roles are less consistent, often being defined specifically for each verb. Nonetheless there are some generalization; the Arg2 is often the benefactive, instrument, attribute, or end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4 the end point. \nHere are some slightly simplified PropBank entries for one sense each of the verbs agree and fall. Such PropBank entries are called frame files; note that the definitions in the frame file for each role (“Other entity agreeing”, “Extent, amount fallen”) are informal glosses intended to be read by humans, rather than being formal definitions. \n(19.11) agree.01 Arg0: Agreer Arg1: Proposition Arg2: Other entity agreeing Ex1: $mathrm { Delta [ _ { A r g 0 } }$ The group] agreed $mathrm { Delta I _ { A r g 1 } }$ it wouldn’t make an offer]. $mathtt { E x 2 }$ : [ArgM-TMP Usually] $mathrm { Delta [ _ { A r g 0 } }$ John] agrees $operatorname { I } _ { mathrm { A r g } 2 }$ with Mary] $mathrm { Delta [ _ { A r g 1 } }$ on everything]. \n(19.12) fall.01 \nArg1: Logical subject, patient, thing falling   \nArg2: Extent, amount fallen   \nArg3: start point   \nArg4: end point, end state of arg1   \nEx1: $mathrm { Delta I _ { A r g 1 } }$ Sales] fell $mathrm { Delta [ _ { A r g 4 } }$ to $$ 25$ million] $mathrm { I } _ { mathrm { A r g 3 } }$ from $$ 27$ million]. $mathtt { E x 2 }$ : $mathrm { Delta [ _ { A r g 1 } }$ The average junk bond] fell $operatorname { I } _ { mathrm { A r g } 2 }$ by $4 . 2 % ]$ . \nNote that there is no Arg0 role for fall, because the normal subject of fall is a PROTO-PATIENT. \nThe PropBank semantic roles can be useful in recovering shallow semantic information about verbal arguments. Consider the verb increase: \n(19.13) increase.01 “go up incrementally” \nArg0: causer of increase   \nArg1: thing increasing   \nArg2: amount increased by, EXT, or MNR   \nArg3: start point   \nArg4: end point \nA PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case $B i g$ Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. \n(19.14) [Arg0 Big Fruit Co. ] increased $mathrm { Delta [ _ { A r g 1 } }$ the price of bananas]. (19.15) $mathrm { Delta [ _ { A r g 1 } }$ The price of bananas] was increased again $mathrm { Delta [ _ { A r g 0 } }$ by Big Fruit Co. ] (19.16) $mathsf { I } _ { mathrm { A r g 1 } }$ The price of bananas] increased $mathrm { [ _ { A r g 2 } } 5 % ]$ . \nPropBank also has a number of non-numbered arguments called ArgMs, (ArgMTMP, ArgM-LOC, etc.) which represent modification or adjunct meanings. These are relatively stable across predicates, so aren’t listed with each frame file. Data labeled with these modifiers can be helpful in training systems to detect temporal, location, or directional modification across predicates. Some of the ArgM’s include: \nTMP when? yesterday evening, now LOC where? at the museum, in San Francisco DIR where to/from? down, to Bangkok MNR how? clearly, with much enthusiasm PRP/CAU why? because ... , in response to the ruling REC themselves, each other ADV miscellaneous PRD secondary predication ...ate the meat raw \nNomBank \nWhile PropBank focuses on verbs, a related project, NomBank (Meyers et al., 2004) adds annotations to noun predicates. For example the noun agreement in Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as the Arg2. This allows semantic role labelers to assign labels to arguments of both verbal and nominal predicates. \n19.5 FrameNet \nWhile making inferences about the semantic commonalities across different sentences with increase is useful, it would be even more useful if we could make such inferences in many more situations, across different verbs, and also between verbs and nouns. For example, we’d like to extract the similarity among these three sentences: \n(19.17) $mathrm { Delta I _ { A r g 1 } }$ The price of bananas] increased $mathrm { [ } _ { mathrm { A r g } 2 } 5 % ]$ .   \n(19.18) $mathsf { I } _ { mathrm { A r g 1 } }$ The price of bananas] rose $mathrm { [ _ { A r g 2 } } 5 % ]$ .   \n(19.19) There has been a $mathrm { [ _ { A r g 2 } } 5 % ]$ rise $mathrm { Delta [ _ { A r g 1 } }$ in the price of bananas]. \nNote that the second example uses the different verb rise, and the third example uses the noun rather than the verb rise. We’d like a system to recognize that the price of bananas is what went up, and that $5 %$ is the amount it went up, no matter whether the $5 %$ appears as the object of the verb increased or as a nominal modifier of the noun rise.",
        "chapter": "Semantic Role Labeling",
        "section": "The Proposition Bank",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The PropBank semantic roles can be useful in recovering shallow semantic information about verbal arguments. Consider the verb increase: \n(19.13) increase.01 “go up incrementally” \nArg0: causer of increase   \nArg1: thing increasing   \nArg2: amount increased by, EXT, or MNR   \nArg3: start point   \nArg4: end point \nA PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case $B i g$ Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. \n(19.14) [Arg0 Big Fruit Co. ] increased $mathrm { Delta [ _ { A r g 1 } }$ the price of bananas]. (19.15) $mathrm { Delta [ _ { A r g 1 } }$ The price of bananas] was increased again $mathrm { Delta [ _ { A r g 0 } }$ by Big Fruit Co. ] (19.16) $mathsf { I } _ { mathrm { A r g 1 } }$ The price of bananas] increased $mathrm { [ _ { A r g 2 } } 5 % ]$ . \nPropBank also has a number of non-numbered arguments called ArgMs, (ArgMTMP, ArgM-LOC, etc.) which represent modification or adjunct meanings. These are relatively stable across predicates, so aren’t listed with each frame file. Data labeled with these modifiers can be helpful in training systems to detect temporal, location, or directional modification across predicates. Some of the ArgM’s include: \nTMP when? yesterday evening, now LOC where? at the museum, in San Francisco DIR where to/from? down, to Bangkok MNR how? clearly, with much enthusiasm PRP/CAU why? because ... , in response to the ruling REC themselves, each other ADV miscellaneous PRD secondary predication ...ate the meat raw \nNomBank \nWhile PropBank focuses on verbs, a related project, NomBank (Meyers et al., 2004) adds annotations to noun predicates. For example the noun agreement in Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as the Arg2. This allows semantic role labelers to assign labels to arguments of both verbal and nominal predicates. \n19.5 FrameNet \nWhile making inferences about the semantic commonalities across different sentences with increase is useful, it would be even more useful if we could make such inferences in many more situations, across different verbs, and also between verbs and nouns. For example, we’d like to extract the similarity among these three sentences: \n(19.17) $mathrm { Delta I _ { A r g 1 } }$ The price of bananas] increased $mathrm { [ } _ { mathrm { A r g } 2 } 5 % ]$ .   \n(19.18) $mathsf { I } _ { mathrm { A r g 1 } }$ The price of bananas] rose $mathrm { [ _ { A r g 2 } } 5 % ]$ .   \n(19.19) There has been a $mathrm { [ _ { A r g 2 } } 5 % ]$ rise $mathrm { Delta [ _ { A r g 1 } }$ in the price of bananas]. \nNote that the second example uses the different verb rise, and the third example uses the noun rather than the verb rise. We’d like a system to recognize that the price of bananas is what went up, and that $5 %$ is the amount it went up, no matter whether the $5 %$ appears as the object of the verb increased or as a nominal modifier of the noun rise. \n\nFrameNet \nThe FrameNet project is another semantic-role-labeling project that attempts to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003, Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank project are specific to an individual verb, roles in the FrameNet project are specific to a frame. \nWhat is a frame? Consider the following set of words: \nreservation, flight, travel, buy, price, cost, fare, rates, meal, plane \nThere are many individual lexical relations of hyponymy, synonymy, and so on between many of the words in this list. The resulting set of relations does not, however, add up to a complete account of how these words are related. They are clearly all defined with respect to a coherent chunk of common-sense background information concerning air travel. \nframe \nWe call the holistic background knowledge that unites these words a frame (Fillmore, 1985). The idea that groups of words are defined with respect to some background information is widespread in artificial intelligence and cognitive science, where besides frame we see related works like a model (Johnson-Laird, 1983), or even script (Schank and Abelson, 1977). \nmodel script \nframe elements \nA frame in FrameNet is a background knowledge structure that defines a set of frame-specific semantic roles, called frame elements, and includes a set of predicates that use these roles. Each word evokes a frame and profiles some aspect of the frame and its elements. The FrameNet dataset includes a set of frames and frame elements, the lexical units associated with each frame, and a set of labeled example sentences. For example, the change position on a scale frame is defined as follows: \nThis frame consists of words that indicate the change of an Item’s position on a scale (the Attribute) from a starting point (Initial value) to an end point (Final value). \nSome of the semantic roles (frame elements) in the frame are defined as in Fig. 19.3. Note that these are separated into core roles, which are frame specific, and non-core roles, which are more like the Arg-M arguments in PropBank, expressing more general properties of time, location, and so on. \nHere are some example sentences: \n(19.20) [ITEM Oil] rose [ATTRIBUTE in price] [DIFFERENCE by $2 %$ ].   \n(19.21) [ITEM It] has increased [FINAL STATE to having them 1 day a month].   \n(19.22) [ITEM Microsoft shares] fell [FINAL VALUE to 7 5/8].   \n(19.23) [ITEM Colon cancer incidence] fell [DIFFERENCE by $5 0 %$ ] [GROUP among men].   \n(19.24) a steady increase [INITIAL VALUE from 9.5] [FINAL VALUE to 14.3] [ITEM in dividends]   \n(19.25) a [DIFFERENCE $5 %$ ] [ITEM dividend] increase... \nNote from these example sentences that the frame includes target words like rise, fall, and increase. In fact, the complete frame consists of the following words: \nFrameNet also codes relationships between frames, allowing frames to inherit from each other, or representing relations between frames like causation (and generalizations among frame elements in different frames can be representing by inheritance as well). Thus, there is a Cause change of position on a scale frame that is linked to the Change of position on a scale frame by the cause relation, but that adds an AGENT role and is used for causative examples such as the following: \n(19.26) [AGENT They] raised [ITEM the price of their soda] [DIFFERENCE by $2 %$ ]. \nTogether, these two frames would allow an understanding system to extract the common event semantics of all the verbal and nominal causative and non-causative usages. \nFrameNets have also been developed for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese. \n19.6 Semantic Role Labeling \nsemantic role labeling \nSemantic role labeling (sometimes shortened as SRL) is the task of automatically finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning, often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets.",
        "chapter": "Semantic Role Labeling",
        "section": "FrameNet",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Recall that the difference between these two models of semantic roles is that FrameNet (19.27) employs many frame-specific frame elements as roles, while PropBank (19.28) uses a smaller number of numbered argument labels that can be interpreted as verb-specific labels, along with the more general ARGM labels. Some examples: \n[You] can’t [blame] [the program] [for being unable to identify it]   \n(19.27) COGNIZER TARGET EVALUEE REASON [The San Francisco Examiner] issued [a special edition] [yesterday]   \n(19.28) ARG0 TARGET ARG1 ARGM-TMP \n19.6.1 A Feature-based Algorithm for Semantic Role Labeling \nA simplified feature-based semantic role labeling algorithm is sketched in Fig. 19.4. Feature-based algorithms—from the very earliest systems like (Simmons, 1973)— begin by parsing, using broad-coverage parsers to assign a parse to the input string. Figure 19.5 shows a parse of (19.28) above. The parse is then traversed to find all words that are predicates. \nFor each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. \nInstead of training a single-stage classifier as in Fig. 19.5, the node-level classification task can be broken down into multiple steps: \n1. Pruning: Since only a small number of the constituents in a sentence are arguments of any given predicate, many systems use simple heuristics to prune unlikely constituents.   \n2. Identification: a binary classification of each node as an argument to be labeled or a NONE.   \n3. Classification: a 1-of- $. N$ classification of all the constituents that were labeled as arguments by the previous stage \nThe separation of identification and classification may lead to better use of features (different features may be useful for the two tasks) or to computational efficiency. \nGlobal Optimization \nThe classification algorithm of Fig. 19.5 classifies each argument separately (‘locally’), making the simplifying assumption that each argument of a predicate can be labeled independently. This assumption is false; there are interactions between arguments that require a more ‘global’ assignment of labels to constituents. For example, constituents in FrameNet and PropBank are required to be non-overlapping. More significantly, the semantic roles of constituents are not independent. For example PropBank does not allow multiple identical arguments; two constituents of the same verb cannot both be labeled ARG0 . \nRole labeling systems thus often add a fourth step to deal with global consistency across the labels in a sentence. For example, the local classifiers can return a list of possible labels associated with probabilities for each constituent, and a second-pass Viterbi decoding or re-ranking approach can be used to choose the best consensus label. Integer linear programming (ILP) is another common way to choose a solution that conforms best to multiple constraints. \nFeatures for Semantic Role Labeling \nMost systems use some generalization of the core set of features introduced by Gildea and Jurafsky (2000). Common basic features templates (demonstrated on the NP-SBJ constituent The San Francisco Examiner in Fig. 19.5) include: \n• The governing predicate, in this case the verb issued. The predicate is a crucial feature since labels are defined only with respect to a particular predicate.   \n• The phrase type of the constituent, in this case, $N P$ (or NP-SBJ). Some semantic roles tend to appear as NPs, others as $S$ or $P P$ , and so on.   \n• The headword of the constituent, Examiner. The headword of a constituent can be computed with standard head rules, such as those given in Chapter 12 in Fig. 12.12. Certain headwords (e.g., pronouns) place strong constraints on the possible semantic roles they are likely to fill.   \n• The headword part of speech of the constituent, NNP.   \n• The path in the parse tree from the constituent to the predicate. This path is marked by the dotted line in Fig. 19.5. Following Gildea and Jurafsky (2000), we can use a simple linear representation of the path, $mathrm { N P uparrow S downarrow V P downarrow V B D . Omega ^ { cdot } }$ $uparrow$ and $downarrow$ represent upward and downward movement in the tree, respectively. The \npath is very useful as a compact representation of many kinds of grammatical function relationships between the constituent and the predicate. • The voice of the clause in which the constituent appears, in this case, active (as contrasted with passive). Passive sentences tend to have strongly different linkings of semantic roles to surface form than do active ones. • The binary linear position of the constituent with respect to the predicate, either before or after. • The subcategorization of the predicate, the set of expected arguments that appear in the verb phrase. We can extract this information by using the phrasestructure rule that expands the immediate parent of the predicate; $mathrm { V P }  mathrm { V B D }$ NP PP for the predicate in Fig. 19.5. • The named entity type of the constituent. • The first words and the last word of the constituent. \nThe following feature vector thus represents the first NP in our example (recall that most observations will have the value NONE rather than, for example, ARG0, since most constituents in the parse tree will not bear a semantic role): \nARG0: [issued, NP, Examiner, NNP, NP S VP VBD, active, before, $mathrm { V P }  mathrm { N P }$ PP, ORG, The, Examiner] \nOther features are often used in addition, such as sets of n-grams inside the constituent, or more complex versions of the path features (the upward or downward halves, or whether particular nodes occur in the path). \nIt’s also possible to use dependency parses instead of constituency parses as the basis of features, for example using dependency parse paths instead of constituency paths. \n19.6.2 A Neural Algorithm for Semantic Role Labeling \nA simple neural approach to SRL is to treat it as a sequence labeling task like namedentity recognition, using the BIO approach. Let’s assume that we are given the predicate and the task is just detecting and labeling spans. Recall that with BIO tagging, we have a begin and end tag for each possible role (B-ARG0, I-ARG0; BARG1, I-ARG1, and so on), plus an outside tag O.",
        "chapter": "Semantic Role Labeling",
        "section": "Semantic Role Labeling",
        "subsection": "A Feature-based Algorithm for Semantic Role Labeling",
        "subsubsection": "N/A"
    },
    {
        "content": "path is very useful as a compact representation of many kinds of grammatical function relationships between the constituent and the predicate. • The voice of the clause in which the constituent appears, in this case, active (as contrasted with passive). Passive sentences tend to have strongly different linkings of semantic roles to surface form than do active ones. • The binary linear position of the constituent with respect to the predicate, either before or after. • The subcategorization of the predicate, the set of expected arguments that appear in the verb phrase. We can extract this information by using the phrasestructure rule that expands the immediate parent of the predicate; $mathrm { V P }  mathrm { V B D }$ NP PP for the predicate in Fig. 19.5. • The named entity type of the constituent. • The first words and the last word of the constituent. \nThe following feature vector thus represents the first NP in our example (recall that most observations will have the value NONE rather than, for example, ARG0, since most constituents in the parse tree will not bear a semantic role): \nARG0: [issued, NP, Examiner, NNP, NP S VP VBD, active, before, $mathrm { V P }  mathrm { N P }$ PP, ORG, The, Examiner] \nOther features are often used in addition, such as sets of n-grams inside the constituent, or more complex versions of the path features (the upward or downward halves, or whether particular nodes occur in the path). \nIt’s also possible to use dependency parses instead of constituency parses as the basis of features, for example using dependency parse paths instead of constituency paths. \n19.6.2 A Neural Algorithm for Semantic Role Labeling \nA simple neural approach to SRL is to treat it as a sequence labeling task like namedentity recognition, using the BIO approach. Let’s assume that we are given the predicate and the task is just detecting and labeling spans. Recall that with BIO tagging, we have a begin and end tag for each possible role (B-ARG0, I-ARG0; BARG1, I-ARG1, and so on), plus an outside tag O. \nAs with all the taggers, the goal is to compute the highest probability tag sequence $hat { y }$ , given the input sequence of words $w$ : \nFig. 19.6 shows a sketch of a standard algorithm from He et al. (2017). Here each input word is mapped to pretrained embeddings, and then each token is concatenated with the predicate embedding and then passed through a feedforward network with a softmax which outputs a distribution over each SRL label. For decoding, a CRF layer can be used instead of the MLP layer on top of the biLSTM output to do global inference, but in practice this doesn’t seem to provide much benefit. \n19.6.3 Evaluation of Semantic Role Labeling \nThe standard evaluation for semantic role labeling is to require that each argument label must be assigned to the exactly correct word sequence or parse constituent, and then compute precision, recall, and $F$ -measure. Identification and classification can also be evaluated separately. Two common datasets used for evaluation are CoNLL2005 (Carreras and Ma`rquez, 2005) and CoNLL-2012 (Pradhan et al., 2013). \n19.7 Selectional Restrictions \nselectional restriction \nWe turn in this section to another way to represent facts about the relationship between predicates and arguments. A selectional restriction is a semantic type constraint that a verb imposes on the kind of concepts that are allowed to fill its argument roles. Consider the two meanings associated with the following example: \n(19.29) I want to eat someplace nearby. \nThere are two possible parses and semantic interpretations for this sentence. In the sensible interpretation, eat is intransitive and the phrase someplace nearby is an adjunct that gives the location of the eating event. In the nonsensical speaker-asGodzilla interpretation, eat is transitive and the phrase someplace nearby is the direct object and the THEME of the eating, like the NP Malaysian food in the following sentences: \n(19.30) I want to eat Malaysian food. \nHow do we know that someplace nearby isn’t the direct object in this sentence? One useful cue is the semantic fact that the THEME of EATING events tends to be something that is edible. This restriction placed by the verb eat on the filler of its THEME argument is a selectional restriction. \nSelectional restrictions are associated with senses, not entire lexemes. We can see this in the following examples of the lexeme serve: \n(19.31) The restaurant serves green-lipped mussels. (19.32) Which airlines serve Denver? \nExample (19.31) illustrates the offering-food sense of serve, which ordinarily restricts its THEME to be some kind of food Example (19.32) illustrates the provides a commercial service to sense of serve, which constrains its THEME to be some type of appropriate location.",
        "chapter": "Semantic Role Labeling",
        "section": "Semantic Role Labeling",
        "subsection": "A Neural Algorithm for Semantic Role Labeling",
        "subsubsection": "N/A"
    },
    {
        "content": "As with all the taggers, the goal is to compute the highest probability tag sequence $hat { y }$ , given the input sequence of words $w$ : \nFig. 19.6 shows a sketch of a standard algorithm from He et al. (2017). Here each input word is mapped to pretrained embeddings, and then each token is concatenated with the predicate embedding and then passed through a feedforward network with a softmax which outputs a distribution over each SRL label. For decoding, a CRF layer can be used instead of the MLP layer on top of the biLSTM output to do global inference, but in practice this doesn’t seem to provide much benefit. \n19.6.3 Evaluation of Semantic Role Labeling \nThe standard evaluation for semantic role labeling is to require that each argument label must be assigned to the exactly correct word sequence or parse constituent, and then compute precision, recall, and $F$ -measure. Identification and classification can also be evaluated separately. Two common datasets used for evaluation are CoNLL2005 (Carreras and Ma`rquez, 2005) and CoNLL-2012 (Pradhan et al., 2013). \n19.7 Selectional Restrictions \nselectional restriction \nWe turn in this section to another way to represent facts about the relationship between predicates and arguments. A selectional restriction is a semantic type constraint that a verb imposes on the kind of concepts that are allowed to fill its argument roles. Consider the two meanings associated with the following example: \n(19.29) I want to eat someplace nearby. \nThere are two possible parses and semantic interpretations for this sentence. In the sensible interpretation, eat is intransitive and the phrase someplace nearby is an adjunct that gives the location of the eating event. In the nonsensical speaker-asGodzilla interpretation, eat is transitive and the phrase someplace nearby is the direct object and the THEME of the eating, like the NP Malaysian food in the following sentences: \n(19.30) I want to eat Malaysian food. \nHow do we know that someplace nearby isn’t the direct object in this sentence? One useful cue is the semantic fact that the THEME of EATING events tends to be something that is edible. This restriction placed by the verb eat on the filler of its THEME argument is a selectional restriction. \nSelectional restrictions are associated with senses, not entire lexemes. We can see this in the following examples of the lexeme serve: \n(19.31) The restaurant serves green-lipped mussels. (19.32) Which airlines serve Denver? \nExample (19.31) illustrates the offering-food sense of serve, which ordinarily restricts its THEME to be some kind of food Example (19.32) illustrates the provides a commercial service to sense of serve, which constrains its THEME to be some type of appropriate location.",
        "chapter": "Semantic Role Labeling",
        "section": "Semantic Role Labeling",
        "subsection": "Evaluation of Semantic Role Labeling",
        "subsubsection": "N/A"
    },
    {
        "content": "Selectional restrictions vary widely in their specificity. The verb imagine, for example, imposes strict requirements on its AGENT role (restricting it to humans and other animate entities) but places very few semantic requirements on its THEME role. A verb like diagonalize, on the other hand, places a very specific constraint on the filler of its THEME role: it has to be a matrix, while the arguments of the adjectives odorless are restricted to concepts that could possess an odor: \n(19.33) In rehearsal, I often ask the musicians to imagine a tennis game.   \n(19.34) Radon is an odorless gas that can’t be detected by human senses.   \n(19.35) To diagonalize a matrix is to find its eigenvalues. \nThese examples illustrate that the set of concepts we need to represent selectional restrictions (being a matrix, being able to possess an odor, etc) is quite open ended. This distinguishes selectional restrictions from other features for representing lexical knowledge, like parts-of-speech, which are quite limited in number. \n19.7.1 Representing Selectional Restrictions \nOne way to capture the semantics of selectional restrictions is to use and extend the event representation of Chapter 15. Recall that the neo-Davidsonian representation of an event consists of a single variable that stands for the event, a predicate denoting the kind of event, and variables and relations for the event roles. Ignoring the issue of the $lambda$ -structures and using thematic roles rather than deep event roles, the semantic contribution of a verb like eat might look like the following: \nWith this representation, all we know about $y$ , the filler of the THEME role, is that it is associated with an Eating event through the Theme relation. To stipulate the selectional restriction that $y$ must be something edible, we simply add a new term to that effect: \nWhen a phrase like ate a hamburger is encountered, a semantic analyzer can form the following kind of representation: \nThis representation is perfectly reasonable since the membership of $y$ in the category Hamburger is consistent with its membership in the category EdibleThing, assuming a reasonable set of facts in the knowledge base. Correspondingly, the representation for a phrase such as ate a takeoff would be ill-formed because membership in an event-like category such as Takeoff would be inconsistent with membership in the category EdibleThing. \nWhile this approach adequately captures the semantics of selectional restrictions, there are two problems with its direct use. First, using FOL to perform the simple task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms can do the job with far less computational cost. The second problem is that this approach presupposes a large, logical knowledge base of facts about the concepts that make up selectional restrictions. Unfortunately, although such common-sense knowledge bases are being developed, none currently have the kind of coverage necessary to the task. \nA more practical approach is to state selectional restrictions in terms of WordNet synsets rather than as logical concepts. Each predicate simply specifies a WordNet synset as the selectional restriction on each of its arguments. A meaning representation is well-formed if the role filler word is a hyponym (subordinate) of this synset. \nFor our ate a hamburger example, for instance, we could set the selectional restriction on the THEME role of the verb eat to the synset ${ { bf f o o d , n u t r i e n t } }$ , glossed as any substance that can be metabolized by an animal to give energy and build tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 19.7 reveals that hamburgers are indeed food. Again, the filler of a role need not match the restriction synset exactly; it just needs to have the synset as one of its superordinates. \nWe can apply this approach to the THEME roles of the verbs imagine, lift, and diagonalize, discussed earlier. Let us restrict imagine’s THEME to the synset {entity}, lift’s THEME to physical entity , and diagonalize to ${ mathrm { m a t r i x } }$ . This arrangement correctly permits imagine a hamburger and lift a hamburger, while also correctly ruling out diagonalize a hamburger. \n19.7.2 Selectional Preferences \nIn the earliest implementations, selectional restrictions were considered strict constraints on the kind of arguments a predicate could take (Katz and Fodor 1963, Hirst 1987). For example, the verb eat might require that its THEME argument be $left[ + mathtt { F O O D } right]$ . Early word sense disambiguation systems used this idea to rule out senses that violated the selectional restrictions of their governing predicates. \nVery quickly, however, it became clear that these selectional restrictions were better represented as preferences rather than strict constraints (Wilks 1975c, Wilks 1975b). For example, selectional restriction violations (like inedible arguments of eat) often occur in well-formed sentences, for example because they are negated (19.36), or because selectional restrictions are overstated (19.37): \n(19.36) But it fell apart in 1931, perhaps because people realized you can’t eat gold for lunch if you’re hungry.   \n(19.37) In his two championship trials, Mr. Kulkarni ate glass on an empty stomach, accompanied only by water and tea. \nModern systems for selectional preferences therefore specify the relation between a predicate and its possible arguments with soft constraints of some kind.",
        "chapter": "Semantic Role Labeling",
        "section": "Selectional Restrictions",
        "subsection": "Representing Selectional Restrictions",
        "subsubsection": "N/A"
    },
    {
        "content": "A more practical approach is to state selectional restrictions in terms of WordNet synsets rather than as logical concepts. Each predicate simply specifies a WordNet synset as the selectional restriction on each of its arguments. A meaning representation is well-formed if the role filler word is a hyponym (subordinate) of this synset. \nFor our ate a hamburger example, for instance, we could set the selectional restriction on the THEME role of the verb eat to the synset ${ { bf f o o d , n u t r i e n t } }$ , glossed as any substance that can be metabolized by an animal to give energy and build tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 19.7 reveals that hamburgers are indeed food. Again, the filler of a role need not match the restriction synset exactly; it just needs to have the synset as one of its superordinates. \nWe can apply this approach to the THEME roles of the verbs imagine, lift, and diagonalize, discussed earlier. Let us restrict imagine’s THEME to the synset {entity}, lift’s THEME to physical entity , and diagonalize to ${ mathrm { m a t r i x } }$ . This arrangement correctly permits imagine a hamburger and lift a hamburger, while also correctly ruling out diagonalize a hamburger. \n19.7.2 Selectional Preferences \nIn the earliest implementations, selectional restrictions were considered strict constraints on the kind of arguments a predicate could take (Katz and Fodor 1963, Hirst 1987). For example, the verb eat might require that its THEME argument be $left[ + mathtt { F O O D } right]$ . Early word sense disambiguation systems used this idea to rule out senses that violated the selectional restrictions of their governing predicates. \nVery quickly, however, it became clear that these selectional restrictions were better represented as preferences rather than strict constraints (Wilks 1975c, Wilks 1975b). For example, selectional restriction violations (like inedible arguments of eat) often occur in well-formed sentences, for example because they are negated (19.36), or because selectional restrictions are overstated (19.37): \n(19.36) But it fell apart in 1931, perhaps because people realized you can’t eat gold for lunch if you’re hungry.   \n(19.37) In his two championship trials, Mr. Kulkarni ate glass on an empty stomach, accompanied only by water and tea. \nModern systems for selectional preferences therefore specify the relation between a predicate and its possible arguments with soft constraints of some kind. \nSelectional Association \nselectional preference strength \nOne of the most influential has been the selectional association model of Resnik (1993). Resnik defines the idea of selectional preference strength as the general amount of information that a predicate tells us about the semantic class of its arguments. For example, the verb eat tells us a lot about the semantic class of its direct objects, since they tend to be edible. The verb $b e$ , by contrast, tells us less about its direct objects. The selectional preference strength can be defined by the difference in information between two distributions: the distribution of expected semantic classes $P ( c )$ (how likely is it that a direct object will fall into class $c$ ) and the distribution of expected semantic classes for the particular verb $P ( c | nu )$ (how likely is it that the direct object of the specific verb $nu$ will fall into semantic class $c$ ). The greater the difference between these distributions, the more information the verb is giving us about possible objects. The difference between these two distributions can be quantified by relative entropy, or the Kullback-Leibler divergence (Kullback and Leibler, 1951). The Kullback-Leibler or KL divergence $D ( P | | Q )$ expresses the difference between two probability distributions $P$ and $boldsymbol { Q }$ \nrelative entropy KL divergence \nThe selectional preference $S _ { R } ( nu )$ uses the KL divergence to express how much information, in bits, the verb $nu$ expresses about the possible semantic class of its argument. \nselectional association \nResnik then defines the selectional association of a particular class and verb as the relative contribution of that class to the general selectional preference of the verb: \nThe selectional association is thus a probabilistic measure of the strength of association between a predicate and a class dominating the argument to the predicate. Resnik estimates the probabilities for these associations by parsing a corpus, counting all the times each predicate occurs with each argument word, and assuming that each word is a partial observation of all the WordNet concepts containing the word. The following table from Resnik (1996) shows some sample high and low selectional associations for verbs and some WordNet semantic classes of their direct objects. \nSelectional Preference via Conditional Probability \nAn alternative to using selectional association between a verb and the WordNet class of its arguments is to use the conditional probability of an argument word given a predicate verb, directly modeling the strength of association of one verb (predicate) with one noun (argument). \n\nThe conditional probability model can be computed by parsing a very large corpus (billions of words), and computing co-occurrence counts: how often a given verb occurs with a given noun in a given relation. The conditional probability of an argument noun given a verb for a particular relation $P ( n | nu , r )$ can then be used as a selectional preference metric for that pair of words (Brockmann and Lapata 2003, Keller and Lapata 2003): \nThe inverse probability $P ( nu | n , r )$ was found to have better performance in some cases (Brockmann and Lapata, 2003): \nAn even simpler approach is to use the simple log co-occurrence frequency of the predicate with the argument log count $( nu , n , r )$ instead of conditional probability; this seems to do better for extracting preferences for syntactic subjects rather than objects (Brockmann and Lapata, 2003). \nEvaluating Selectional Preferences \npseudowords \nOne way to evaluate models of selectional preferences is to use pseudowords (Gale et al. 1992c, Schu¨tze 1992a). A pseudoword is an artificial word created by concatenating a test word in some context (say banana) with a confounder word (say door) to create banana-door). The task of the system is to identify which of the two words is the original word. To evaluate a selectional preference model (for example on the relationship between a verb and a direct object) we take a test corpus and select all verb tokens. For each verb token (say drive) we select the direct object (e.g., car), concatenated with a confounder word that is its nearest neighbor, the noun with the frequency closest to the original (say house), to make car/house). We then use the selectional preference model to choose which of car and house are more preferred objects of drive, and compute how often the model chooses the correct original object (e.g., car) (Chambers and Jurafsky, 2010). \nAnother evaluation metric is to get human preferences for a test set of verbargument pairs, and have them rate their degree of plausibility. This is usually done by using magnitude estimation, a technique from psychophysics, in which subjects rate the plausibility of an argument proportional to a modulus item. A selectional preference model can then be evaluated by its correlation with the human preferences (Keller and Lapata, 2003). \n19.8 Primitive Decomposition of Predicates \nOne way of thinking about the semantic roles we have discussed through the chapter is that they help us define the roles that arguments play in a decompositional way, based on finite lists of thematic roles (agent, patient, instrument, proto-agent, protopatient, etc.). This idea of decomposing meaning into sets of primitive semantics",
        "chapter": "Semantic Role Labeling",
        "section": "Selectional Restrictions",
        "subsection": "Selectional Preferences",
        "subsubsection": "N/A"
    },
    {
        "content": "The conditional probability model can be computed by parsing a very large corpus (billions of words), and computing co-occurrence counts: how often a given verb occurs with a given noun in a given relation. The conditional probability of an argument noun given a verb for a particular relation $P ( n | nu , r )$ can then be used as a selectional preference metric for that pair of words (Brockmann and Lapata 2003, Keller and Lapata 2003): \nThe inverse probability $P ( nu | n , r )$ was found to have better performance in some cases (Brockmann and Lapata, 2003): \nAn even simpler approach is to use the simple log co-occurrence frequency of the predicate with the argument log count $( nu , n , r )$ instead of conditional probability; this seems to do better for extracting preferences for syntactic subjects rather than objects (Brockmann and Lapata, 2003). \nEvaluating Selectional Preferences \npseudowords \nOne way to evaluate models of selectional preferences is to use pseudowords (Gale et al. 1992c, Schu¨tze 1992a). A pseudoword is an artificial word created by concatenating a test word in some context (say banana) with a confounder word (say door) to create banana-door). The task of the system is to identify which of the two words is the original word. To evaluate a selectional preference model (for example on the relationship between a verb and a direct object) we take a test corpus and select all verb tokens. For each verb token (say drive) we select the direct object (e.g., car), concatenated with a confounder word that is its nearest neighbor, the noun with the frequency closest to the original (say house), to make car/house). We then use the selectional preference model to choose which of car and house are more preferred objects of drive, and compute how often the model chooses the correct original object (e.g., car) (Chambers and Jurafsky, 2010). \nAnother evaluation metric is to get human preferences for a test set of verbargument pairs, and have them rate their degree of plausibility. This is usually done by using magnitude estimation, a technique from psychophysics, in which subjects rate the plausibility of an argument proportional to a modulus item. A selectional preference model can then be evaluated by its correlation with the human preferences (Keller and Lapata, 2003). \n19.8 Primitive Decomposition of Predicates \nOne way of thinking about the semantic roles we have discussed through the chapter is that they help us define the roles that arguments play in a decompositional way, based on finite lists of thematic roles (agent, patient, instrument, proto-agent, protopatient, etc.). This idea of decomposing meaning into sets of primitive semantics \ncomponential analysis \nelements or features, called primitive decomposition or componential analysis, has been taken even further, and focused particularly on predicates. \nConsider these examples of the verb kill:   \n(19.41) Jim killed his philodendron.   \n(19.42) Jim did something to cause his philodendron to become not alive. \nThere is a truth-conditional (‘propositional semantics’) perspective from which these two sentences have the same meaning. Assuming this equivalence, we could represent the meaning of kill as: \nthus using semantic primitives like do, cause, become not, and alive. \nIndeed, one such set of potential semantic primitives has been used to account for some of the verbal alternations discussed in Section 19.2 (Lakoff 1965, Dowty 1979). Consider the following examples. \n(19.44) John opened the door. $Rightarrow$ CAUSE(John, BECOME(OPEN(door))) (19.45) The door opened. $Rightarrow$ BECOME(OPEN(door)) (19.46) The door is open. $Rightarrow$ OPEN(door) \nThe decompositional approach asserts that a single state-like predicate associated with open underlies all of these examples. The differences among the meanings of these examples arises from the combination of this single predicate with the primitives CAUSE and BECOME. \nWhile this approach to primitive decomposition can explain the similarity between states and actions or causative and non-causative predicates, it still relies on having a large number of predicates like open. More radical approaches choose to break down these predicates as well. One such approach to verbal predicate decomposition that played a role in early natural language systems is conceptual dependency (CD), a set of ten primitive predicates, shown in Fig. 19.8. \nBelow is an example sentence along with its CD representation. The verb brought is translated into the two primitives ATRANS and PTRANS to indicate that the waiter both physically conveyed the check to Mary and passed control of it to her. Note that CD also associates a fixed set of thematic roles with each primitive to represent the various participants in the action. \n(19.47) The waiter brought Mary the check. \n∃x, y Atrans(x) ∧ Actor(x,Waiter) ∧ Ob ject(x,Check) ∧ To(x, Mary) ∧Ptrans(y) ∧ Actor(y,Waiter) ∧ Ob ject(y,Check) ∧ To(y, Mary) \n19.9 Summary \n• Semantic roles are abstract models of the role an argument plays in the event described by the predicate.   \n• Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.   \n• Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.   \n• Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class. \nBibliographical and Historical Notes \nAlthough the idea of semantic roles dates back to P¯a .nini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesnie`re’s groundbreaking E´l´ements de Syntaxe Structurale (Tesni`ere, 1959) in which the term ‘dependency’ was introduced and the foundations were laid for dependency grammar. Following Tesnie`re’s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. \nThe idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser.",
        "chapter": "Semantic Role Labeling",
        "section": "Primitive Decomposition of Predicates",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "∃x, y Atrans(x) ∧ Actor(x,Waiter) ∧ Ob ject(x,Check) ∧ To(x, Mary) ∧Ptrans(y) ∧ Actor(y,Waiter) ∧ Ob ject(y,Check) ∧ To(y, Mary) \n19.9 Summary \n• Semantic roles are abstract models of the role an argument plays in the event described by the predicate.   \n• Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.   \n• Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.   \n• Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class. \nBibliographical and Historical Notes \nAlthough the idea of semantic roles dates back to P¯a .nini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesnie`re’s groundbreaking E´l´ements de Syntaxe Structurale (Tesni`ere, 1959) in which the term ‘dependency’ was introduced and the foundations were laid for dependency grammar. Following Tesnie`re’s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. \nThe idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser.",
        "chapter": "Semantic Role Labeling",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "∃x, y Atrans(x) ∧ Actor(x,Waiter) ∧ Ob ject(x,Check) ∧ To(x, Mary) ∧Ptrans(y) ∧ Actor(y,Waiter) ∧ Ob ject(y,Check) ∧ To(y, Mary) \n19.9 Summary \n• Semantic roles are abstract models of the role an argument plays in the event described by the predicate.   \n• Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.   \n• Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.   \n• Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class. \nBibliographical and Historical Notes \nAlthough the idea of semantic roles dates back to P¯a .nini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesnie`re’s groundbreaking E´l´ements de Syntaxe Structurale (Tesni`ere, 1959) in which the term ‘dependency’ was introduced and the foundations were laid for dependency grammar. Following Tesnie`re’s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. \nThe idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser.",
        "chapter": "Semantic Role Labeling",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "∃x, y Atrans(x) ∧ Actor(x,Waiter) ∧ Ob ject(x,Check) ∧ To(x, Mary) ∧Ptrans(y) ∧ Actor(y,Waiter) ∧ Ob ject(y,Check) ∧ To(y, Mary) \n19.9 Summary \n• Semantic roles are abstract models of the role an argument plays in the event described by the predicate.   \n• Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.   \n• Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.   \n• Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class. \nBibliographical and Historical Notes \nAlthough the idea of semantic roles dates back to P¯a .nini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesnie`re’s groundbreaking E´l´ements de Syntaxe Structurale (Tesni`ere, 1959) in which the term ‘dependency’ was introduced and the foundations were laid for dependency grammar. Following Tesnie`re’s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. \nThe idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. \nEach verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). \nBy 1977 case representation was widely used and taught in AI and NLP courses, and was described as a standard of natural language processing in the first edition of Winston’s 1977 textbook Artificial Intelligence. \nIn the 1980s Fillmore proposed his model of frame semantics, later describing the intuition as follows: \n“The idea behind frame semantics is that speakers are aware of possibly quite complex situation types, packages of connected expectations, that go by various names—frames, schemas, scenarios, scripts, cultural narratives, memes—and the words in our language are understood with such frames as their presupposed background.” (Fillmore, 2012, p. 712) \nThe word frame seemed to be in the air for a suite of related notions proposed at about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as well as related notions with other names like scripts (Schank and Abelson, 1975) and schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison). Fillmore was also influenced by the semantic field theorists and by a visit to the Yale AI lab where he took notice of the lists of slots and fillers used by early information extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s Fillmore drew on these insights to begin the FrameNet corpus annotation project. \nAt the same time, Beth Levin drew on her early case frame dictionaries (Levin, 1977) to develop her book which summarized sets of verb classes defined by shared argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus created by Martha Palmer and colleagues (Palmer et al., 2005). \nThe combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and Ma`rquez et al. (2008). \nThe use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and $mathrm { { X u } }$ (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with $mathbf { A } ^ { * }$ decoding that make it possible to apply a wide variety of global constraints in SRL decoding. \nMost semantic role labeling schemes only work within a single sentence, focusing on the object of the verbal (or nominal, in the case of NomBank) predicate. \nimplicit argument \nHowever, in many cases, a verbal or nominal predicate may have an implicit argument: one that appears only in a contextual sentence, or perhaps not at all and must be inferred. In the two sentences This house has a new owner. The sale was finalized 10 days ago. the sale in the second sentence has no ARG1, but a reasonable reader would infer that the Arg1 should be the house mentioned in the prior sentence. Finding these arguments, implicit argument detection (sometimes shortened as iSRL) was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do et al. (2017) for more recent neural models. \nTo avoid the need for huge labeled training sets, unsupervised approaches for semantic role labeling attempt to induce the set of semantic roles by clustering over arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev (2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khoddam (2014). \niSRL \nRecent innovations in frame labeling include connotation frames, which mark richer information about the argument of predicates. Connotation frames mark the sentiment of the writer or reader toward the arguments (for example using the verb survive in he survived a bombing expresses the writer’s sympathy toward the subject he and negative sentiment toward the bombing. See Chapter 20 for more details. \nSelectional preference has been widely studied beyond the selectional association models of Resnik (1993) and Resnik (1996). Methods have included clustering (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic models (Se´aghdha 2010, Ritter et al. 2010b), and constraints can be expressed at the level of words or classes (Agirre and Martinez, 2001). Selectional preferences have also been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al. 2013, Do et al. 2017). \nExercises \nCHAPTER 20 aLnedxicConsnfotratSieontiment, Affect, \nSome day we’ll be able to measure the power of words Maya Angelou \naffective \nsubjectivity \nIn this chapter we turn to tools for interpreting affective meaning, extending our study of sentiment analysis in Chapter 4. We use the word ‘affective’, following the tradition in affective computing (Picard, 1995) to mean emotion, sentiment, personality, mood, and attitudes. Affective meaning is closely related to subjectivity, the study of a speaker or writer’s evaluations, opinions, emotions, and speculations (Wiebe et al., 1999). \nHow should affective meaning be defined? One influential typology of affective states comes from Scherer (2000), who defines each class of affective states by factors like its cognitive realization and time course (Fig. 20.1). \nEmotion: Relatively brief episode of response to the evaluation of an external or internal event as being of major significance. (angry, sad, joyful, fearful, ashamed, proud, elated, desperate)   \nMood: Diffuse affect state, most pronounced as change in subjective feeling, of low intensity but relatively long duration, often without apparent cause. (cheerful, gloomy, irritable, listless, depressed, buoyant)   \nInterpersonal stance: Affective stance taken toward another person in a specific interaction, coloring the interpersonal exchange in that situation. (distant, cold, warm, supportive, contemptuous, friendly)   \nAttitude: Relatively enduring, affectively colored beliefs, preferences, and predispositions towards objects or persons. (liking, loving, hating, valuing, desiring)   \nPersonality traits: Emotionally laden, stable personality dispositions and behavior tendencies, typical for a person. (nervous, anxious, reckless, morose, hostile, jealous) \nWe can design extractors for each of these kinds of affective states. Chapter 4 already introduced sentiment analysis, the task of extracting the positive or negative orientation that a writer expresses in a text. This corresponds in Scherer’s typology to the extraction of attitudes: figuring out what people like or dislike, from affectrich texts like consumer reviews of books or movies, newspaper editorials, or public sentiment in blogs or tweets. \nDetecting emotion and moods is useful for detecting whether a student is confused, engaged, or certain when interacting with a tutorial system, whether a caller to a help line is frustrated, whether someone’s blog posts or tweets indicated depression. Detecting emotions like fear in novels, for example, could help us trace what groups or situations are feared and how that changes over time.",
        "chapter": "Semantic Role Labeling",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "connotations \nDetecting different interpersonal stances can be useful when extracting information from human-human conversations. The goal here is to detect stances like friendliness or awkwardness in interviews or friendly conversations, for example for summarizing meetings or finding parts of a conversation where people are especially excited or engaged, conversational hot spots that can help in meeting summarization. Detecting the personality of a user—such as whether the user is an extrovert or the extent to which they are open to experience— can help improve conversational agents, which seem to work better if they match users’ personality expectations (Mairesse and Walker, 2008). And affect is important for generation as well as recognition; synthesizing affect is important for conversational agents in various domains, including literacy tutors such as children’s storybooks, or computer games. \nIn Chapter 4 we introduced the use of naive Bayes classification to classify a document’s sentiment. Various classifiers have been successfully applied to many of these tasks, using all the words in the training set as input to a classifier which then determines the affect status of the text. \nIn this chapter we focus on an alternative model, in which instead of using every word as a feature, we focus only on certain words, ones that carry particularly strong cues to affect or sentiment. We call these lists of words affective lexicons or sentiment lexicons. These lexicons presuppose a fact about semantics: that words have affective meanings or connotations. The word connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. In addition to their ability to help determine the affective status of a text, connotation lexicons can be useful features for other kinds of affective tasks, and for computational social science analysis. \nIn the next sections we introduce basic theories of emotion, show how sentiment lexicons are a special case of emotion lexicons, and mention some useful lexicons. We then survey three ways for building lexicons: human labeling, semi-supervised, and supervised. Finally, we talk about how to detect affect toward a particular entity, and introduce connotation frames. \n20.1 Defining Emotion \nemotion \nOne of the most important affective classes is emotion, which Scherer (2000) defines as a “relatively brief episode of response to the evaluation of an external or internal event as being of major significance”. \nDetecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. \nComputational models of emotion in NLP have mainly been based on two families of theories of emotion (out of the many studied in the field of affective science). In one of these families, emotions are viewed as fixed atomic units, limited in number, and from which others are generated, often called basic emotions (Tomkins \nbasic emotions \n1962, Plutchik 1962), a model dating back to Darwin. Perhaps the most well-known of this family of theories are the 6 emotions proposed by Ekman (e.g., Ekman 1999) to be universally present in all cultures: surprise, happiness, anger, fear, disgust, sadness. Another atomic theory is the Plutchik (1980) wheel of emotion, consisting of 8 basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with the emotions derived from them, shown in Fig. 20.2. \nThe second class of emotion theories widely used in NLP views emotion as a space in 2 or 3 dimensions (Russell, 1980). Most models include the two dimensions valence and arousal, and many add a third, dominance. These can be defined as: \nvalence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus \nSentiment can be viewed as a special case of this second view of emotions as points in space. In particular, the valence dimension, measuring how pleasant or unpleasant a word is, is often used directly as a measure of sentiment. \nIn these lexicon-based models of affect, the affective meaning of a word is generally fixed, irrespective of the linguistic context in which a word is used, or the dialect or culture of the speaker. By contrast, other models in affective science represent emotions as much richer processes involving cognition (Barrett et al., 2007). In appraisal theory, for example, emotions are complex processes, in which a person considers how an event is congruent with their goals, taking into account variables like the agency, certainty, urgency, novelty and control associated with the event (Moors et al., 2013). Computational models in NLP taking into account these richer theories of emotion will likely play an important role in future work.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Defining Emotion",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "1962, Plutchik 1962), a model dating back to Darwin. Perhaps the most well-known of this family of theories are the 6 emotions proposed by Ekman (e.g., Ekman 1999) to be universally present in all cultures: surprise, happiness, anger, fear, disgust, sadness. Another atomic theory is the Plutchik (1980) wheel of emotion, consisting of 8 basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with the emotions derived from them, shown in Fig. 20.2. \nThe second class of emotion theories widely used in NLP views emotion as a space in 2 or 3 dimensions (Russell, 1980). Most models include the two dimensions valence and arousal, and many add a third, dominance. These can be defined as: \nvalence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus \nSentiment can be viewed as a special case of this second view of emotions as points in space. In particular, the valence dimension, measuring how pleasant or unpleasant a word is, is often used directly as a measure of sentiment. \nIn these lexicon-based models of affect, the affective meaning of a word is generally fixed, irrespective of the linguistic context in which a word is used, or the dialect or culture of the speaker. By contrast, other models in affective science represent emotions as much richer processes involving cognition (Barrett et al., 2007). In appraisal theory, for example, emotions are complex processes, in which a person considers how an event is congruent with their goals, taking into account variables like the agency, certainty, urgency, novelty and control associated with the event (Moors et al., 2013). Computational models in NLP taking into account these richer theories of emotion will likely play an important role in future work. \n20.2 Available Sentiment and Affect Lexicons \nGeneral Inquirer \nA wide variety of affect lexicons have been created and released. The most basic lexicons label words along one dimension of semantic variability, generally called “sentiment” or “valence”. \nIn the simplest lexicons this dimension is represented in a binary fashion, with a wordlist for positive words and a wordlist for negative words. The oldest is the General Inquirer (Stone et al., 1966), which drew on content analysis and on early work in the cognitive psychology of word meaning (Osgood et al., 1957). The General Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe, 2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for reliability (strongly subjective or weakly subjective). The polarity lexicon of $mathrm { H u }$ and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product reviews, labeled using a bootstrapping method from WordNet. \nSlightly more general than these sentiment lexicons are lexicons that assign each word a value on all three affective dimensions. The NRC Valence, Arousal, and Dominance (VAD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dominance scores to 20,000 words. Some examples are shown in Fig. 20.4. \nEmoLex \nThe NRC Word-Emotion Association Lexicon, also called EmoLex (Mohammad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions defined above. The lexicon includes around 14,000 words including words from prior lexicons as well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for some sample words: \nFor a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon (Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy, and sadness; Fig. 20.5 shows examples. \nLIWC \nLIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture aspects of lexical meaning relevant for social psychological tasks. In addition to sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 20.6. \nThere are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated, as well as lexicons for categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation. \nAnother useful feature for various tasks is the distinction between concrete words like banana or bathrobe and abstract words like belief and although. The lexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5 of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5, belief 1.19, although 1.07, and in between words like brisk a 2.5. \n20.3 Creating Affect Lexicons by Human Labeling \ncrowdsourcing \nThe earliest method used to build affect lexicons, and still in common use, is to have humans label each word. This is now most commonly done via crowdsourcing: breaking the task into small pieces and distributing them to a large number of annotators. Let’s take a look at some of the methodological choices for two crowdsourced emotion lexicons.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Available Sentiment and Affect Lexicons",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "For a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon (Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy, and sadness; Fig. 20.5 shows examples. \nLIWC \nLIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture aspects of lexical meaning relevant for social psychological tasks. In addition to sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 20.6. \nThere are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated, as well as lexicons for categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation. \nAnother useful feature for various tasks is the distinction between concrete words like banana or bathrobe and abstract words like belief and although. The lexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5 of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5, belief 1.19, although 1.07, and in between words like brisk a 2.5. \n20.3 Creating Affect Lexicons by Human Labeling \ncrowdsourcing \nThe earliest method used to build affect lexicons, and still in common use, is to have humans label each word. This is now most commonly done via crowdsourcing: breaking the task into small pieces and distributing them to a large number of annotators. Let’s take a look at some of the methodological choices for two crowdsourced emotion lexicons. \n\nThe NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013), labeled emotions in two steps. To ensure that the annotators were judging the correct sense of the word, they first answered a multiple-choice synonym question that primed the correct sense of the word (without requiring the annotator to read a potentially confusing sense definition). These were created automatically using the headwords associated with the thesaurus category of the sense in question in the Macquarie dictionary and the headwords of 3 random distractor categories. An example: \nWhich word is closest in meaning (most related) to startle? \n• automobile • shake • honesty • entertain \nFor each word (e.g. startle), the annotator was then asked to rate how associated that word is with each of the 8 emotions (joy, fear, anger, etc.). The associations were rated on a scale of not, weakly, moderately, and strongly associated. Outlier ratings were removed, and then each term was assigned the class chosen by the majority of the annotators, with ties broken by choosing the stronger intensity, and then the 4 levels were mapped into a binary label for each word (no and weak mapped to 0, moderate and strong mapped to 1). \nThe NRC VAD Lexicon (Mohammad, 2018a) was built by selecting words and emoticons from prior lexicons and annotating them with crowd-sourcing using bestworst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In bestworst scaling, annotators are given N items (usually 4) and are asked which item is the best (highest) and which is the worst (lowest) in terms of some property. The set of words used to describe the ends of the scales are taken from prior literature. For valence, for example, the raters were asked: \nQ1. Which of the four words below is associated with the MOST happiness / pleasure / positiveness / satisfaction / contentedness / hopefulness OR LEAST unhappiness / annoyance / negativeness / dissatisfaction / melancholy / despair? (Four words listed as options.) Q2. Which of the four words below is associated with the LEAST happiness / pleasure / positiveness / satisfaction / contentedness / hopefulness OR MOST unhappiness / annoyance / negativeness / dissatisfaction / melancholy / despair? (Four words listed as options.) \n\nsplit-half reliability \nThe score for each word in the lexicon is the proportion of times the item was chosen as the best (highest V/A/D) minus the proportion of times the item was chosen as the worst (lowest V/A/D). The agreement between annotations are evaluated by splithalf reliability: split the corpus in half and compute the correlations between the annotations in the two halves. \n20.4 Semi-supervised Induction of Affect Lexicons \nAnother common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word $w$ by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. \n20.4.1 Semantic Axis Methods \nOne of the most well-known lexicon induction methods, the Turney and Littman (2003) algorithm, is given seed words like good or bad, and then for each word $w$ to be labeled, measures both how similar it is to good and how different it is from bad. Here we describe a slight extension of the algorithm due to An et al. (2018), which is based on computing a semantic axis. \nIn the first step, we choose seed words by hand. There are two methods for dealing with the fact that the affect of a word is different in different contexts: (1) start with a single large seed lexicon and rely on the induction algorithm to fine-tune it to the domain, or (2) choose different seed words for different genres. Hellrich et al. (2019) suggests that for modeling affect across different historical time periods, starting with a large modern affect dictionary is better than small seedsets tuned to be stable across time. As an example of the second approach, Hamilton et al. (2016a) define one set of seed words for general sentiment analysis, a different set for Twitter, and yet another set for sentiment in financial text:",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Creating Affect Lexicons by Human Labeling",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "split-half reliability \nThe score for each word in the lexicon is the proportion of times the item was chosen as the best (highest V/A/D) minus the proportion of times the item was chosen as the worst (lowest V/A/D). The agreement between annotations are evaluated by splithalf reliability: split the corpus in half and compute the correlations between the annotations in the two halves. \n20.4 Semi-supervised Induction of Affect Lexicons \nAnother common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word $w$ by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. \n20.4.1 Semantic Axis Methods \nOne of the most well-known lexicon induction methods, the Turney and Littman (2003) algorithm, is given seed words like good or bad, and then for each word $w$ to be labeled, measures both how similar it is to good and how different it is from bad. Here we describe a slight extension of the algorithm due to An et al. (2018), which is based on computing a semantic axis. \nIn the first step, we choose seed words by hand. There are two methods for dealing with the fact that the affect of a word is different in different contexts: (1) start with a single large seed lexicon and rely on the induction algorithm to fine-tune it to the domain, or (2) choose different seed words for different genres. Hellrich et al. (2019) suggests that for modeling affect across different historical time periods, starting with a large modern affect dictionary is better than small seedsets tuned to be stable across time. As an example of the second approach, Hamilton et al. (2016a) define one set of seed words for general sentiment analysis, a different set for Twitter, and yet another set for sentiment in financial text: \non a specific corpus (for example using a financial corpus if a finance lexicon is the goal), or we can fine-tune off-the-shelf embeddings to a corpus. Fine-tuning is especially important if we have a very specific genre of text but don’t have enough data to train good embeddings. In fine-tuning, we begin with off-the-shelf embeddings like word2vec, and continue training them on the small target corpus. \nOnce we have embeddings for each pole word, we create an embedding that represents each pole by taking the centroid of the embeddings of each of the seed words; recall that the centroid is the multidimensional version of the mean. Given a set of embeddings for the positive seed words $S ^ { + } = { E ( w _ { 1 } ^ { + } ) , E ( w _ { 2 } ^ { + } ) , . . . , E ( w _ { n } ^ { + } ) }$ , and embeddings for the negative seed words $S ^ { - } = { E ( w _ { 1 } ^ { - } ) , E ( w _ { 2 } ^ { - } ) , . . . , E ( w _ { m } ^ { - } ) }$ , the pole centroids are: \nThe semantic axis defined by the poles is computed just by subtracting the two vectors: \n$pmb { mathsf { v } } _ { a x i s }$ , the semantic axis, is a vector in the direction of positive sentiment. Finally, we compute (via cosine similarity) the angle between the vector in the direction of positive sentiment and the direction of $w$ ’s embedding. A higher cosine means that $w$ is more aligned with $S ^ { + }$ than $S ^ { - }$ . \nIf a dictionary of words with sentiment scores is sufficient, we’re done! Or if we need to group words into a positive and a negative lexicon, we can use a threshold or other method to give us discrete lexicons. \n20.4.2 Label Propagation \nAn alternative family of methods defines lexicons by propagating sentiment labels on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown (1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of Hamilton et al. (2016a), which has four steps: \n1. Define a graph: Given word embeddings, build a weighted lexical graph by connecting each word with its $k$ nearest neighbors (according to cosine similarity). The weights of the edge between words $w _ { i }$ and $w _ { j }$ are set as: \n2. Define a seed set: Choose positive and negative seed words. \n3. Propagate polarities from the seed set: Now we perform a random walk on this graph, starting at the seed set. In a random walk, we start at a node and",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Semi-supervised Induction of Affect Lexicons",
        "subsection": "Semantic Axis Methods",
        "subsubsection": "N/A"
    },
    {
        "content": "on a specific corpus (for example using a financial corpus if a finance lexicon is the goal), or we can fine-tune off-the-shelf embeddings to a corpus. Fine-tuning is especially important if we have a very specific genre of text but don’t have enough data to train good embeddings. In fine-tuning, we begin with off-the-shelf embeddings like word2vec, and continue training them on the small target corpus. \nOnce we have embeddings for each pole word, we create an embedding that represents each pole by taking the centroid of the embeddings of each of the seed words; recall that the centroid is the multidimensional version of the mean. Given a set of embeddings for the positive seed words $S ^ { + } = { E ( w _ { 1 } ^ { + } ) , E ( w _ { 2 } ^ { + } ) , . . . , E ( w _ { n } ^ { + } ) }$ , and embeddings for the negative seed words $S ^ { - } = { E ( w _ { 1 } ^ { - } ) , E ( w _ { 2 } ^ { - } ) , . . . , E ( w _ { m } ^ { - } ) }$ , the pole centroids are: \nThe semantic axis defined by the poles is computed just by subtracting the two vectors: \n$pmb { mathsf { v } } _ { a x i s }$ , the semantic axis, is a vector in the direction of positive sentiment. Finally, we compute (via cosine similarity) the angle between the vector in the direction of positive sentiment and the direction of $w$ ’s embedding. A higher cosine means that $w$ is more aligned with $S ^ { + }$ than $S ^ { - }$ . \nIf a dictionary of words with sentiment scores is sufficient, we’re done! Or if we need to group words into a positive and a negative lexicon, we can use a threshold or other method to give us discrete lexicons. \n20.4.2 Label Propagation \nAn alternative family of methods defines lexicons by propagating sentiment labels on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown (1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of Hamilton et al. (2016a), which has four steps: \n1. Define a graph: Given word embeddings, build a weighted lexical graph by connecting each word with its $k$ nearest neighbors (according to cosine similarity). The weights of the edge between words $w _ { i }$ and $w _ { j }$ are set as: \n2. Define a seed set: Choose positive and negative seed words. \n3. Propagate polarities from the seed set: Now we perform a random walk on this graph, starting at the seed set. In a random walk, we start at a node and \nthen choose a node to move to with probability proportional to the edge probability. A word’s polarity score for a seed set is proportional to the probability of a random walk from the seed set landing on that word (Fig. 20.7). \n4. Create word scores: We walk from both positive and negative seed sets, resulting in positive (rawscore $^ + ( w _ { i } ) )$ ) and negative (rawscore $^ - ( w _ { i } ) )$ ) raw label scores. We then combine these values into a positive-polarity score as: \nIt’s often helpful to standardize the scores to have zero mean and unit variance within a corpus. \n5. Assign confidence to each score: Because sentiment scores are influenced by the seed set, we’d like to know how much the score of a word would change if a different seed set is used. We can use bootstrap sampling to get confidence regions, by computing the propagation $B$ times over random subsets of the positive and negative seed sets (for example using $B = 5 0$ and choosing 7 of the 10 seed words each time). The standard deviation of the bootstrap sampled polarity scores gives a confidence measure. \n20.4.3 Other Methods \nThe core of semisupervised algorithms is the metric for measuring similarity with the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) approaches above used embedding cosine as the distance metric: words were labeled as positive basically if their embeddings had high cosines with positive seeds and low cosines with negative seeds. Other methods have chosen other kinds of distance metrics besides embedding cosine. \nFor example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic cues; two adjectives are considered similar if they were frequently conjoined by and and rarely conjoined by but. This is based on the intuition that adjectives conjoined by the words and tend to have the same polarity; positive adjectives are generally coordinated with positive, negative with negative: \nfair and legitimate, corrupt and brutal but less often positive adjectives coordinated with negative: \n*fair and brutal, *corrupt and legitimate By contrast, adjectives conjoined by but are likely to be of opposite polarity:",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Semi-supervised Induction of Affect Lexicons",
        "subsection": "Label Propagation",
        "subsubsection": "N/A"
    },
    {
        "content": "then choose a node to move to with probability proportional to the edge probability. A word’s polarity score for a seed set is proportional to the probability of a random walk from the seed set landing on that word (Fig. 20.7). \n4. Create word scores: We walk from both positive and negative seed sets, resulting in positive (rawscore $^ + ( w _ { i } ) )$ ) and negative (rawscore $^ - ( w _ { i } ) )$ ) raw label scores. We then combine these values into a positive-polarity score as: \nIt’s often helpful to standardize the scores to have zero mean and unit variance within a corpus. \n5. Assign confidence to each score: Because sentiment scores are influenced by the seed set, we’d like to know how much the score of a word would change if a different seed set is used. We can use bootstrap sampling to get confidence regions, by computing the propagation $B$ times over random subsets of the positive and negative seed sets (for example using $B = 5 0$ and choosing 7 of the 10 seed words each time). The standard deviation of the bootstrap sampled polarity scores gives a confidence measure. \n20.4.3 Other Methods \nThe core of semisupervised algorithms is the metric for measuring similarity with the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) approaches above used embedding cosine as the distance metric: words were labeled as positive basically if their embeddings had high cosines with positive seeds and low cosines with negative seeds. Other methods have chosen other kinds of distance metrics besides embedding cosine. \nFor example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic cues; two adjectives are considered similar if they were frequently conjoined by and and rarely conjoined by but. This is based on the intuition that adjectives conjoined by the words and tend to have the same polarity; positive adjectives are generally coordinated with positive, negative with negative: \nfair and legitimate, corrupt and brutal but less often positive adjectives coordinated with negative: \n*fair and brutal, *corrupt and legitimate By contrast, adjectives conjoined by but are likely to be of opposite polarity: \nfair but brutal \nAnother cue to opposite polarity comes from morphological negation (un-, im-, -less). Adjectives with the same root but differing in a morphological negative (adequate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity. \nYet another method for finding words that have a similar polarity to seed words is to make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b). A word’s synonyms presumably share its polarity while a word’s antonyms probably have the opposite polarity. After a seed lexicon is built, each lexicon is updated as follows, possibly iterated. \n$mathrm { L e x ^ { + } }$ : Add synonyms of positive words (well) and antonyms (like fine) of negative words   \nLex−: Add synonyms of negative words (awful) and antonyms (like evil) of positive words \nAn extension of this algorithm assigns polarity to WordNet senses, called SentiWordNet (Baccianella et al., 2010). Fig. 20.8 shows some examples. \nIn this algorithm, polarity is assigned to entire synsets rather than words. A positive lexicon is built from all the synsets associated with 7 positive words, and a negative lexicon from synsets associated with 7 negative words. A classifier is then trained from this data to take a WordNet gloss and decide if the sense being defined is positive, negative or neutral. A further step (involving a random-walk algorithm) assigns a score to each WordNet synset for its degree of positivity, negativity, and neutrality. \nIn summary, semisupervised algorithms use a human-defined set of seed words for the two poles of a dimension, and use similarity metrics like embedding cosine, coordination, morphology, or thesaurus structure to score words by how similar they are to the positive seeds and how dissimilar to the negative seeds. \n20.5 Supervised Learning of Word Sentiment \nSemi-supervised methods require only minimal human supervision (in the form of seed sets). But sometimes a supervision signal exists in the world and can be made use of. One such signal is the scores associated with online reviews. \nThe web contains an enormous number of online reviews for restaurants, movies, books, or other products, each of which have the text of the review along with an associated review score: a value that may range from 1 star to 5 stars, or scoring 1 to 10. Fig. 20.9 shows samples extracted from restaurant, book, and movie reviews.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Semi-supervised Induction of Affect Lexicons",
        "subsection": "Other Methods",
        "subsubsection": "N/A"
    },
    {
        "content": "Potts diagram \nhood $P ( w | c )$ over all categories gives a Potts score of 0.10. The word disappointing thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The Potts diagram (Potts, 2011) is a visualization of these word scores, representing the prior sentiment of a word as a distribution over the rating categories. \nFig. 20.10 shows the Potts diagrams for 3 positive and 3 negative scalar adjectives. Note that the curve for strongly positive scalars have the shape of the letter J, while strongly negative scalars look like a reverse J. By contrast, weakly positive and negative scalars have a hump-shape, with the maximum either below the mean (weakly negative words like disappointing) or above the mean (weakly positive words like good). These shapes offer an illuminating typology of affective meaning. \nFig. 20.11 shows the Potts diagrams for emphasizing and attenuating adverbs. Note that emphatics tend to have a J-shape (most likely to occur in the most positive reviews) or a U-shape (most likely to occur in the strongly positive and negative). Attenuators all have the hump-shape, emphasizing the middle of the scale and downplaying both extremes. The diagrams can be used both as a typology of lexical sentiment, and also play a role in modeling sentiment compositionality. \nIn addition to functions like posterior $P ( c | boldsymbol { w } )$ , likelihood $P ( w | c )$ , or normalized likelihood (Eq. 20.6) many other functions of the count of a word occurring with a sentiment label have been used. We’ll introduce some of these on page 440, including ideas like normalizing the counts per writer in Eq. 20.14. \n20.5.1 Log Odds Ratio Informative Dirichlet Prior \nOne thing we often want to do with word polarity is to distinguish between words that are more likely to be used in one category of texts than in another. We may, for example, want to know the words most associated with 1 star reviews versus those associated with 5 star reviews. These differences may not be just related to sentiment. We might want to find words used more often by Democratic than Republican members of Congress, or words used more often in menus of expensive restaurants than cheap restaurants. \n\nGiven two classes of documents, to find words more associated with one category than another, we could measure the difference in frequencies (is a word $w$ more frequent in class $A$ or class $B mathrm { ? }$ ). Or instead of the difference in frequencies we could compute the ratio of frequencies, or compute the log odds ratio (the log of the ratio between the odds of the two words). We could then sort words by whichever association measure we pick, ranging from words overrepresented in category $A$ to words overrepresented in category $B$ . \nThe problem with simple log-likelihood or log odds methods is that they don’t work well for very rare words or very frequent words; for words that are very frequent, all differences seem large, and for words that are very rare, no differences seem large. \nIn this section we walk through the details of one solution to this problem: the “log odds ratio informative Dirichlet prior” method of Monroe et al. (2008) that is a particularly useful method for finding words that are statistically overrepresented in one particular category of texts compared to another. It’s based on the idea of using another large corpus to get a prior estimate of what we expect the frequency of each word to be. \nLet’s start with the goal: assume we want to know whether the word horrible occurs more in corpus $i$ or corpus $j$ . We could compute the log likelihood ratio, using $f ^ { i } ( w )$ to mean the frequency of word $w$ in corpus $i$ , and $n ^ { i }$ to mean the total number of words in corpus $i$ : \nlog odds ratio \nInstead, let’s compute the log odds ratio: does horrible have higher odds in $i$ or in \n$j$ : \nThe Dirichlet intuition is to use a large background corpus to get a prior estimate of what we expect the frequency of each word $w$ to be. We’ll do this very simply by adding the counts from that corpus to the numerator and denominator, so that we’re essentially shrinking the counts toward that prior. It’s like asking how large are the differences between $i$ and $j$ given what we would expect given their frequencies in a well-estimated large background corpus. \nThe method estimates the difference between the frequency of word $w$ in two corpora $i$ and $j$ via the prior-modified log odds ratio for $w$ , $delta _ { w } ^ { ( i - j ) }$ , which is estimated as: \n(where $n ^ { i }$ is the size of corpus $i , n ^ { j }$ is the size of corpus $j , f _ { w } ^ { i }$ is the count of word $w$ in corpus $i$ , $f _ { w } ^ { j }$ is the count of word $w$ in corpus $j , alpha _ { 0 }$ is the size of the background corpus, and $alpha _ { w }$ is the count of word $w$ in the background corpus.) \nIn addition, Monroe et al. (2008) make use of an estimate for the variance of the log–odds–ratio: \nThe final statistic for a word is then the $textbf { em z }$ –score of its log–odds–ratio: \nThe Monroe et al. (2008) method thus modifies the commonly used log odds ratio in two ways: it uses the $mathbf { z }$ -scores of the log odds ratio, which controls for the amount of variance in a word’s frequency, and it uses counts from a background corpus to provide a prior count for words. \nFig. 20.12 shows the method applied to a dataset of restaurant reviews from Yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews (Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the 1-star reviews using negative sentiment words like worse, bad, awful and the 5-star reviews using positive sentiment words like great, best, amazing. But there are other illuminating differences. 1-star reviews use logical negation (no, not), while 5-star reviews use emphatics and emphasize universality (very, highly, every, always). 1- star reviews use first person plurals (we, us, our) while 5 star reviews use the second person. 1-star reviews talk about people (manager, waiter, customer) while 5-star reviews talk about dessert and properties of expensive restaurants like courses and atmosphere. See Jurafsky et al. (2014) for more details. \n20.6 Using Lexicons for Sentiment Recognition \nIn Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection. \nIn the simplest case, lexicons can be used when we don’t have sufficient training data to build a supervised sentiment analyzer; it can often be expensive to have a human assign sentiment to each document to train the supervised classifier. \nIn such situations, lexicons can be used in a rule-based algorithm for classification. The simplest version is just to use the ratio of positive to negative words: if a document has more positive than negative words (using the lexicon to decide the polarity of each word in the document), it is classified as positive. Often a threshold $lambda$ is used, in which a document is classified as positive only if the ratio is greater than $lambda$ . If the sentiment lexicon includes positive and negative weights for each word, $theta _ { w } ^ { + }$ and $theta _ { w } ^ { - }$ , these can be used as well. Here’s a simple such sentiment algorithm: \nIf supervised training data is available, these counts computed from sentiment lexicons, sometimes weighted or normalized in various ways, can also be used as features in a classifier along with other lexical or non-lexical features. We return to such algorithms in Section 20.7.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Supervised Learning of Word Sentiment",
        "subsection": "Log Odds Ratio Informative Dirichlet Prior",
        "subsubsection": "N/A"
    },
    {
        "content": "20.6 Using Lexicons for Sentiment Recognition \nIn Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection. \nIn the simplest case, lexicons can be used when we don’t have sufficient training data to build a supervised sentiment analyzer; it can often be expensive to have a human assign sentiment to each document to train the supervised classifier. \nIn such situations, lexicons can be used in a rule-based algorithm for classification. The simplest version is just to use the ratio of positive to negative words: if a document has more positive than negative words (using the lexicon to decide the polarity of each word in the document), it is classified as positive. Often a threshold $lambda$ is used, in which a document is classified as positive only if the ratio is greater than $lambda$ . If the sentiment lexicon includes positive and negative weights for each word, $theta _ { w } ^ { + }$ and $theta _ { w } ^ { - }$ , these can be used as well. Here’s a simple such sentiment algorithm: \nIf supervised training data is available, these counts computed from sentiment lexicons, sometimes weighted or normalized in various ways, can also be used as features in a classifier along with other lexical or non-lexical features. We return to such algorithms in Section 20.7. \n20.7 Using Lexicons for Affect Recognition \nDetection of emotion (and the other kinds of affective meaning described by Scherer (2000)) can be done by generalizing the algorithms described above for detecting sentiment. \nThe most common algorithms involve supervised classification: a training set is labeled for the affective meaning to be detected, and a classifier is built using features extracted from the training set. As with sentiment analysis, if the training set is large enough, and the test set is sufficiently similar to the training set, simply using all the words or all the bigrams as features in a powerful classifier like SVM or logistic regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose performance is hard to beat. Thus we can treat affective meaning classification of a text sample as simple document classification. \nSome modifications are nonetheless often necessary for very large datasets. For example, the Schwartz et al. (2013) study of personality, gender, and age using 700 million words of Facebook posts used only a subset of the n-grams of lengths 1- 3. Only words and phrases used by at least $1 %$ of the subjects were included as features, and 2-grams and 3-grams were only kept if they had sufficiently high PMI (PMI greater than $^ { 2 * }$ length, where length is the number of words): \nVarious weights can be used for the features, including the raw count in the training set, or some normalized probability or log probability. Schwartz et al. (2013), for example, turn feature counts into phrase likelihoods by normalizing them by each subject’s total word use. \nIf the training data is sparser, or not as similar to the test set, any of the lexicons we’ve discussed can play a helpful role, either alone or in combination with all the words and $mathfrak { n }$ -grams. \nMany possible values can be used for lexicon features. The simplest is just an indicator function, in which the value of a feature $f _ { L }$ takes the value 1 if a particular text has any word from the relevant lexicon $L$ . Using the notation of Chapter 4, in which a feature value is defined for a particular output class $c$ and document $x$ . \nAlternatively the value of a feature $f _ { L }$ for a particular lexicon $L$ can be the total number of word tokens in the document that occur in $L$ : \nFor lexica in which each word is associated with a score or weight, the count can be multiplied by a weight $theta _ { w } ^ { L }$ :",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Using Lexicons for Sentiment Recognition",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "20.7 Using Lexicons for Affect Recognition \nDetection of emotion (and the other kinds of affective meaning described by Scherer (2000)) can be done by generalizing the algorithms described above for detecting sentiment. \nThe most common algorithms involve supervised classification: a training set is labeled for the affective meaning to be detected, and a classifier is built using features extracted from the training set. As with sentiment analysis, if the training set is large enough, and the test set is sufficiently similar to the training set, simply using all the words or all the bigrams as features in a powerful classifier like SVM or logistic regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose performance is hard to beat. Thus we can treat affective meaning classification of a text sample as simple document classification. \nSome modifications are nonetheless often necessary for very large datasets. For example, the Schwartz et al. (2013) study of personality, gender, and age using 700 million words of Facebook posts used only a subset of the n-grams of lengths 1- 3. Only words and phrases used by at least $1 %$ of the subjects were included as features, and 2-grams and 3-grams were only kept if they had sufficiently high PMI (PMI greater than $^ { 2 * }$ length, where length is the number of words): \nVarious weights can be used for the features, including the raw count in the training set, or some normalized probability or log probability. Schwartz et al. (2013), for example, turn feature counts into phrase likelihoods by normalizing them by each subject’s total word use. \nIf the training data is sparser, or not as similar to the test set, any of the lexicons we’ve discussed can play a helpful role, either alone or in combination with all the words and $mathfrak { n }$ -grams. \nMany possible values can be used for lexicon features. The simplest is just an indicator function, in which the value of a feature $f _ { L }$ takes the value 1 if a particular text has any word from the relevant lexicon $L$ . Using the notation of Chapter 4, in which a feature value is defined for a particular output class $c$ and document $x$ . \nAlternatively the value of a feature $f _ { L }$ for a particular lexicon $L$ can be the total number of word tokens in the document that occur in $L$ : \nFor lexica in which each word is associated with a score or weight, the count can be multiplied by a weight $theta _ { w } ^ { L }$ : \nCounts can alternatively be logged or normalized per writer as in Eq. 20.14. \nHowever they are defined, these lexicon features are then used in a supervised classifier to predict the desired affective category for the text or document. Once a classifier is trained, we can examine which lexicon features are associated with which classes. For a classifier like logistic regression the feature weight gives an indication of how associated the feature is with the class. \n20.8 Lexicon-based methods for Entity-Centric Affect \nWhat if we want to get an affect score not for an entire document, but for a particular entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines affect lexicons with contextual embeddings to assign an affect score to an entity in text. In the context of affect about people, they relabel the Valence/Arousal/Dominance dimension as Sentiment/Agency/Power. The algorithm first trains classifiers to map embeddings to scores: \n1. For each word $w$ in the training corpus: \n(a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual embedding e for each instance of the word. No additional fine-tuning is done.   \n(b) Average over the e embeddings of each instance of $w$ to obtain a single embedding vector for one training point $w$ .   \n(c) Use the NRC VAD Lexicon to get S, A, and $mathrm { bf P }$ scores for $w$ . \n2. Train (three) regression models on all words $w$ to predict V, A, D scores from a word’s average embedding. \nNow given an entity mention $m$ in a text, we assign affect scores as follows: \n1. Use the same pretrained LM to get contextual embeddings for $m$ in context. 2. Feed this embedding through the 3 regression models to get S, A, $mathrm { bf P }$ scores for the entity. \nThis results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 20.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference. \n20.9 Connotation Frames \nconnotation frame \nThe lexicons we’ve described so far define a word as a point in affective space. A connotation frame, by contrast, is a lexicon that incorporates a richer kind of grammatical structure, by combining affective lexicons with the frame semantic lexicons of Chapter 10. The basic insight of connotation frame lexicons is that a predicate like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016, Rashkin et al. 2017). \nConsider sentences like: \n(20.15) Country A violated the sovereignty of Country B (20.16) the teenager ... survived the Boston Marathon bombing”",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Using Lexicons for Affect Recognition",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Counts can alternatively be logged or normalized per writer as in Eq. 20.14. \nHowever they are defined, these lexicon features are then used in a supervised classifier to predict the desired affective category for the text or document. Once a classifier is trained, we can examine which lexicon features are associated with which classes. For a classifier like logistic regression the feature weight gives an indication of how associated the feature is with the class. \n20.8 Lexicon-based methods for Entity-Centric Affect \nWhat if we want to get an affect score not for an entire document, but for a particular entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines affect lexicons with contextual embeddings to assign an affect score to an entity in text. In the context of affect about people, they relabel the Valence/Arousal/Dominance dimension as Sentiment/Agency/Power. The algorithm first trains classifiers to map embeddings to scores: \n1. For each word $w$ in the training corpus: \n(a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual embedding e for each instance of the word. No additional fine-tuning is done.   \n(b) Average over the e embeddings of each instance of $w$ to obtain a single embedding vector for one training point $w$ .   \n(c) Use the NRC VAD Lexicon to get S, A, and $mathrm { bf P }$ scores for $w$ . \n2. Train (three) regression models on all words $w$ to predict V, A, D scores from a word’s average embedding. \nNow given an entity mention $m$ in a text, we assign affect scores as follows: \n1. Use the same pretrained LM to get contextual embeddings for $m$ in context. 2. Feed this embedding through the 3 regression models to get S, A, $mathrm { bf P }$ scores for the entity. \nThis results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 20.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference. \n20.9 Connotation Frames \nconnotation frame \nThe lexicons we’ve described so far define a word as a point in affective space. A connotation frame, by contrast, is a lexicon that incorporates a richer kind of grammatical structure, by combining affective lexicons with the frame semantic lexicons of Chapter 10. The basic insight of connotation frame lexicons is that a predicate like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016, Rashkin et al. 2017). \nConsider sentences like: \n(20.15) Country A violated the sovereignty of Country B (20.16) the teenager ... survived the Boston Marathon bombing”",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Lexicon-based methods for Entity-Centric Affect",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Counts can alternatively be logged or normalized per writer as in Eq. 20.14. \nHowever they are defined, these lexicon features are then used in a supervised classifier to predict the desired affective category for the text or document. Once a classifier is trained, we can examine which lexicon features are associated with which classes. For a classifier like logistic regression the feature weight gives an indication of how associated the feature is with the class. \n20.8 Lexicon-based methods for Entity-Centric Affect \nWhat if we want to get an affect score not for an entire document, but for a particular entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines affect lexicons with contextual embeddings to assign an affect score to an entity in text. In the context of affect about people, they relabel the Valence/Arousal/Dominance dimension as Sentiment/Agency/Power. The algorithm first trains classifiers to map embeddings to scores: \n1. For each word $w$ in the training corpus: \n(a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual embedding e for each instance of the word. No additional fine-tuning is done.   \n(b) Average over the e embeddings of each instance of $w$ to obtain a single embedding vector for one training point $w$ .   \n(c) Use the NRC VAD Lexicon to get S, A, and $mathrm { bf P }$ scores for $w$ . \n2. Train (three) regression models on all words $w$ to predict V, A, D scores from a word’s average embedding. \nNow given an entity mention $m$ in a text, we assign affect scores as follows: \n1. Use the same pretrained LM to get contextual embeddings for $m$ in context. 2. Feed this embedding through the 3 regression models to get S, A, $mathrm { bf P }$ scores for the entity. \nThis results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 20.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference. \n20.9 Connotation Frames \nconnotation frame \nThe lexicons we’ve described so far define a word as a point in affective space. A connotation frame, by contrast, is a lexicon that incorporates a richer kind of grammatical structure, by combining affective lexicons with the frame semantic lexicons of Chapter 10. The basic insight of connotation frame lexicons is that a predicate like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016, Rashkin et al. 2017). \nConsider sentences like: \n(20.15) Country A violated the sovereignty of Country B (20.16) the teenager ... survived the Boston Marathon bombing” \n\nBy using the verb violate in (20.15), the author is expressing their sympathvieys wDiteh Country B, portraying Country B as a victim, and expressing antagonismRatochwealrd the agent Country A. By contrast, in using the verb survive, the author of (20.16) is expressing that the bombing is a negative experience, and the subject of the sentence, the teenager, iserafusly,mwpaethceatinc cshpaercauctleart.eTtheastetahsepecotsrpofrcaonunsoetda iton are inherent in the meaningtroafitnheEvLerMbso vainoldatBeEanRdTsuprovritvrea,yasthsehomwans npoFiwge.r2f0u.l1.4. \nThe connotation frame lexicons of Rashkin et al. (2016) and Rashkeisnt atgael.n (2017) also express other connotative aspects of the predicate toward eamchovairegupment, including the effect (something bad happened to x) value: (x is valuable), and mental state: ( $mathbf { dot { X } }$ is distressed by the event). Connotation frames can also mark the power differentFiianlablleyt,wewene hqeualrigtautmiveenltys ausnianlgyzthe hvoerwb iwmepll oeumr eans that the theme argumenmt ehtahsogdreactaeprtpuorewseratfhfaencthediamgents),ioansd tbhye agneanlcyzionfgeach argument (waited is low agency). Fig. 20.15 shows a visualization from Sap et al. (2l0o17w)-. \nConnotation frames can be built by hand (Sap et al., 2017), or they can bderlievaerrn.eId by supervised learning (Rashkin et al., 2016), for example using hand-labelbedt twraeiening data to superviasepocsliatisosinfioefrslefsosrpeoawcehr tohfatnhteheintdhievmideu(a“lthreltrait-ions,Fieg.ugr.e, 3w: Setahmepr S(writer $ mathrm { R o l e l } $ )buisn $^ +$ )o.  -n, caondt atshte,n“Hime pdreomviangdeadccthueratcriybuvnia globwaitlhchoingsh ranintost across all relations. \n\n20.10 Summary \n• Many kinds of affective states can be distinguished, including emorost”iothnes thmeomoeds(, attitudes (whviicahteinfrcolumdtehesewnteillm-kenot)w, ninBtercphedreslotensatl(sBteacnhcdee,l,and pwerirtseorniamliptlyi.e   \n• Emotion can19b8e6)r.epIrne epnartteidculbayr fiwxedfiantdo tmhiact uhnigiths aogfetencnycalled basic emotions, or as points in space defined by dimensions like valence and arousal.   \n• Words have cmonvineos (aet.igo.,naSlnaoswpeWcths re) atcecdidteonttahlleysep asfsfetchteive satgartees,meantd sth0i.s3 connotationalBeacshpdeeclt toefstwaonrd amlseoanbiencgaucsaenebvenremproevsiesntewidthin lexicons.   \n• Affective lexicons can be built by hand, using crowd sourcinveg btodleanboetles hwe affective content of each word.   \n• Lexicons can2beCbouinltnowtiatth osenmFir-saumpesrvoifsePdo,wbeorotasntrdapping froFomr sexeadmpwloe,rdas using similarity mAegterincsylike embedding cosine.   \n• Lexicons canWbeecrleatrentewdo n eaw cuollnynostuatpioernvriesleatdiomnsa,npnoerw,erwhen a convenient training signalndc angebnecfyo(uenxda imnptlehse iwnoFrilgdu,rseu3c)h, as rant enxgpsaans-signed by users on a review site.   \n• Words can bewaitshsipglnaecedhwolediegrhsttsoinavaoliedxgiecnodnerbybiuasiingthvearcionu-s funhcitgihonasgeonfcwyoarsd counts in tratienxitn(ge.tge.,xtXs  asncduerdatYi;oanmetxraicmspliektea lkoig sohdodwsnratiao iangfeonrcmy(aAtiGv)e Dirichlet prinort.h   \n• Affect can be detected, just like sentiment, by using standard s5u6p%e ainsde 5t1ex% classificationPotewcehrniDqiuffesr,eunstinalgs allMthaneywvoerdbs iormpbliygrtahemsaui-n a tteixvtelays. eDaetsuprietse. Additional fetahtourirteys lceavnelbseofd tahwe nagfreontmacnodutnhtesmoef rweloartdivse ntolexic9o4n%s   \n• Lexicons can al2shotbtep:u/s/ehdotmoesd.ectse.cwtasfhfienctgitno na. erdule/-˜bmassaepd/ classinfioetratboryspriacrkeliyngs the simple mmaojovriety-bsieansti/ment based on counts of words in eachSolemxeicon.t   \n• Connotationhformaesm.ces.weaxsphreisnsgrtiocnh.erdure/l˜amtisoanps/ moofvaiffee-cbtivaes/.meanincgltuhdae  taheprseudbicate encodes about its arguments.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Connotation Frames",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "20.10 Summary \n• Many kinds of affective states can be distinguished, including emorost”iothnes thmeomoeds(, attitudes (whviicahteinfrcolumdtehesewnteillm-kenot)w, ninBtercphedreslotensatl(sBteacnhcdee,l,and pwerirtseorniamliptlyi.e   \n• Emotion can19b8e6)r.epIrne epnartteidculbayr fiwxedfiantdo tmhiact uhnigiths aogfetencnycalled basic emotions, or as points in space defined by dimensions like valence and arousal.   \n• Words have cmonvineos (aet.igo.,naSlnaoswpeWcths re) atcecdidteonttahlleysep asfsfetchteive satgartees,meantd sth0i.s3 connotationalBeacshpdeeclt toefstwaonrd amlseoanbiencgaucsaenebvenremproevsiesntewidthin lexicons.   \n• Affective lexicons can be built by hand, using crowd sourcinveg btodleanboetles hwe affective content of each word.   \n• Lexicons can2beCbouinltnowtiatth osenmFir-saumpesrvoifsePdo,wbeorotasntrdapping froFomr sexeadmpwloe,rdas using similarity mAegterincsylike embedding cosine.   \n• Lexicons canWbeecrleatrentewdo n eaw cuollnynostuatpioernvriesleatdiomnsa,npnoerw,erwhen a convenient training signalndc angebnecfyo(uenxda imnptlehse iwnoFrilgdu,rseu3c)h, as rant enxgpsaans-signed by users on a review site.   \n• Words can bewaitshsipglnaecedhwolediegrhsttsoinavaoliedxgiecnodnerbybiuasiingthvearcionu-s funhcitgihonasgeonfcwyoarsd counts in tratienxitn(ge.tge.,xtXs  asncduerdatYi;oanmetxraicmspliektea lkoig sohdodwsnratiao iangfeonrcmy(aAtiGv)e Dirichlet prinort.h   \n• Affect can be detected, just like sentiment, by using standard s5u6p%e ainsde 5t1ex% classificationPotewcehrniDqiuffesr,eunstinalgs allMthaneywvoerdbs iormpbliygrtahemsaui-n a tteixvtelays. eDaetsuprietse. Additional fetahtourirteys lceavnelbseofd tahwe nagfreontmacnodutnhtesmoef rweloartdivse ntolexic9o4n%s   \n• Lexicons can al2shotbtep:u/s/ehdotmoesd.ectse.cwtasfhfienctgitno na. erdule/-˜bmassaepd/ classinfioetratboryspriacrkeliyngs the simple mmaojovriety-bsieansti/ment based on counts of words in eachSolemxeicon.t   \n• Connotationhformaesm.ces.weaxsphreisnsgrtiocnh.erdure/l˜amtisoanps/ moofvaiffee-cbtivaes/.meanincgltuhdae  taheprseudbicate encodes about its arguments.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "20.10 Summary \n• Many kinds of affective states can be distinguished, including emorost”iothnes thmeomoeds(, attitudes (whviicahteinfrcolumdtehesewnteillm-kenot)w, ninBtercphedreslotensatl(sBteacnhcdee,l,and pwerirtseorniamliptlyi.e   \n• Emotion can19b8e6)r.epIrne epnartteidculbayr fiwxedfiantdo tmhiact uhnigiths aogfetencnycalled basic emotions, or as points in space defined by dimensions like valence and arousal.   \n• Words have cmonvineos (aet.igo.,naSlnaoswpeWcths re) atcecdidteonttahlleysep asfsfetchteive satgartees,meantd sth0i.s3 connotationalBeacshpdeeclt toefstwaonrd amlseoanbiencgaucsaenebvenremproevsiesntewidthin lexicons.   \n• Affective lexicons can be built by hand, using crowd sourcinveg btodleanboetles hwe affective content of each word.   \n• Lexicons can2beCbouinltnowtiatth osenmFir-saumpesrvoifsePdo,wbeorotasntrdapping froFomr sexeadmpwloe,rdas using similarity mAegterincsylike embedding cosine.   \n• Lexicons canWbeecrleatrentewdo n eaw cuollnynostuatpioernvriesleatdiomnsa,npnoerw,erwhen a convenient training signalndc angebnecfyo(uenxda imnptlehse iwnoFrilgdu,rseu3c)h, as rant enxgpsaans-signed by users on a review site.   \n• Words can bewaitshsipglnaecedhwolediegrhsttsoinavaoliedxgiecnodnerbybiuasiingthvearcionu-s funhcitgihonasgeonfcwyoarsd counts in tratienxitn(ge.tge.,xtXs  asncduerdatYi;oanmetxraicmspliektea lkoig sohdodwsnratiao iangfeonrcmy(aAtiGv)e Dirichlet prinort.h   \n• Affect can be detected, just like sentiment, by using standard s5u6p%e ainsde 5t1ex% classificationPotewcehrniDqiuffesr,eunstinalgs allMthaneywvoerdbs iormpbliygrtahemsaui-n a tteixvtelays. eDaetsuprietse. Additional fetahtourirteys lceavnelbseofd tahwe nagfreontmacnodutnhtesmoef rweloartdivse ntolexic9o4n%s   \n• Lexicons can al2shotbtep:u/s/ehdotmoesd.ectse.cwtasfhfienctgitno na. erdule/-˜bmassaepd/ classinfioetratboryspriacrkeliyngs the simple mmaojovriety-bsieansti/ment based on counts of words in eachSolemxeicon.t   \n• Connotationhformaesm.ces.weaxsphreisnsgrtiocnh.erdure/l˜amtisoanps/ moofvaiffee-cbtivaes/.meanincgltuhdae  taheprseudbicate encodes about its arguments.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "20.10 Summary \n• Many kinds of affective states can be distinguished, including emorost”iothnes thmeomoeds(, attitudes (whviicahteinfrcolumdtehesewnteillm-kenot)w, ninBtercphedreslotensatl(sBteacnhcdee,l,and pwerirtseorniamliptlyi.e   \n• Emotion can19b8e6)r.epIrne epnartteidculbayr fiwxedfiantdo tmhiact uhnigiths aogfetencnycalled basic emotions, or as points in space defined by dimensions like valence and arousal.   \n• Words have cmonvineos (aet.igo.,naSlnaoswpeWcths re) atcecdidteonttahlleysep asfsfetchteive satgartees,meantd sth0i.s3 connotationalBeacshpdeeclt toefstwaonrd amlseoanbiencgaucsaenebvenremproevsiesntewidthin lexicons.   \n• Affective lexicons can be built by hand, using crowd sourcinveg btodleanboetles hwe affective content of each word.   \n• Lexicons can2beCbouinltnowtiatth osenmFir-saumpesrvoifsePdo,wbeorotasntrdapping froFomr sexeadmpwloe,rdas using similarity mAegterincsylike embedding cosine.   \n• Lexicons canWbeecrleatrentewdo n eaw cuollnynostuatpioernvriesleatdiomnsa,npnoerw,erwhen a convenient training signalndc angebnecfyo(uenxda imnptlehse iwnoFrilgdu,rseu3c)h, as rant enxgpsaans-signed by users on a review site.   \n• Words can bewaitshsipglnaecedhwolediegrhsttsoinavaoliedxgiecnodnerbybiuasiingthvearcionu-s funhcitgihonasgeonfcwyoarsd counts in tratienxitn(ge.tge.,xtXs  asncduerdatYi;oanmetxraicmspliektea lkoig sohdodwsnratiao iangfeonrcmy(aAtiGv)e Dirichlet prinort.h   \n• Affect can be detected, just like sentiment, by using standard s5u6p%e ainsde 5t1ex% classificationPotewcehrniDqiuffesr,eunstinalgs allMthaneywvoerdbs iormpbliygrtahemsaui-n a tteixvtelays. eDaetsuprietse. Additional fetahtourirteys lceavnelbseofd tahwe nagfreontmacnodutnhtesmoef rweloartdivse ntolexic9o4n%s   \n• Lexicons can al2shotbtep:u/s/ehdotmoesd.ectse.cwtasfhfienctgitno na. erdule/-˜bmassaepd/ classinfioetratboryspriacrkeliyngs the simple mmaojovriety-bsieansti/ment based on counts of words in eachSolemxeicon.t   \n• Connotationhformaesm.ces.weaxsphreisnsgrtiocnh.erdure/l˜amtisoanps/ moofvaiffee-cbtivaes/.meanincgltuhdae  taheprseudbicate encodes about its arguments. \nBibliographical and Historical Notes \nsubjectivity \nThe idea of formally representing the subjective meaning of words began with Osgood et al. (1957), the same pioneering study that first proposed the vector space model of meaning described in Chapter 6. Osgood et al. (1957) had participants rate words on various scales, and ran factor analysis on the ratings. The most significant factor they uncovered was the evaluative dimension, which distinguished between pairs like good/bad, valuable/worthless, pleasant/unpleasant. This work influenced the development of early dictionaries of sentiment and affective meaning in the field of content analysis (Stone et al., 1966). \nWiebe (1994) began an influential line of work on detecting subjectivity in text, beginning with the task of identifying subjective sentences and the subjective characters who are described in the text as holding private states, beliefs or attitudes. Learned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and McKeown (1997) were shown to be a useful feature in subjectivity detection (Hatzivassiloglou and Wiebe 2000, Wiebe 2000). \nThe term sentiment seems to have been introduced in 2001 by Das and Chen (2001), to describe the task of measuring market sentiment by looking at the words in stock trading message boards. In the same paper Das and Chen (2001) also proposed the use of a sentiment lexicon. The list of words in the lexicon was created by hand, but each word was assigned weights according to how much it discriminated a particular class (say buy versus sell) by maximizing across-class variation and minimizing within-class variation. The term sentiment, and the use of lexicons, caught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) first showed the power of using all the words without a sentiment lexicon; see also Wang and Manning (2012). \nMost of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a, Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997, Fast et al. 2016). \nMuch recent work focuses on ways to learn embeddings that directly encode sentiment or other properties, such as the DENSIFIER algorithm of Rothe et al. (2016) that learns to transform the embedding space to focus on sentiment (or other) information. \nExercises \n20.1 Show that the relationship between a word $w$ and a category $c$ in the Potts Score in Eq. 20.6 is a variant of the pointwise mutual information $mathrm { p m i } ( w , c )$ without the log term. \n21 Coreference Resolution \nand even Stigand, the patriotic archbishop of Canterbury, found it advisable–”’ ‘Found WHAT?’ said the Duck. ‘Found IT,’ the Mouse replied rather crossly: ‘of course you know what “it”means.’ ‘I know what “it”means well enough, when I find a thing,’ said the Duck: ‘it’s generally a frog or a worm. The question is, what did the archbishop find?’ \nLewis Carroll, Alice in Wonderland \nmention referent corefer \n\nAn important component of language processing is knowing who is being talked about in a text. Consider the following passage: \n(21.1) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $$ 2.3$ million, as the 38-year-old became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks. \nEach of the underlined phrases in this passage is used by the writer to refer to a person named Victoria Chen. We call linguistic expressions like her or Victoria Chen mentions or referring expressions, and the discourse entity that is referred to (Victoria Chen) the referent. (To distinguish between referring expressions and their referents, we italicize the former.)1 Two or more referring expressions that are used to refer to the same discourse entity are said to corefer; thus, Victoria Chen and she corefer in (21.1). \ndiscourse model \nCoreference is an important component of natural language processing. A dialogue system that has just told the user “There is a 2pm flight on United and a 4pm one on Cathay Pacific” must know which flight the user means by “I’ll take the second one”. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence “She was born in Warsaw”. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence ‘“Me encanta el conocimiento”, dice.’ should be translated as ‘“I love knowledge”, he says’, or ‘“I love knowledge”, she says’. Indeed, this example comes from an actual news article in El Pa´ıs about a female professor and was mistranslated as “he” in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). \nNatural language processing systems (and humans) interpret linguistic expressions with respect to a discourse model (Karttunen, 1969). A discourse model (Fig. 21.1) is a mental model that the understander builds incrementally when interpreting a text, containing representations of the entities referred to in the text, as well as properties of the entities and relations among them. When a referent is first mentioned in a discourse, we say that a representation for it is evoked into the model. Upon subsequent mention, this representation is accessed from the model.",
        "chapter": "Lexicons for Sentiment, Affect, and Connotation",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "21.1 Coreference Phenomena: Linguistic Background \nWe now offer some linguistic background on reference phenomena. We introduce the four types of referring expressions (definite and indefinite NPs, pronouns, and names), describe how these are used to evoke and access entities in the discourse model, and talk about linguistic features of the anaphor/antecedent relation (like number/gender agreement, or properties of verb semantics). \n21.1.1 Types of Referring Expressions \nIndefinite Noun Phrases: The most common form of indefinite reference in English is marked with the determiner $a$ (or an), but it can also be marked by a quantifier such as some or even the determiner this. Indefinite reference generally introduces into the discourse context entities that are new to the hearer. \n(21.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose. b. He had gone round one day to bring her some walnuts. c. I saw this beautiful cauliflower today. \nDefinite Noun Phrases: Definite reference, such as via NPs that use the English article the, refers to an entity that is identifiable to the hearer. An entity can be identifiable to the hearer because it has been mentioned previously in the text and thus is already represented in the discourse model: \n(21.7) It concerns a white stallion which I have sold to an officer. But the pedigree of the white stallion was not fully established. \nAlternatively, an entity can be identifiable because it is contained in the hearer’s set of beliefs about the world, or the uniqueness of the object is implied by the description itself, in which case it evokes a representation of the referent into the discourse model, as in (21.9): \n(21.8) I read about it in the New York Times. (21.9) Have you seen the car keys? \nThese last uses are quite common; more than half of definite NPs in newswire texts are non-anaphoric, often because they are the first time an entity is mentioned (Poesio and Vieira 1998, Bean and Riloff 1999). \nPronouns: Another form of definite reference is pronominalization, used for entities that are extremely salient in the discourse, (as we discuss below): \n(21.10) Emma smiled and chatted as cheerfully as she could, \ncataphora \nPronouns can also participate in cataphora, in which they are mentioned before their referents are, as in (21.11). \n(21.11) Even before she saw it, Dorothy had been thinking about the Emerald City every day. \nbound \nHere, the pronouns she and it both occur before their referents are introduced. \nPronouns also appear in quantified contexts in which they are considered to be bound, as in (21.12). \n(21.12) Every dancer brought her left arm forward. \nUnder the relevant reading, her does not refer to some woman in context, but instead behaves like a variable bound to the quantified expression every dancer. We are not concerned with the bound interpretation of pronouns in this chapter. \nIn some languages, pronouns can appear as clitics attached to a word, like lo (‘it’) in this Spanish example from AnCora (Recasens and Mart´ı, 2010): \n(21.13) La intencio´n es reconocer el gran prestigio que tiene la marato´n y unirlo con esta gran carrera. ‘The aim is to recognize the great prestige that the Marathon has and join|it with this great race.” \nDemonstrative Pronouns: Demonstrative pronouns this and that can appear either alone or as determiners, for instance, this ingredient, that spice: \n(21.14) I just bought a copy of Thoreau’s Walden. I had bought one five years ago. That one had been very tattered; this one was in much better condition. \nNote that this NP is ambiguous; in colloquial spoken English, it can be indefinite, as in (21.6), or definite, as in (21.14). \nZero Anaphora: Instead of using a pronoun, in some languages (including Chinese, Japanese, and Italian) it is possible to have an anaphor that has no lexical realization at all, called a zero anaphor or zero pronoun, as in the following Italian and Japanese examples from Poesio et al. (2016): \n(21.15) EN $[ mathrm { J o h n } ] _ { i }$ went to visit some friends. On the way $[ mathrm { h e } ] _ { i }$ bought some wine. IT [Giovanni]i ando` a far visita a degli amici. Per via $phi _ { i }$ compro` del vino. JA [John]i-wa yujin-o houmon-sita. Tochu-de $phi _ { i }$ wain-o ka-tta. \nor this Chinese example: \n(21.16) [我] 前一会精神上太紧张。[0] 现在比较平静了 [I] was too nervous a while ago. ... [0] am now calmer. \nZero anaphors complicate the task of mention detection in these languages. \nNames: Names (such as of people, locations, or organizations) can be used to refer to both new and old entities in the discourse: \na. Miss Woodhouse certainly had not done him justice. b. International Business Machines sought patent compensation from Amazon; IBM had previously sued other companies. \ninformation status discourse-new discourse-old \n21.1.2 Information Status \nThe way referring expressions are used to evoke new referents into the discourse (introducing new information), or access old entities from the model (old information), is called their information status or information structure. Entities can be discourse-new or discourse-old, and indeed it is common to distinguish at least three kinds of entities informationally (Prince, 1981): \nnew NPs: \nbrand new NPs: these introduce entities that are discourse-new and hearernew like a fruit or some walnuts. unused NPs: these introduce entities that are discourse-new but hearer-old (like Hong Kong, Marie Curie, or the New York Times. old NPs: also called evoked NPs, these introduce entities that already in the discourse model, hence are both discourse-old and hearer-old, like it in “I went to a new restaurant. It was...”.",
        "chapter": "Coreference Resolution",
        "section": "Coreference Phenomena: Linguistic Background",
        "subsection": "Types of Referring Expressions",
        "subsubsection": "N/A"
    },
    {
        "content": "In some languages, pronouns can appear as clitics attached to a word, like lo (‘it’) in this Spanish example from AnCora (Recasens and Mart´ı, 2010): \n(21.13) La intencio´n es reconocer el gran prestigio que tiene la marato´n y unirlo con esta gran carrera. ‘The aim is to recognize the great prestige that the Marathon has and join|it with this great race.” \nDemonstrative Pronouns: Demonstrative pronouns this and that can appear either alone or as determiners, for instance, this ingredient, that spice: \n(21.14) I just bought a copy of Thoreau’s Walden. I had bought one five years ago. That one had been very tattered; this one was in much better condition. \nNote that this NP is ambiguous; in colloquial spoken English, it can be indefinite, as in (21.6), or definite, as in (21.14). \nZero Anaphora: Instead of using a pronoun, in some languages (including Chinese, Japanese, and Italian) it is possible to have an anaphor that has no lexical realization at all, called a zero anaphor or zero pronoun, as in the following Italian and Japanese examples from Poesio et al. (2016): \n(21.15) EN $[ mathrm { J o h n } ] _ { i }$ went to visit some friends. On the way $[ mathrm { h e } ] _ { i }$ bought some wine. IT [Giovanni]i ando` a far visita a degli amici. Per via $phi _ { i }$ compro` del vino. JA [John]i-wa yujin-o houmon-sita. Tochu-de $phi _ { i }$ wain-o ka-tta. \nor this Chinese example: \n(21.16) [我] 前一会精神上太紧张。[0] 现在比较平静了 [I] was too nervous a while ago. ... [0] am now calmer. \nZero anaphors complicate the task of mention detection in these languages. \nNames: Names (such as of people, locations, or organizations) can be used to refer to both new and old entities in the discourse: \na. Miss Woodhouse certainly had not done him justice. b. International Business Machines sought patent compensation from Amazon; IBM had previously sued other companies. \ninformation status discourse-new discourse-old \n21.1.2 Information Status \nThe way referring expressions are used to evoke new referents into the discourse (introducing new information), or access old entities from the model (old information), is called their information status or information structure. Entities can be discourse-new or discourse-old, and indeed it is common to distinguish at least three kinds of entities informationally (Prince, 1981): \nnew NPs: \nbrand new NPs: these introduce entities that are discourse-new and hearernew like a fruit or some walnuts. unused NPs: these introduce entities that are discourse-new but hearer-old (like Hong Kong, Marie Curie, or the New York Times. old NPs: also called evoked NPs, these introduce entities that already in the discourse model, hence are both discourse-old and hearer-old, like it in “I went to a new restaurant. It was...”. \ninferrables: these introduce entities that are neither hearer-old nor discourse-old, but the hearer can infer their existence by reasoning based on other entities that are in the discourse. Consider the following examples: \nbridging inference \n(21.18) I went to a superb restaurant yesterday. The chef had just opened it.   \n(21.19) Mix flour, butter and water. Knead the dough until shiny. \nNeither the chef nor the dough were in the discourse model based on the first sentence of either example, but the reader can make a bridging inference that these entities should be added to the discourse model and associated with the restaurant and the ingredients, based on world knowledge that restaurants have chefs and dough is the result of mixing flour and liquid (Haviland and Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018). \ngiven-new \naccessible \nsalience \nThe form of an NP gives strong clues to its information status. We often talk about an entity’s position on the given-new dimension, the extent to which the referent is given (salient in the discourse, easier for the hearer to call to mind, predictable by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976, Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) i.e., very salient in the hearer’s mind or easy to call to mind, can be referred to with less linguistic material. For example pronouns are used only when the referent has a high degree of activation or salience in the discourse model.4 By contrast, less salient entities, like a new referent being introduced to the discourse, will need to be introduced with a longer and more explicit referring expression to help the hearer recover the referent. \nThus when an entity is first introduced into a discourse its mentions are likely to have full names, titles or roles, or appositive or restrictive relative clauses, as in the introduction of our protagonist in (21.1): Victoria Chen, CFO of Megabucks Banking. As an entity is discussed over a discourse, it becomes more salient to the hearer and its mentions on average typically becomes shorter and less informative, for example with a shortened name (for example Ms. Chen), a definite description (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b, Reichman 1985, Fox 1993). \n21.1.3 Complications: Non-Referring Expressions \nMany noun phrases or other nominals are not referring expressions, although they may bear a confusing superficial resemblance. For example in some of the earliest computational work on reference resolution, Karttunen (1969) pointed out that the NP a car in the following example does not create a discourse referent: \n(21.20) Janet doesn’t have a car. \nand cannot be referred back to by anaphoric it or the car: \n(21.21) $^ { * } I t$ is a Toyota.   \n(21.22) *The car is red. \nWe summarize here four common types of structures that are not counted as mentions in coreference tasks and hence complicate the task of mention-detection:",
        "chapter": "Coreference Resolution",
        "section": "Coreference Phenomena: Linguistic Background",
        "subsection": "Information Status",
        "subsubsection": "N/A"
    },
    {
        "content": "inferrables: these introduce entities that are neither hearer-old nor discourse-old, but the hearer can infer their existence by reasoning based on other entities that are in the discourse. Consider the following examples: \nbridging inference \n(21.18) I went to a superb restaurant yesterday. The chef had just opened it.   \n(21.19) Mix flour, butter and water. Knead the dough until shiny. \nNeither the chef nor the dough were in the discourse model based on the first sentence of either example, but the reader can make a bridging inference that these entities should be added to the discourse model and associated with the restaurant and the ingredients, based on world knowledge that restaurants have chefs and dough is the result of mixing flour and liquid (Haviland and Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018). \ngiven-new \naccessible \nsalience \nThe form of an NP gives strong clues to its information status. We often talk about an entity’s position on the given-new dimension, the extent to which the referent is given (salient in the discourse, easier for the hearer to call to mind, predictable by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976, Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) i.e., very salient in the hearer’s mind or easy to call to mind, can be referred to with less linguistic material. For example pronouns are used only when the referent has a high degree of activation or salience in the discourse model.4 By contrast, less salient entities, like a new referent being introduced to the discourse, will need to be introduced with a longer and more explicit referring expression to help the hearer recover the referent. \nThus when an entity is first introduced into a discourse its mentions are likely to have full names, titles or roles, or appositive or restrictive relative clauses, as in the introduction of our protagonist in (21.1): Victoria Chen, CFO of Megabucks Banking. As an entity is discussed over a discourse, it becomes more salient to the hearer and its mentions on average typically becomes shorter and less informative, for example with a shortened name (for example Ms. Chen), a definite description (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b, Reichman 1985, Fox 1993). \n21.1.3 Complications: Non-Referring Expressions \nMany noun phrases or other nominals are not referring expressions, although they may bear a confusing superficial resemblance. For example in some of the earliest computational work on reference resolution, Karttunen (1969) pointed out that the NP a car in the following example does not create a discourse referent: \n(21.20) Janet doesn’t have a car. \nand cannot be referred back to by anaphoric it or the car: \n(21.21) $^ { * } I t$ is a Toyota.   \n(21.22) *The car is red. \nWe summarize here four common types of structures that are not counted as mentions in coreference tasks and hence complicate the task of mention-detection: \nAppositives: An appositional structure is a noun phrase that appears next to a head noun phrase, describing the head. In English they often appear in commas, like “a unit of UAL” appearing in apposition to the NP United, or CFO of Megabucks Banking in apposition to Victoria Chen. \n(21.23) Victoria Chen, CFO of Megabucks Banking, saw ...   \n(21.24) United, a unit of UAL, matched the fares. \nAppositional NPs are not referring expressions, instead functioning as a kind of supplementary parenthetical description of the head NP. Nonetheless, sometimes it is useful to link these phrases to an entity they describe, and so some datasets like OntoNotes mark appositional relationships. \nPredicative and Prenominal NPs: Predicative or attributive NPs describe properties of the head noun. In United is a unit of UAL, the NP a unit of UAL describes a property of United, rather than referring to a distinct entity. Thus they are not marked as mentions in coreference tasks; in our example the NPs $$ 2.3$ million and the company’s president, are attributive, describing properties of her pay and the 38-year-old; Example (21.27) shows a Chinese example in which the predicate NP (中国最大的城市; China’s biggest city) is not a mention. \n(21.25) her pay jumped to $$ 2.3$ million (21.26) the 38-year-old became the company’s president (21.27) 上海是[中国最大的城市] [Shanghai is China’s biggest city] \nExpletives: Many uses of pronouns like it in English and corresponding pronouns in other languages are not referential. Such expletive or pleonastic cases include it is raining, in idioms like hit it off, or in particular syntactic situations like clefts (21.28a) or extraposition (21.28b): \na. It was Emma Goldman who founded Mother Earth b. It surprised me that there was a herring hanging on her wall. \nGenerics: Another kind of expression that does not refer back to an entity explicitly evoked in the text is generic reference. Consider (21.29). \n(21.29) I love mangos. They are very tasty. \nHere, they refers, not to a particular mango or set of mangos, but instead to the class of mangos in general. The pronoun you can also be used generically: \n(21.30) In July in San Francisco you have to wear a jacket. \n21.1.4 Linguistic Properties of the Coreference Relation \nNow that we have seen the linguistic properties of individual referring expressions we turn to properties of the antecedent/anaphor pair. Understanding these properties is helpful both in designing novel features and performing error analyses. \nNumber Agreement: Referring expressions and their referents must generally agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plural, and you is unspecified for number. So a plural antecedent like the chefs cannot generally corefer with a singular anaphor like she. However, algorithms cannot enforce number agreement too strictly. First, semantically plural entities can be referred to by either $i t$ or they: \n(21.31) IBM announced a new machine translation product yesterday. They have been working on it for 20 years.",
        "chapter": "Coreference Resolution",
        "section": "Coreference Phenomena: Linguistic Background",
        "subsection": "Complications: Non-Referring Expressions",
        "subsubsection": "N/A"
    },
    {
        "content": "Appositives: An appositional structure is a noun phrase that appears next to a head noun phrase, describing the head. In English they often appear in commas, like “a unit of UAL” appearing in apposition to the NP United, or CFO of Megabucks Banking in apposition to Victoria Chen. \n(21.23) Victoria Chen, CFO of Megabucks Banking, saw ...   \n(21.24) United, a unit of UAL, matched the fares. \nAppositional NPs are not referring expressions, instead functioning as a kind of supplementary parenthetical description of the head NP. Nonetheless, sometimes it is useful to link these phrases to an entity they describe, and so some datasets like OntoNotes mark appositional relationships. \nPredicative and Prenominal NPs: Predicative or attributive NPs describe properties of the head noun. In United is a unit of UAL, the NP a unit of UAL describes a property of United, rather than referring to a distinct entity. Thus they are not marked as mentions in coreference tasks; in our example the NPs $$ 2.3$ million and the company’s president, are attributive, describing properties of her pay and the 38-year-old; Example (21.27) shows a Chinese example in which the predicate NP (中国最大的城市; China’s biggest city) is not a mention. \n(21.25) her pay jumped to $$ 2.3$ million (21.26) the 38-year-old became the company’s president (21.27) 上海是[中国最大的城市] [Shanghai is China’s biggest city] \nExpletives: Many uses of pronouns like it in English and corresponding pronouns in other languages are not referential. Such expletive or pleonastic cases include it is raining, in idioms like hit it off, or in particular syntactic situations like clefts (21.28a) or extraposition (21.28b): \na. It was Emma Goldman who founded Mother Earth b. It surprised me that there was a herring hanging on her wall. \nGenerics: Another kind of expression that does not refer back to an entity explicitly evoked in the text is generic reference. Consider (21.29). \n(21.29) I love mangos. They are very tasty. \nHere, they refers, not to a particular mango or set of mangos, but instead to the class of mangos in general. The pronoun you can also be used generically: \n(21.30) In July in San Francisco you have to wear a jacket. \n21.1.4 Linguistic Properties of the Coreference Relation \nNow that we have seen the linguistic properties of individual referring expressions we turn to properties of the antecedent/anaphor pair. Understanding these properties is helpful both in designing novel features and performing error analyses. \nNumber Agreement: Referring expressions and their referents must generally agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plural, and you is unspecified for number. So a plural antecedent like the chefs cannot generally corefer with a singular anaphor like she. However, algorithms cannot enforce number agreement too strictly. First, semantically plural entities can be referred to by either $i t$ or they: \n(21.31) IBM announced a new machine translation product yesterday. They have been working on it for 20 years. \nsingular they \nSecond, singular they has become much more common, in which they is used to describe singular individuals, often useful because they is gender neutral. Although recently increasing, singular they is quite old, part of English for many centuries.5 \nPerson Agreement: English distinguishes between first, second, and third person, and a pronoun’s antecedent must agree with the pronoun in person. Thus a third person pronoun (he, she, they, him, her, them, his, her, their) must have a third person antecedent (one of the above or any other noun phrase). However, phenomena like quotation can cause exceptions; in this example I, my, and she are coreferent: \n(21.32) “I voted for Nader because he was most aligned with my values,” she said. \nGender or Noun Class Agreement: In many languages, all nouns have grammatical gender or noun class6 and pronouns generally agree with the grammatical gender of their antecedent. In English this occurs only with third-person singular pronouns, which distinguish between male (he, him, his), female (she, her), and nonpersonal $( i t )$ grammatical genders. Non-binary pronouns like ze or hir may also occur in more recent texts. Knowing which gender to associate with a name in text can be complex, and may require world knowledge about the individual. Some examples: \n(21.33) Maryam has a theorem. She is exciting. (she $ c =$ Maryam, not the theorem) (21.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam) \nBinding Theory Constraints: The binding theory is a name for syntactic constraints on the relations between a mention and an antecedent in the same sentence (Chomsky, 1981). Oversimplifying a bit, reflexive pronouns like himself and herself corefer with the subject of the most immediate clause that contains them (21.35), whereas nonreflexives cannot corefer with this subject (21.36). \nreflexive \n(21.35) Janet bought herself a bottle of fish sauce. [herself $fallingdotseq$ Janet] (21.36) Janet bought her a bottle of fish sauce. [her=Janet] \nRecency: Entities introduced in recent utterances tend to be more salient than those introduced from utterances further back. Thus, in (21.37), the pronoun $i t$ is more likely to refer to Jim’s map than the doctor’s map. \n(21.37) The doctor found an old map in the captain’s chest. Jim found an even older map hidden on the shelf. It described an island. \nGrammatical Role: Entities mentioned in subject position are more salient than those in object position, which are in turn more salient than those mentioned in oblique positions. Thus although the first sentence in (21.38) and (21.39) expresses roughly the same propositional content, the preferred referent for the pronoun he varies with the subject—John in (21.38) and Bill in (21.39). \n(21.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of rum. [ he $mathbf { tau } = mathbf { tau }$ Billy ] \n(21.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of rum. $[ { mathrm { ~ h e } } = { mathrm { J i m } } $ ] \nVerb Semantics: Some verbs semantically emphasize one of their arguments, biasing the interpretation of subsequent pronouns. Compare (21.40) and (21.41). \n(21.40) John telephoned Bill. He lost the laptop.   \n(21.41) John criticized Bill. He lost the laptop. \nThese examples differ only in the verb used in the first sentence, yet “he” in (21.40) is typically resolved to John, whereas “he” in (21.41) is resolved to Bill. This may be partly due to the link between implicit causality and saliency: the implicit cause of a “criticizing” event is its object, whereas the implicit cause of a “telephoning” event is its subject. In such verbs, the entity which is the implicit cause may be more salient. \nSelectional Restrictions: Many other kinds of semantic knowledge can play a role in referent preference. For example, the selectional restrictions that a verb places on its arguments (Chapter 10) can help eliminate referents, as in (21.42). \n(21.42) I ate the soup in my new bowl after cooking it for hours \nThere are two possible referents for it, the soup and the bowl. The verb eat, however, requires that its direct object denote something edible, and this constraint can rule out bowl as a possible referent. \n21.2 Coreference Tasks and Datasets \nWe can formulate the task of coreference resolution as follows: Given a text $T$ , find all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference annotations on $T$ . \nLet’s return to our coreference example, now using superscript numbers for each coreference chain (cluster), and subscript letters for individual mentions in the cluster: \n(21.43) [Victoria $mathrm { C h e n l } _ { a } ^ { 1 }$ , CFO of [Megabucks Banking $ ] _ { a } ^ { 2 }$ , saw $[ [ mathrm { h e r } ] _ { b } ^ { 1 }$ pay $] _ { a } ^ { 3 }$ jump to $$ 2.3$ million, as [the 38-year-old $| _ { c } ^ { 1 }$ also became [[the company $] _ { b } ^ { 2 }$ ’s president. It is widely known that [she $] _ { d } ^ { 1 }$ came to [Megabucks] $] _ { c } ^ { 2 }$ from rival [Lotsabucks $] _ { a } ^ { 4 }$ . \nAssuming example (21.43) was the entirety of the article, the chains for her pay and Lotsabucks are singleton mentions: \n1. Victoria Chen, her, the 38-year-old, She   \n2. {Megabucks Banking, the company, Megabucks}   \n3. { her pay}   \n4. Lotsabucks \nFor most coreference evaluation campaigns, the input to the system is the raw text of articles, and systems must detect mentions and then link them into clusters. Solving this task requires dealing with pronominal anaphora (figuring out that her refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic $I t$ in It has been ten years), dealing with definite noun phrases to figure out that the 38-year-old is coreferent with Victoria Chen, and that the company is the same as Megabucks. And we need to deal with names, to realize that Megabucks is the same as Megabucks Banking.",
        "chapter": "Coreference Resolution",
        "section": "Coreference Phenomena: Linguistic Background",
        "subsection": "Linguistic Properties of the Coreference Relation",
        "subsubsection": "N/A"
    },
    {
        "content": "Verb Semantics: Some verbs semantically emphasize one of their arguments, biasing the interpretation of subsequent pronouns. Compare (21.40) and (21.41). \n(21.40) John telephoned Bill. He lost the laptop.   \n(21.41) John criticized Bill. He lost the laptop. \nThese examples differ only in the verb used in the first sentence, yet “he” in (21.40) is typically resolved to John, whereas “he” in (21.41) is resolved to Bill. This may be partly due to the link between implicit causality and saliency: the implicit cause of a “criticizing” event is its object, whereas the implicit cause of a “telephoning” event is its subject. In such verbs, the entity which is the implicit cause may be more salient. \nSelectional Restrictions: Many other kinds of semantic knowledge can play a role in referent preference. For example, the selectional restrictions that a verb places on its arguments (Chapter 10) can help eliminate referents, as in (21.42). \n(21.42) I ate the soup in my new bowl after cooking it for hours \nThere are two possible referents for it, the soup and the bowl. The verb eat, however, requires that its direct object denote something edible, and this constraint can rule out bowl as a possible referent. \n21.2 Coreference Tasks and Datasets \nWe can formulate the task of coreference resolution as follows: Given a text $T$ , find all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference annotations on $T$ . \nLet’s return to our coreference example, now using superscript numbers for each coreference chain (cluster), and subscript letters for individual mentions in the cluster: \n(21.43) [Victoria $mathrm { C h e n l } _ { a } ^ { 1 }$ , CFO of [Megabucks Banking $ ] _ { a } ^ { 2 }$ , saw $[ [ mathrm { h e r } ] _ { b } ^ { 1 }$ pay $] _ { a } ^ { 3 }$ jump to $$ 2.3$ million, as [the 38-year-old $| _ { c } ^ { 1 }$ also became [[the company $] _ { b } ^ { 2 }$ ’s president. It is widely known that [she $] _ { d } ^ { 1 }$ came to [Megabucks] $] _ { c } ^ { 2 }$ from rival [Lotsabucks $] _ { a } ^ { 4 }$ . \nAssuming example (21.43) was the entirety of the article, the chains for her pay and Lotsabucks are singleton mentions: \n1. Victoria Chen, her, the 38-year-old, She   \n2. {Megabucks Banking, the company, Megabucks}   \n3. { her pay}   \n4. Lotsabucks \nFor most coreference evaluation campaigns, the input to the system is the raw text of articles, and systems must detect mentions and then link them into clusters. Solving this task requires dealing with pronominal anaphora (figuring out that her refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic $I t$ in It has been ten years), dealing with definite noun phrases to figure out that the 38-year-old is coreferent with Victoria Chen, and that the company is the same as Megabucks. And we need to deal with names, to realize that Megabucks is the same as Megabucks Banking. \nExactly what counts as a mention and what links are annotated differs from task to task and dataset to dataset. For example some coreference datasets do not label singletons, making the task much simpler. Resolvers can achieve much higher scores on corpora without singletons, since singletons constitute the majority of mentions in running text, and they are often hard to distinguish from non-referential NPs. Some tasks use gold mention-detection (i.e. the system is given human-labeled mention boundaries and the task is just to cluster these gold mentions), which eliminates the need to detect and segment mentions from running text. \nCoreference is usually evaluated by the CoNLL F1 score, which combines three metrics: MUC, $B ^ { 3 }$ , and $C E A F _ { e }$ ; Section 21.7 gives the details. \nLet’s mention a few characteristics of one popular coreference dataset, OntoNotes (Pradhan et al. $2 0 0 7 mathrm { c }$ , Pradhan et al. 2007a), and the CoNLL 2012 Shared Task based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese and English coreference datasets of roughly one million words each, consisting of newswire, magazine articles, broadcast news, broadcast conversations, web data and conversational speech data, as well as about 300,000 words of annotated Arabic newswire. The most important distinguishing characteristic of OntoNotes is that it does not label singletons, simplifying the coreference task, since singletons represent $6 0 % - 7 0 %$ of all entities. In other ways, it is similar to other coreference datasets. Referring expression NPs that are coreferent are marked as mentions, but generics and pleonastic pronouns are not marked. Appositive clauses are not marked as separate mentions, but they are included in the mention. Thus in the NP, “Richard Godown, president of the Industrial Biotechnology Association” the mention is the entire phrase. Prenominal modifiers are annotated as separate entities only if they are proper nouns. Thus wheat is not an entity in wheat fields, but UN is an entity in UN policy (but not adjectives like American in American policy). \nA number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart´ı, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations. \n21.3 Mention Detection \nmention detection \nThe first stage of coreference is mention detection: finding the spans of text that constitute each mention. Mention detection algorithms are usually very liberal in proposing candidate mentions (i.e., emphasizing recall), and only filtering later. For example many systems run parsers and named entity taggers on the text and extract every span that is either an NP, a possessive pronoun, or a named entity. \nDoing so from our sample text repeated in (21.44): \n(21.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $$ 2.3$ million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks.",
        "chapter": "Coreference Resolution",
        "section": "Coreference Tasks and Datasets",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Exactly what counts as a mention and what links are annotated differs from task to task and dataset to dataset. For example some coreference datasets do not label singletons, making the task much simpler. Resolvers can achieve much higher scores on corpora without singletons, since singletons constitute the majority of mentions in running text, and they are often hard to distinguish from non-referential NPs. Some tasks use gold mention-detection (i.e. the system is given human-labeled mention boundaries and the task is just to cluster these gold mentions), which eliminates the need to detect and segment mentions from running text. \nCoreference is usually evaluated by the CoNLL F1 score, which combines three metrics: MUC, $B ^ { 3 }$ , and $C E A F _ { e }$ ; Section 21.7 gives the details. \nLet’s mention a few characteristics of one popular coreference dataset, OntoNotes (Pradhan et al. $2 0 0 7 mathrm { c }$ , Pradhan et al. 2007a), and the CoNLL 2012 Shared Task based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese and English coreference datasets of roughly one million words each, consisting of newswire, magazine articles, broadcast news, broadcast conversations, web data and conversational speech data, as well as about 300,000 words of annotated Arabic newswire. The most important distinguishing characteristic of OntoNotes is that it does not label singletons, simplifying the coreference task, since singletons represent $6 0 % - 7 0 %$ of all entities. In other ways, it is similar to other coreference datasets. Referring expression NPs that are coreferent are marked as mentions, but generics and pleonastic pronouns are not marked. Appositive clauses are not marked as separate mentions, but they are included in the mention. Thus in the NP, “Richard Godown, president of the Industrial Biotechnology Association” the mention is the entire phrase. Prenominal modifiers are annotated as separate entities only if they are proper nouns. Thus wheat is not an entity in wheat fields, but UN is an entity in UN policy (but not adjectives like American in American policy). \nA number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart´ı, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations. \n21.3 Mention Detection \nmention detection \nThe first stage of coreference is mention detection: finding the spans of text that constitute each mention. Mention detection algorithms are usually very liberal in proposing candidate mentions (i.e., emphasizing recall), and only filtering later. For example many systems run parsers and named entity taggers on the text and extract every span that is either an NP, a possessive pronoun, or a named entity. \nDoing so from our sample text repeated in (21.44): \n(21.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $$ 2.3$ million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks. \n\nmight result in the following list of 13 potential mentions: \nMore recent mention detection systems are even more generous; the span-based algorithm we will describe in Section 21.6 first extracts literally all n-gram spans of words up to ${ bf N } = { bf 1 0 }$ . Of course recall from Section 21.1.3 that many NPs—and the overwhelming majority of random n-gram spans—are not referring expressions. Therefore all such mention detection systems need to eventually filter out pleonastic/expletive pronouns like $I t$ above, appositives like CFO of Megabucks Banking Inc, or predicate nominals like the company’s president or $$ 2.3$ million. \nSome of this filtering can be done by rules. Early rule-based systems designed regular expressions to deal with pleonastic $i t$ , like the following rules from Lappin and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe, know, anticipate) to capture pleonastic $i t$ in “It is thought that ketchup...”, or modal adjectives (e.g., necessary, possible, certain, important), for, e.g., “It is likely that I...”. Such rules are sometimes used as part of modern systems: \nIt is Modaladjective that S It is Modaladjective (for NP) to VP It is Cogv-ed that S It seems/appears/means/follows (that) S \nMention-detection rules are sometimes designed specifically for particular evaluation campaigns. For OntoNotes, for example, mentions are not embedded within larger mentions, and while numeric quantities are annotated, they are rarely coreferential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a common first pass rule-based mention detection algorithm (Lee et al., 2013) is: \n1. Take all NPs, possessive pronouns, and named entities. 2. Remove numeric quantities (100 dollars, $8 %$ ), mentions embedded in larger mentions, adjectival forms of nations, and stop words (like there). 3. Remove pleonastic $i t$ based on regular expression patterns. \nRule-based systems, however, are generally insufficient to deal with mentiondetection, and so modern systems incorporate some sort of learned mention detection component, such as a referentiality classifier, an anaphoricity classifier— detecting whether an NP is an anaphor—or a discourse-new classifier— detecting whether a mention is discourse-new and a potential antecedent for a future anaphor. \nAn anaphoricity detector, for example, can draw its positive training examples from any span that is labeled as an anaphoric referring expression in hand-labeled datasets like OntoNotes, ARRAU, or AnCora. Any other NP or named entity can be marked as a negative training example. Anaphoricity classifiers use features of the candidate mention such as its head word, surrounding words, definiteness, animacy, length, position in the sentence/discourse, many of which were first proposed in early work by $mathrm { N g }$ and Cardie (2002a); see Section 21.5 for more on features. \nReferentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference system. The end result of such a filtering mention detection system on our example above might be the following filtered set of 9 potential mentions: \nIt turns out, however, that hard filtering of mentions based on an anaphoricity or referentiality classifier leads to poor performance. If the anaphoricity classifier threshold is set too high, too many mentions are filtered out and recall suffers. If the classifier threshold is set too low, too many pleonastic or non-referential mentions are included and precision suffers. \nThe modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model $mathrm { N g } ~ 2 0 0 5 mathrm { b }$ , Denis and Baldridge 2007, Rahman and $mathrm { N g }  2 0 0 9 .$ ). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We’ll describe this method in detail in Section 21.6. 7 \nDespite these advances, correctly detecting referential mentions seems to still be an unsolved problem, since systems incorrectly marking pleonastic pronouns like it and other non-referential NPs as coreferent is a large source of errors of modern coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube 2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a). \nMention, referentiality, or anaphoricity detection is thus an important open area of investigation. Other sources of knowledge may turn out to be helpful, especially in combination with unsupervised and semisupervised algorithms, which also mitigate the expense of labeled datasets. In early work, for example Bean and Riloff (1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by extracting and generalizing over the first NPs in a text, which are guaranteed to be non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in the training data but never appear as gold mentions to help find non-referential NPs. Bergsma et al. (2008b) use web counts as a semisupervised way to augment standard features for anaphoricity detection for English $i t$ , an important task because $i t$ is both common and ambiguous; between a quarter and half it examples are non-anaphoric. Consider the following two examples: \n(21.45) You can make [it] in advance. [anaphoric] (21.46) You can make [it] in Hollywood. [non-anaphoric] \nThe it in make it is non-anaphoric, part of the idiom make it. Bergsma et al. (2008b) turn the context around each example into patterns, like “make * in advance” from (21.45), and “make * in Hollywood” from (21.46). They then use Google n-grams to enumerate all the words that can replace it in the patterns. Non-anaphoric contexts tend to only have it in the wildcard positions, while anaphoric contexts occur with many other NPs (for example make them in advance is just as frequent in their data as make it in advance, but make them in Hollywood did not occur at all). These n-gram contexts can be used as features in a supervised anaphoricity classifier. \n\n21.4 Architectures for Coreference Algorithms \nModern systems for coreference are based on supervised neural machine learning, supervised from hand-labeled datasets like OntoNotes. In this section we overview the various architecture of modern systems, using the categorization of $mathrm { N g }$ (2010), which distinguishes algorithms based on whether they make each coreference decision in a way that is entity-based—representing each entity in the discourse model— or only mention-based—considering each mention independently, and whether they use ranking models to directly compare potential antecedents. Afterwards, we go into more detail on one state-of-the-art algorithm in Section 21.6. \n21.4.1 The Mention-Pair Architecture \nmention-pair \nmention-pair \nWe begin with the mention-pair architecture, the simplest and most influential coreference architecture, which introduces many of the features of more complex algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that— as its name suggests—is given a pair of mentions, a candidate anaphor and a candidate antecedent, and makes a binary classification decision: coreferring or not. \nLet’s consider the task of this classifier for the pronoun she in our example, and assume the slightly simplified set of potential antecedents in Fig. 21.2. \nFor each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary classifier computes a probability: whether or not the mention is the antecedent of she. We want this probability to be high for actual antecedents (Victoria Chen, her, the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay). \nEarly classifiers used hand-built features (Section 21.5); more recent classifiers use neural representation learning (Section 21.6) \nFor training, we need a heuristic for selecting training samples; since most pairs of mentions in a document are not coreferent, selecting every pair would lead to a massive overabundance of negative samples. The most common heuristic, from (Soon et al., 2001), is to choose the closest antecedent as a positive example, and all pairs in between as the negative examples. More formally, for each anaphor mention $m _ { i }$ we create \n• one positive instance $( m _ { i } , m _ { j } )$ where $m _ { j }$ is the closest antecedent to $m _ { i }$ , and • a negative instance $( m _ { i } , m _ { k } )$ for each $m _ { k }$ between $m _ { j }$ and $m _ { i }$",
        "chapter": "Coreference Resolution",
        "section": "Mention Detection",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "21.4 Architectures for Coreference Algorithms \nModern systems for coreference are based on supervised neural machine learning, supervised from hand-labeled datasets like OntoNotes. In this section we overview the various architecture of modern systems, using the categorization of $mathrm { N g }$ (2010), which distinguishes algorithms based on whether they make each coreference decision in a way that is entity-based—representing each entity in the discourse model— or only mention-based—considering each mention independently, and whether they use ranking models to directly compare potential antecedents. Afterwards, we go into more detail on one state-of-the-art algorithm in Section 21.6. \n21.4.1 The Mention-Pair Architecture \nmention-pair \nmention-pair \nWe begin with the mention-pair architecture, the simplest and most influential coreference architecture, which introduces many of the features of more complex algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that— as its name suggests—is given a pair of mentions, a candidate anaphor and a candidate antecedent, and makes a binary classification decision: coreferring or not. \nLet’s consider the task of this classifier for the pronoun she in our example, and assume the slightly simplified set of potential antecedents in Fig. 21.2. \nFor each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary classifier computes a probability: whether or not the mention is the antecedent of she. We want this probability to be high for actual antecedents (Victoria Chen, her, the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay). \nEarly classifiers used hand-built features (Section 21.5); more recent classifiers use neural representation learning (Section 21.6) \nFor training, we need a heuristic for selecting training samples; since most pairs of mentions in a document are not coreferent, selecting every pair would lead to a massive overabundance of negative samples. The most common heuristic, from (Soon et al., 2001), is to choose the closest antecedent as a positive example, and all pairs in between as the negative examples. More formally, for each anaphor mention $m _ { i }$ we create \n• one positive instance $( m _ { i } , m _ { j } )$ where $m _ { j }$ is the closest antecedent to $m _ { i }$ , and • a negative instance $( m _ { i } , m _ { k } )$ for each $m _ { k }$ between $m _ { j }$ and $m _ { i }$ \n\nThus for the anaphor she, we would choose (she, her) as the positive example and no negative examples. Similarly, for the anaphor the company we would choose (the company, Megabucks) as the positive example and (the company, she) (the company, the 38-year-old) (the company, her pay) and (the company, her) as negative examples. \nOnce the classifier is trained, it is applied to each test sentence in a clustering step. For each mention $i$ in a document, the classifier considers each of the prior $i - 1$ mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to left (from mention $i - 1$ down to mention 1) and the first antecedent with probability $> . 5$ is linked to $i$ . If no antecedent has probably $> 0 . 5$ , no antecedent is selected for $i$ . In best-first clustering, the classifier is run on all $i - 1$ antecedents and the most probable preceding mention is chosen as the antecedent for $i$ . The transitive closure of the pairwise relation is taken as the cluster. \nWhile the mention-pair model has the advantage of simplicity, it has two main problems. First, the classifier doesn’t directly compare candidate antecedents to each other, so it’s not trained to decide, between two likely antecedents, which one is in fact better. Second, it ignores the discourse model, looking only at mentions, not entities. Each classifier decision is made completely locally to the pair, without being able to take into account other mentions of the same entity. The next two models each address one of these two flaws. \n21.4.2 The Mention-Rank Architecture \nThe mention ranking model directly compares candidate antecedents to each other, choosing the highest-scoring antecedent for each anaphor. \nIn early formulations, for mention $i$ , the classifier decides which of the ${ 1 , . . . , i -$ 1} prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose $i$ is in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on $i$ . Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and $mathrm { N g }$ , 2009). \nSo in modern mention-ranking systems, for the ith mention (anaphor), we have an associated random variable $y _ { i }$ ranging over the values $Y ( i ) = { 1 , . . . , i - 1 , epsilon }$ . The value $epsilon$ is a special dummy mention meaning that $i$ does not have an antecedent (i.e., is either discourse-new and starts a new coref chain, or is non-anaphoric). \nAt test time, for a given mention $i$ the model computes one softmax over all the antecedents (plus $epsilon$ ) giving a probability for each candidate antecedent (or none).",
        "chapter": "Coreference Resolution",
        "section": "Architectures for Coreference Algorithms",
        "subsection": "The Mention-Pair Architecture",
        "subsubsection": "N/A"
    },
    {
        "content": "Thus for the anaphor she, we would choose (she, her) as the positive example and no negative examples. Similarly, for the anaphor the company we would choose (the company, Megabucks) as the positive example and (the company, she) (the company, the 38-year-old) (the company, her pay) and (the company, her) as negative examples. \nOnce the classifier is trained, it is applied to each test sentence in a clustering step. For each mention $i$ in a document, the classifier considers each of the prior $i - 1$ mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to left (from mention $i - 1$ down to mention 1) and the first antecedent with probability $> . 5$ is linked to $i$ . If no antecedent has probably $> 0 . 5$ , no antecedent is selected for $i$ . In best-first clustering, the classifier is run on all $i - 1$ antecedents and the most probable preceding mention is chosen as the antecedent for $i$ . The transitive closure of the pairwise relation is taken as the cluster. \nWhile the mention-pair model has the advantage of simplicity, it has two main problems. First, the classifier doesn’t directly compare candidate antecedents to each other, so it’s not trained to decide, between two likely antecedents, which one is in fact better. Second, it ignores the discourse model, looking only at mentions, not entities. Each classifier decision is made completely locally to the pair, without being able to take into account other mentions of the same entity. The next two models each address one of these two flaws. \n21.4.2 The Mention-Rank Architecture \nThe mention ranking model directly compares candidate antecedents to each other, choosing the highest-scoring antecedent for each anaphor. \nIn early formulations, for mention $i$ , the classifier decides which of the ${ 1 , . . . , i -$ 1} prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose $i$ is in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on $i$ . Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and $mathrm { N g }$ , 2009). \nSo in modern mention-ranking systems, for the ith mention (anaphor), we have an associated random variable $y _ { i }$ ranging over the values $Y ( i ) = { 1 , . . . , i - 1 , epsilon }$ . The value $epsilon$ is a special dummy mention meaning that $i$ does not have an antecedent (i.e., is either discourse-new and starts a new coref chain, or is non-anaphoric). \nAt test time, for a given mention $i$ the model computes one softmax over all the antecedents (plus $epsilon$ ) giving a probability for each candidate antecedent (or none). \nFig. 21.3 shows an example of the computation for the single candidate anaphor she. \nOnce the antecedent is classified for each anaphor, transitive closure can be run over the pairwise decisions to get a complete clustering. \nTraining is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don’t know which of all the possible gold antecedents to use for training. Instead, the best antecedent for each mention is latent; that is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the details in Section 21.6. \nMention-ranking models can be implemented with hand-build features or with neural representation learning (which might also incorporate some hand-built features). we’ll explore both directions in Section 21.5 and Section 21.6. \n21.4.3 Entity-based Models \nBoth the mention-pair and mention-ranking models make their decisions about mentions. By contrast, entity-based models link each mention not to a previous mention but to a previous discourse entity (cluster of mentions). \nA mention-ranking model can be turned into an entity-ranking model simply by having the classifier make its decisions over clusters of mentions rather than individual mentions (Rahman and $mathrm { N g }$ , 2009). \nFor traditional feature-based models, this can be done by extracting features over clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper, (D)efinite, (I)ndefinite, $( mathrm { P r } )$ onoun, so that a cluster composed of Victoria, her, the 38-year-old would have the shape $P mathrm { - } P r mathrm { - } D$ (Bj¨orkelund and Kuhn, 2014). An entitybased model that includes a mention-pair classifier can use as features aggregates of mention-pair probabilities, for example computing the average probability of coreference over all mention-pairs in the two clusters (Clark and Manning 2015). \nNeural models can learn representations of clusters automatically, for example by using an RNN over the sequence of cluster mentions to encode a state corresponding to a cluster representation (Wiseman et al., 2016), or by learning distributed representations for pairs of clusters by pooling over learned representations of mention pairs (Clark and Manning, 2016b). \nHowever, although entity-based models are more expressive, the use of clusterlevel information in practice has not led to large gains in performance, so mentionranking models are still more commonly used. \n21.5 Classifiers using hand-built features \nHand-designed features play an important role in coreference, whether as the sole input to classification in pre-neural classifiers, or as augmentations to the automatic representation learning used in state-of-the-art neural systems like the one we’ll describe in Section 21.6.",
        "chapter": "Coreference Resolution",
        "section": "Architectures for Coreference Algorithms",
        "subsection": "The Mention-Rank Architecture",
        "subsubsection": "N/A"
    },
    {
        "content": "Fig. 21.3 shows an example of the computation for the single candidate anaphor she. \nOnce the antecedent is classified for each anaphor, transitive closure can be run over the pairwise decisions to get a complete clustering. \nTraining is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don’t know which of all the possible gold antecedents to use for training. Instead, the best antecedent for each mention is latent; that is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the details in Section 21.6. \nMention-ranking models can be implemented with hand-build features or with neural representation learning (which might also incorporate some hand-built features). we’ll explore both directions in Section 21.5 and Section 21.6. \n21.4.3 Entity-based Models \nBoth the mention-pair and mention-ranking models make their decisions about mentions. By contrast, entity-based models link each mention not to a previous mention but to a previous discourse entity (cluster of mentions). \nA mention-ranking model can be turned into an entity-ranking model simply by having the classifier make its decisions over clusters of mentions rather than individual mentions (Rahman and $mathrm { N g }$ , 2009). \nFor traditional feature-based models, this can be done by extracting features over clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper, (D)efinite, (I)ndefinite, $( mathrm { P r } )$ onoun, so that a cluster composed of Victoria, her, the 38-year-old would have the shape $P mathrm { - } P r mathrm { - } D$ (Bj¨orkelund and Kuhn, 2014). An entitybased model that includes a mention-pair classifier can use as features aggregates of mention-pair probabilities, for example computing the average probability of coreference over all mention-pairs in the two clusters (Clark and Manning 2015). \nNeural models can learn representations of clusters automatically, for example by using an RNN over the sequence of cluster mentions to encode a state corresponding to a cluster representation (Wiseman et al., 2016), or by learning distributed representations for pairs of clusters by pooling over learned representations of mention pairs (Clark and Manning, 2016b). \nHowever, although entity-based models are more expressive, the use of clusterlevel information in practice has not led to large gains in performance, so mentionranking models are still more commonly used. \n21.5 Classifiers using hand-built features \nHand-designed features play an important role in coreference, whether as the sole input to classification in pre-neural classifiers, or as augmentations to the automatic representation learning used in state-of-the-art neural systems like the one we’ll describe in Section 21.6.",
        "chapter": "Coreference Resolution",
        "section": "Architectures for Coreference Algorithms",
        "subsection": "Entity-based Models",
        "subsubsection": "N/A"
    },
    {
        "content": "Fig. 21.3 shows an example of the computation for the single candidate anaphor she. \nOnce the antecedent is classified for each anaphor, transitive closure can be run over the pairwise decisions to get a complete clustering. \nTraining is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don’t know which of all the possible gold antecedents to use for training. Instead, the best antecedent for each mention is latent; that is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the details in Section 21.6. \nMention-ranking models can be implemented with hand-build features or with neural representation learning (which might also incorporate some hand-built features). we’ll explore both directions in Section 21.5 and Section 21.6. \n21.4.3 Entity-based Models \nBoth the mention-pair and mention-ranking models make their decisions about mentions. By contrast, entity-based models link each mention not to a previous mention but to a previous discourse entity (cluster of mentions). \nA mention-ranking model can be turned into an entity-ranking model simply by having the classifier make its decisions over clusters of mentions rather than individual mentions (Rahman and $mathrm { N g }$ , 2009). \nFor traditional feature-based models, this can be done by extracting features over clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper, (D)efinite, (I)ndefinite, $( mathrm { P r } )$ onoun, so that a cluster composed of Victoria, her, the 38-year-old would have the shape $P mathrm { - } P r mathrm { - } D$ (Bj¨orkelund and Kuhn, 2014). An entitybased model that includes a mention-pair classifier can use as features aggregates of mention-pair probabilities, for example computing the average probability of coreference over all mention-pairs in the two clusters (Clark and Manning 2015). \nNeural models can learn representations of clusters automatically, for example by using an RNN over the sequence of cluster mentions to encode a state corresponding to a cluster representation (Wiseman et al., 2016), or by learning distributed representations for pairs of clusters by pooling over learned representations of mention pairs (Clark and Manning, 2016b). \nHowever, although entity-based models are more expressive, the use of clusterlevel information in practice has not led to large gains in performance, so mentionranking models are still more commonly used. \n21.5 Classifiers using hand-built features \nHand-designed features play an important role in coreference, whether as the sole input to classification in pre-neural classifiers, or as augmentations to the automatic representation learning used in state-of-the-art neural systems like the one we’ll describe in Section 21.6. \n\nIn this section we describe features commonly used in logistic regression, SVM, or random forest classifiers for coreference resolution. \nGiven an anaphor mention and a potential antecedent mention, most feature based classifiers make use of three types of features: (i) features of the anaphor, (ii) features of the candidate antecedent, and (iii) features of the relationship between the pair. Entity-based models can make additional use of two additional classes: (iv) feature of all mentions from the antecedent’s entity cluster, and (v) features of the relation between the anaphor and the mentions in the antecedent entity cluster. \nthat would be computed for the potential anaphor “she” and potential antecedent “Victoria Chen” in our example sentence, repeated below: \n(21.47) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $$ 2.3$ million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks. \nFeatures that prior work has found to be particularly useful are exact string match, entity headword agreement, mention distance, as well as (for pronouns) exact attribute match and i-within-i, and (for nominals and proper names) word inclusion and cosine. For lexical features (like head words) it is common to only use words that appear enough times (perhaps more than 20 times), backing off to parts of speech for rare words. \nIt is crucial in feature-based systems to use conjunctions of features; one experiment suggested that moving from individual features in a classifier to conjunctions of multiple features increased F1 by 4 points (Lee et al., 2017a). Specific conjunctions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using decision tree or random forest classifiers ( $mathrm { N g }$ and Cardie 2002a, Lee et al. 2017a). \nFinally, some of these features can also be used in neural models as well. Modern neural systems (Section 21.6) use contextual word embeddings, so they don’t benefit from adding shallow features like string or head match, grammatical role, or mention types. However other features like mention length, distance between mentions, or genre can complement neural contextual embedding models nicely. \n21.6 A neural mention-ranking algorithm \nIn this section we describe the neural e2e-coref algorithms of Lee et al. (2017b) (simplified and extended a bit, drawing on Joshi et al. (2019) and others). This is a mention-ranking algorithm that considers all possible spans of text in the document, assigns a mention-score to each span, prunes the mentions based on this score, then assigns coreference links to the remaining mentions. \nMore formally, given a document $D$ with $T$ words, the model considers all of the $frac { T ( T - 1 ) } { 2 }$ text spans in $D$ (unigrams, bigrams, trigrams, 4-grams, etc; in practice we only consider spans up a maximum length around 10). The task is to assign to each span $i$ an antecedent $y _ { i }$ , a random variable ranging over the values $begin{array} { r } { Y ( i ) = } end{array}$ ${ 1 , . . . , i - 1 , epsilon }$ ; each previous span and a special dummy token $epsilon$ . Choosing the dummy token means that $i$ does not have an antecedent, either because $i$ is discoursenew and starts a new coreference chain, or because $i$ is non-anaphoric. \nFor each pair of spans $i$ and $j$ , the system assigns a score $s ( i , j )$ for the coreference link between span $i$ and span $j$ . The system then learns a distribution $P ( y _ { i } )$ over the antecedents for span $i$ : \nThis score $s ( i , j )$ includes three factors that we’ll define below: $m ( i )$ ; whether span $i$ is a mention; $m ( j )$ ; whether span $j$ is a mention; and $c ( i , j )$ ; whether $j$ is the antecedent of $i$ :",
        "chapter": "Coreference Resolution",
        "section": "Classifiers using hand-built features",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "For the dummy antecedent $epsilon$ , the score $s ( i , epsilon )$ is fixed to 0. This way if any nondummy scores are positive, the model predicts the highest-scoring antecedent, but if all the scores are negative it abstains. \n21.6.1 Computing span representations \nTo compute the the two functions $m ( i )$ and $c ( i , j )$ which score a span $i$ or a pair of spans $( i , j )$ , we’ll need a way to represent a span. The ${ mathrm { e 2 e } } .$ -coref family of algorithms represents each span by trying to capture 3 words/tokens: the first word, the last word, and the most important word. We first run each paragraph or subdocument through an encoder (like BERT) to generate embeddings $h _ { i }$ for each token $i$ . The span $i$ is then represented by a vector $pmb { mathrm { g } } _ { i }$ that is a concatenation of the encoder output embedding for the first (start) token of the span, the encoder output for the last (end) token of the span, and a third vector which is an attention-based representation: \nThe goal of the attention vector is to represent which word/token is the likely syntactic head-word of the span; we saw in the prior section that head-words are a useful feature; a matching head-word is a good indicator of coreference. The attention representation is computed as usual; the system learns a weight vector $pmb { w } _ { alpha }$ , and computes its dot product with the hidden state $mathbf { h } _ { t }$ transformed by a FFNN: \nThe attention score is normalized into a distribution via a softmax: \nAnd then the attention distribution is used to create a vector ${ mathbf { h } } _ { mathrm { A T T } ( i ) }$ which is an attention-weighted sum of words in span $i$ : \n21.6.2 Computing the mention and antecedent scores m and c \nNow that we know how to compute the vector $pmb { mathsf { g } } _ { i }$ for representing span $i ,$ , we can see the details of the two scoring functions $m ( i )$ and $c ( i , j )$ . Both are computed by feedforward networks: \nAt inference time, this mention score $m$ is used as a filter to keep only the best few mentions. \nWe then compute the antecedent score for high-scoring mentions. The antecedent score $c ( i , j )$ takes as input a representation of the spans $i$ and $j$ , but also the elementwise similarity of the two spans to each other $mathbf { g } _ { i } circ mathbf { g } _ { j }$ (here $0$ is element-wise multiplication). Fig. 21.6 shows the computation of the score $s$ for the three possible antecedents of the company in the example sentence from Fig. 21.5. \nGiven the set of mentions, the joint distribution of antecedents for each document is computed in a forward pass, and we can then do transitive closure on the antecedents to create a final clustering for the document. \nFig. 21.7 shows example predictions from the model, showing the attention weights, which Lee et al. (2017b) find correlate with traditional semantic heads. Note that the model gets the second example wrong, presumably because attendants and pilot likely have nearby word embeddings. \nFigure 21.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per example, showing one correct example and one mistake. Bold, parenthesized spans are mentions in the predicted cluster. The amount of red color on a word indicates the head-finding attention weight $a _ { i , t }$ in (21.52). Figure adapted from Lee et al. (2017b).",
        "chapter": "Coreference Resolution",
        "section": "A neural mention-ranking algorithm",
        "subsection": "Computing span representations",
        "subsubsection": "N/A"
    },
    {
        "content": "21.6.2 Computing the mention and antecedent scores m and c \nNow that we know how to compute the vector $pmb { mathsf { g } } _ { i }$ for representing span $i ,$ , we can see the details of the two scoring functions $m ( i )$ and $c ( i , j )$ . Both are computed by feedforward networks: \nAt inference time, this mention score $m$ is used as a filter to keep only the best few mentions. \nWe then compute the antecedent score for high-scoring mentions. The antecedent score $c ( i , j )$ takes as input a representation of the spans $i$ and $j$ , but also the elementwise similarity of the two spans to each other $mathbf { g } _ { i } circ mathbf { g } _ { j }$ (here $0$ is element-wise multiplication). Fig. 21.6 shows the computation of the score $s$ for the three possible antecedents of the company in the example sentence from Fig. 21.5. \nGiven the set of mentions, the joint distribution of antecedents for each document is computed in a forward pass, and we can then do transitive closure on the antecedents to create a final clustering for the document. \nFig. 21.7 shows example predictions from the model, showing the attention weights, which Lee et al. (2017b) find correlate with traditional semantic heads. Note that the model gets the second example wrong, presumably because attendants and pilot likely have nearby word embeddings. \nFigure 21.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per example, showing one correct example and one mistake. Bold, parenthesized spans are mentions in the predicted cluster. The amount of red color on a word indicates the head-finding attention weight $a _ { i , t }$ in (21.52). Figure adapted from Lee et al. (2017b). \n21.6.3 Learning \nFor training, we don’t have a single gold antecedent for each mention; instead the coreference labeling only gives us each entire cluster of coreferent mentions; so a mention only has a latent antecedent. We therefore use a loss function that maximizes the sum of the coreference probability of any of the legal antecedents. For a given mention i with possible antecedents $Y ( i )$ , let $mathrm { G O L D } ( i )$ be the set of mentions in the gold cluster containing $i$ . Since the set of mentions occurring before $i$ is $Y ( i )$ , the set of mentions in that gold cluster that also occur before $i$ is $Y ( i ) cap mathrm { G O L D } ( i )$ . We therefore want to maximize: \nIf a mention $i$ is not in a gold cluster $mathrm {  G O L D } ( i ) = epsilon$ . \nTo turn this probability into a loss function, we’ll use the cross-entropy loss function we defined in Eq. 5.22 in Chapter 5, by taking the $- log$ of the probability. If we then sum over all mentions, we get the final loss function for training: \n21.7 Evaluation of Coreference Resolution \nWe evaluate coreference algorithms model-theoretically, comparing a set of hypothesis chains or clusters $H$ produced by the system against a set of gold or reference chains or clusters $R$ from a human labeling, and reporting precision and recall. \nHowever, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based $B ^ { 3 }$ metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016). \nLet’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995) is based on the number of coreference links (pairs of mentions) common to $H$ and $R$ . Precision is the number of common links divided by the number of links in $H$ . Recall is the number of common links divided by the number of links in $R$ ; This makes MUC biased toward systems that produce large chains (and fewer entities), and it ignores singletons, since they don’t involve links. \n$mathbf { B } ^ { 3 }$ is mention-based rather than link-based. For each mention in the reference chain, we compute a precision and recall, and then we take a weighted sum over all $N$ mentions in the document to compute a precision and recall for the entire task. For a given mention $i$ , let $R$ be the reference chain that includes $i$ , and $H$ the hypothesis chain that has $i$ . The set of correct mentions in $H$ is $H cap R$ . Precision for mention $i$ is thus $frac { | H cap R | } { | H | }$ , and recall for mention $i$ thus $frac { | H cap R | } { | R | }$ . The tot∩al precision is the weighted sum of the precision for mention $i$ , weighted by a weight $w _ { i }$ . The total recall is the weighted sum of the recall for mention $i$ , weighted by a weight $w _ { i }$ . Equivalently:",
        "chapter": "Coreference Resolution",
        "section": "A neural mention-ranking algorithm",
        "subsection": "Computing the mention and antecedent scores m and c ",
        "subsubsection": "N/A"
    },
    {
        "content": "21.6.3 Learning \nFor training, we don’t have a single gold antecedent for each mention; instead the coreference labeling only gives us each entire cluster of coreferent mentions; so a mention only has a latent antecedent. We therefore use a loss function that maximizes the sum of the coreference probability of any of the legal antecedents. For a given mention i with possible antecedents $Y ( i )$ , let $mathrm { G O L D } ( i )$ be the set of mentions in the gold cluster containing $i$ . Since the set of mentions occurring before $i$ is $Y ( i )$ , the set of mentions in that gold cluster that also occur before $i$ is $Y ( i ) cap mathrm { G O L D } ( i )$ . We therefore want to maximize: \nIf a mention $i$ is not in a gold cluster $mathrm {  G O L D } ( i ) = epsilon$ . \nTo turn this probability into a loss function, we’ll use the cross-entropy loss function we defined in Eq. 5.22 in Chapter 5, by taking the $- log$ of the probability. If we then sum over all mentions, we get the final loss function for training: \n21.7 Evaluation of Coreference Resolution \nWe evaluate coreference algorithms model-theoretically, comparing a set of hypothesis chains or clusters $H$ produced by the system against a set of gold or reference chains or clusters $R$ from a human labeling, and reporting precision and recall. \nHowever, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based $B ^ { 3 }$ metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016). \nLet’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995) is based on the number of coreference links (pairs of mentions) common to $H$ and $R$ . Precision is the number of common links divided by the number of links in $H$ . Recall is the number of common links divided by the number of links in $R$ ; This makes MUC biased toward systems that produce large chains (and fewer entities), and it ignores singletons, since they don’t involve links. \n$mathbf { B } ^ { 3 }$ is mention-based rather than link-based. For each mention in the reference chain, we compute a precision and recall, and then we take a weighted sum over all $N$ mentions in the document to compute a precision and recall for the entire task. For a given mention $i$ , let $R$ be the reference chain that includes $i$ , and $H$ the hypothesis chain that has $i$ . The set of correct mentions in $H$ is $H cap R$ . Precision for mention $i$ is thus $frac { | H cap R | } { | H | }$ , and recall for mention $i$ thus $frac { | H cap R | } { | R | }$ . The tot∩al precision is the weighted sum of the precision for mention $i$ , weighted by a weight $w _ { i }$ . The total recall is the weighted sum of the recall for mention $i$ , weighted by a weight $w _ { i }$ . Equivalently:",
        "chapter": "Coreference Resolution",
        "section": "A neural mention-ranking algorithm",
        "subsection": "Learning",
        "subsubsection": "N/A"
    },
    {
        "content": "21.6.3 Learning \nFor training, we don’t have a single gold antecedent for each mention; instead the coreference labeling only gives us each entire cluster of coreferent mentions; so a mention only has a latent antecedent. We therefore use a loss function that maximizes the sum of the coreference probability of any of the legal antecedents. For a given mention i with possible antecedents $Y ( i )$ , let $mathrm { G O L D } ( i )$ be the set of mentions in the gold cluster containing $i$ . Since the set of mentions occurring before $i$ is $Y ( i )$ , the set of mentions in that gold cluster that also occur before $i$ is $Y ( i ) cap mathrm { G O L D } ( i )$ . We therefore want to maximize: \nIf a mention $i$ is not in a gold cluster $mathrm {  G O L D } ( i ) = epsilon$ . \nTo turn this probability into a loss function, we’ll use the cross-entropy loss function we defined in Eq. 5.22 in Chapter 5, by taking the $- log$ of the probability. If we then sum over all mentions, we get the final loss function for training: \n21.7 Evaluation of Coreference Resolution \nWe evaluate coreference algorithms model-theoretically, comparing a set of hypothesis chains or clusters $H$ produced by the system against a set of gold or reference chains or clusters $R$ from a human labeling, and reporting precision and recall. \nHowever, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based $B ^ { 3 }$ metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016). \nLet’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995) is based on the number of coreference links (pairs of mentions) common to $H$ and $R$ . Precision is the number of common links divided by the number of links in $H$ . Recall is the number of common links divided by the number of links in $R$ ; This makes MUC biased toward systems that produce large chains (and fewer entities), and it ignores singletons, since they don’t involve links. \n$mathbf { B } ^ { 3 }$ is mention-based rather than link-based. For each mention in the reference chain, we compute a precision and recall, and then we take a weighted sum over all $N$ mentions in the document to compute a precision and recall for the entire task. For a given mention $i$ , let $R$ be the reference chain that includes $i$ , and $H$ the hypothesis chain that has $i$ . The set of correct mentions in $H$ is $H cap R$ . Precision for mention $i$ is thus $frac { | H cap R | } { | H | }$ , and recall for mention $i$ thus $frac { | H cap R | } { | R | }$ . The tot∩al precision is the weighted sum of the precision for mention $i$ , weighted by a weight $w _ { i }$ . The total recall is the weighted sum of the recall for mention $i$ , weighted by a weight $w _ { i }$ . Equivalently: \n\nThe weight $w _ { i }$ for each entity can be set to different values to produce different versions of the algorithm. \nFollowing a proposal from Denis and Baldridge (2009), the CoNLL coreference competitions were scored based on the average of MUC, CEAF-e, and ${ bf B } ^ { 3 }$ (Pradhan et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed description of the entire set of metrics; reference implementations of these should be used rather than attempting to reimplement from scratch (Pradhan et al., 2014). \nAlternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name itself, or is linked with a wrong name, is not useful for this task. We might instead want a metric that weights each mention by how informative it is (with names being most informative) (Chen and $mathrm { N g }$ , 2013) or a metric that considers a hypothesis to match a gold chain only if it contains at least one variant of a name (the NEC F1 metric of Agarwal et al. (2019)). \n21.8 Winograd Schema problems \nFrom early on in the field, researchers have noted that some cases of coreference are quite difficult, seeming to require world knowledge or sophisticated reasoning to solve. The problem was most famously pointed out by Winograd (1972) with the following example: \n(21.58) The city council denied the demonstrators a permit because \na. they feared violence.   \nb. they advocated violence. \nWinograd noticed that the antecedent that most readers preferred for the pronoun they in continuation (a) was the city council, but in (b) was the demonstrators. He suggested that this requires understanding that the second clause is intended as an explanation of the first clause, and also that our cultural frames suggest that city councils are perhaps more likely than demonstrators to fear violence and that demonstrators might be more likely to advocate violence. \nIn an attempt to get the field of NLP to focus more on methods involving world knowledge and common-sense reasoning, Levesque (2011) proposed a challenge task called the Winograd Schema Challenge.8 The problems in the challenge task are coreference problems designed to be easily disambiguated by the human reader, but hopefully not solvable by simple techniques such as selectional restrictions, or other basic word association methods.",
        "chapter": "Coreference Resolution",
        "section": "Evaluation of Coreference Resolution",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The weight $w _ { i }$ for each entity can be set to different values to produce different versions of the algorithm. \nFollowing a proposal from Denis and Baldridge (2009), the CoNLL coreference competitions were scored based on the average of MUC, CEAF-e, and ${ bf B } ^ { 3 }$ (Pradhan et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed description of the entire set of metrics; reference implementations of these should be used rather than attempting to reimplement from scratch (Pradhan et al., 2014). \nAlternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name itself, or is linked with a wrong name, is not useful for this task. We might instead want a metric that weights each mention by how informative it is (with names being most informative) (Chen and $mathrm { N g }$ , 2013) or a metric that considers a hypothesis to match a gold chain only if it contains at least one variant of a name (the NEC F1 metric of Agarwal et al. (2019)). \n21.8 Winograd Schema problems \nFrom early on in the field, researchers have noted that some cases of coreference are quite difficult, seeming to require world knowledge or sophisticated reasoning to solve. The problem was most famously pointed out by Winograd (1972) with the following example: \n(21.58) The city council denied the demonstrators a permit because \na. they feared violence.   \nb. they advocated violence. \nWinograd noticed that the antecedent that most readers preferred for the pronoun they in continuation (a) was the city council, but in (b) was the demonstrators. He suggested that this requires understanding that the second clause is intended as an explanation of the first clause, and also that our cultural frames suggest that city councils are perhaps more likely than demonstrators to fear violence and that demonstrators might be more likely to advocate violence. \nIn an attempt to get the field of NLP to focus more on methods involving world knowledge and common-sense reasoning, Levesque (2011) proposed a challenge task called the Winograd Schema Challenge.8 The problems in the challenge task are coreference problems designed to be easily disambiguated by the human reader, but hopefully not solvable by simple techniques such as selectional restrictions, or other basic word association methods. \n\nThe problems are framed as a pair of statements that differ in a single word or phrase, and a coreference question: \n(21.59) The trophy didn’t fit into the suitcase because it was too large. Question: What was too large? Answer: The trophy   \n(21.60) The trophy didn’t fit into the suitcase because it was too small. Question: What was too small? Answer: The suitcase \nThe problems have the following characteristics: \n1. The problems each have two parties   \n2. A pronoun preferentially refers to one of the parties, but could grammatically also refer to the other   \n3. A question asks which party the pronoun refers to   \n4. If one word in the question is changed, the human-preferred answer changes to the other party \nThe kind of world knowledge that might be needed to solve the problems can vary. In the trophy/suitcase example, it is knowledge about the physical world; that a bigger object cannot fit into a smaller object. In the original Winograd sentence, it is stereotypes about social actors like politicians and protesters. In examples like the following, it is knowledge about human actions like turn-taking or thanking. \n(21.61) Bill passed the gameboy to John because his turn was [over/next]. Whose turn was [over/next]? Answers: Bill/John   \n(21.62) Joan made sure to thank Susan for all the help she had [given/received]. Who had [given/received] help? Answers: Susan/Joan. \nAlthough the Winograd Schema was designed to require common-sense reasoning, a large percentage of the original set of problem can be solved by pretrained language models, fine-tuned on Winograd Schema sentences (Kocijan et al., 2019). Large pretrained language models encode an enormous amount of world or common-sense knowledge! The current trend is therefore to propose new datasets with increasingly difficult Winograd-like coreference resolution problems like KNOWREF (Emami et al., 2019), with examples like: \n(21.63) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the gap wasn’t all that big. \nIn the end, it seems likely that some combination of language modeling and knowledge will prove fruitful; indeed, it seems that knowledge-based models overfit less to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018), \n21.9 Gender Bias in Coreference \nAs with other aspects of language processing, coreference models exhibit gender and other biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018). For example the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Winograd Schema paradigm to test the extent to which coreference algorithms are biased toward linking gendered pronouns with antecedents consistent with cultural stereotypes. As we summarized in Chapter 6, embeddings replicate societal biases in their training test, such as associating men with historically sterotypical male occupations like doctors, and women with stereotypical female occupations like secretaries (Caliskan et al. 2017, Garg et al. 2018).",
        "chapter": "Coreference Resolution",
        "section": "Winograd Schema problems",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The problems are framed as a pair of statements that differ in a single word or phrase, and a coreference question: \n(21.59) The trophy didn’t fit into the suitcase because it was too large. Question: What was too large? Answer: The trophy   \n(21.60) The trophy didn’t fit into the suitcase because it was too small. Question: What was too small? Answer: The suitcase \nThe problems have the following characteristics: \n1. The problems each have two parties   \n2. A pronoun preferentially refers to one of the parties, but could grammatically also refer to the other   \n3. A question asks which party the pronoun refers to   \n4. If one word in the question is changed, the human-preferred answer changes to the other party \nThe kind of world knowledge that might be needed to solve the problems can vary. In the trophy/suitcase example, it is knowledge about the physical world; that a bigger object cannot fit into a smaller object. In the original Winograd sentence, it is stereotypes about social actors like politicians and protesters. In examples like the following, it is knowledge about human actions like turn-taking or thanking. \n(21.61) Bill passed the gameboy to John because his turn was [over/next]. Whose turn was [over/next]? Answers: Bill/John   \n(21.62) Joan made sure to thank Susan for all the help she had [given/received]. Who had [given/received] help? Answers: Susan/Joan. \nAlthough the Winograd Schema was designed to require common-sense reasoning, a large percentage of the original set of problem can be solved by pretrained language models, fine-tuned on Winograd Schema sentences (Kocijan et al., 2019). Large pretrained language models encode an enormous amount of world or common-sense knowledge! The current trend is therefore to propose new datasets with increasingly difficult Winograd-like coreference resolution problems like KNOWREF (Emami et al., 2019), with examples like: \n(21.63) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the gap wasn’t all that big. \nIn the end, it seems likely that some combination of language modeling and knowledge will prove fruitful; indeed, it seems that knowledge-based models overfit less to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018), \n21.9 Gender Bias in Coreference \nAs with other aspects of language processing, coreference models exhibit gender and other biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018). For example the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Winograd Schema paradigm to test the extent to which coreference algorithms are biased toward linking gendered pronouns with antecedents consistent with cultural stereotypes. As we summarized in Chapter 6, embeddings replicate societal biases in their training test, such as associating men with historically sterotypical male occupations like doctors, and women with stereotypical female occupations like secretaries (Caliskan et al. 2017, Garg et al. 2018). \n\nA WinoBias sentence contain two mentions corresponding to stereotypicallymale and stereotypically-female occupations and a gendered pronoun that must be linked to one of them. The sentence cannot be disambiguated by the gender of the pronoun, but a biased model might be distracted by this cue. Here is an example sentence: \n(21.64) The secretary called the physician $cdot i$ and told $mathrm { h i m } _ { i }$ about a new patient [pro-stereotypical]   \n(21.65) The secretary called the physician $cdot i$ and told heri about a new patient [anti-stereotypical] \nZhao et al. (2018a) consider a coreference system to be biased if it is more accurate at linking pronouns consistent with gender stereotypical occupations (e.g., him with physician in (21.64)) than linking pronouns inconsistent with gender-stereotypical occupations (e.g., her with physician in (21.65)). They show that coreference systems of all architectures (rule-based, feature-based machine learned, and end-toend-neural) all show significant bias, performing on average $2 1 mathrm { F } _ { 1 }$ points worse in the anti-stereotypical cases. \nOne possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias. \nWebster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only $20 %$ of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following: \n(21.66) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving back from Karuizawa to Kitami where she had spent her junior days. \nWebster et al. (2018) show that modern coreference algorithms perform significantly worse on resolving feminine pronouns than masculine pronouns in GAP. Kurita et al. (2019) shows that a system based on BERT contextualized word representations shows similar bias.",
        "chapter": "Coreference Resolution",
        "section": "Gender Bias in Coreference",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A WinoBias sentence contain two mentions corresponding to stereotypicallymale and stereotypically-female occupations and a gendered pronoun that must be linked to one of them. The sentence cannot be disambiguated by the gender of the pronoun, but a biased model might be distracted by this cue. Here is an example sentence: \n(21.64) The secretary called the physician $cdot i$ and told $mathrm { h i m } _ { i }$ about a new patient [pro-stereotypical]   \n(21.65) The secretary called the physician $cdot i$ and told heri about a new patient [anti-stereotypical] \nZhao et al. (2018a) consider a coreference system to be biased if it is more accurate at linking pronouns consistent with gender stereotypical occupations (e.g., him with physician in (21.64)) than linking pronouns inconsistent with gender-stereotypical occupations (e.g., her with physician in (21.65)). They show that coreference systems of all architectures (rule-based, feature-based machine learned, and end-toend-neural) all show significant bias, performing on average $2 1 mathrm { F } _ { 1 }$ points worse in the anti-stereotypical cases. \nOne possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias. \nWebster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only $20 %$ of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following: \n(21.66) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving back from Karuizawa to Kitami where she had spent her junior days. \nWebster et al. (2018) show that modern coreference algorithms perform significantly worse on resolving feminine pronouns than masculine pronouns in GAP. Kurita et al. (2019) shows that a system based on BERT contextualized word representations shows similar bias.",
        "chapter": "Coreference Resolution",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A WinoBias sentence contain two mentions corresponding to stereotypicallymale and stereotypically-female occupations and a gendered pronoun that must be linked to one of them. The sentence cannot be disambiguated by the gender of the pronoun, but a biased model might be distracted by this cue. Here is an example sentence: \n(21.64) The secretary called the physician $cdot i$ and told $mathrm { h i m } _ { i }$ about a new patient [pro-stereotypical]   \n(21.65) The secretary called the physician $cdot i$ and told heri about a new patient [anti-stereotypical] \nZhao et al. (2018a) consider a coreference system to be biased if it is more accurate at linking pronouns consistent with gender stereotypical occupations (e.g., him with physician in (21.64)) than linking pronouns inconsistent with gender-stereotypical occupations (e.g., her with physician in (21.65)). They show that coreference systems of all architectures (rule-based, feature-based machine learned, and end-toend-neural) all show significant bias, performing on average $2 1 mathrm { F } _ { 1 }$ points worse in the anti-stereotypical cases. \nOne possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias. \nWebster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only $20 %$ of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following: \n(21.66) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving back from Karuizawa to Kitami where she had spent her junior days. \nWebster et al. (2018) show that modern coreference algorithms perform significantly worse on resolving feminine pronouns than masculine pronouns in GAP. Kurita et al. (2019) shows that a system based on BERT contextualized word representations shows similar bias. \n21.10 Summary \nThis chapter introduced the task of coreference resolution. \n• This is the task of linking together mentions in text which corefer, i.e. refer to the same discourse entity in the discourse model, resulting in a set of coreference chains (also called clusters or entities).   \n• Mentions can be definite NPs or indefinite NPs, pronouns (including zero pronouns) or names.   \n• The surface form of an entity mention is linked to its information status (new, old, or inferrable), and how accessible or salient the entity is.   \n• Some NPs are not referring expressions, such as pleonastic $i t$ in $I t$ is raining.   \n• Many corpora have human-labeled coreference annotations that can be used for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan.   \n• Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.   \n• Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers.   \n• Modern coreference systems tend to be end-to-end, performing mention detection and coreference in a single end-to-end architecture.   \n• Algorithms learn representations for text spans and heads, and learn to compare anaphor spans with candidate antecedent spans.   \n• Coreference systems are evaluated by comparing with gold entity labels using precision/recall metrics like MUC, $mathbf { B } ^ { 3 }$ , CEAF, BLANC, or LEA.   \n• The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.   \n• Coreference systems exhibit gender bias which can be evaluated using datasets like Winobias and GAP. \nBibliographical and Historical Notes \nCoreference has been part of natural language processing since the 1970s (Woods et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation of coreference was formulated by Karttunen (1969) (at the 3rd COLING conference), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But it was Bonnie Webber’s 1978 dissertation and following work (Webber 1983) that explored the model’s computational aspects, providing fundamental insights into how entities are represented in the discourse model and the ways in which they can license subsequent reference. Many of the examples she provided continue to challenge theories of reference to this day. \nThe Hobbs algorithm9 is a tree-search algorithm that was the first in a long series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be understood from a simplified version due to Kehler et al. (2004) that just searches through the list of NPs in the current and prior sentences. This simplified Hobbs algorithm searches NPs in the following order: “(i) in the current sentence from right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in the current sentence from left-to-right, starting with the first noun group to the right of the pronoun (for cataphora). The first noun group that agrees with the pronoun with respect to number, gender, and person is chosen as the antecedent” (Kehler et al., 2004). \n\nLappin and Leass (1994) was an influential entity-based system that used weights to combine syntactic and other features, extended soon after by Kennedy and Boguraev (1996) whose system avoids the need for full syntactic parses. \nApproximately contemporaneously centering (Grosz et al., 1995) was applied to pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of work followed focused on centering’s use in coreference (Kameyama 1986, Di Eugenio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler 1997a, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how centering can be integrated with coherence-driven theories of pronoun interpretation. See Chapter 22 for the use of centering in measuring discourse coherence. \nCoreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC7 corpora), and set the tone for much later work, choosing to focus exclusively on the simplest cases of identity coreference (ignoring difficult cases like bridging, metonymy, and part-whole) and drawing the community toward supervised machine learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that were widely used for model training and evaluation. \nThis DARPA work influenced the community toward supervised learning beginning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by $mathrm { N g }$ and Cardie (2002b), and a series of machine learning models followed over the next 15 years. These often focused separately on pronominal anaphora resolution (Kehler et al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999, $mathrm { N g }$ and Cardie 2002b, $mathrm { N g } 2 0 0 5 mathrm { a } )$ and definite NP reference (Poesio and Vieira 1998, Vieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff 1999, Bean and Riloff 2004, $mathrm { N g }$ and Cardie 2002a, $ N mathrm { g } 2 0 0 4 ,$ ), or singleton detection (de Marneffe et al., 2015). \nThe move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of $mathrm { N g }$ (2005b) to use a dummy antecedent for mention-ranking, allowing ‘non-referential’ to be a choice for coreference classifiers, Denis and Baldridge’s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and $mathrm { N g }$ (2009) proposal to train the two models jointly with a single objective. \nSimple rule-based systems for coreference returned to prominence in the 2010s, partly because of their ability to encode entity-based features in a high-precision way (Zhou et al. 2004b, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al. 2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an inability to deal with the semantics necessary to correctly handle cases of common noun coreference. \n\nA return to supervised learning led to a number of advances in mention-ranking models which were also extended into neural architectures, for example using reinforcement learning to directly optimize coreference evaluation models Clark and Manning (2016a), doing end-to-end coreference all the way from span extraction (Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take advantage of global entity-level information (Clark and Manning 2016b, Wiseman et al. 2016, Lee et al. 2018). \nCoreference is also related to the task of entity linking discussed in Chapter 23. Coreference can help entity linking by giving more possible surface forms to help link to the right Wikipedia page, and conversely entity linking can help improve coreference resolution. Consider this example from Hajishirzi et al. (2013): \n(21.67) [Michael Eisner]1 and [Donald Tsang]2 announced the grand opening of [[Hong Kong]3 Disneyland $] _ { 4 }$ yesterday. [Eisner]1 thanked [the President]2 and welcomed [fans]5 to [the park]4. \nIntegrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). \nThe coreference task as we introduced it involves a simplifying assumption that the relationship between an anaphor and its antecedent is one of identity: the two coreferring mentions refer to the identical discourse referent. In real texts, the relationship can be more complex, where different aspects of a discourse referent can be neutralized or refocused. For example (21.68) (Recasens et al., 2011) shows an example of metonymy, in which the capital city Washington is used metonymically to refer to the US. (21.69-21.70) show other examples (Recasens et al., 2011): \n(21.68) a strict interpretation of a policy requires The U.S. to notify foreign dictators of certain coup plots ... Washington rejected the bid ...   \n(21.69) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New Year. In the South, everyone was celebrating New Year; to the North, it was a regular day.   \n(21.70) In France, the president is elected for a term of seven years, while in the United States he is elected for a term of four years. \nFor further linguistic discussions of these complications of coreference see Pustejovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and Turner (2008), Versley (2008), and Barker (2010). \n$mathrm { N g }$ (2017) offers a useful compact history of machine learning models in coreference resolution. There are three excellent book-length surveys of anaphora/coreference resolution, covering different time periods: Hirst (1981) (early work until about 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015). \nAndy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there are some remnants of Andy’s lovely prose still in this third-edition coreference chapter.",
        "chapter": "Coreference Resolution",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A return to supervised learning led to a number of advances in mention-ranking models which were also extended into neural architectures, for example using reinforcement learning to directly optimize coreference evaluation models Clark and Manning (2016a), doing end-to-end coreference all the way from span extraction (Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take advantage of global entity-level information (Clark and Manning 2016b, Wiseman et al. 2016, Lee et al. 2018). \nCoreference is also related to the task of entity linking discussed in Chapter 23. Coreference can help entity linking by giving more possible surface forms to help link to the right Wikipedia page, and conversely entity linking can help improve coreference resolution. Consider this example from Hajishirzi et al. (2013): \n(21.67) [Michael Eisner]1 and [Donald Tsang]2 announced the grand opening of [[Hong Kong]3 Disneyland $] _ { 4 }$ yesterday. [Eisner]1 thanked [the President]2 and welcomed [fans]5 to [the park]4. \nIntegrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). \nThe coreference task as we introduced it involves a simplifying assumption that the relationship between an anaphor and its antecedent is one of identity: the two coreferring mentions refer to the identical discourse referent. In real texts, the relationship can be more complex, where different aspects of a discourse referent can be neutralized or refocused. For example (21.68) (Recasens et al., 2011) shows an example of metonymy, in which the capital city Washington is used metonymically to refer to the US. (21.69-21.70) show other examples (Recasens et al., 2011): \n(21.68) a strict interpretation of a policy requires The U.S. to notify foreign dictators of certain coup plots ... Washington rejected the bid ...   \n(21.69) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New Year. In the South, everyone was celebrating New Year; to the North, it was a regular day.   \n(21.70) In France, the president is elected for a term of seven years, while in the United States he is elected for a term of four years. \nFor further linguistic discussions of these complications of coreference see Pustejovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and Turner (2008), Versley (2008), and Barker (2010). \n$mathrm { N g }$ (2017) offers a useful compact history of machine learning models in coreference resolution. There are three excellent book-length surveys of anaphora/coreference resolution, covering different time periods: Hirst (1981) (early work until about 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015). \nAndy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there are some remnants of Andy’s lovely prose still in this third-edition coreference chapter. \n\nExercises \n22 Discourse Coherence \nlocal global \nAnd even in our wildest and most wandering reveries, nay in our very dreams, we shall find, if we reflect, that the imagination ran not altogether at adventures, but that there was still a connection upheld among the different ideas, which succeeded each other. Were the loosest and freest conversation to be transcribed, there would immediately be transcribed, there would immediately be observed something which connected it in all its transitions. \nDavid Hume, An enquiry concerning human understanding, 1748 \nOrson Welles’ movie Citizen Kane was groundbreaking in many ways, perhaps most notably in its structure. The story of the life of fictional media magnate Charles Foster Kane, the movie does not proceed in chronological order through Kane’s life. Instead, the film begins with Kane’s death (famously murmuring “Rosebud”) and is structured around flashbacks to his life inserted among scenes of a reporter investigating his death. The novel idea that the structure of a movie does not have to linearly follow the structure of the real timeline made apparent for 20th century cinematography the infinite possibilities and impact of different kinds of coherent narrative structures. \nBut coherent structure is not just a fact about movies or works of art. Like movies, language does not normally consist of isolated, unrelated sentences, but instead of collocated, structured, coherent groups of sentences. We refer to such a coherent structured group of sentences as a discourse, and we use the word coherence to refer to the relationship between sentences that makes real discourses different than just random assemblages of sentences. The chapter you are now reading is an example of a discourse, as is a news article, a conversation, a thread on social media, a Wikipedia page, and your favorite novel. \nWhat makes a discourse coherent? If you created a text by taking random sentences each from many different sources and pasted them together, would that be a coherent discourse? Almost certainly not. Real discourses exhibit both local coherence and global coherence. Let’s consider three ways in which real discourses are locally coherent; \nFirst, sentences or clauses in real discourses are related to nearby sentences in systematic ways. Consider this example from Hobbs (1979): \n(22.1) John took a train from Paris to Istanbul. He likes spinach. \nThis sequence is incoherent because it is unclear to a reader why the second sentence follows the first; what does liking spinach have to do with train trips? In fact, a reader might go to some effort to try to figure out how the discourse could be coherent; perhaps there is a French spinach shortage? The very fact that hearers try to identify such connections suggests that human discourse comprehension involves the need to establish this kind of coherence. \nBy contrast, in the following coherent example: \n(22.2) Jane took a train from Paris to Istanbul. She had to attend a conference.",
        "chapter": "Coreference Resolution",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "22.1 Coherence Relations \ncoherence relation \nRecall from the introduction the difference between passages (22.5) and (22.6). \n(22.5) Jane took a train from Paris to Istanbul. She likes spinach.   \n(22.6) Jane took a train from Paris to Istanbul. She had to attend a conference. \nThe reason (22.6) is more coherent is that the reader can form a connection between the two sentences, in which the second sentence provides a potential REASON for the first sentences. This link is harder to form for (22.5). These connections between text spans in a discourse can be specified as a set of coherence relations. The next two sections describe two commonly used models of coherence relations and associated corpora: Rhetorical Structure Theory (RST), and the Penn Discourse TreeBank (PDTB). \nRST nucleus satellite \n22.1.1 Rhetorical Structure Theory \nThe most commonly used model of discourse organization is Rhetorical Structure Theory (RST) (Mann and Thompson, 1987). In RST relations are defined between two spans of text, generally a nucleus and a satellite. The nucleus is the unit that is more central to the writer’s purpose and that is interpretable independently; the satellite is less central and generally is only interpretable with respect to the nucleus. Some symmetric relations, however, hold between two nuclei. \nBelow are a few examples of RST coherence relations, with definitions adapted from the RST Treebank Manual (Carlson and Marcu, 2001). \nReason: The nucleus is an action carried out by an animate agent and the satellite is the reason for the nucleus. \n(22.7) [NUC Jane took a train from Paris to Istanbul.] [SAT She had to attend a conference.] \nElaboration: The satellite gives additional information or detail about the situation presented in the nucleus. \n(22.8) [NUC Dorothy was from Kansas.] [SAT She lived in the midst of the great Kansas prairies.] \nEvidence: The satellite gives additional information or detail about the situation presented in the nucleus. The information is presented with the goal of convince the reader to accept the information presented in the nucleus. \n(22.9) [NUC Kevin must be here.] [SAT His car is parked outside.] \nAttribution: The satellite gives the source of attribution for an instance of reported speech in the nucleus. \n(22.10) [SAT Analysts estimated] [NUC that sales at U.S. stores declined in the quarter, too] \nList: In this multinuclear relation, a series of nuclei is given, without contrast or explicit comparison: \n(22.11) [NUC Billy Bones was the mate; ] [NUC Long John, he was quartermaster] \nRST relations are traditionally represented graphically; the asymmetric NucleusSatellite relation is represented with an arrow from the satellite to the nucleus: \nWe can also talk about the coherence of a larger text by considering the hierarchical structure between coherence relations. Figure 22.1 shows the rhetorical structure of a paragraph from Marcu (2000a) for the text in (22.12) from the Scientific American magazine. \n(22.12) With its distant orbit–50 percent farther from the sun than Earth–and slim atmospheric blanket, Mars experiences frigid weather conditions. Surface temperatures typically average about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator and can dip to -123 degrees C near the poles. Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure. \nFigure 22.1 A discourse tree for the Scientific American text in (22.12), from Marcu (2000a). Note that asymmetric relations are represented with a curved arrow from the satellite to the nucleus. \n\nThe leaves in the Fig. 22.1 tree correspond to text spans of a sentence, clause or EDU phrase that are called elementary discourse units or EDUs in RST; these units can also be referred to as discourse segments. Because these units may correspond to arbitrary spans of text, determining the boundaries of an EDU is an important task for extracting coherence relations. Roughly speaking, one can think of discourse segments as being analogous to constituents in sentence syntax, and indeed as we’ll see in Section 22.2 we generally draw on parsing algorithms to infer discourse structure. \n\nThere are corpora for many discourse coherence models; the RST Discourse TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It consists of 385 English language documents selected from the Penn Treebank, with full RST parses for each one, using a large set of 78 distinct relations, grouped into 16 classes. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian Portuguese (Braud et al., 2017). \nNow that we’ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. \n22.1.2 Penn Discourse TreeBank (PDTB) \nPDTB \nThe Penn Discourse TreeBank (PDTB) is a second commonly used dataset that embodies another model of coherence relations (Miltsakaki et al. 2004, Prasad et al. 2008, Prasad et al. 2014). PDTB labeling is lexically grounded. Instead of asking annotators to directly tag the coherence relation between text spans, they were given a list of discourse connectives, words that signal discourse relations, like because, although, when, since, or as a result. In a part of a text where these words marked a coherence relation between two text spans, the connective and the spans were then annotated, as in Fig. 22.13, where the phrase as a result signals a causal relationship between what PDTB calls Arg1 (the first two sentences, here in italics) and Arg2 (the third sentence, here in bold). \n(22.13) Jewelry displays in department stores were often cluttered and uninspired. And the merchandise was, well, fake. As a result, marketers of faux gems steadily lost space in department stores to more fashionable rivals—cosmetics makers.   \n(22.14) In July, the Environmental Protection Agency imposed a gradual ban on virtually all uses of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. \nNot all coherence relations are marked by an explicit discourse connective, and so the PDTB also annotates pairs of neighboring sentences with no explicit signal, like (22.14). The annotator first chooses the word or phrase that could have been its signal (in this case as a result), and then labels its sense. For example for the ambiguous discourse connective since annotators marked whether it is using a CAUSAL or a TEMPORAL sense. \nThe final dataset contains roughly 18,000 explicit relations and 16,000 implicit relations. Fig. 22.2 shows examples from each of the 4 major semantic classes, while Fig. 22.3 shows the full tagset. \nUnlike the RST Discourse Treebank, which integrates these pairwise coherence relations into a global tree structure spanning an entire discourse, the PDTB does not annotate anything above the span-pair level, making no commitment with respect to higher-level discourse structure. \nThere are also treebanks using similar methods for other languages; (22.15) shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015). Because Chinese has a smaller percentage of explicit discourse connectives than English (only $2 2 %$ of all discourse relations are marked with explicit connectives, compared to $47 %$ in English), annotators labeled this corpus by directly mapping pairs of sentences to 11 sense tags, without starting with a lexical discourse connector.",
        "chapter": "Discourse Coherence",
        "section": "Coherence Relations",
        "subsection": "Rhetorical Structure Theory",
        "subsubsection": "N/A"
    },
    {
        "content": "There are corpora for many discourse coherence models; the RST Discourse TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It consists of 385 English language documents selected from the Penn Treebank, with full RST parses for each one, using a large set of 78 distinct relations, grouped into 16 classes. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian Portuguese (Braud et al., 2017). \nNow that we’ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. \n22.1.2 Penn Discourse TreeBank (PDTB) \nPDTB \nThe Penn Discourse TreeBank (PDTB) is a second commonly used dataset that embodies another model of coherence relations (Miltsakaki et al. 2004, Prasad et al. 2008, Prasad et al. 2014). PDTB labeling is lexically grounded. Instead of asking annotators to directly tag the coherence relation between text spans, they were given a list of discourse connectives, words that signal discourse relations, like because, although, when, since, or as a result. In a part of a text where these words marked a coherence relation between two text spans, the connective and the spans were then annotated, as in Fig. 22.13, where the phrase as a result signals a causal relationship between what PDTB calls Arg1 (the first two sentences, here in italics) and Arg2 (the third sentence, here in bold). \n(22.13) Jewelry displays in department stores were often cluttered and uninspired. And the merchandise was, well, fake. As a result, marketers of faux gems steadily lost space in department stores to more fashionable rivals—cosmetics makers.   \n(22.14) In July, the Environmental Protection Agency imposed a gradual ban on virtually all uses of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. \nNot all coherence relations are marked by an explicit discourse connective, and so the PDTB also annotates pairs of neighboring sentences with no explicit signal, like (22.14). The annotator first chooses the word or phrase that could have been its signal (in this case as a result), and then labels its sense. For example for the ambiguous discourse connective since annotators marked whether it is using a CAUSAL or a TEMPORAL sense. \nThe final dataset contains roughly 18,000 explicit relations and 16,000 implicit relations. Fig. 22.2 shows examples from each of the 4 major semantic classes, while Fig. 22.3 shows the full tagset. \nUnlike the RST Discourse Treebank, which integrates these pairwise coherence relations into a global tree structure spanning an entire discourse, the PDTB does not annotate anything above the span-pair level, making no commitment with respect to higher-level discourse structure. \nThere are also treebanks using similar methods for other languages; (22.15) shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015). Because Chinese has a smaller percentage of explicit discourse connectives than English (only $2 2 %$ of all discourse relations are marked with explicit connectives, compared to $47 %$ in English), annotators labeled this corpus by directly mapping pairs of sentences to 11 sense tags, without starting with a lexical discourse connector. \n\n(22.15) [Conn 为] [Arg2 推动图们江地区开发] ，[Arg1 韩国捐款一百万美元 设立了图们江发展基金] “[In order to] [Arg2 promote the development of the Tumen River region], [Arg1 South Korea donated one million dollars to establish the Tumen River Development Fund].” \nThese discourse treebanks have been used for shared tasks on multilingual discourse parsing (Xue et al., 2016). \n22.2 Discourse Structure Parsing \ndiscourse parsing \nGiven a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse",
        "chapter": "Discourse Coherence",
        "section": "Coherence Relations",
        "subsection": "Penn Discourse TreeBank (PDTB)",
        "subsubsection": "N/A"
    },
    {
        "content": "tree as we do for RST). \n22.2.1 EDU segmentation for RST parsing \nRST parsing is generally done in two stages. The first stage, EDU segmentation, extracts the start and end of each EDU. The output of this stage would be a labeling like the following: \n(22.16) [Mr. Rambo $mathrm { s a y s l } _ { mathrm { e } 1 }$ [that a 3.2-acre propert $boldsymbol { mathrm { y l } } _ { mathrm { e } 2 }$ [overlooking the San Fernando $mathrm { Delta V a l l e y l e 3 }$ [is priced at $$ 4$ million $mathrm { l e } 4$ [because the late actor Erroll Flynn once lived there.]e5 \nSince EDUs roughly correspond to clauses, early models of EDU segmentation first ran a syntactic parser, and then post-processed the output. Modern systems generally use neural sequence models supervised by the gold EDU segmentation in datasets like the RST Discourse Treebank. Fig. 22.4 shows an example architecture simplified from the algorithm of Lukasik et al. (2020) that predicts for each token whether or not it is a break. Here the input sentence is passed through an encoder and then passed through a linear layer and a softmax to produce a sequence of 0s and 1, where 1 indicates the start of an EDU. \n22.2.2 RST parsing \nTools for building RST coherence structure for a discourse have long been based on syntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern RST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we saw in Chapter 14, using representation learning to build representations for each span, and training a parser to choose the correct shift and reduce actions based on the gold parses in the training set. \nWe’ll describe the shift-reduce parser of $mathrm { Y u }$ et al. (2018). The parser state consists of a stack and a queue, and produces this structure by taking a series of actions on the states. Actions include: \n• shift: pushes the first EDU in the queue onto the stack creating a single-node subtree. • reduce(l,d): merges the top two subtrees on the stack, where $l$ is the coherence relation label, and $d$ is the nuclearity direction, $d in { N N , N S , S N }$ . \nAs well as the pop root operation, to remove the final tree from the stack. \nFig. 22.6 shows the actions the parser takes to build the structure in Fig. 22.5.",
        "chapter": "Discourse Coherence",
        "section": "Discourse Structure Parsing",
        "subsection": "EDU segmentation for RST parsing",
        "subsubsection": "N/A"
    },
    {
        "content": "tree as we do for RST). \n22.2.1 EDU segmentation for RST parsing \nRST parsing is generally done in two stages. The first stage, EDU segmentation, extracts the start and end of each EDU. The output of this stage would be a labeling like the following: \n(22.16) [Mr. Rambo $mathrm { s a y s l } _ { mathrm { e } 1 }$ [that a 3.2-acre propert $boldsymbol { mathrm { y l } } _ { mathrm { e } 2 }$ [overlooking the San Fernando $mathrm { Delta V a l l e y l e 3 }$ [is priced at $$ 4$ million $mathrm { l e } 4$ [because the late actor Erroll Flynn once lived there.]e5 \nSince EDUs roughly correspond to clauses, early models of EDU segmentation first ran a syntactic parser, and then post-processed the output. Modern systems generally use neural sequence models supervised by the gold EDU segmentation in datasets like the RST Discourse Treebank. Fig. 22.4 shows an example architecture simplified from the algorithm of Lukasik et al. (2020) that predicts for each token whether or not it is a break. Here the input sentence is passed through an encoder and then passed through a linear layer and a softmax to produce a sequence of 0s and 1, where 1 indicates the start of an EDU. \n22.2.2 RST parsing \nTools for building RST coherence structure for a discourse have long been based on syntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern RST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we saw in Chapter 14, using representation learning to build representations for each span, and training a parser to choose the correct shift and reduce actions based on the gold parses in the training set. \nWe’ll describe the shift-reduce parser of $mathrm { Y u }$ et al. (2018). The parser state consists of a stack and a queue, and produces this structure by taking a series of actions on the states. Actions include: \n• shift: pushes the first EDU in the queue onto the stack creating a single-node subtree. • reduce(l,d): merges the top two subtrees on the stack, where $l$ is the coherence relation label, and $d$ is the nuclearity direction, $d in { N N , N S , S N }$ . \nAs well as the pop root operation, to remove the final tree from the stack. \nFig. 22.6 shows the actions the parser takes to build the structure in Fig. 22.5. \n$e _ { 1 }$ : American Telephone & Telegraph Co. said it $e _ { 2 }$ : will lay off 75 to 85 technicians here , effective Nov. 1. e3: The workers install , maintain and repair its private branch exchanges, $e _ { 4 }$ : which are large intracompany telephone networks. \neTsthieg $mathrm { Y u }$ ehteail.m(p2li0c1it8)syunsteasxafneaetnucroederx-tdraectoidoenraprpcrhoitaechtufroer, RwSheTreptahrse negn. Idnera irasepraraenseientitmsonpt-thbyeasisnetpdautnte,euparanldomtfohdweeolfi dfnsoaranhsditsaEteaDsUrke,spuwrsehiisnceghnatishiaebrlfaeucltlho rcienalscoubrlitpL.o TaTteMhevraTerhieoa efinersxittpilboi iLntSshTiyeMsrtaelracmyhe:ircarlebpir-edsiernetcs itohnealwoLrSdTs iMnsi(dBe -aLnSETDMUs,)atnodetnhceodsectoenxtds,reapnrdesfeunrtsh othne-bEasDeUd smeqoudelncwe.i hGidvyenamniicnporuatcslen. Bnacse $w _ { 1 } , w _ { 2 } , . . . , w _ { m }$ ,otshedwmorodselc,anwebestruedprye-t spwernhotipecodhsaersdeuimsmuopavl e(cisbtytshsytenatfiiacrxseftemaEbteuDrdeUdsin gnWst,ehcceoomqnbudieuncuatteieoxnpnsetrowiitmtheencthsatroancktae,rsfteoarmndbmaeirdndgiRnaSgsTiodnr targls,onorectoanl.t,ex2t0u0al3)e.mFbiersdtd, nwges)evrealsulatiengthien paenrfionrpmutanwcoerdofreopurrespernotaptoisoendsterqaunesnitcieo $pmb { times } _ { 1 } ^ { w } , pmb { times } _ { 2 } ^ { w } , . . . , pmb { times } _ { m } ^ { w }$ .icThemr esruglteosf the  tworpdt-lwevoelsbuibLtSrTeeMs os tnhtehnea steaqcuke,ncwe hofe $boldsymbol { mathsf { h } } ^ { boldsymbol { w } }$ lvailsuaes:d \nA(nL iEeDt Ual.o,f2s0p1a5n $w _ { s } , w _ { s + 1 } , . . . , w _ { t }$ wthilelnbhearseblieLasSeTdMforutppubtlircepurnesdenr tahtieonA $pmb { mathsf { h } } _ { s } ^ { w } , pmb { mathsf { h } } _ { s + 1 } ^ { w } , . . . , pmb { mathsf { h } } _ { t } ^ { w }$ , and is represented by average pooling: \nsThiencsleucdoindg tahye rtruasnesi tihoins-ibnapsuetdtonceourmalpumteoad efil,natlhre pdryensaenmtiact ornaoclfethsetrsaeteqguyenacnedoft suE,reDweUhxtrererapecrteisoaencnthaptlipiornoesa $ { mathbf { h } } ^ { e }$ .:c \nsTehde dDeicsocdoeurrisethPeanrasifneegdforward network $boldsymbol { mathsf { w } }$ that outputs an action $o$ based on a concatenation of the top three subtrees on the stack $( s _ { o } , s _ { 1 } , s _ { 2 } )$ plus the first EDU in the queue $left( q _ { 0 } right)$ : \n..w.,hehren}t,heanredprtehsendtaetciondeorf tphredEiDctUs nonextthes qeupeuaec ihtoeqa0ns,coc2om0n1es4d)i.d Ioenc telhydeofronomltohtweh ene etnracnosdietir,onansdystheemtfhorreeRhSiTd deisncvoeucrtsoersparespirnegs,eantidntghepnaritinatlrotdreuecse aorue cnoemurpaultneed wbyo average pooling over the encoder output for the EDUs in those trees: \nTraining first maps each RST gold parse tree into a sequence of oracle actions, and then uses the standard cross-entropy loss (with $l _ { 2 }$ regularization) to train the system to take such actions. Give a state $s$ and oracle action $a$ , we first compute the decoder output using Eq. 22.20, apply a softmax to get probabilities: \nand then computing the cross-entropy loss: \nRST discourse parsers are evaluated on the test section of the RST Discourse Treebank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu, 2000b). It is standard to first transform the gold RST trees into right-branching binary trees, and to report four metrics: trees with no labels (S for Span), labeled with nuclei (N), with relations (R), or both (F for Full), for each metric computing micro-averaged $mathrm { F } _ { 1 }$ over all spans from all documents (Marcu 2000b, Morey et al. 2017). \n22.2.3 PDTB discourse parsing \nPDTB discourse parsing, the task of detecting PDTB coherence relations between spans, is sometimes called shallow discourse parsing because the task just involves flat relationships between text spans, rather than the full trees of RST parsing. \nThe set of four subtasks for PDTB discourse parsing was laid out by Lin et al. (2014) in the first complete system, with separate tasks for explicit (tasks 1-3) and implicit (task 4) connectives: \n1. Find the discourse connectives (disambiguating them from non-discourse uses)   \n2. Find the two spans for each connective   \n3. Label the relationship between these spans   \n4. Assign a relation between every adjacent pair of sentences \nMany systems have been proposed for Task 4: taking a pair of adjacent sentences as input and assign a coherence relation sense label as output. The setup often follows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each adjacent span one of the 11 second-level PDTB tags or none (removing the 5 very rare tags of the 16 shown in italics in Fig. 22.3). \nA simple but very strong algorithm for Task 4 is to represent each of the two spans by BERT embeddings and take the last layer hidden state corresponding to the position of the [CLS] token, pass this through a single layer tanh feedforward network and then a softmax for sense classification (Nie et al., 2019). \nEach of the other tasks also have been addressed. Task 1 is to disambiguating discourse connectives from their non-discourse use. For example as Pitler and Nenkova (2009) point out, the word and is a discourse connective linking the two clauses by an elaboration/expansion relation in (22.24) while it’s a non-discourse NP conjunction in (22.25): \n(22.24) Selling picked up as previous buyers bailed out of their positions and aggressive short sellers—anticipating further declines—moved in. (22.25) My favorite colors are blue and green.",
        "chapter": "Discourse Coherence",
        "section": "Discourse Structure Parsing",
        "subsection": "RST parsing",
        "subsubsection": "N/A"
    },
    {
        "content": "Training first maps each RST gold parse tree into a sequence of oracle actions, and then uses the standard cross-entropy loss (with $l _ { 2 }$ regularization) to train the system to take such actions. Give a state $s$ and oracle action $a$ , we first compute the decoder output using Eq. 22.20, apply a softmax to get probabilities: \nand then computing the cross-entropy loss: \nRST discourse parsers are evaluated on the test section of the RST Discourse Treebank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu, 2000b). It is standard to first transform the gold RST trees into right-branching binary trees, and to report four metrics: trees with no labels (S for Span), labeled with nuclei (N), with relations (R), or both (F for Full), for each metric computing micro-averaged $mathrm { F } _ { 1 }$ over all spans from all documents (Marcu 2000b, Morey et al. 2017). \n22.2.3 PDTB discourse parsing \nPDTB discourse parsing, the task of detecting PDTB coherence relations between spans, is sometimes called shallow discourse parsing because the task just involves flat relationships between text spans, rather than the full trees of RST parsing. \nThe set of four subtasks for PDTB discourse parsing was laid out by Lin et al. (2014) in the first complete system, with separate tasks for explicit (tasks 1-3) and implicit (task 4) connectives: \n1. Find the discourse connectives (disambiguating them from non-discourse uses)   \n2. Find the two spans for each connective   \n3. Label the relationship between these spans   \n4. Assign a relation between every adjacent pair of sentences \nMany systems have been proposed for Task 4: taking a pair of adjacent sentences as input and assign a coherence relation sense label as output. The setup often follows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each adjacent span one of the 11 second-level PDTB tags or none (removing the 5 very rare tags of the 16 shown in italics in Fig. 22.3). \nA simple but very strong algorithm for Task 4 is to represent each of the two spans by BERT embeddings and take the last layer hidden state corresponding to the position of the [CLS] token, pass this through a single layer tanh feedforward network and then a softmax for sense classification (Nie et al., 2019). \nEach of the other tasks also have been addressed. Task 1 is to disambiguating discourse connectives from their non-discourse use. For example as Pitler and Nenkova (2009) point out, the word and is a discourse connective linking the two clauses by an elaboration/expansion relation in (22.24) while it’s a non-discourse NP conjunction in (22.25): \n(22.24) Selling picked up as previous buyers bailed out of their positions and aggressive short sellers—anticipating further declines—moved in. (22.25) My favorite colors are blue and green. \nSimilarly, once is a discourse connective indicating a temporal relation in (22.26), but simply a non-discourse adverb meaning ‘formerly’ and modifying used in (22.27): \n(22.26) The asbestos fiber, crocidolite, is unusually resilient once it enters the lungs, with even brief exposures to it causing symptoms that show up decades later, researchers said.   \n(22.27) A form of asbestos once used to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than 30 years ago, researchers reported. \nDetermining whether a word is a discourse connective is thus a special case of word sense disambiguation. Early work on disambiguation showed that the 4 PDTB high-level sense classes could be disambiguated with high $( 9 4 % )$ accuracy used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent work performs the task end-to-end from word inputs using a biLSTM-CRF with BIO outputs (B-CONN, I-CONN, O) (Yu et al., 2019). \nFor task 2, PDTB spans can be identified with the same sequence models used to find RST EDUs: a biLSTM sequence model with pretrained contextual embedding (BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a baseline at finding spans, since $9 3 %$ of relations are either completely within a single sentence or span two adjacent sentences, with one argument in each sentence (Biran and McKeown, 2015). \n22.3 Centering and Entity-Based Coherence \nentity-based \nA second way a discourse can be coherent is by virtue of being “about” some entity. This idea that at each point in the discourse some entity is salient, and a discourse is coherent by continuing to discuss the same entity, appears early in functional linguistics and the psychology of discourse (Chafe 1976, Kintsch and Van Dijk 1978), and soon made its way to computational models. In this section we introduce two models of this kind of entity-based coherence: Centering Theory (Grosz et al., 1995), and the entity grid model of Barzilay and Lapata (2008). \nCentering Theory \n22.3.1 Centering \nCentering Theory (Grosz et al., 1995) is a theory of both discourse salience and discourse coherence. As a model of discourse salience, Centering proposes that at any given point in the discourse one of the entities in the discourse model is salient: it is being “centered” on. As a model of discourse coherence, Centering proposes that discourses in which adjacent sentences CONTINUE to maintain the same salient entity are more coherent than those which SHIFT back and forth between multiple entities (we will see that CONTINUE and SHIFT are technical terms in the theory). \nThe following two texts from Grosz et al. (1995) which have exactly the same propositional content but different saliences, can help in understanding the main Centering intuition. \n(22.28) a. John went to his favorite music store to buy a piano. b. He had frequented the store for many years. c. He was excited that he could finally buy a piano. d. He arrived just as the store was closing for the day.",
        "chapter": "Discourse Coherence",
        "section": "Discourse Structure Parsing",
        "subsection": "PDTB discourse parsing",
        "subsubsection": "N/A"
    },
    {
        "content": "Similarly, once is a discourse connective indicating a temporal relation in (22.26), but simply a non-discourse adverb meaning ‘formerly’ and modifying used in (22.27): \n(22.26) The asbestos fiber, crocidolite, is unusually resilient once it enters the lungs, with even brief exposures to it causing symptoms that show up decades later, researchers said.   \n(22.27) A form of asbestos once used to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than 30 years ago, researchers reported. \nDetermining whether a word is a discourse connective is thus a special case of word sense disambiguation. Early work on disambiguation showed that the 4 PDTB high-level sense classes could be disambiguated with high $( 9 4 % )$ accuracy used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent work performs the task end-to-end from word inputs using a biLSTM-CRF with BIO outputs (B-CONN, I-CONN, O) (Yu et al., 2019). \nFor task 2, PDTB spans can be identified with the same sequence models used to find RST EDUs: a biLSTM sequence model with pretrained contextual embedding (BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a baseline at finding spans, since $9 3 %$ of relations are either completely within a single sentence or span two adjacent sentences, with one argument in each sentence (Biran and McKeown, 2015). \n22.3 Centering and Entity-Based Coherence \nentity-based \nA second way a discourse can be coherent is by virtue of being “about” some entity. This idea that at each point in the discourse some entity is salient, and a discourse is coherent by continuing to discuss the same entity, appears early in functional linguistics and the psychology of discourse (Chafe 1976, Kintsch and Van Dijk 1978), and soon made its way to computational models. In this section we introduce two models of this kind of entity-based coherence: Centering Theory (Grosz et al., 1995), and the entity grid model of Barzilay and Lapata (2008). \nCentering Theory \n22.3.1 Centering \nCentering Theory (Grosz et al., 1995) is a theory of both discourse salience and discourse coherence. As a model of discourse salience, Centering proposes that at any given point in the discourse one of the entities in the discourse model is salient: it is being “centered” on. As a model of discourse coherence, Centering proposes that discourses in which adjacent sentences CONTINUE to maintain the same salient entity are more coherent than those which SHIFT back and forth between multiple entities (we will see that CONTINUE and SHIFT are technical terms in the theory). \nThe following two texts from Grosz et al. (1995) which have exactly the same propositional content but different saliences, can help in understanding the main Centering intuition. \n(22.28) a. John went to his favorite music store to buy a piano. b. He had frequented the store for many years. c. He was excited that he could finally buy a piano. d. He arrived just as the store was closing for the day. \nbackwardlooking center \nforward-looking center \n(22.29) a. John went to his favorite music store to buy a piano. b. It was a store John had frequented for many years. c. He was excited that he could finally buy a piano. d. It was closing just as John arrived. \nWhile these two texts differ only in how the two entities (John and the store) are realized in the sentences, the discourse in (22.28) is intuitively more coherent than the one in (22.29). As Grosz et al. (1995) point out, this is because the discourse in (22.28) is clearly about one individual, John, describing his actions and feelings. The discourse in (22.29), by contrast, focuses first on John, then the store, then back to John, then to the store again. It lacks the “aboutness” of the first discourse. \nCentering Theory realizes this intuition by maintaining two representations for each utterance $U _ { n }$ . The backward-looking center of $U _ { n }$ , denoted as $C _ { b } ( U _ { n } )$ , represents the current salient entity, the one being focused on in the discourse after $U _ { n }$ is interpreted. The forward-looking centers of $U _ { n }$ , denoted as $C _ { f } ( U _ { n } )$ , are a set of potential future salient entities, the discourse entities evoked by $U _ { n }$ any of which could serve as $C _ { b }$ (the salient entity) of the following utterance, i.e. $C _ { b } ( U _ { n + 1 } )$ . \nThe set of forward-looking centers $C _ { f } ( U _ { n } )$ are ranked according to factors like discourse salience and grammatical role (for example subjects are higher ranked than objects, which are higher ranked than all other grammatical roles). We call the highest-ranked forward-looking center $C _ { p }$ (for “preferred center”). $C _ { p }$ is a kind of prediction about what entity will be talked about next. Sometimes the next utterance indeed talks about this entity, but sometimes another entity becomes salient instead. \nWe’ll use here the algorithm for centering presented in Brennan et al. (1987), which defines four intersentential relationships between a pair of utterances $U _ { n }$ and $U _ { n + 1 }$ that depend on the relationship between $C _ { b } ( U _ { n + 1 } )$ , $C _ { b } ( U _ { n } )$ , and $C _ { p } ( U _ { n + 1 } )$ ; these are shown in Fig. 22.7. \nThe following rules are used by the algorithm: \nRule 1: If any element of $C _ { f } ( U _ { n } )$ is realized by a pronoun in utterance $U _ { n + 1 }$ , then $C _ { b } ( U _ { n + 1 } )$ must be realized as a pronoun also.   \nRule 2: Transition states are ordered. Continue is preferred to Retain is preferred to Smooth-Shift is preferred to Rough-Shift. \nRule 1 captures the intuition that pronominalization (including zero-anaphora) is a common way to mark discourse salience. If there are multiple pronouns in an utterance realizing entities from the previous utterance, one of these pronouns must realize the backward center $C _ { b }$ ; if there is only one pronoun, it must be $C _ { b }$ . \nRule 2 captures the intuition that discourses that continue to center the same entity are more coherent than ones that repeatedly shift to other centers. The transition table is based on two factors: whether the backward-looking center $C _ { b }$ is the same from $U _ { n }$ to $U _ { n + 1 }$ and whether this discourse entity is the one that was preferred $( C _ { p } )$ from $U _ { n }$ . If both of these hold, a CONTINUE relation, the speaker has been talking about the same entity and is going to continue talking about that entity. In a RETAIN relation, the speaker intends to SHIFT to a new entity in a future utterance and meanwhile places the current entity in a lower rank $C _ { f }$ . In a SHIFT relation, the speaker is shifting to a new salient entity. \n\nLet’s walk though the start of (22.28) again, repeated as (22.30), showing the representations after each utterance is processed. \n(22.30) John went to his favorite music store to buy a piano. $( U _ { 1 } )$ He was excited that he could finally buy a piano. $( U _ { 2 } )$ ) He arrived just as the store was closing for the day. $( U _ { 3 } )$ . It was closing just as John arrived $( U _ { 4 } )$ ) \nUsing the grammatical role hierarchy to order the $mathbf { C } _ { f }$ , for sentence $U _ { 1 }$ we get: \n$C _ { f } ( U _ { 1 } )$ : John, music store, piano   \n$C _ { p } ( U _ { 1 } )$ : John   \n$C _ { b } ( U _ { 1 } )$ : undefined \nand then for sentence $U _ { 2 }$ : \nCf (U2): John, piano   \n$C _ { p } ( U _ { 2 } )$ : John   \n$C _ { b } ( U _ { 2 } )$ : John   \nResult: Continue $( C _ { p } ( U _ { 2 } ) { = } C _ { b } ( U _ { 2 } ) ; C _ { b } ( U _ { 1 } )$ undefined) \nThe transition from $U _ { 1 }$ to $U _ { 2 }$ is thus a CONTINUE. Completing this example is left as exercise (1) for the reader \n22.3.2 Entity Grid model \nCentering embodies a particular theory of how entity mentioning leads to coherence: that salient entities appear in subject position or are pronominalized, and that discourses are salient by means of continuing to mention the same entity in such ways. \nThe entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. \nThe model is based around an entity grid, a two-dimensional array that represents the distribution of entity mentions across sentences. The rows represent sentences, and the columns represent discourse entities (most versions of the entity grid model focus just on nominal mentions). Each cell represents the possible appearance of an entity in a sentence, and the values represent whether the entity appears and its grammatical role. Grammatical roles are subject (S), object (O), neither (X), or absent $( - )$ ; in the implementation of Barzilay and Lapata (2008), subjects of passives are represented with O, leading to a representation with some of the characteristics of thematic roles. \nFig. 22.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 22.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is $mathrm { ~ O ~ -- ~ } X$ , showing that the trial appears in the first sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference rerseatonlauklt.edo2n0h0itg4o;hcePlruotsehtseiaronteohtbejaemlc  s2n,t0o0w4hd)i,csbhcouitunrtshueirseninast tenieortsan(akCnehdoaphtiegorhne2rf2ot)rhasonuwtrhelemlroaedste.pl .aFroAsrinegxoabthvmie sesnotleuntcieosn tfo rgiedtegnrtaifmyimnagtiecnatlitryolcelas.s",
        "chapter": "Discourse Coherence",
        "section": "Centering and Entity-Based Coherence",
        "subsection": "Centering",
        "subsubsection": "N/A"
    },
    {
        "content": "Let’s walk though the start of (22.28) again, repeated as (22.30), showing the representations after each utterance is processed. \n(22.30) John went to his favorite music store to buy a piano. $( U _ { 1 } )$ He was excited that he could finally buy a piano. $( U _ { 2 } )$ ) He arrived just as the store was closing for the day. $( U _ { 3 } )$ . It was closing just as John arrived $( U _ { 4 } )$ ) \nUsing the grammatical role hierarchy to order the $mathbf { C } _ { f }$ , for sentence $U _ { 1 }$ we get: \n$C _ { f } ( U _ { 1 } )$ : John, music store, piano   \n$C _ { p } ( U _ { 1 } )$ : John   \n$C _ { b } ( U _ { 1 } )$ : undefined \nand then for sentence $U _ { 2 }$ : \nCf (U2): John, piano   \n$C _ { p } ( U _ { 2 } )$ : John   \n$C _ { b } ( U _ { 2 } )$ : John   \nResult: Continue $( C _ { p } ( U _ { 2 } ) { = } C _ { b } ( U _ { 2 } ) ; C _ { b } ( U _ { 1 } )$ undefined) \nThe transition from $U _ { 1 }$ to $U _ { 2 }$ is thus a CONTINUE. Completing this example is left as exercise (1) for the reader \n22.3.2 Entity Grid model \nCentering embodies a particular theory of how entity mentioning leads to coherence: that salient entities appear in subject position or are pronominalized, and that discourses are salient by means of continuing to mention the same entity in such ways. \nThe entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. \nThe model is based around an entity grid, a two-dimensional array that represents the distribution of entity mentions across sentences. The rows represent sentences, and the columns represent discourse entities (most versions of the entity grid model focus just on nominal mentions). Each cell represents the possible appearance of an entity in a sentence, and the values represent whether the entity appears and its grammatical role. Grammatical roles are subject (S), object (O), neither (X), or absent $( - )$ ; in the implementation of Barzilay and Lapata (2008), subjects of passives are represented with O, leading to a representation with some of the characteristics of thematic roles. \nFig. 22.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 22.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is $mathrm { ~ O ~ -- ~ } X$ , showing that the trial appears in the first sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference rerseatonlauklt.edo2n0h0itg4o;hcePlruotsehtseiaronteohtbejaemlc  s2n,t0o0w4hd)i,csbhcouitunrtshueirseninast tenieortsan(akCnehdoaphtiegorhne2rf2ot)rhasonuwtrhelemlroaedste.pl .aFroAsrinegxoabthvmie sesnotleuntcieosn tfo rgiedtegnrtaifmyimnagtiecnatlitryolcelas.s \n\noIonl thaetrdeestuelrtimnignegsridw,hcicohlunmonusn tphart asres rdefnesreto(ltihke stahemecoelnutimtyn fnora dMoicuromseonft). indicate entities that are mentioned often in the texts; sparse columns (like the column for earnings) indicate entities that are mentioned rarely. \noIntrtahdeiectnotirtypgarirdwmisoedcella,scsiofihcearteinocnes iasnmdecaosnusrterducbtsy apapttaertintsi onf lonctalh ensteittyoftrNaPnssitAoioufrnu.n Fpaoemrieemntxeanltmas,spslewu,mDpetemipopanlrotuymnedNnetg iyasinadg soCuabrjdaeicpet’psron(2a0sce0h2ni)tectnhocraet te1hr, andncieds tiehbseuontluontinotonmseyensnttitioiTnhecedoshiynesrtseenmnt tedenexctcsiede2ex;shtihwbiihstesishctehrtetaitwrnoarneNsgiPtuisloanr et[Secso–r]e.fleTerechtnetdrbaiynsegixtripioldonitstoianprgoe oathgwyu.seSaslotehmqueoefonlfcetexhsi ${ mathbf { s } , mathbf { o } ~ mathbf { X } , - } ^ { n }$ awla,rhseiecfmhoarcnmatniacl,ibzaendedxiptnroascCiteieondtnearlsinecgaotnTuthirenesuo.roIytusiassceclaloisnesftrdroaoimn teshaecohnMctUroaClnu(sm6it–ni7.o dsEatocafhs trlaoncsdaitliyfoioenclduhsasitna tapedr-joafbc-atehbnietl-isaterytn;tptehenrecfeopsr.omGbarnibcdielsi(to7yf0.oc4foFh[-Serm–e]natisntuertxehtesonagrieMdlUifkrCeo-l6myatFnoidgh.6a32v.2e4.s8onismMe0Ud0eC8 (itcolcucmurnss6(it.ie.m,ecsolou tmonfs hweit7h5 utostaal freawnsigtiaopns,s soufclhenags hMtiwcro)s.ofFt ign. 2T2a.b1le0 s1)hoawnsd hme distprairbsueticonluomvenrs twrahniscihtiownilsl cofonlseinstgtmh o2stfloyrotfhgeatpesxt(soefe Fmiagr.ke2t2s.9an(dsheoarwninags itnheTafibrlset roTwa $d _ { 1 }$ ),3 and 2 other documents. \nThe transitions and their probabilities can then be used as features for a machine leairning model. This model can be a texti jclassifi1er jtrai2nedij to prodmuceij human-labeled coihnegrreindcxe s.cTohriesf(efaotrureexavemcptloer frreoprmesheuntmatinosnliasbeulsienfgulelyacahmtenxtabalsectohmeraecnht noer lienacronhearlegnotr)i. hBmuts s(useceh doautraeisxpeexrpiemnesnitvse itno gSaetchtieor.nsB4ar–z6i)l.aFyuarnthdeLr ampoartea, it20a0ll5o)wisn trhoeducoends a esriamtipolnifyoifnlga ignenonvuamtiboenr: ocfothrearnesnictieonmsowdehlischcaconulbde tproatienetdialbly usenlcfo-svuerpenrovieslioen: tradiinsterdibtuot odinstpiantgteurisnhs tehlevnaantufroarlcorhiegrienacleoarsdsers omfesnetnotreontches icnohaerdeinsceo-urrelsae efrdotams a modified order (such as a randomized order). We turn to these evaluations in the next section. \n\n22.3.3 Evaluating Neural and Entity-based coherence \nEntity-based coherence models, as well as the neural models we introduce in the next section, are generally evaluated in one of two ways. \nFirst, we can have humans rate the coherence of a document and train a classifier to predict these human ratings, which can be categorial (high/low, or high/mid/low) or continuous. This is the best evaluation to use if we have some end task in mind, like essay grading, where human raters are the correct definition of the final label. \nAlternatively, since it’s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering. \nSelf-supervision has been implemented in 3 ways. In the sentence order discrimination task (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentence. A model is considered correct for an (original, permuted) test pair if it ranks the original document higher. Given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. \nIn the sentence insertion task (Chen et al., 2007) we take a document, remove one of the $n$ sentences $s$ , and create $n - 1$ copies of the document with $s$ inserted into each position. The task is to decide which of the $n$ documents is the one with the original ordering, distinguishing the original position for $s$ from all other positions. Insertion is harder than discrimination since we are comparing documents that differ by only one sentence. \nFinally, in the sentence order reconstruction task (Lapata, 2003), we take a document, randomize the sentences, and train the model to put them back in the correct order. Again given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. Reordering is of course a much harder task than simple classification. \n22.4 Representation learning models for local coherence \nlexical cohesion \nThe third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so. \nTextTiling \nThe field was pioneered by a series of unsupervised models in the 1990s of this kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semantically related words in nearby sentences. Morris and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that occurred through a discourse and that were related in Roget’s Thesaurus (by being in the same category, or linked categories). They showed that the number and density of chain correlated with the topic structure. The TextTiling algorithm of Hearst (1997) computed the cosine between neighboring text spans (the normalized dot product of vectors of raw word counts), again showing that sentences or paragraph in a subtopic have high cosine with each other, but not with sentences in a neighboring subtopic.",
        "chapter": "Discourse Coherence",
        "section": "Centering and Entity-Based Coherence",
        "subsection": "Entity Grid model",
        "subsubsection": "N/A"
    },
    {
        "content": "22.3.3 Evaluating Neural and Entity-based coherence \nEntity-based coherence models, as well as the neural models we introduce in the next section, are generally evaluated in one of two ways. \nFirst, we can have humans rate the coherence of a document and train a classifier to predict these human ratings, which can be categorial (high/low, or high/mid/low) or continuous. This is the best evaluation to use if we have some end task in mind, like essay grading, where human raters are the correct definition of the final label. \nAlternatively, since it’s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering. \nSelf-supervision has been implemented in 3 ways. In the sentence order discrimination task (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentence. A model is considered correct for an (original, permuted) test pair if it ranks the original document higher. Given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. \nIn the sentence insertion task (Chen et al., 2007) we take a document, remove one of the $n$ sentences $s$ , and create $n - 1$ copies of the document with $s$ inserted into each position. The task is to decide which of the $n$ documents is the one with the original ordering, distinguishing the original position for $s$ from all other positions. Insertion is harder than discrimination since we are comparing documents that differ by only one sentence. \nFinally, in the sentence order reconstruction task (Lapata, 2003), we take a document, randomize the sentences, and train the model to put them back in the correct order. Again given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. Reordering is of course a much harder task than simple classification. \n22.4 Representation learning models for local coherence \nlexical cohesion \nThe third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so. \nTextTiling \nThe field was pioneered by a series of unsupervised models in the 1990s of this kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semantically related words in nearby sentences. Morris and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that occurred through a discourse and that were related in Roget’s Thesaurus (by being in the same category, or linked categories). They showed that the number and density of chain correlated with the topic structure. The TextTiling algorithm of Hearst (1997) computed the cosine between neighboring text spans (the normalized dot product of vectors of raw word counts), again showing that sentences or paragraph in a subtopic have high cosine with each other, but not with sentences in a neighboring subtopic.",
        "chapter": "Discourse Coherence",
        "section": "Centering and Entity-Based Coherence",
        "subsection": "Evaluating Neural and Entity-based coherence",
        "subsubsection": "N/A"
    },
    {
        "content": "22.3.3 Evaluating Neural and Entity-based coherence \nEntity-based coherence models, as well as the neural models we introduce in the next section, are generally evaluated in one of two ways. \nFirst, we can have humans rate the coherence of a document and train a classifier to predict these human ratings, which can be categorial (high/low, or high/mid/low) or continuous. This is the best evaluation to use if we have some end task in mind, like essay grading, where human raters are the correct definition of the final label. \nAlternatively, since it’s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering. \nSelf-supervision has been implemented in 3 ways. In the sentence order discrimination task (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentence. A model is considered correct for an (original, permuted) test pair if it ranks the original document higher. Given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. \nIn the sentence insertion task (Chen et al., 2007) we take a document, remove one of the $n$ sentences $s$ , and create $n - 1$ copies of the document with $s$ inserted into each position. The task is to decide which of the $n$ documents is the one with the original ordering, distinguishing the original position for $s$ from all other positions. Insertion is harder than discrimination since we are comparing documents that differ by only one sentence. \nFinally, in the sentence order reconstruction task (Lapata, 2003), we take a document, randomize the sentences, and train the model to put them back in the correct order. Again given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. Reordering is of course a much harder task than simple classification. \n22.4 Representation learning models for local coherence \nlexical cohesion \nThe third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so. \nTextTiling \nThe field was pioneered by a series of unsupervised models in the 1990s of this kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semantically related words in nearby sentences. Morris and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that occurred through a discourse and that were related in Roget’s Thesaurus (by being in the same category, or linked categories). They showed that the number and density of chain correlated with the topic structure. The TextTiling algorithm of Hearst (1997) computed the cosine between neighboring text spans (the normalized dot product of vectors of raw word counts), again showing that sentences or paragraph in a subtopic have high cosine with each other, but not with sentences in a neighboring subtopic. \n\nA third early model, the LSA Coherence method of Foltz et al. (1998) was the first to use embeddings, modeling the coherence between two sentences as the cosine between their LSA sentence embedding vectors1, computing embeddings for a sentence $s$ by summing the embeddings of its words $w$ : \nand defining the overall coherence of a text as the average similarity over all pairs of adjacent sentences $s _ { i }$ and $s _ { i + 1 }$ : \nModern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. \nHere we present one such model, the local coherence discriminator (LCD) (Xu et al., 2019). Like early models, LCD computes the coherence of a text as the average of coherence scores between consecutive pairs of sentences. But unlike the early unsupervised models, LCD is a self-supervised model trained to discriminate consecutive sentence pairs $( s _ { i } , s _ { i + 1 } )$ in the training documents (assumed to be coherent) from (constructed) incoherent pairs $left( s _ { i } , s ^ { prime } right)$ . All consecutive pairs are positive examples, and the negative (incoherent) partner for a sentence $s _ { i }$ is another sentence uniformly sampled from the same document as $s _ { i }$ . \nFig. 22.11 describes the architecture of the model $f _ { theta }$ , which takes a sentence pair and returns a score, higher scores for more coherent pairs. Given an input sentence pair $s$ and $t$ , the model computes sentence embeddings s and t (using any sentence embeddings algorithm), and then concatenates four features of the pair: (1) the concatenation of the two vectors (2) their difference $mathsf { pmb { s } } - mathsf { pmb { t } }$ ; (3) the absolute value of their difference $left| mathsf { pmb { s } } - mathsf { pmb { t } } right|$ ; (4) their element-wise product s $odot$ t. These are passed through a one-layer feedforward network to output the coherence score. \nThe model is trained to make this coherence score higher for real pairs than for negative pairs. More formally, the training objective for a corpus $C$ of documents $d$ , each of which consists of a list of sentences $s _ { i }$ , is: \n$mathbb { E } _ { p ( s ^ { prime } mid s _ { i } ) }$ is the expectation with respect to the negative sampling distribution conditioned on $s _ { i }$ : given a sentence $s _ { i }$ the algorithms samples a negative sentence $s ^ { prime }$ umneifnot tmoly over the other sentences in the same document. $L$ is a loss function that tpakoessetwo scor4e.s2, onPerfoer-tarpaiosnietidveGpaeinrearnadtiovne foMroadnelgatsivtehpeair, with the goal of encouraging $f ^ { + } = f _ { theta } ( s _ { i } , s _ { i + 1 } )$ oE nbecohidgehrand $f ^ { - } = f _ { boldsymbol { theta } } ( s _ { i } , s ^ { prime } ) )$ to be low. Fig. 22.11 ukse the margin loss $l ( f ^ { + } , f ^ { - } ) = operatorname* { m a x } ( 0 , eta - f ^ { + } + f ^ { - } )$ where $eta$ is the margin hyperparkam−eter. \n\n$mathrm { { X u } }$ het al. t(e2n01c9e ealnscogdievre, rausnegfiunlgbafsreolimn tahlgeormithosmt shiatmiptslieslftihcas quite high ipresr, arnmdance ianvemraeagseuriGnlgopVe p(lPexeintyn: ntrgationnane RaNl.N, l2a0n1g4u)agemboede-l on the data, aonudt ctohamtpute tdhienlgosg tliokelmihoroedsofpsehnistetincae $s _ { i }$ dinstuwpoerwvaiysse,donocr guinvseun-the preceding context (conditional log likelihood) and once with no context (marginal log likelihood). The difference between these values tells us how much the preceding context improved the predictability of $s _ { i }$ , a predictability measure of coherence. \ncTroavi-ning smiondcel gteonpereadticvte omngoedrelcsoncteaxn otfhtaen buse tcuornsecduitinvteopairs of senttiecnec, swcean resuelnttienecve nenstcrodnegre,rgdeisnceoruartsieverecproehsernetanticoensm. oFdoerlecxanmple a Transowremesreleanguabge lmeovdeerlatgreaid bdywiothur cmonotdraesl tvoe beentnenficte frbjoemct teheto predict text up to a distance of $pm 2$ sentences improves performance on various discourse coherence tasks (Iter et al., 2020). \nLanguage-model style models are generally evaluated by the methods of Sectiaoinls22o.n3.3, al2t0ho1u8g). tAhefytecrainaitlisaolibzeateivoalnu,atwede ofrnetehzeRtShTe agnednePrDa-TB coherence relation tasks.t \n22.5 Global Coherence \nhA sd sdcios-urse tmhues cahlsooicceohoefrseegnltoebnaclley erantchoerd tehr.an just at the level of pairs of sentences. Consider stories, for example. The narrative structure of stories is one of the oldest kin5ds ofEgxlopbealricomhernetnsce to be studied. In his influential Morphology of the Folktale, Propp (1968) models the discourse structure of Russian folktales via a kind of plot5.g1rammEavra. uHiastimoondeTlaisnkclsudes a set of character categories he called ndroafmtahteis personae, like Hero, Villain, Donor, or Helper, and a set of events he tchaleletdwfounctioFnosll(loikwein“gVilNlaginuyceonmamnitds kJiodtnya(p2pi0n1g7”,) “aDnodnorthtesr sprHe-ro”, or “Hero eisSpursTu;ed”) tvhiaotuhs vweotrok,occwuer ienvaplaurtaitceulaoruro dmero,daellosngonwitheotdhiesr- components. tPerovpaplusehows tchriatmtihneaptliotns oafnedacihnsoefrtthieofnaitraystkasl. Ahedsdtiutidioensacllayn,bewre presented as a sequence of these functions, different tales choosing different subsets of functions, but always in the same order. Indeed Lakoff (1972b) showed that Propp’s model amounted to a discourse grammar of stories, and in recent computational work Finlayson (2016) demonstrates that some of these Proppian functions could be induced from corpora of folktale texts by detecting events that have similar actions across stories. Bamman et al. (2013) showed that generalizations over dramatis personae could be induced from movie plot summaries on Wikipedia. Their model induced latent personae from features like the actions the character takes (e.g., Villains strangle), the actions done to them (e.g., Villains are foiled and arrested) or the descriptive words used of them (Villains are evil).",
        "chapter": "Discourse Coherence",
        "section": "Representation learning models for local coherence",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "In this section we introduce two kinds of such global discourse structure that have been widely studied computationally. The first is the structure of arguments: the way people attempt to convince each other in persuasive essays by offering claims and supporting premises. The second is somewhat related: the structure of scientific papers, and the way authors present their goals, results, and relationship to prior work in their papers. \n22.5.1 Argumentation Structure \nargumentation mining \npathos ethos logos \nclaims premises \nargumentative relations \nThe first type of global discourse structure is the structure of arguments. Analyzing people’s argumentation computationally is often called argumentation mining. \nThe study of arguments dates back to Aristotle, who in his Rhetorics described three components of a good argument: pathos (appealing to the emotions of the listener), ethos (appealing to the speaker’s personal character), and logos (the logical structure of the argument). \nMost of the discourse structure studies of argumentation have focused on logos, particularly via building and training on annotated datasets of persuasive essays or other arguments (Reed et al. 2008, Stab and Gurevych 2014a, Peldszus and Stede 2016, Habernal and Gurevych 2017, Musi et al. 2018). Such corpora, for example, often include annotations of argumentative components like claims (the central component of the argument that is controversial and needs support) and premises (the reasons given by the author to persuade the reader by supporting or attacking the claim or other premises), as well as the argumentative relations between them like SUPPORT and ATTACK. \nConsider the following example of a persuasive essay from Stab and Gurevych (2014b). The first sentence (1) presents a claim (in bold). (2) and (3) present two premises supporting the claim. (4) gives a premise supporting premise (3). \n“(1) Museums and art galleries provide a better understanding about arts than Internet. (2) In most museums and art galleries, detailed descriptions in terms of the background, history and author are provided. (3) Seeing an artwork online is not the same as watching it with our own eyes, as (4) the picture online does not show the texture or three-dimensional structure of the art, which is important to study.” \nThus this example has three argumentative relations: SUPPORT(2,1), SUPPORT(3,1) and SUPPORT(4,3). Fig. 22.12 shows the structure of a much more complex argument. \nWhile argumentation mining is clearly related to rhetorical structure and other kinds of coherence relations, arguments tend to be much less local; often a persuasive essay will have only a single main claim, with premises spread throughout the text, without the local coherence we see in coherence relations. \nargumentation schemes \nAlgorithms for detecting argumentation structure often include classifiers for distinguishing claims, premises, or non-argumentation, together with relation6c2l9assifiers for deciding if two spans have the SUPPORT, ATTACK, or neither relation (Peldszus and Stede, 2013). While these are the main focus of much computational work, there is also preliminary efforts on annotating and detecting richer semantic relationships (Park and Cardie 2014, Hidey et al. 2017) such as detecting argumentation schemes, larger-scale structures for argument like argument from example, or argument from cause to effect, or argument from consequences (Feng and Hirst, 2011). \npersuasion \nAnother important line of research is studying how these argument structure (or other features) are associated with the success or persuasiveness of an argument (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it is Aristotle’s logos that is most related to discourse structure, Aristotle’s ethos and pathos techniques are particularly relevant in the detection of mechanisms of this sort of persuasion. For example scholars have investigated the linguistic realization of features studied by social scientists like reciprocity (people return favors), social proof (people follow others’ choices), authority (people are influenced by those with power), and scarcity (people value things that are scarce), all of which can be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown (2017) showed that these features could be combined with argumentation structure to predict who influences whom on social media, Althoff et al. (2014) found that linguistic models of reciprocity and authority predicted success in online requests, while the semisupervised model of Yang et al. (2019) detected mentions of scarcity, commitment, and social identity to predict the success of peer-to-peer lending platforms. \nSee Stede and Schneider (2018) for a comprehensive survey of argument mining. \n22.5.2 The structure of scientific discourse \nScientific papers have a very specific global structure: somewhere in the course of the paper the authors must indicate a scientific goal, develop a method for a solution, provide evidence for the solution, and compare to prior work. One popular",
        "chapter": "Discourse Coherence",
        "section": "Global Coherence",
        "subsection": "Argumentation Structure",
        "subsubsection": "N/A"
    },
    {
        "content": "argumentation schemes \nAlgorithms for detecting argumentation structure often include classifiers for distinguishing claims, premises, or non-argumentation, together with relation6c2l9assifiers for deciding if two spans have the SUPPORT, ATTACK, or neither relation (Peldszus and Stede, 2013). While these are the main focus of much computational work, there is also preliminary efforts on annotating and detecting richer semantic relationships (Park and Cardie 2014, Hidey et al. 2017) such as detecting argumentation schemes, larger-scale structures for argument like argument from example, or argument from cause to effect, or argument from consequences (Feng and Hirst, 2011). \npersuasion \nAnother important line of research is studying how these argument structure (or other features) are associated with the success or persuasiveness of an argument (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it is Aristotle’s logos that is most related to discourse structure, Aristotle’s ethos and pathos techniques are particularly relevant in the detection of mechanisms of this sort of persuasion. For example scholars have investigated the linguistic realization of features studied by social scientists like reciprocity (people return favors), social proof (people follow others’ choices), authority (people are influenced by those with power), and scarcity (people value things that are scarce), all of which can be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown (2017) showed that these features could be combined with argumentation structure to predict who influences whom on social media, Althoff et al. (2014) found that linguistic models of reciprocity and authority predicted success in online requests, while the semisupervised model of Yang et al. (2019) detected mentions of scarcity, commitment, and social identity to predict the success of peer-to-peer lending platforms. \nSee Stede and Schneider (2018) for a comprehensive survey of argument mining. \n22.5.2 The structure of scientific discourse \nScientific papers have a very specific global structure: somewhere in the course of the paper the authors must indicate a scientific goal, develop a method for a solution, provide evidence for the solution, and compare to prior work. One popular \nargumentative zoning \nannotation scheme for modeling these rhetorical goals is the argumentative zoning model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the idea that each scientific paper tries to make a knowledge claim about a new piece of knowledge being added to the repository of the field (Myers, 1992). Sentences in a scientific paper can be assigned one of 15 tags; Fig. 22.13 shows 7 (shortened) examples of labeled sentences. \nTeufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientific articles from computational linguistics and chemistry, which can be used as supervision for training standard sentence-classification architecture to assign the 15 labels. \n22.6 Summary \nIn this chapter we introduced local and global models for discourse coherence. \n• Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. • Various sets of coherence relations and rhetorical relations have been proposed. The relations in Rhetorical Structure Theory (RST) hold between spans of text and are structured into a tree. Because of this, shift-reduce and other parsing algorithms are generally used to assign these structures. The Penn Discourse Treebank (PDTB) labels only relations between pairs of spans, and the labels are generally assigned by sequence models. • Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence.",
        "chapter": "Discourse Coherence",
        "section": "Global Coherence",
        "subsection": "The structure of scientific discourse",
        "subsubsection": "N/A"
    },
    {
        "content": "argumentative zoning \nannotation scheme for modeling these rhetorical goals is the argumentative zoning model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the idea that each scientific paper tries to make a knowledge claim about a new piece of knowledge being added to the repository of the field (Myers, 1992). Sentences in a scientific paper can be assigned one of 15 tags; Fig. 22.13 shows 7 (shortened) examples of labeled sentences. \nTeufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientific articles from computational linguistics and chemistry, which can be used as supervision for training standard sentence-classification architecture to assign the 15 labels. \n22.6 Summary \nIn this chapter we introduced local and global models for discourse coherence. \n• Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. • Various sets of coherence relations and rhetorical relations have been proposed. The relations in Rhetorical Structure Theory (RST) hold between spans of text and are structured into a tree. Because of this, shift-reduce and other parsing algorithms are generally used to assign these structures. The Penn Discourse Treebank (PDTB) labels only relations between pairs of spans, and the labels are generally assigned by sequence models. • Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence.",
        "chapter": "Discourse Coherence",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "argumentative zoning \nannotation scheme for modeling these rhetorical goals is the argumentative zoning model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the idea that each scientific paper tries to make a knowledge claim about a new piece of knowledge being added to the repository of the field (Myers, 1992). Sentences in a scientific paper can be assigned one of 15 tags; Fig. 22.13 shows 7 (shortened) examples of labeled sentences. \nTeufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientific articles from computational linguistics and chemistry, which can be used as supervision for training standard sentence-classification architecture to assign the 15 labels. \n22.6 Summary \nIn this chapter we introduced local and global models for discourse coherence. \n• Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. • Various sets of coherence relations and rhetorical relations have been proposed. The relations in Rhetorical Structure Theory (RST) hold between spans of text and are structured into a tree. Because of this, shift-reduce and other parsing algorithms are generally used to assign these structures. The Penn Discourse Treebank (PDTB) labels only relations between pairs of spans, and the labels are generally assigned by sequence models. • Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence.",
        "chapter": "Discourse Coherence",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "argumentative zoning \nannotation scheme for modeling these rhetorical goals is the argumentative zoning model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the idea that each scientific paper tries to make a knowledge claim about a new piece of knowledge being added to the repository of the field (Myers, 1992). Sentences in a scientific paper can be assigned one of 15 tags; Fig. 22.13 shows 7 (shortened) examples of labeled sentences. \nTeufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientific articles from computational linguistics and chemistry, which can be used as supervision for training standard sentence-classification architecture to assign the 15 labels. \n22.6 Summary \nIn this chapter we introduced local and global models for discourse coherence. \n• Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. • Various sets of coherence relations and rhetorical relations have been proposed. The relations in Rhetorical Structure Theory (RST) hold between spans of text and are structured into a tree. Because of this, shift-reduce and other parsing algorithms are generally used to assign these structures. The Penn Discourse Treebank (PDTB) labels only relations between pairs of spans, and the labels are generally assigned by sequence models. • Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence. \n• Many different genres have different types of global coherence. Persuasive essays have claims and premises that are extracted in the field of argument mining, scientific articles have structure related to aims, methods, results, and comparisons. \nBibliographical and Historical Notes \nCoherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for the hearer, and the investigations by Mann and Thompson (1987) of the discourse structure of large texts. Other approaches to coherence relations and their extraction include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue that coherence structure includes crossed bracketings, which make it impossible to represent as a tree, and propose a graph representation instead. A compendium of over 350 relations that have been proposed in the literature can be found in Hovy (1990). \nRST parsing was first proposed by Marcu (1997), and early work was rule-based, focused on discourse markers (Marcu, 2000a). The creation of the RST Discourse TreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety of machine learning algorithms, beginning with the shift-reduce parser of Marcu (1999) that used decision trees to choose actions, and continuing with a wide variety of machine learned parsing methods (Soricut and Marcu 2003, Sagae 2009, Hernault et al. 2010, Feng and Hirst 2014, Surdeanu et al. 2015, Joty et al. 2015) and chunkers (Sporleder and Lapata, 2005). Subba and Di Eugenio (2009) integrated sophisticated semantic information into RST parsing. Ji and Eisenstein (2014) first applied neural models to RST parsing neural models, leading to the modern set of neural RST models (Li et al. 2014, Li et al. 2016d, Braud et al. 2017, Yu et al. 2018, inter alia) as well as neural segmenters (Wang et al. 2018b). and neural PDTB parsing models (Ji and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017). \nBarzilay and Lapata (2005) pioneered the idea of self-supervision for coherence: training a coherence model to distinguish true orderings of sentences from random permutations. Li et al. (2014) first applied this paradigm to neural sentencerepresentation, and many neural self-supervised models followed (Li and Jurafsky 2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al. 2020) \nAnother aspect of global coherence is the global topic structure of a text, the way the topics shift over the course of the document. Barzilay and Lee (2004) introduced an HMM model for capturing topics for coherence, and later work expanded this intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li and Jurafsky 2017). \nThe relationship between explicit and implicit discourse connectives has been a fruitful one for research. Marcu and Echihabi (2002) first proposed to use sentences with explicit relations to help provide training data for implicit relations, by removing the explicit relations and trying to re-predict them as a way of improving performance on implicit connectives; this idea was refined by Sporleder and Lascarides (2005), (Pitler et al., 2009), and Rutherford and Xue (2015). This relationship can also be used as a way to create discourse-aware representations. The DisSent algorithm (Nie et al., 2019) creates the task of predicting explicit discourse markers between two sentences. They show that representations learned to be good at this task also function as powerful sentence representations for other discourse tasks. \n\nThe idea of entity-based coherence seems to have arisen in multiple fields in the mid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse processing (Kintsch and Van Dijk, 1978), and in the roughly contemporaneous work of Grosz, Sidner, Joshi, and their colleagues. Grosz (1977a) addressed the focus of attention that conversational participants maintain as the discourse unfolds. She defined two levels of focus; entities relevant to the entire discourse were said to be in global focus, whereas entities that are locally in focus (i.e., most central to a particular utterance) were said to be in immediate focus. Sidner 1979; 1983 described a method for tracking (immediate) discourse foci and their use in resolving pronouns and demonstrative noun phrases. She made a distinction between the current discourse focus and potential foci, which are the predecessors to the backwardand forward-looking centers of Centering theory, respectively. The name and further roots of the centering approach lie in papers by Joshi and Kuhn (1979) and Joshi and Weinstein (1981), who addressed the relationship between immediate focus and the inferences required to integrate the current utterance into the discourse model. Grosz et al. (1983) integrated this work with the prior work of Sidner and Grosz. This led to a manuscript on centering which, while widely circulated since 1986, remained unpublished until Grosz et al. (1995). A collection of centering papers appears in Walker et al. (1998b). See Karamanis et al. (2004) and Poesio et al. (2004) for a deeper exploration of centering and its parameterizations, and the History section of Chapter 22 for more on the use of centering on coreference. \nThe grid model of entity-based coherence was first proposed by Barzilay and Lapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then extended by them Barzilay and Lapata (2008) and others with additional features (Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that projects entities into a global graph for the discourse (Guinaudeau and Strube 2013, Mesgar and Strube 2016), and a convolutional model to capture longer-range entity dependencies (Nguyen and Joty, 2017). \nTheories of discourse coherence have also been used in algorithms for interpreting discourse-level linguistic phenomena, including verb phrase ellipsis and gapping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher 1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship between coherence relations and discourse connectives can be found in Knott and Dale (1994). \nUseful surveys of discourse processing and structure include Stede (2011) and Webber et al. (2012). \nAndy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there are some remnants of Andy’s lovely prose still in this third-edition coherence chapter. \nExercises \n22.1 Finish the Centering Theory processing of the last two utterances of (22.30), and show how (22.29) would be processed. Does the algorithm indeed mark (22.29) as less coherent?   \n22.2 Select an editorial column from your favorite newspaper, and determine the discourse structure for a 10–20 sentence portion. What problems did you encounter? Were you helped by superficial cues the speaker included (e.g., discourse connectives) in any places? \n23 Question Answering \nThe quest for knowledge is deeply human, and so it is not surprising that practically as soon as there were computers we were asking them questions. By the early 1960s, systems used the two major paradigms of question answering—informationretrieval-based and knowledge-based—to answer questions about baseball statistics or scientific facts. Even imaginary computers got into the act. Deep Thought, the computer that Douglas Adams invented in The Hitchhiker’s Guide to the Galaxy, managed to answer “the Ultimate Question Of Life, The Universe, and Everything”.1 In 2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!, surpassing humans at answering questions like: \nWILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA” INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL \nQuestion answering systems are designed to fill human information needs that might arise in situations like talking to a virtual assistant, interacting with a search engine, or querying a database. Most question answering systems focus on a particular subset of these information needs: factoid questions, questions that can be answered with simple facts expressed in short texts, like the following: \n(23.1) Where is the Louvre Museum located? (23.2) What is the average age of the onset of autism? \nIn this chapter we describe the two major paradigms for factoid question answering. Information-retrieval (IR) based QA, sometimes called open domain QA, relies on the vast amount of text on the web or in collections of scientific papers like PubMed. Given a user question, information retrieval is used to find relevant passages. Then neural reading comprehension algorithms read these retrieved passages and draw an answer directly from spans of text. \nIn the second paradigm, knowledge-based question answering, a system instead builds a semantic representation of the query, such as mapping What states border Texas? to the logical representation: $lambda x . s t a t e ( x ) wedge b o r d e r s ( x , t e x a s )$ , or When was Ada Lovelace born? to the gapped relation: birth-year (Ada Lovelace, ?x). These meaning representations are then used to query databases of facts. \nWe’ll also briefly discuss two other QA paradigms. We’ll see how to query a language model directly to answer a question, relying on the fact that huge pretrained language models have already encoded a lot of factoids. And we’ll sketch classic pre-neural hybrid question-answering algorithms that combine information from IRbased and knowledge-based sources. \nWe’ll explore the possibilities and limitations of all these approaches, along the way also introducing two technologies that are key for question answering but also relevant throughout NLP: information retrieval (a key component of IR-based QA) and entity linking (similarly key for knowledge-based QA). We’ll start in the next section by introducing the task of information retrieval.",
        "chapter": "Discourse Coherence",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "The focus of this chapter is factoid question answering, but there are many other QA tasks the interested reader could pursue, including long-form question answering (answering questions like “why” questions that require generating long answers), community question answering, (using datasets of community-created question-answer pairs like Quora or Stack Overflow), or even answering questions on human exams like the New York Regents Science Exam (Clark et al., 2019) as an NLP/AI benchmark to measure progress in the field. \n23.1 Information Retrieval \ninformation retrieval IR \nInformation retrieval or IR is the name of the field encompassing the retrieval of all manner of media based on user information needs. The resulting IR system is often called a search engine. Our goal in this section is to give a sufficient overview of IR to see its application to question answering. Readers with more interest specifically in information retrieval should see the Historical Notes section at the end of the chapter and textbooks like Manning et al. (2008). \nad hoc retrieval \ndocument \ncollection term query \nThe IR task we consider is called ad hoc retrieval, in which a user poses a query to a retrieval system, which then returns an ordered set of documents from some collection. A document refers to whatever unit of text the system indexes and retrieves (web pages, scientific papers, news articles, or even shorter passages like paragraphs). A collection refers to a set of documents being used to satisfy user requests. A term refers to a word in a collection, but it may also include phrases. Finally, a query represents a user’s information need expressed as a set of terms. The high-level architecture of an ad hoc retrieval engine is shown in Fig. 23.1. \nThe basic IR architecture uses the vector space model we introduced in Chapter 6, in which we map queries and document to vectors based on unigram word counts, and use the cosine similarity between the vectors to rank potential documents (Salton, 1971). This is thus an example of the bag-of-words model introduced in Chapter 4, since words are considered independently of their positions. \n23.1.1 Term weighting and document scoring \nLet’s look at the details of how the match between a document and query is scored. \nterm weight \nWe don’t use raw word counts in IR, instead computing a term weight for each document word. Two term weighting schemes are common: the tf-idf weighting introduced in Chapter 6, and a slightly more powerful variant called BM25. \nWe’ll reintroduce tf-idf here so readers don’t need to look back at Chapter 6. Tf-idf (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, the term frequency tf and the indirect document frequency idf. \nThe term frequency tells us how frequent the word is; words that occur more often in a document are likely to be informative about the document’s contents. We usually use the $log _ { 1 0 }$ of the word frequency, rather than the raw count. The intuition is that a word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document. Because we can’t take the log of 0, we normally add 1 to the count:3 \nIf we use log weighting, terms which occur 0 times in a document would have ${ mathrm { t f } } = log _ { 1 0 } ( 1 ) = 0$ , 10 times in a document ${ mathrm { t f } } = log _ { 1 0 } ( 1 1 ) = 1 . 0 4 $ , 100 times ${ mathrm { t f } } =$ $log _ { 1 0 } ( 1 0 1 ) = 2 . 0 0 4$ , 1000 times $mathrm { t f } = 3 . 0 0 0 4 4$ , and so on. \nThe document frequency $operatorname { d f } _ { t }$ of a term $t$ is the number of documents it occurs in. Terms that occur in only a few documents are useful for discriminating those documents from the rest of the collection; terms that occur across the entire collection aren’t as helpful. The inverse document frequency or idf term weight (Sparck Jones, 1972) is defined as: \nwhere $N$ is the total number of documents in the collection, and $operatorname { d f } _ { t }$ is the number of documents in which term $t$ occurs. The fewer documents in which a term occurs, the higher this weight; the lowest weight of 0 is assigned to terms that occur in every document. \nHere are some idf values for some words in the corpus of Shakespeare plays, ranging from extremely informative words that occur in only one play like Romeo, to those that occur in a few like salad or Falstaff, to those that are very common like fool or so common as to be completely non-discriminative since they occur in all 37 plays like good or sweet.4 \nThe tf-idf value for word $t$ in document $d$ is then the product of term frequency $mathbf { t f } _ { t , d }$ and IDF: \n23.1.2 Document Scoring \nWe score document $d$ by the cosine of its vector $mathbf { d }$ with the query vector $pmb q$ : \nAnother way to think of the cosine computation is as the dot product of unit vectors; we first normalize both the query and document vector to unit vectors, by dividing by their lengths, and then take the dot product: \nWe can spell out Eq. 23.7, using the tf-idf values and spelling out the dot product as a sum of products: \nIn practice, it’s common to approximate Eq. 23.8 by simplifying the query processing. Queries are usually very short, so each query word is likely to have a count of 1. And the cosine normalization for the query (the division by $| q | ,$ ) will be the same for all documents, so won’t change the ranking between any two documents $D _ { i }$ and $D _ { j }$ So we generally use the following simple score for a document $d$ given a query $q$ : \nLet’s walk through an example of a tiny query against a collection of 4 nano documents, computing tf-idf values and seeing the rank of the documents. We’ll assume all words in the following query and documents are downcased and punctuation is removed: \nQuery: sweet love   \nDoc 1: Sweet sweet nurse! Love?   \nDoc 2: Sweet sorrow   \nDoc 3: How sweet is love?   \nDoc 4: Nurse! \nFig. 23.2 shows the computation of the tf-idf values and the document vector length $| d |$ for the first two documents using Eq. 23.3, Eq. 23.4, and Eq. 23.5 (computations for documents 3 and 4 are left as an exercise for the reader). \nFig. 23.3 shows the scores of the 4 documents, reranked according to Eq. 23.9. The ranking follows intuitively from the vector space model. Document 1, which has both terms including two instances of sweet, is the highest ranked, above document 3 which has a larger length $| d |$ in the denominator, and also a smaller tf for sweet. Document 3 is missing one of the terms, and Document 4 is missing both.",
        "chapter": "Question Answering",
        "section": "Information Retrieval",
        "subsection": "Term weighting and document scoring",
        "subsubsection": "N/A"
    },
    {
        "content": "The tf-idf value for word $t$ in document $d$ is then the product of term frequency $mathbf { t f } _ { t , d }$ and IDF: \n23.1.2 Document Scoring \nWe score document $d$ by the cosine of its vector $mathbf { d }$ with the query vector $pmb q$ : \nAnother way to think of the cosine computation is as the dot product of unit vectors; we first normalize both the query and document vector to unit vectors, by dividing by their lengths, and then take the dot product: \nWe can spell out Eq. 23.7, using the tf-idf values and spelling out the dot product as a sum of products: \nIn practice, it’s common to approximate Eq. 23.8 by simplifying the query processing. Queries are usually very short, so each query word is likely to have a count of 1. And the cosine normalization for the query (the division by $| q | ,$ ) will be the same for all documents, so won’t change the ranking between any two documents $D _ { i }$ and $D _ { j }$ So we generally use the following simple score for a document $d$ given a query $q$ : \nLet’s walk through an example of a tiny query against a collection of 4 nano documents, computing tf-idf values and seeing the rank of the documents. We’ll assume all words in the following query and documents are downcased and punctuation is removed: \nQuery: sweet love   \nDoc 1: Sweet sweet nurse! Love?   \nDoc 2: Sweet sorrow   \nDoc 3: How sweet is love?   \nDoc 4: Nurse! \nFig. 23.2 shows the computation of the tf-idf values and the document vector length $| d |$ for the first two documents using Eq. 23.3, Eq. 23.4, and Eq. 23.5 (computations for documents 3 and 4 are left as an exercise for the reader). \nFig. 23.3 shows the scores of the 4 documents, reranked according to Eq. 23.9. The ranking follows intuitively from the vector space model. Document 1, which has both terms including two instances of sweet, is the highest ranked, above document 3 which has a larger length $| d |$ in the denominator, and also a smaller tf for sweet. Document 3 is missing one of the terms, and Document 4 is missing both. \nA slightly more complex variant in the tf-idf family is the BM25 weighting scheme (sometimes called Okapi BM25 after the Okapi IR system in which it was introduced (Robertson et al., 1995)). BM25 adds two parameters: $k$ , a knob that adjust the balance between term frequency and IDF, and $b$ , which controls the importance of document length normalization. The BM25 score of a document $d$ given a query $q$ is: \nwhere $| d _ { mathrm { a v g } } |$ is the length of the average document. When $k$ is 0, BM25 reverts to no use of term frequency, just a binary selection of terms in the query (plus idf). A large $k$ results in raw term frequency (plus idf). $b$ ranges from 1 (scaling by document length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable values are $mathbf { k } = [ 1 . 2 , 2 ]$ and ${ bf b } = 0 . 7 5$ . Kamphuis et al. (2020) is a useful summary of the many minor variants of BM25. \nStop words In the past it was common to remove high-frequency words from both the query and document before representing them. The list of such high-frequency words to be removed is called a stop list. The intuition is that high-frequency terms (often function words like the, a, to) carry little semantic weight and may not help with retrieval, and can also help shrink the inverted index files we describe below. The downside of using a stop list is that it makes it difficult to search for phrases that contain words in the stop list. For example, common stop lists would reduce the phrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists is much less common, partly due to improved efficiency and partly because much of their function is already handled by IDF weighting, which downweights function words that occur in every document. Nonetheless, stop word removal is occasionally useful in various NLP tasks so is worth keeping in mind.",
        "chapter": "Question Answering",
        "section": "Information Retrieval",
        "subsection": "Document Scoring",
        "subsubsection": "N/A"
    },
    {
        "content": "A slightly more complex variant in the tf-idf family is the BM25 weighting scheme (sometimes called Okapi BM25 after the Okapi IR system in which it was introduced (Robertson et al., 1995)). BM25 adds two parameters: $k$ , a knob that adjust the balance between term frequency and IDF, and $b$ , which controls the importance of document length normalization. The BM25 score of a document $d$ given a query $q$ is: \nwhere $| d _ { mathrm { a v g } } |$ is the length of the average document. When $k$ is 0, BM25 reverts to no use of term frequency, just a binary selection of terms in the query (plus idf). A large $k$ results in raw term frequency (plus idf). $b$ ranges from 1 (scaling by document length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable values are $mathbf { k } = [ 1 . 2 , 2 ]$ and ${ bf b } = 0 . 7 5$ . Kamphuis et al. (2020) is a useful summary of the many minor variants of BM25. \nStop words In the past it was common to remove high-frequency words from both the query and document before representing them. The list of such high-frequency words to be removed is called a stop list. The intuition is that high-frequency terms (often function words like the, a, to) carry little semantic weight and may not help with retrieval, and can also help shrink the inverted index files we describe below. The downside of using a stop list is that it makes it difficult to search for phrases that contain words in the stop list. For example, common stop lists would reduce the phrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists is much less common, partly due to improved efficiency and partly because much of their function is already handled by IDF weighting, which downweights function words that occur in every document. Nonetheless, stop word removal is occasionally useful in various NLP tasks so is worth keeping in mind. \n23.1.3 Inverted Index \nIn order to compute scores, we need to efficiently find documents that contain words in the query. (As we saw in Fig. 23.3, any document that contains none of the query terms will have a score of 0 and can be ignored.) The basic search problem in IR is thus to find all documents $d in C$ that contain a term $q in Q$ . \ninverted index \nThe data structure for this task is the inverted index, which we use for making this search efficient, and also conveniently storing useful information like the document frequency and the count of each term in each document. \npostings \nAn inverted index, given a query term, gives a list of documents that contain the term. It consists of two parts, a dictionary and the postings. The dictionary is a list of terms (designed to be efficiently accessed), each pointing to a postings list for the term. A postings list is the list of document IDs associated with each term, which can also contain information like the term frequency or even the exact positions of terms in the document. The dictionary can also start the document frequency for each term For example, a simple inverted index for our 4 sample documents above, with each word containing its document frequency in ${ }$ , and a pointer to a postings list that contains document IDs and term counts in $[ ]$ , might look like the following: \nGiven a list of terms in query, we can very efficiently get lists of all candidate documents, together with the information necessary to compute the tf-idf scores we need. \nThere are alternatives to the inverted index. For the question-answering domain of finding Wikipedia pages to match a user query, Chen et al. (2017a) show that indexing based on bigrams works better than unigrams, and use efficient hashing algorithms rather than the inverted index to make the search efficient. \n23.1.4 Evaluation of Information-Retrieval Systems \nWe measure the performance of ranked retrieval systems using the same precision and recall metrics we have been using. We make the assumption that each document returned by the IR system is either relevant to our purposes or not relevant. Precision is the fraction of the returned documents that are relevant, and recall is the fraction of all relevant documents that are returned. More formally, let’s assume a system returns $T$ ranked documents in response to an information request, a subset $R$ of these are relevant, a disjoint subset, $N$ , are the remaining irrelevant documents, and $U$ documents in the collection as a whole are relevant to this request. Precision and recall are then defined as: \nUnfortunately, these metrics don’t adequately measure the performance of a system that ranks the documents it returns. If we are comparing the performance of two ranked retrieval systems, we need a metric that prefers the one that ranks the relevant documents higher. We need to adapt precision and recall to capture how well a system does at putting relevant documents higher in the ranking.",
        "chapter": "Question Answering",
        "section": "Information Retrieval",
        "subsection": "Inverted Index",
        "subsubsection": "N/A"
    },
    {
        "content": "23.1.3 Inverted Index \nIn order to compute scores, we need to efficiently find documents that contain words in the query. (As we saw in Fig. 23.3, any document that contains none of the query terms will have a score of 0 and can be ignored.) The basic search problem in IR is thus to find all documents $d in C$ that contain a term $q in Q$ . \ninverted index \nThe data structure for this task is the inverted index, which we use for making this search efficient, and also conveniently storing useful information like the document frequency and the count of each term in each document. \npostings \nAn inverted index, given a query term, gives a list of documents that contain the term. It consists of two parts, a dictionary and the postings. The dictionary is a list of terms (designed to be efficiently accessed), each pointing to a postings list for the term. A postings list is the list of document IDs associated with each term, which can also contain information like the term frequency or even the exact positions of terms in the document. The dictionary can also start the document frequency for each term For example, a simple inverted index for our 4 sample documents above, with each word containing its document frequency in ${ }$ , and a pointer to a postings list that contains document IDs and term counts in $[ ]$ , might look like the following: \nGiven a list of terms in query, we can very efficiently get lists of all candidate documents, together with the information necessary to compute the tf-idf scores we need. \nThere are alternatives to the inverted index. For the question-answering domain of finding Wikipedia pages to match a user query, Chen et al. (2017a) show that indexing based on bigrams works better than unigrams, and use efficient hashing algorithms rather than the inverted index to make the search efficient. \n23.1.4 Evaluation of Information-Retrieval Systems \nWe measure the performance of ranked retrieval systems using the same precision and recall metrics we have been using. We make the assumption that each document returned by the IR system is either relevant to our purposes or not relevant. Precision is the fraction of the returned documents that are relevant, and recall is the fraction of all relevant documents that are returned. More formally, let’s assume a system returns $T$ ranked documents in response to an information request, a subset $R$ of these are relevant, a disjoint subset, $N$ , are the remaining irrelevant documents, and $U$ documents in the collection as a whole are relevant to this request. Precision and recall are then defined as: \nUnfortunately, these metrics don’t adequately measure the performance of a system that ranks the documents it returns. If we are comparing the performance of two ranked retrieval systems, we need a metric that prefers the one that ranks the relevant documents higher. We need to adapt precision and recall to capture how well a system does at putting relevant documents higher in the ranking. \nLet’s turn to an example. Assume the table in Fig. 23.4 gives rank-specific precision and recall values calculated as we proceed down through a set of ranked documents for a particular query; the precisions are the fraction of relevant documents seen at a given rank, and recalls the fraction of relevant documents found at the same rank. The recall measures in this example are based on this query having 9 relevant documents in the collection as a whole. \nNote that recall is non-decreasing; when a relevant document is encountered, \ninterpolated precision \nrecall increases, and when a non-relevant document is found it remains unchanged. Precision, on the other hand, jumps up and down, increasing when relevant documents are found, and decreasing otherwise. The most common way to visualize precision and recall is to plot precision against recall in a precision-recall curve, like the one shown in Fig. 23.5 for the data in table 23.4. \nFig. 23.5 shows the values for a single query. But we’ll need to combine values for all the queries, and in a way that lets us compare one system to another. One way of doing this is to plot averaged precision values at 11 fixed levels of recall (0 to 100, in steps of 10). Since we’re not likely to have datapoints at these exact levels, we use interpolated precision values for the 11 recall values from the data points we do have. We can accomplish this by choosing the maximum precision value achieved at any level of recall at or above the one we’re calculating. In other words, \nThis interpolation scheme not only lets us average performance over a set of queries, but also helps smooth over the irregular precision values in the original data. It is designed to give systems the benefit of the doubt by assigning the maximum precision value achieved at higher levels of recall from the one being measured. Fig. 23.6 and Fig. 23.7 show the resulting interpolated data points from our example. \nGiven curves such as that in Fig. 23.7 we can compare two systems or approaches by comparing their curves. Clearly, curves that are higher in precision across all recall values are preferred. However, these curves can also provide insight into the overall behavior of a system. Systems that are higher in precision toward the left may favor precision over recall, while systems that are more geared towards recall will be higher at higher levels of recall (to the right). \nA second way to evaluate ranked retrieval is mean average precision (MAP), which provides a single metric that can be used to compare competing systems or approaches. In this approach, we again descend through the ranked list of items, but now we note the precision only at those points where a relevant item has been encountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 23.4). For a single query, we average these individual precision measurements over the return set (up to some fixed cutoff). More formally, if we assume that $R _ { r }$ is the set of relevant documents at or above $r$ , then the average precision (AP) for a single query is \nwhere $P r e c i s i o n _ { r } ( d )$ is the precision measured at the rank at which document $d$ was found. For an ensemble of queries $boldsymbol { Q }$ , we then average over these averages, to get our final MAP measure: \nThe MAP for the single query (hence ${ bf Pi } = mathrm { A P } _ { bf Pi } ^ { bf Pi }$ ) in Fig. 23.4 is 0.6. \n23.1.5 IR with Dense Vectors \nThe classic tf-idf or BM25 algorithms for IR have long been known to have a conceptual flaw: they work only if there is exact overlap of words between the query and document. In other words, the user posing a query (or asking a question) needs to guess exactly what words the writer of the answer might have used to discuss the issue. As Lin et al. (2021) put it, the user might decide to search for a tragic love story but Shakespeare writes instead about star-crossed lovers. This is called the vocabulary mismatch problem (Furnas et al., 1987). \nThe solution to this problem is to use an approach that can handle synonymy: instead of (sparse) word-count vectors, using (dense) embeddings. This idea was proposed quite early with the LSI approach (Deerwester et al., 1990), but modern methods all make use of encoders like BERT. In what is sometimes called a biencoder we use two separate encoder models, one to encode the query and one to encode the document, and use the dot product between these two vectors as the score (Fig. 23.8. For example, if we used BERT, we would have two encoders $mathbf { B E R T } _ { Q }$ and $mathbf { B E R T } _ { D }$ and we could represent the query and document as the [CLS] token of the respective encoders (Karpukhin et al., 2020):",
        "chapter": "Question Answering",
        "section": "Information Retrieval",
        "subsection": "Evaluation of Information-Retrieval Systems",
        "subsubsection": "N/A"
    },
    {
        "content": "where $P r e c i s i o n _ { r } ( d )$ is the precision measured at the rank at which document $d$ was found. For an ensemble of queries $boldsymbol { Q }$ , we then average over these averages, to get our final MAP measure: \nThe MAP for the single query (hence ${ bf Pi } = mathrm { A P } _ { bf Pi } ^ { bf Pi }$ ) in Fig. 23.4 is 0.6. \n23.1.5 IR with Dense Vectors \nThe classic tf-idf or BM25 algorithms for IR have long been known to have a conceptual flaw: they work only if there is exact overlap of words between the query and document. In other words, the user posing a query (or asking a question) needs to guess exactly what words the writer of the answer might have used to discuss the issue. As Lin et al. (2021) put it, the user might decide to search for a tragic love story but Shakespeare writes instead about star-crossed lovers. This is called the vocabulary mismatch problem (Furnas et al., 1987). \nThe solution to this problem is to use an approach that can handle synonymy: instead of (sparse) word-count vectors, using (dense) embeddings. This idea was proposed quite early with the LSI approach (Deerwester et al., 1990), but modern methods all make use of encoders like BERT. In what is sometimes called a biencoder we use two separate encoder models, one to encode the query and one to encode the document, and use the dot product between these two vectors as the score (Fig. 23.8. For example, if we used BERT, we would have two encoders $mathbf { B E R T } _ { Q }$ and $mathbf { B E R T } _ { D }$ and we could represent the query and document as the [CLS] token of the respective encoders (Karpukhin et al., 2020): \nMore complex versions can use other ways to represent the encoded text, such as using average pooling over the BERT outputs of all tokens instead of using the CLS token, or can add extra weight matrices after the encoding or dot product steps (Liu et al. 2016a, Lee et al. 2019). \nUsing dense vectors for IR or the retriever component of question answerers is still an open area of research. Among the many areas of active research are how to do the fine-tuning of the encoder modules on the IR task (generally by fine-tuning on query-document combinations, with various clever ways to get negative examples), and how to deal with the fact that documents are often longer than encoders like BERT can process (generally by breaking up documents into passages). \nEfficiency is also an issue. At the core of every IR engine is the need to rank every possible document for its similarity to the query. For sparse word-count vectors, the inverted index allows this very efficiently. For dense vector algorithms like those based on BERT or other Transformer encoders, finding the set of dense document vectors that have the highest dot product with a dense query vector is an example of nearest neighbor search. Modern systems therefore make use of approximate nearest neighbor vector search algorithms like Faiss (Johnson et al., 2017). \n23.2 IR-based Factoid Question Answering \nFaiss \nIR-based QA \nThe goal of IR-based QA (sometimes called open domain QA) is to answer a user’s question by finding short text segments from the web or some other large collection of documents. Figure 23.9 shows some sample factoid questions and their answers. \nretrieve and read \nThe dominant paradigm for IR-based QA is the retrieve and read model shown in Fig. 23.10. In the first stage of this 2-stage model we retrieve relevant passages from a text collection, usually using a search engines of the type we saw in the previous section. In the second stage, a neural reading comprehension algorithm passes over each passage and finds spans that are likely to answer the question. \nreading comprehension \nSome question answering systems focus only on the second task, the reading comprehension task. Reading comprehension systems are given a factoid question $q$ and a passage $p$ that could contain the answer, and return an answer $s$ (or perhaps declare that there is no answer in the passage, or in some setups make a choice from a set of possible answers). Of course this setup does not match the information need of users who have a question they need answered (after all, if a user knew which passage contained the answer, they could just read it themselves). Instead, this task was originally modeled on children’s reading comprehension tests—pedagogical instruments in which a child is given a passage to read and must answer questions about it—as a way to evaluate natural language processing performance (Hirschman et al., 1999). Reading comprehension systems are still used that way, but have also evolved to function as the second stage of the modern retrieve and read model.",
        "chapter": "Question Answering",
        "section": "Information Retrieval",
        "subsection": "IR with Dense Vectors",
        "subsubsection": "N/A"
    },
    {
        "content": "Other question answering systems address the entire retrieve and read task; they are given a factoid question and a large document collection (such as Wikipedia or a crawl of the web) and return an answer, usually a span of text extracted from a document. This task is often called open domain QA. \nIn the next few sections we’ll lay out the various pieces of IR-based QA, starting with some commonly used datasets. \n23.2.1 IR-based QA: Datasets \nDatasets for IR-based QA are most commonly created by first developing reading comprehension datasets containing tuples of (passage, question, answer). Reading comprehension systems can use the datasets to train a reader that is given a passage and a question, and predicts a span in the passage as the answer. Including the passage from which the answer is to be extracted eliminates the need for reading comprehension systems to deal with IR. \nFor example the Stanford Question Answering Dataset (SQuAD) consists of passages from Wikipedia and associated questions whose answers are spans from the passage (Rajpurkar et al. 2016). Squad 2.0 in addition adds some questions that are designed to be unanswerable (Rajpurkar et al. 2018), with a total of just over 150,000 questions. Fig. 23.11 shows a (shortened) excerpt from a SQUAD 2.0 passage together with three questions and their gold answer spans. \nHotpotQA \nSQuAD was built by having humans read a given Wikipedia passage, write questions about the passage, and choose a specific answer span. \nOther datasets are created by similar techniques but try to make the questions more complex. The HotpotQA dataset (Yang et al., 2018) was created by showing crowd workers multiple context documents and asked to come up with questions that require reasoning about all of the documents. \nThe fact that questions in datasets like SQuAD or HotpotQA are created by annotators who have first read the passage may make their questions easier to answer, since the annotator may (subconsciously) make use of words from the answer text. \nNatural Questions \nA solution to this possible bias is to make datasets from questions that were not written with a passage in mind. The TriviaQA dataset (Joshi et al., 2017) contains 94K questions written by trivia enthusiasts, together with supporting documents from Wikipedia and the web resulting in 650K question-answer-evidence triples. \nThe Natural Questions dataset (Kwiatkowski et al., 2019) incorporates real anonymized queries to the Google search engine. Annotators are presented a query, along with a Wikipedia page from the top 5 search results, and annotate a paragraphlength long answer and a short span answer, or mark null if the text doesn’t contain the paragraph. For example the question “When are hops added to the brewing process?” has the short answer the boiling process and a long answer which the surrounding entire paragraph from the Wikipedia page on Brewing. In using this dataset, a reading comprehension model is given a question and a Wikipedia page and must return a long answer, short answer, or ’no answer’ response. \nThe above datasets are all in English. The TyDi QA dataset contains 204K question-answer pairs from 11 typologically diverse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020). In the TYDI QA task, a system is given a question and the passages from a Wikipedia article and must (a) select the passage containing the answer (or NULL if no passage contains the answer), and (b) mark the minimal answer span (or NULL). Many questions have no answer. The various languages in the dataset bring up challenges for QA systems like morphological variation between the question and the answer, or complex issue with word segmentation or multiple alphabets. \nIn the reading comprehension task, a system is given a question and the passage in which the answer should be found. In the full two-stage QA task, however, systems are not given a passage, but are required to do their own retrieval from some document collection. A common way to create open-domain QA datasets is to modify a reading comprehension dataset. For research purposes this is most commonly done by using QA datasets that annotate Wikipedia (like SQuAD or HotpotQA). For training, the entire (question, passage, answer) triple is used to train the reader. But at inference time, the passages are removed and system is given only the question, together with access to the entire Wikipedia corpus. The system must then do IR to find a set of pages and then read them.",
        "chapter": "Question Answering",
        "section": "IR-based Factoid Question Answering",
        "subsection": "IR-based QA: Datasets",
        "subsubsection": "N/A"
    },
    {
        "content": "Natural Questions \nA solution to this possible bias is to make datasets from questions that were not written with a passage in mind. The TriviaQA dataset (Joshi et al., 2017) contains 94K questions written by trivia enthusiasts, together with supporting documents from Wikipedia and the web resulting in 650K question-answer-evidence triples. \nThe Natural Questions dataset (Kwiatkowski et al., 2019) incorporates real anonymized queries to the Google search engine. Annotators are presented a query, along with a Wikipedia page from the top 5 search results, and annotate a paragraphlength long answer and a short span answer, or mark null if the text doesn’t contain the paragraph. For example the question “When are hops added to the brewing process?” has the short answer the boiling process and a long answer which the surrounding entire paragraph from the Wikipedia page on Brewing. In using this dataset, a reading comprehension model is given a question and a Wikipedia page and must return a long answer, short answer, or ’no answer’ response. \nThe above datasets are all in English. The TyDi QA dataset contains 204K question-answer pairs from 11 typologically diverse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020). In the TYDI QA task, a system is given a question and the passages from a Wikipedia article and must (a) select the passage containing the answer (or NULL if no passage contains the answer), and (b) mark the minimal answer span (or NULL). Many questions have no answer. The various languages in the dataset bring up challenges for QA systems like morphological variation between the question and the answer, or complex issue with word segmentation or multiple alphabets. \nIn the reading comprehension task, a system is given a question and the passage in which the answer should be found. In the full two-stage QA task, however, systems are not given a passage, but are required to do their own retrieval from some document collection. A common way to create open-domain QA datasets is to modify a reading comprehension dataset. For research purposes this is most commonly done by using QA datasets that annotate Wikipedia (like SQuAD or HotpotQA). For training, the entire (question, passage, answer) triple is used to train the reader. But at inference time, the passages are removed and system is given only the question, together with access to the entire Wikipedia corpus. The system must then do IR to find a set of pages and then read them. \n23.2.2 IR-based QA: Reader (Answer Span Extraction) \nextractive QA \nThe first stage of IR-based QA is a retriever, for example of the type we saw in Section 23.1. The second stage of IR-based question answering is the reader. The reader’s job is to take a passage as input and produce the answer. In the extractive QA we discuss here, the answer is a span of text in the passage.5 For example given a question like “How tall is Mt. Everest?” and a passage that contains the clause Reaching 29,029 feet at its summit, a reader will output 29,029 feet. \nspan \nThe answer extraction task is commonly modeled by span labeling: identifying in the passage a span (a continuous string of text) that constitutes an answer. Neural algorithms for reading comprehension are given a question $q$ of $n$ tokens $q _ { 1 } , . . . , q _ { n }$ and a passage $p$ of $m$ tokens $p _ { 1 } , . . . , p _ { m }$ . Their goal is thus to compute the probability $P ( a | q , p )$ that each possible span $a$ is the answer. \nIf each span $a$ starts at position $a _ { s }$ and ends at position $a _ { e }$ , we make the simplifying assumption that this probability can be estimated as $P ( a | q , p ) = P _ { mathrm { s t a r t } } ( a _ { s } | q , p ) P _ { mathrm { e n d } } ( a _ { e } | q , p )$ . Thus for for each token $p _ { i }$ in the passage we’ll compute two probabilities: $p _ { mathrm { s t a r t } } ( i )$ that $p _ { i }$ is the start of the answer span, and $p _ { mathrm { e n d } } ( i )$ that $p _ { i }$ is the end of the answer span. \nA standard baseline algorithm for reading comprehension is to pass the question and passage to any encoder like BERT (Fig. 23.12), as strings separated with a [SEP] token, resulting in an encoding token embedding for every passage token $p _ { i }$ . \nFor span-based question answering, we represent the question as the first sequence and the passage as the second sequence. We’ll also need to add a linear layer that will be trained in the fine-tuning phase to predict the start and end position of the span. We’ll add two new special vectors: a span-start embedding $S$ and a span-end embedding $E$ , which will be learned in fine-tuning. To get a span-start probability for each output token $p _ { i } ^ { prime }$ , we compute the dot product between $S$ and $p _ { i } ^ { prime }$ and then use a softmax to normalize over all tokens $p _ { i } ^ { prime }$ in the passage: \nWe do the analogous thing to compute a span-end probability: \nThe score of a candidate span from position $i$ to $j$ is $S cdot p _ { i } ^ { prime } { + } E cdot p _ { j } ^ { prime }$ , and the highest scoring span in which $j geq i$ is chosen is the model prediction. \nThe training loss for fine-tuning is the negative sum of the log-likelihoods of the correct start and end positions for each instance: \nMany datasets (like SQuAD 2.0 and Natural Questions) also contain (question, passage) pairs in which the answer is not contained in the passage. We thus also need a way to estimate the probability that the answer to a question is not in the document. This is standardly done by treating questions with no answer as having the [CLS] token as the answer, and hence the answer span start and end index will point at [CLS] (Devlin et al., 2019). \nFor many datasets the annotated documents/passages are longer than the maximum 512 input tokens BERT allows, such as Natural Questions whose gold passages are full Wikipedia pages. In such cases, following Alberti et al. (2019), we can create multiple pseudo-passage observations from the labeled Wikipedia page. Each observation is formed by concatenating [CLS], the question, [SEP], and tokens from the document. We walk through the document, sliding a window of size 512 (or rather, 512 minus the question length $n$ minus special tokens) and packing the window of tokens into each next pseudo-passage. The answer span for the observation is either labeled [CLS] $left[ = right.$ no answer in this particular window) or the gold-labeled span is marked. The same process can be used for inference, breaking up each retrieved document into separate observation passages and labeling each observation. The answer can be chosen as the span with the highest probability (or nil if no span is more probable than [CLS]). \n23.3 Entity Linking \nentity linking \nWe’ve now seen the first major paradigm for question answering, IR-based QA. Before we turn to the second major paradigm for question answering, knowledgebased question answering, we introduce the important core technology of entity linking, since it is required for any knowledge-based QA algorithm. \nEntity linking is the task of associating a mention in text with the representation of some real-world entity in an ontology (Ji and Grishman, 2011). \nwikification \nThe most common ontology for factoid question-answering is Wikipedia, since Wikipedia is often the source of the text that answers the question. In this usage, each unique Wikipedia page acts as the unique id for a particular entity. This task of deciding which Wikipedia page corresponding to an individual is being referred to by a text mention has its own name: wikification (Mihalcea and Csomai, 2007). \nSince the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We’ll give two algorithms, one simple classic baseline that uses anchor dictionaries and information from the Wikipedia graph structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,",
        "chapter": "Question Answering",
        "section": "IR-based Factoid Question Answering",
        "subsection": "IR-based QA: Reader (Answer Span Extraction)",
        "subsubsection": "N/A"
    },
    {
        "content": "2020). We’ll focus here mainly on the application of entity linking to questions rather than other genres. \n23.3.1 Linking based on Anchor Dictionaries and Web Graph \nAs a simple baseline we introduce the TAGME linker (Ferragina and Scaiella, 2011) for Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008). Wikification algorithms define the set of entities as the set of Wikipedia pages, so we’ll refer to each Wikipedia page as a unique entity $e$ . TAGME first creates a catalog of all entities (i.e. all Wikipedia pages, removing some disambiguation and other meta-pages) and indexes them in a standard IR engine like Lucene. For each page $e$ , the algorithm computes an in-link count $mathrm { i n } ( e )$ : the total number of in-links from other Wikipedia pages that point to $e$ . These counts can be derived from Wikipedia dumps. \nFinally, the algorithm requires an anchor dictionary. An anchor dictionary lists for each Wikipedia page, its anchor texts: the hyperlinked spans of text on other pages that point to it. For example, the web page for Stanford University, http://www.stanford.edu, might be pointed to from another page using anchor texts like Stanford or Stanford University: \n<a href=\"http://www.stanford.edu\" $>$ Stanford University</a> \nWe compute a Wikipedia anchor dictionary by including, for each Wikipedia page $e$ , $e$ ’s title as well as all the anchor texts from all Wikipedia pages that point to $e$ . For each anchor string a we’ll also compute its total frequency freq $( a )$ in Wikipedia (including non-anchor uses), the number of times $a$ occurs as a link (which we’ll call $l i n k ( a ) )$ , and its link probability linkprob $( a ) = operatorname* { l i n k } ( a ) / operatorname* { f r e q } ( a )$ . Some cleanup of the final anchor dictionary is required, for example removing anchor strings composed only of numbers or single characters, that are very rare, or that are very unlikely to be useful entities because they have a very low linkprob. \nMention Detection Given a question (or other text we are trying to link), TAGME detects mentions by querying the anchor dictionary for each token sequence up to 6 words. This large set of sequences is pruned with some simple heuristics (for example pruning substrings if they have small linkprobs). The question: \nWhen was Ada Lovelace born? \nmight give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans like Lovelace might be pruned as having too low a linkprob, and but spans like born have such a low linkprob that they would not be in the anchor dictionary at all. \nMention Disambiguation If a mention span is unambiguous (points to only one entity/Wikipedia page), we are done with entity linking! However, many spans are ambiguous, matching anchors for multiple Wikipedia entities/pages. The TAGME algorithm uses two factors for disambiguating ambiguous spans, which have been referred to as prior probability and relatedness/coherence. The first factor is $p ( e | a )$ , the probability with which the span refers to a particular entity. For each page $e in$ $mathcal { E } ( a )$ , the probability $p ( e | a )$ that anchor $a$ points to $e$ , is the ratio of the number of links into $e$ with anchor text $a$ to the total number of occurrences of $a$ as an anchor: \nLet’s see how that factor works in linking entities in the following question: \nWhat Chinese Dynasty came before the Yuan? \nThe most common association for the span Yuan in the anchor dictionary is the name of the Chinese currency, i.e., the probability $p$ (Yuan currency| yuan) is very high. Rarer Wikipedia associations for Yuan include the common Chinese last name, a language spoken in Thailand, and the correct entity in this case, the name of the Chinese dynasty. So if we chose based only on $p ( e | a )$ , we would make the wrong disambiguation and miss the correct link, Yuan dynasty. \nTo help in just this sort of case, TAGME uses a second factor, the relatedness of this entity to other entities in the input question. In our example, the fact that the question also contains the span Chinese Dynasty, which has a high probability link to the page Dynasties in Chinese history, ought to help match Yuan dynasty. \nLet’s see how this works. Given a question $q$ , for each candidate anchors span $a$ detected in $q$ , we assign a relatedness score to each possible entity $e in mathcal { E } ( a )$ of $a$ . The relatedness score of the link $a  e$ is the weighted average relatedness between $e$ and all other entities in $q$ . Two entities are considered related to the extent their Wikipedia pages share many in-links. More formally, the relatedness between two entities $A$ and $B$ is computed as \nwhere $mathrm { i n } ( x )$ is the set of Wikipedia pages pointing to $x$ and $W$ is the set of all Wikipedia pages in the collection. \nThe vote given by anchor $b$ to the candidate annotation $a  X$ is the average, over all the possible entities of $b$ , of their relatedness to $X$ , weighted by their prior probability: \nThe total relatedness score for $a to X$ is the sum of the votes of all the other anchors detected in $q$ : \nTo score $a  X$ , we combine relatedness and prior by choosing the entity $X$ that has the highest relatedness $( a  X )$ ), finding other entities within a small $epsilon$ of this value, and from this set, choosing the entity with the highest prior $P ( X | a )$ . The result of this step is a single entity assigned to each span in $q$ . \nThe TAGME algorithm has one further step of pruning spurious anchor/entity pairs, assigning a score averaging link probability with the coherence. \nFinally, pairs are pruned if $operatorname { s c o r e } ( a to X ) < lambda$ , where the threshold $lambda$ is set on a held-out set. \n23.3.2 Neural Graph-based linking \nMore recent entity linking models are based on biencoders, encoding a candidate mention span, encoding an entity, and computing the dot product between the encodings. This allows embeddings for all the entities in the knowledge base to be precomputed and cached (Wu et al., 2020). Let’s sketch the ELQ linking algorithm of Li et al. (2020), which is given a question $q$ and a set of candidate entities from Wikipedia with associated Wikipedia text, and outputs tuples $left( e , m _ { s } , m _ { e } right)$ of entity id, mention start, and mention end. As Fig. 23.13 shows, it does this by encoding each Wikipedia entity using text from Wikipedia, encoding each mention span using text from the question, and computing their similarity, as we describe below. \nEntity Mention Detection To get an $h$ -dimensional embedding for each question token, the algorithm runs the question through BERT in the normal way: \nIt then computes the likelihood of each span $[ i , j ]$ in $q$ being an entity mention, in a way similar to the span-based algorithm we saw for the reader above. First we compute the score for $i / j$ being the start/end of a mention: \nwhere $pmb { w } _ { mathrm { s t a r t } }$ and $boldsymbol { mathsf { w } } _ { mathrm { e n d } }$ are vectors learned during training. Next, another trainable embedding, $pmb { mathsf { w } } _ { mathrm { m e n t i o n } }$ is used to compute a score for each token being part of a mention: \nMention probabilities are then computed by combining these three scores:",
        "chapter": "Question Answering",
        "section": "Entity Linking",
        "subsection": "Linking based on Anchor Dictionaries and Web Graph",
        "subsubsection": "N/A"
    },
    {
        "content": "23.3.2 Neural Graph-based linking \nMore recent entity linking models are based on biencoders, encoding a candidate mention span, encoding an entity, and computing the dot product between the encodings. This allows embeddings for all the entities in the knowledge base to be precomputed and cached (Wu et al., 2020). Let’s sketch the ELQ linking algorithm of Li et al. (2020), which is given a question $q$ and a set of candidate entities from Wikipedia with associated Wikipedia text, and outputs tuples $left( e , m _ { s } , m _ { e } right)$ of entity id, mention start, and mention end. As Fig. 23.13 shows, it does this by encoding each Wikipedia entity using text from Wikipedia, encoding each mention span using text from the question, and computing their similarity, as we describe below. \nEntity Mention Detection To get an $h$ -dimensional embedding for each question token, the algorithm runs the question through BERT in the normal way: \nIt then computes the likelihood of each span $[ i , j ]$ in $q$ being an entity mention, in a way similar to the span-based algorithm we saw for the reader above. First we compute the score for $i / j$ being the start/end of a mention: \nwhere $pmb { w } _ { mathrm { s t a r t } }$ and $boldsymbol { mathsf { w } } _ { mathrm { e n d } }$ are vectors learned during training. Next, another trainable embedding, $pmb { mathsf { w } } _ { mathrm { m e n t i o n } }$ is used to compute a score for each token being part of a mention: \nMention probabilities are then computed by combining these three scores: \nEntity Linking To link mentions to entities, we next compute embeddings for each entity in the set $mathcal { E } = e _ { 1 } , cdots , e _ { i } , cdots , e _ { w }$ of all Wikipedia entities. For each entity $e _ { i }$ we’ll get text from the entity’s Wikipedia page, the title $t { big ( } e _ { i } { big ) }$ and the first 128 tokens of the Wikipedia page which we’ll call the description $d ( e _ { i } )$ . This is again run through BERT, taking the output of the CLS token BERT[CLS] as the entity representation: \nMention spans can be linked to entities by computing, for each entity $e$ and span $[ i , j ]$ , the dot product similarity between the span encoding (the average of the token embeddings) and the entity encoding. \nFinally, we take a softmax to get a distribution over entities for each span: \nTraining The ELQ mention detection and entity linking algorithm is fully supervised. This means, unlike the anchor dictionary algorithms from Section 23.3.1, it requires datasets with entity boundaries marked and linked. Two such labeled datasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions (Berant et al., 2013) dataset derived from Google search questions, and GraphQuestions (Su et al., 2016). Both have had entity spans in the questions marked and linked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled versions $mathrm { W e b Q S P _ { E L } }$ and GraphQEL (Li et al., 2020). \nGiven a training set, the ELQ mention detection and entity linking phases are trained jointly, optimizing the sum of their losses. The mention detection loss is a binary cross-entropy loss \nwith $y _ { [ i , j ] } = 1$ if $[ i , j ]$ is a gold mention span, else 0. The entity linking loss is: \nwhere $e _ { g }$ is the gold entity for mention $[ i , j ]$ . \nSee the end of the chapter for more discussion of other applications of entity linking outside of question answering. \n23.4 Knowledge-based Question Answering \nWhile an enormous amount of information is encoded in the vast amount of text on the web, information obviously also exists in more structured forms. We use the term knowledge-based question answering for the idea of answering a natural language question by mapping it to a query over a structured database. Like the textbased paradigm for question answering, this approach dates back to the earliest days of natural language processing, with systems like BASEBALL (Green et al., 1961) that answered questions from a structured database of baseball games and stats.",
        "chapter": "Question Answering",
        "section": "Entity Linking",
        "subsection": "Neural Graph-based linking",
        "subsubsection": "N/A"
    },
    {
        "content": "Two common paradigms are used for knowledge-based QA. The first, graphbased QA, models the knowledge base as a graph, often with entities as nodes and relations or propositions as edges between nodes. The second, QA by semantic parsing, using the semantic parsing methods we saw in Chapter 16. Both of these methods require some sort of entity linking that we described in the prior section. \n23.4.1 Knowledge-Based QA from RDF triple stores \nLet’s introduce the components of a simple knowledge-based QA system after entity linking has been performed. We’ll focus on the very simplest case of graph-based QA, in which the dataset is a set of factoids in the form of RDF triples, and the task is to answer questions about one of the missing arguments. Recall from Chapter 17 that an RDF triple is a 3-tuple, a predicate with two arguments, expressing some simple relation or proposition. Popular such ontologies are often derived from Wikipedia; DBpedia (Bizer et al., 2009) has over 2 billion RDF triples, or Freebase (Bollacker et al., 2008), now part of Wikidata (Vrandecˇic´ and Kro¨tzsch, 2014). Consider an RDF triple like the following: \nsubject predicate object Ada Lovelace birth-year 1815 \nThis triple can be used to answer text questions like “When was Ada Lovelace born?” or “Who was born in 1815?”. \nA number of such question datasets exist. SimpleQuestions (Bordes et al., 2015) contains 100K questions written by annotators based on triples from Freebase. For example, the question ”What American cartoonist is the creator of Andy Lippincott?”. was written based on the triple (andy lippincott, character created by, garry trudeau). FreebaseQA (Jiang et al., 2019), aligns the trivia questions from TriviaQA (Joshi et al., 2017) and other sources with triples in Freebase, aligning for example the trivia question “Which 18th century author wrote Clarissa (or The Character History of a Young Lady), said to be the longest novel in the English language?” with the triple (Clarissa, book.written-work.author, Samuel Richardson). Another such family of datasets starts from WEBQUESTIONS (Berant et al., 2013), which contains 5,810 questions asked by web users, each beginning with a wh-word, containing exactly one entity, and paired with handwritten answers drawn from the Freebase page of the question’s entity. WEBQUESTIONSSP (Yih et al., 2016) augments WEBQUESTIONS with human-created semantic parses (SPARQL queries) for those questions answerable using Freebase. COMPLEXWEBQUESTIONS augments the dataset with compositional and other kinds of complex questions, resulting in 34,689 questions, along with answers, web snippets, and SPARQL queries (Talmor and Berant, 2018). \nLet’s assume we’ve already done the stage of entity linking introduced in the prior section. Thus we’ve mapped already from a textual mention like Ada Lovelace to the canonical entity ID in the knowledge base. For simple triple relation question answering, the next step is to determine which relation is being asked about, mapping from a string like “When was ... born” to canonical relations in the knowledge base like birth-year. We might sketch the combined task as: \n“When was Ada Lovelace born?” $$ birth-year (Ada Lovelace, ?x) \n“What is the capital of England?” $$ capital-city(?x, England) \nThe next step is relation detection and linking. For simple questions, where we assume the question has only a single relation, relation detection and linking can be done in a way resembling the neural entity linking models: computing similarity (generally by dot product) between the encoding of the question text and an encoding for each possible relation. For example, in the algorithm of (Lukovnikov et al., 2019), the CLS output of a BERT model is used to represent the question span for the purposes of relation detection, and a separate vector is trained for each relation $r _ { i }$ . The probability of a particular relation $r _ { i }$ is then computed by softmax over the dot products: \nRanking of answers Most algorithms have a final stage which takes the top $j$ entities and the top $k$ relations returned by the entity and relation inference steps, searches the knowledge base for triples containing those entities and relations, and then ranks those triples. This ranking can be heuristic, for example scoring each entity/relation pairs based on the string similarity between the mention span and the entities text aliases, or favoring entities that have a high in-degree (are linked to by many relations). Or the ranking can be done by training a classifier to take the concatenated entity/relation encodings and predict a probability. \n23.4.2 QA by Semantic Parsing \nThe second kind of knowledge-based QA uses a semantic parser to map the question to a structured program to produce an answer. These logical forms can take the form of some version of predicate calculus, a query language like SQL or SPARQL, or some other executable program like the examples in Fig. 23.14. \nThe logical form of the question is thus either in the form of a query or can easily be converted into one (predicate calculus can be converted to SQL, for example). The database can be a full relational database, or some other structured knowledge store. \nAs we saw in Chapter 16, semantic parsing algorithms can be supervised fully with questions paired with a hand-built logical form, or can be weakly supervised by questions paired with an answer (the denotation), in which the logical form is modeled only as a latent variable. \nFor the fully supervised case, we can get a set of questions paired with their correct logical form from datasets like the GEOQUERY dataset of questions about US geography (Zelle and Mooney, 1996), the DROP dataset of complex questions (on history and football games) that require reasoning (Dua et al. 2019), or the ATIS dataset of flight queries, all of which have versions with SQL or other logical forms (Iyer et al. 2017, Wolfson et al. 2020, Oren et al. 2020). \nThe task is then to take those pairs of training tuples and produce a system that maps from new questions to their logical forms. A common baseline algorithm is a simple sequence-to-sequence model, for example using BERT to represent question tokens, passing them to an encoder-decoder (Chapter 10), as sketched in Fig. 23.15. Any other of the semantic parsing algorithms described in Chapter 16 would also be appropriate.",
        "chapter": "Question Answering",
        "section": "Knowledge-based Question Answering",
        "subsection": "Knowledge-Based QA from RDF triple stores",
        "subsubsection": "N/A"
    },
    {
        "content": "“What is the capital of England?” $$ capital-city(?x, England) \nThe next step is relation detection and linking. For simple questions, where we assume the question has only a single relation, relation detection and linking can be done in a way resembling the neural entity linking models: computing similarity (generally by dot product) between the encoding of the question text and an encoding for each possible relation. For example, in the algorithm of (Lukovnikov et al., 2019), the CLS output of a BERT model is used to represent the question span for the purposes of relation detection, and a separate vector is trained for each relation $r _ { i }$ . The probability of a particular relation $r _ { i }$ is then computed by softmax over the dot products: \nRanking of answers Most algorithms have a final stage which takes the top $j$ entities and the top $k$ relations returned by the entity and relation inference steps, searches the knowledge base for triples containing those entities and relations, and then ranks those triples. This ranking can be heuristic, for example scoring each entity/relation pairs based on the string similarity between the mention span and the entities text aliases, or favoring entities that have a high in-degree (are linked to by many relations). Or the ranking can be done by training a classifier to take the concatenated entity/relation encodings and predict a probability. \n23.4.2 QA by Semantic Parsing \nThe second kind of knowledge-based QA uses a semantic parser to map the question to a structured program to produce an answer. These logical forms can take the form of some version of predicate calculus, a query language like SQL or SPARQL, or some other executable program like the examples in Fig. 23.14. \nThe logical form of the question is thus either in the form of a query or can easily be converted into one (predicate calculus can be converted to SQL, for example). The database can be a full relational database, or some other structured knowledge store. \nAs we saw in Chapter 16, semantic parsing algorithms can be supervised fully with questions paired with a hand-built logical form, or can be weakly supervised by questions paired with an answer (the denotation), in which the logical form is modeled only as a latent variable. \nFor the fully supervised case, we can get a set of questions paired with their correct logical form from datasets like the GEOQUERY dataset of questions about US geography (Zelle and Mooney, 1996), the DROP dataset of complex questions (on history and football games) that require reasoning (Dua et al. 2019), or the ATIS dataset of flight queries, all of which have versions with SQL or other logical forms (Iyer et al. 2017, Wolfson et al. 2020, Oren et al. 2020). \nThe task is then to take those pairs of training tuples and produce a system that maps from new questions to their logical forms. A common baseline algorithm is a simple sequence-to-sequence model, for example using BERT to represent question tokens, passing them to an encoder-decoder (Chapter 10), as sketched in Fig. 23.15. Any other of the semantic parsing algorithms described in Chapter 16 would also be appropriate. \nFigure 23.14 Sample logical forms produced by a semantic parser for question answering, including two questions from the GeoQuery database of questions on U.S. Geography (Zelle and Mooney, 1996) with predicate calculus representations, one ATIS question with SQL (Iyer et al., 2017), a program over Freebase relations, and a program in QDMR, the Question Decomposition Meaning Representation (Wolfson et al., 2020). \n23.5 Using Language Models to do QA \nAn alternative approach to doing QA is to query a pretrained language model, forcing a model to answer a question solely from information stored in its parameters. For example Roberts et al. (2020) use the T5 language model, which is an encoderdecoder architecture pretrained to fill in masked spans of task. Fig. 23.16 shows the architecture; the deleted spans are marked by $< M >$ , and the system is trained to have the decoder generating the missing spans (separated by $< M >$ ). \nRoberts et al. (2020) then finetune the T5 system to the question answering task, by giving it a question, and training it to output the answer text in the decoder. Using the largest 11-billion-parameter T5 model does competitively, although not quite as well as systems designed specifically for question answering. \nLanguage modeling is not yet a complete solution for question answering; for example in addition to not working quite as well, they suffer from poor interpretability (unlike standard QA systems, for example, they currently can’t give users more context by telling them what passage the answer came from). Nonetheless, the study of extracting answer from language models is an intriguing area for future question",
        "chapter": "Question Answering",
        "section": "Knowledge-based Question Answering",
        "subsection": "QA by Semantic Parsing",
        "subsubsection": "N/A"
    },
    {
        "content": "Figure 23.14 Sample logical forms produced by a semantic parser for question answering, including two questions from the GeoQuery database of questions on U.S. Geography (Zelle and Mooney, 1996) with predicate calculus representations, one ATIS question with SQL (Iyer et al., 2017), a program over Freebase relations, and a program in QDMR, the Question Decomposition Meaning Representation (Wolfson et al., 2020). \n23.5 Using Language Models to do QA \nAn alternative approach to doing QA is to query a pretrained language model, forcing a model to answer a question solely from information stored in its parameters. For example Roberts et al. (2020) use the T5 language model, which is an encoderdecoder architecture pretrained to fill in masked spans of task. Fig. 23.16 shows the architecture; the deleted spans are marked by $< M >$ , and the system is trained to have the decoder generating the missing spans (separated by $< M >$ ). \nRoberts et al. (2020) then finetune the T5 system to the question answering task, by giving it a question, and training it to output the answer text in the decoder. Using the largest 11-billion-parameter T5 model does competitively, although not quite as well as systems designed specifically for question answering. \nLanguage modeling is not yet a complete solution for question answering; for example in addition to not working quite as well, they suffer from poor interpretability (unlike standard QA systems, for example, they currently can’t give users more context by telling them what passage the answer came from). Nonetheless, the study of extracting answer from language models is an intriguing area for future question \nanswer research. \n23.6 Classic QA Models \nWhile neural architectures are the state of the art for question answering, pre-neural architectures using hybrids of rules and feature-based classifiers can sometimes achieve higher performance. Here we summarize one influential classic system, the Watson DeepQA system from IBM that won the Jeopardy! challenge in 2011 (Fig. 23.17). Let’s consider how it handles these Jeopardy! examples, each with a category followed by a question: \nQuestion (2) Candidate Answer Generation (3) Candidate (4) Answer Confidence From Text Resources Candidate Scoring Merging Docaunmdent Answer ACnasnwdeirdate Candidate Rankding   \n(P1)roQcueesstiinogn Retrieval passages EDAxontcruhaomcretinetoxtnitles CAaCnCAasdAaniwndsdeisdawdritwedearetar CoCnafin+ddeindcaete te ERveitdrienvcael EAqMnuisevrwagelersnt Docu eDntocument and scoring Lexical AnCsawnedridate   \nAnswer Type Confidence QDuetesctition From Structured Data CAaCnasdniwdeiadrteate Text TDimBeP efrdoiam CAansdiwdearte ReLgorgeistsiicon   \nParsing Relation CAansdiwdearte Text ATnyspwe r CoAannfisdiewdneacrte ARnasnwker Retrieval Candidate Sources Caonfidideantcee   \nNa TmaegdgiEnngtity Answer SFpacebforokm Ans+wer DBPedia Freebase Answer and   \nRelation Extraction Confidence Coreference \nPoets and Poetry: He was a bank clerk in the Yukon before he published “Songs of a Sourdough” in 1907.",
        "chapter": "Question Answering",
        "section": "Using Language Models to do QA",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "answer research. \n23.6 Classic QA Models \nWhile neural architectures are the state of the art for question answering, pre-neural architectures using hybrids of rules and feature-based classifiers can sometimes achieve higher performance. Here we summarize one influential classic system, the Watson DeepQA system from IBM that won the Jeopardy! challenge in 2011 (Fig. 23.17). Let’s consider how it handles these Jeopardy! examples, each with a category followed by a question: \nQuestion (2) Candidate Answer Generation (3) Candidate (4) Answer Confidence From Text Resources Candidate Scoring Merging Docaunmdent Answer ACnasnwdeirdate Candidate Rankding   \n(P1)roQcueesstiinogn Retrieval passages EDAxontcruhaomcretinetoxtnitles CAaCnCAasdAaniwndsdeisdawdritwedearetar CoCnafin+ddeindcaete te ERveitdrienvcael EAqMnuisevrwagelersnt Docu eDntocument and scoring Lexical AnCsawnedridate   \nAnswer Type Confidence QDuetesctition From Structured Data CAaCnasdniwdeiadrteate Text TDimBeP efrdoiam CAansdiwdearte ReLgorgeistsiicon   \nParsing Relation CAansdiwdearte Text ATnyspwe r CoAannfisdiewdneacrte ARnasnwker Retrieval Candidate Sources Caonfidideantcee   \nNa TmaegdgiEnngtity Answer SFpacebforokm Ans+wer DBPedia Freebase Answer and   \nRelation Extraction Confidence Coreference \nPoets and Poetry: He was a bank clerk in the Yukon before he published “Songs of a Sourdough” in 1907. \nTHEATRE: A new play based on this Sir Arthur Conan Doyle canine classic opened on the London stage in 2007. \nQuestion Processing In this stage the questions are parsed, named entities are extracted (Sir Arthur Conan Doyle identified as a PERSON, Yukon as a GEOPOLITICAL ENTITY, “Songs of a Sourdough” as a COMPOSITION), coreference is run (he is linked with clerk). \nfocus \nThe question focus, shown in bold in both examples, is extracted. The focus is the string of words in the question that corefers with the answer. It is likely to be replaced by the answer in any answer string found and so can be used to align with a supporting passage. In DeepQA The focus is extracted by handwritten rules—made possible by the relatively stylized syntax of Jeopardy! questions—such as a rule extracting any noun phrase with determiner “this” as in the Conan Doyle example, and rules extracting pronouns like she, he, hers, him, as in the poet example. \nThe lexical answer type (shown in blue above) is a word or words which tell us something about the semantic type of the answer. Because of the wide variety of questions in Jeopardy!, DeepQA chooses a wide variety of words to be answer types, rather than a small set of named entities. These lexical answer types are again extracted by rules: the default rule is to choose the syntactic headword of the focus. Other rules improve this default choice. For example additional lexical answer types can be words in the question that are coreferent with or have a particular syntactic relation with the focus, such as headwords of appositives or predicative nominatives of the focus. In some cases even the Jeopardy! category can act as a lexical answer type, if it refers to a type of entity that is compatible with the other lexical answer types. Thus in the first case above, he, poet, and clerk are all lexical answer types. In addition to using the rules directly as a classifier, they can instead be used as features in a logistic regression classifier that can return a probability as well as a lexical answer type. These answer types will be used in the later ‘candidate answer scoring’ phase as a source of evidence for each candidate. Relations like the following are also extracted: \nauthorof(focus,“Songs of a sourdough”) publish (e1, he, “Songs of a sourdough”) in (e2, e1, 1907) temporallink(publish(...), 1907) \nFinally the question is classified by type (definition question, multiple-choice, puzzle, fill-in-the-blank). This is generally done by writing pattern-matching regular expressions over words or parse trees. \nCandidate Answer Generation Next we combine the processed question with external documents and other knowledge sources to suggest many candidate answers from both text documents and structured knowledge bases. We can query structured resources like DBpedia or IMDB with the relation and the known entity, just as we saw in Section 23.4. Thus if we have extracted the relation authorof(focus,\"Songs of a sourdough\"), we can query a triple store with authorof(?x,\"Songs of a sourdough\") to return an author. \nTo extract answers from text DeepQA uses simple versions of Retrieve and Read. For example for the IR stage, DeepQA generates a query from the question by eliminating stop words, and then upweighting any terms which occur in any relation with the focus. For example from this query: \nMOVIE-“ING”: Robert Redford and Paul Newman starred in this depressionera grifter flick. (Answer: “The Sting”) \nthe following weighted query might be passed to a standard IR system: \n(2.0 Robert Redford) (2.0 Paul Newman) star depression era grifter (1.5 flick) \nDeepQA also makes use of the convenient fact that the vast majority of Jeopardy! answers are the title of a Wikipedia document. To find these titles, we can do a second text retrieval pass specifically on Wikipedia documents. Then instead of extracting passages from the retrieved Wikipedia document, we directly return the titles of the highly ranked retrieved documents as the possible answers. \nOnce we have a set of passages, we need to extract candidate answers. If the document happens to be a Wikipedia page, we can just take the title, but for other texts, like news documents, we need other approaches. Two common approaches are to extract all anchor texts in the document (anchor text is the text between $< a >$ and $< / mathsf { a } >$ used to point to a URL in an HTML page), or to extract all noun phrases in the passage that are Wikipedia document titles. \nCandidate Answer Scoring Next DeepQA uses many sources of evidence to score each candidate. This includes a classifier that scores whether the candidate answer can be interpreted as a subclass or instance of the potential answer type. Consider the candidate “difficulty swallowing” and the lexical answer type “manifestation”. DeepQA first matches each of these words with possible entities in ontologies like DBpedia and WordNet. Thus the candidate “difficulty swallowing” is matched with the DBpedia entity “Dysphagia”, and then that instance is mapped to the WordNet type “Symptom”. The answer type “manifestation” is mapped to the WordNet type “Condition”. The system looks for a hyponymy, or synonymy link, in this case finding hyponymy between “Symptom” and “Condition”. \nOther scorers are based on using time and space relations extracted from DBpedia or other structured databases. For example, we can extract temporal properties of the entity (when was a person born, when died) and then compare to time expressions in the question. If a time expression in the question occurs chronologically before a person was born, that would be evidence against this person being the answer to the question. \nFinally, we can use text retrieval to help retrieve evidence supporting a candidate answer. We can retrieve passages with terms matching the question, then replace the focus in the question with the candidate answer and measure the overlapping words or ordering of the passage with the modified question. \nThe output of this stage is a set of candidate answers, each with a vector of scoring features. \nAnswer Merging and Scoring DeepQA finally merges equivalent candidate answers. Thus if we had extracted two candidate answers J.F.K. and John F. Kennedy, this stage would merge the two into a single candidate, for example using the anchor dictionaries described above for entity linking, which will list many synonyms for Wikipedia titles (e.g., JFK, John F. Kennedy, Senator John F. Kennedy, President Kennedy, Jack Kennedy). We then merge the evidence for each variant, combining the scoring feature vectors for the merged candidates into a single vector. \nNow we have a set of candidates, each with a feature vector. A classifier takes each feature vector and assigns a confidence value to this candidate answer. The classifier is trained on thousands of candidate answers, each labeled for whether it is correct or incorrect, together with their feature vectors, and learns to predict a probability of being a correct answer. Since, in training, there are far more incorrect answers than correct answers, we need to use one of the standard techniques for dealing with very imbalanced data. DeepQA uses instance weighting, assigning an instance weight of .5 for each incorrect answer example in training. The candidate answers are then sorted by this confidence value, resulting in a single best answer. \n\nDeepQA’s fundamental intuition is thus to propose a very large number of candidate answers from both text-based and knowledge-based sources and then use a rich variety of evidence features for scoring these candidates. See the papers mentioned at the end of the chapter for more details. \n23.7 Evaluation of Factoid Answers \nmean reciprocal rank MRR \nFactoid question answering is commonly evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is designed for systems that return a short ranked list of answers or passages for each test set question, which we can compare against the (human-labeled) correct answer. First, each test set question is scored with the reciprocal of the rank of the first correct answer. For example if the system returned five answers to a question but the first three are wrong (so the highest-ranked correct answer is ranked fourth), the reciprocal rank for that question is $frac { 1 } { 4 }$ . The score for questions that return no correct answer is 0. The MRR of a system is the average of the scores for each question in the test set. In some versions of MRR, questions with a score of zero are ignored in this calculation. More formally, for a system returning ranked answers to each question in a test set $boldsymbol { Q }$ , (or in the alternate version, let $boldsymbol { Q }$ be the subset of test set questions that have non-zero scores). MRR is then defined as \nReading comprehension systems on datasets like SQuAD are evaluated (first ignoring punctuation and articles like $a$ , an, the) via two metrics (Rajpurkar et al., 2016): \n• Exact match: The $%$ of predicted answers that match the gold answer exactly. • $mathbf { F } _ { 1 }$ score: The average word/token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute $mathrm { F } _ { 1 }$ for each question, then return the average $mathrm { F } _ { 1 }$ over all questions. \nA number of test sets are available for question answering. Early systems used the TREC QA dataset: https://trec.nist.gov/data/qa/t8_qadata.html. More recent competitions uses the datasets described in Section 23.2.1. Other recent datasets include the AI2 Reasoning Challenge (ARC) (Clark et al., 2018) of multiple choice questions designed to be hard to answer from simple lexical methods, like this question \nWhich property of a mineral can be determined just by looking at it? (A) luster [correct] (B) mass (C) weight (D) hardness \nin which the correct answer luster is unlikely to co-occur frequently with phrases like looking at it, while the word mineral is highly associated with the incorrect answer hardness.",
        "chapter": "Question Answering",
        "section": "Classic QA Models",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "DeepQA’s fundamental intuition is thus to propose a very large number of candidate answers from both text-based and knowledge-based sources and then use a rich variety of evidence features for scoring these candidates. See the papers mentioned at the end of the chapter for more details. \n23.7 Evaluation of Factoid Answers \nmean reciprocal rank MRR \nFactoid question answering is commonly evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is designed for systems that return a short ranked list of answers or passages for each test set question, which we can compare against the (human-labeled) correct answer. First, each test set question is scored with the reciprocal of the rank of the first correct answer. For example if the system returned five answers to a question but the first three are wrong (so the highest-ranked correct answer is ranked fourth), the reciprocal rank for that question is $frac { 1 } { 4 }$ . The score for questions that return no correct answer is 0. The MRR of a system is the average of the scores for each question in the test set. In some versions of MRR, questions with a score of zero are ignored in this calculation. More formally, for a system returning ranked answers to each question in a test set $boldsymbol { Q }$ , (or in the alternate version, let $boldsymbol { Q }$ be the subset of test set questions that have non-zero scores). MRR is then defined as \nReading comprehension systems on datasets like SQuAD are evaluated (first ignoring punctuation and articles like $a$ , an, the) via two metrics (Rajpurkar et al., 2016): \n• Exact match: The $%$ of predicted answers that match the gold answer exactly. • $mathbf { F } _ { 1 }$ score: The average word/token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute $mathrm { F } _ { 1 }$ for each question, then return the average $mathrm { F } _ { 1 }$ over all questions. \nA number of test sets are available for question answering. Early systems used the TREC QA dataset: https://trec.nist.gov/data/qa/t8_qadata.html. More recent competitions uses the datasets described in Section 23.2.1. Other recent datasets include the AI2 Reasoning Challenge (ARC) (Clark et al., 2018) of multiple choice questions designed to be hard to answer from simple lexical methods, like this question \nWhich property of a mineral can be determined just by looking at it? (A) luster [correct] (B) mass (C) weight (D) hardness \nin which the correct answer luster is unlikely to co-occur frequently with phrases like looking at it, while the word mineral is highly associated with the incorrect answer hardness.",
        "chapter": "Question Answering",
        "section": "Evaluation of Factoid Answers",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "DeepQA’s fundamental intuition is thus to propose a very large number of candidate answers from both text-based and knowledge-based sources and then use a rich variety of evidence features for scoring these candidates. See the papers mentioned at the end of the chapter for more details. \n23.7 Evaluation of Factoid Answers \nmean reciprocal rank MRR \nFactoid question answering is commonly evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is designed for systems that return a short ranked list of answers or passages for each test set question, which we can compare against the (human-labeled) correct answer. First, each test set question is scored with the reciprocal of the rank of the first correct answer. For example if the system returned five answers to a question but the first three are wrong (so the highest-ranked correct answer is ranked fourth), the reciprocal rank for that question is $frac { 1 } { 4 }$ . The score for questions that return no correct answer is 0. The MRR of a system is the average of the scores for each question in the test set. In some versions of MRR, questions with a score of zero are ignored in this calculation. More formally, for a system returning ranked answers to each question in a test set $boldsymbol { Q }$ , (or in the alternate version, let $boldsymbol { Q }$ be the subset of test set questions that have non-zero scores). MRR is then defined as \nReading comprehension systems on datasets like SQuAD are evaluated (first ignoring punctuation and articles like $a$ , an, the) via two metrics (Rajpurkar et al., 2016): \n• Exact match: The $%$ of predicted answers that match the gold answer exactly. • $mathbf { F } _ { 1 }$ score: The average word/token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute $mathrm { F } _ { 1 }$ for each question, then return the average $mathrm { F } _ { 1 }$ over all questions. \nA number of test sets are available for question answering. Early systems used the TREC QA dataset: https://trec.nist.gov/data/qa/t8_qadata.html. More recent competitions uses the datasets described in Section 23.2.1. Other recent datasets include the AI2 Reasoning Challenge (ARC) (Clark et al., 2018) of multiple choice questions designed to be hard to answer from simple lexical methods, like this question \nWhich property of a mineral can be determined just by looking at it? (A) luster [correct] (B) mass (C) weight (D) hardness \nin which the correct answer luster is unlikely to co-occur frequently with phrases like looking at it, while the word mineral is highly associated with the incorrect answer hardness.",
        "chapter": "Question Answering",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "DeepQA’s fundamental intuition is thus to propose a very large number of candidate answers from both text-based and knowledge-based sources and then use a rich variety of evidence features for scoring these candidates. See the papers mentioned at the end of the chapter for more details. \n23.7 Evaluation of Factoid Answers \nmean reciprocal rank MRR \nFactoid question answering is commonly evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is designed for systems that return a short ranked list of answers or passages for each test set question, which we can compare against the (human-labeled) correct answer. First, each test set question is scored with the reciprocal of the rank of the first correct answer. For example if the system returned five answers to a question but the first three are wrong (so the highest-ranked correct answer is ranked fourth), the reciprocal rank for that question is $frac { 1 } { 4 }$ . The score for questions that return no correct answer is 0. The MRR of a system is the average of the scores for each question in the test set. In some versions of MRR, questions with a score of zero are ignored in this calculation. More formally, for a system returning ranked answers to each question in a test set $boldsymbol { Q }$ , (or in the alternate version, let $boldsymbol { Q }$ be the subset of test set questions that have non-zero scores). MRR is then defined as \nReading comprehension systems on datasets like SQuAD are evaluated (first ignoring punctuation and articles like $a$ , an, the) via two metrics (Rajpurkar et al., 2016): \n• Exact match: The $%$ of predicted answers that match the gold answer exactly. • $mathbf { F } _ { 1 }$ score: The average word/token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute $mathrm { F } _ { 1 }$ for each question, then return the average $mathrm { F } _ { 1 }$ over all questions. \nA number of test sets are available for question answering. Early systems used the TREC QA dataset: https://trec.nist.gov/data/qa/t8_qadata.html. More recent competitions uses the datasets described in Section 23.2.1. Other recent datasets include the AI2 Reasoning Challenge (ARC) (Clark et al., 2018) of multiple choice questions designed to be hard to answer from simple lexical methods, like this question \nWhich property of a mineral can be determined just by looking at it? (A) luster [correct] (B) mass (C) weight (D) hardness \nin which the correct answer luster is unlikely to co-occur frequently with phrases like looking at it, while the word mineral is highly associated with the incorrect answer hardness. \nBibliographical and Historical Notes \nQuestion answering was one of the earliest NLP tasks, and early versions of the textbased and knowledge-based paradigms were developed by the very early 1960s. The text-based algorithms generally relied on simple parsing of the question and of the sentences in the document, and then looking for matches. This approach was used very early on (Phillips, 1960) but perhaps the most complete early system, and one that strikingly prefigures modern relation-based systems, was the Protosynthex system of Simmons et al. (1964). Given a question, Protosynthex first formed a query from the content words in the question, and then retrieved candidate answer sentences in the document, ranked by their frequency-weighted term overlap with the question. The query and each retrieved sentence were then parsed with dependency parsers, and the sentence whose structure best matches the question structure selected. Thus the question What do worms eat? would match worms eat grass: both have the subject worms as a dependent of eat, in the version of dependency grammar used at the time, while birds eat worms has birds as the subject: \nThe alternative knowledge-based paradigm was implemented in the BASEBALL system (Green et al., 1961). This system answered questions about baseball games like “Where did the Red Sox play on July $7 ^ { , , }$ by querying a structured database of game information. The database was stored as a kind of attribute-value matrix with values for attributes of each game: \nMonth $mathbf { Sigma } = mathbf { Sigma }$ July Place $mathbf { Sigma } = mathbf { Sigma }$ Boston Day $= 7$ Game Serial No. $= 9 6$ (Team $mathbf { Sigma } = mathbf { Sigma }$ Red Sox, Score $mathit { Theta } = mathit { Theta } 5 mathit { check { Psi } } .$ ) (Team $mathbf { Sigma } = mathbf { Sigma }$ Yankees, Score = 3) \nEach question was constituency-parsed using the algorithm of Zellig Harris’s TDAP project at the University of Pennsylvania, essentially a cascade of finite-state transducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen 1999). Then in a content analysis phase each word or phrase was associated with a program that computed parts of its meaning. Thus the phrase ‘Where’ had code to assign the semantics $mathsf { P l a c e } = ?$ , with the result that the question “Where did the Red Sox play on July $7 \"$ was assigned the meaning Place $= ?$ Team $mathbf { Sigma } = mathbf { Sigma }$ Red Sox Month $mathbf { Sigma } = mathbf { Sigma }$ July $tt D a y mathrm { ~ = ~ } 7$ \nThe question is then matched against the database to return the answer. Simmons (1965) summarizes other early QA systems. \nAnother important progenitor of the knowledge-based paradigm for questionanswering is work that used predicate calculus as the meaning representation language. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to be a natural language interface to a database of chemical facts about lunar geology. It could answer questions like Do any samples have greater than 13 percent aluminum by parsing them into a logical form \n\n(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16 (NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT)))) \nBy a couple decades later, drawing on new machine learning approaches in NLP, Zelle and Mooney (1996) proposed to treat knowledge-based QA as a semantic parsing task, by creating the Prolog-based GEOQUERY dataset of questions about US geography. This model was extended by Zettlemoyer and Collins (2005) and 2007. By a decade later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia and Liang 2016), and then to knowledge-based question answering by mapping text to SQL (Iyer et al., 2017). \nMeanwhile, the information-retrieval paradigm for question answering was influenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees and Harman, 2005). TREC added an influential QA track in 1999, which led to a wide variety of factoid and non-factoid systems competing in annual evaluations. \nAt that same time, Hirschman et al. (1999) introduced the idea of using children’s reading comprehension tests to evaluate machine text comprehension algorithms. They acquired a corpus of 120 passages with 5 questions each designed for 3rd-6th grade children, built an answer extraction system, and measured how well the answers given by their system corresponded to the answer key from the test’s publisher. Their algorithm focused on word overlap as a feature; later algorithms added named entity features and more complex similarity between the question and the answer span (Riloff and Thelen 2000, $mathrm { N g }$ et al. 2000). \nThe DeepQA component of the Watson Jeopardy! system was a large and sophisticated feature-based system developed just before neural systems became common. It is described in a series of papers in volume 56 of the IBM Journal of Research and Development, e.g., Ferrucci (2012). \nNeural reading comprehension systems drew on the insight common to early systems that answer finding should focus on question-passage similarity. Many of the architectural outlines of these modern neural systems were laid out in Hermann et al. (2015a), Chen et al. (2017a), and Seo et al. (2017). These systems focused on datasets like Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their successors, usually using separate IR algorithms as input to neural reading comprehension systems. Some recent systems include the IR component as part of a single end-to-end architecture (Lee et al., 2019). \nOther question-answering tasks include Quiz Bowl, which has timing considerations since the question can be interrupted (Boyd-Graber et al., 2018). Question answering is also an important function of modern personal assistant dialog systems; see Chapter 24. \nExercises \nCHAPTER 24 Chatbots & Dialogue Systems \nLes lois de la conversation sont en ge´ne´ral de ne s’y appesantir sur aucun objet, mais de passer le´ge`rement, sans effort et sans affectation, d’un sujet a` un autre ; de savoir y parler de choses frivoles comme de choses se´rieuses \n[The rules of conversation are, in general, not to dwell on any one subject, but to pass lightly from one to another without effort and without affectation; to know how to speak about trivial topics as well as serious ones;] The 18th C. Encyclopedia of Diderot, start of the entry on conversation \n“What ho!” I said.   \n“What ho!” said Motty.   \n“What ho! What ho!”   \n“What ho! What ho! What ho!”   \nAfter that it seemed rather difficult to go on with the conversation.” P.G. Wodehouse, My Man Jeeves \nconversation dialogue \nThe literature of the fantastic abounds in inanimate objects magically endowed with sentience and the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s Frankenstein, there is something deeply moving about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so lifelike that he tapped it on the knee and commanded it to speak. Perhaps this shouldn’t be surprising. Language is the mark of humanity and sentience, and conversation or dialogue is the most fundamental and specially privileged arena of language. It is the first kind of language we learn as children, and for most of us, it is the kind of language we most commonly indulge in, whether we are ordering curry for lunch or buying spinach, participating in business meetings or talking with our families, booking airline flights or complaining about the weather. \n\ndialogue system conversational agent \nThis chapter introduces the fundamental algorithms of dialogue systems, or conversational agents. These programs communicate with users in natural language (text, speech, or both), and fall into two classes. Task-oriented dialogue agents use conversation with users to help complete tasks. Dialogue agents in digital assistants (Siri, Alexa, Google Now/Home, Cortana, etc.), give directions, control appliances, find restaurants, or make calls. Conversational agents can answer questions on corporate websites, interface with robots, and even be used for social good: DoNotPay is a “robot lawyer” that helps people challenge incorrect parking fines, apply for emergency housing, or claim asylum if they are refugees. By contrast, chatbots are systems designed for extended conversations, set up to mimic the unstructured conversations or ‘chats’ characteristic of human-human interaction, mainly for entertainment, but also for practical purposes like making task-oriented agents more natural.1 In Section 24.2 we’ll discuss the three major chatbot architectures: rule-based systems, information retrieval systems, and encoder-decoder generators. In Section 24.3 we turn to task-oriented agents, introducing the frame-based architecture (the GUS architecture) that underlies most task-based systems.",
        "chapter": "Question Answering",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "24.1 Properties of Human Conversation \nConversation between humans is an intricate and complex joint activity. Before we attempt to design a conversational agent to converse with humans, it is crucial to understand something about how humans converse with each other. Consider some of the phenomena that occur in the conversation between a human travel agent and a human client excerpted in Fig. 24.1. \n$overline { { mathbf { C } _ { 1 } } }$ : . . . I need to travel in May.   \n$mathbf { A } _ { 2 }$ : And, what day in May did you want to travel?   \n$mathbf { C } _ { 3 }$ : OK uh I need to be there for a meeting that’s from the 12th to the 15th. ${ bf A } _ { 4 }$ : And you’re flying into what city?   \n${ bf C } _ { 5 }$ : Seattle.   \n$mathbf { A } _ { 6 }$ : And what time would you like to leave Pittsburgh?   \n$mathrm { C } _ { 7 }$ : Uh hmm I don’t think there’s many options for non-stop.   \n${ bf A } _ { 8 }$ : Right. There’s three non-stops today.   \n$mathbf { C } _ { 9 }$ : What are they?   \n$mathbf { A } _ { 1 0 }$ : The first one departs PGH at 10:00am arrives Seattle at 12:05 their time. The second flight departs PGH at $5 { : } 5 5 mathrm { p m }$ , arrives Seattle at $8 mathrm { p m }$ . And the last flight departs PGH at $8 { : } 1 5 mathrm { p m }$ arrives Seattle at $1 0 { : } 2 8 mathrm { p m }$ .   \n$mathbf { C } _ { 1 1 }$ : OK I’ll take the 5ish flight on the night before on the 11th.   \n$mathbf { A } _ { 1 2 }$ : On the 11th? OK. Departing at $5 { : } 5 5 mathrm { p m }$ arrives Seattle at 8pm, U.S. Air flight 115.   \n$mathbf { C } _ { 1 3 }$ : OK.   \n$mathbf { A } _ { 1 4 }$ : And you said returning on May 15th?   \n$mathbf { C } _ { 1 5 }$ : Uh, yeah, at the end of the day.   \n$mathbf { A } _ { 1 6 }$ : OK. There’s #two non-stops . . . #   \n$mathbf { C } _ { 1 7 }$ : #Act. . . actually #, what day of the week is the 15th? $mathbf { A } _ { 1 8 }$ : It’s a Friday.   \nC19: Uh hmm. I would consider staying there an extra day til Sunday.   \n$mathbf { A } _ { 2 0 }$ : OK. . . OK. On Sunday I have . . \nTurns \nturn A dialogue is a sequence of turns $mathrm { nabla { C _ { 1 } } }$ , $mathbf { A } _ { 2 }$ , $mathbf { C } _ { 3 }$ , and so on), each a single contribution from one speaker to the dialogue (as if in a game: I take a turn, then you take a turn, \nthen me, and so on). There are 20 turns in Fig. 24.1. A turn can consist of a sentence (like $mathbf { C } _ { 1 } mathbf { Psi } _ { }$ ), although it might be as short as a single word $( mathbf { C } _ { 1 3 } )$ or as long as multiple sentences $left( mathbf { A } _ { 1 0 } right)$ . \nendpointing \nTurn structure has important implications for spoken dialogue. A system has to know when to stop talking; the client interrupts (in $mathbf { A } _ { 1 6 }$ and $mathbf { C } _ { 1 7 }$ ), so the system must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are able to (most of the time) detect when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This task— called endpointing or endpoint detection— can be quite challenging because of noise and because people often pause in the middle of turns. \nSpeech Acts \nspeech acts \nA key insight into conversation—due originally to the philosopher Wittgenstein (1953) but worked out more fully by Austin (1962)—is that each utterance in a dialogue is a kind of action being performed by the speaker. These actions are commonly called speech acts or dialog acts: here’s one taxonomy consisting of 4 major classes (Bach and Harnish, 1979): \nConstatives: committing the speaker to something’s being the case (answering, claiming, confirming, denying, disagreeing, stating)   \nDirectives: attempts by the speaker to get the addressee to do something (advising, asking, forbidding, inviting, ordering, requesting)   \nCommissives: committing the speaker to some future course of action (promising, planning, vowing, betting, opposing)   \nAcknowledgments: express the speaker’s attitude regarding the hearer with respect to some social action (apologizing, greeting, thanking, accepting an acknowledgment) \ncommon ground grounding \nA user asking a person or a dialogue system to do something (‘Turn up the music’) is issuing a DIRECTIVE. Asking a question that requires an answer is also a way of issuing a DIRECTIVE: in a sense when the system says $( mathbf { A } _ { 2 } )$ “what day in May did you want to travel?” it’s as if the system is (very politely) commanding the user to answer. By contrast, a user stating a constraint (like $mathbf { C } _ { 1 }$ ‘I need to travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing an ACKNOWLEDGMENT. The speech act expresses an important component of the intention of the speaker (or writer) in saying what they said. \nGrounding \nA dialogue is not just a series of independent speech acts, but rather a collective act performed by the speaker and the hearer. Like all collective acts, it’s important for the participants to establish what they both agree on, called the common ground (Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Grounding means acknowledging that the hearer has understood the speaker; like an ACK used to confirm receipt in data communications (Clark, 1996). (People need grounding for non-linguistic actions as well; the reason an elevator button lights up when it’s pressed is to acknowledge that the elevator has indeed been called (Norman, 1988).) \nHumans constantly ground each other’s utterances. We can ground by explicitly saying “OK”, as the agent does in ${ bf A } _ { 8 }$ or ${ bf A } _ { 1 0 }$ . Or we can ground by repeating what the other person says; in utterance $mathbf { A } _ { 2 }$ the agent repeats “in May”, demonstrating her understanding to the client. Or notice that when the client answers a question, the agent begins the next question with “And”. The “And” implies that the new question is ‘in addition’ to the old question, again indicating to the client that the agent has successfully understood the answer to the last question. \n\nconversational analysis \nSubdialogues and Dialogue Structure \nadjacency pair \nConversations have structure. Consider, for example, the local structure between speech acts discussed in the field of conversational analysis (Sacks et al., 1974). QUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed by ACCEPTANCE (or REJECTION). COMPLIMENTS (“Nice jacket!”) often give rise to DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs are composed of a first pair part and a second pair part (Schegloff, 1968), and these expectations can help systems decide what actions to take. \nside sequence subdialogue \nHowever, dialogue acts aren’t always followed immediately by their second pair part. The two parts can be separated by a side sequence (Jefferson 1972) or subdialogue. For example utterances $mathbf { C } _ { 1 7 }$ to $mathbf { A } _ { 2 0 }$ constitute a correction subdialogue (Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998): \n$mathbf { C } _ { 1 7 }$ : #Act. . . actually#, what day of the week is the 15th?   \n$mathbf { A } _ { 1 8 }$ : It’s a Friday.   \n$mathbf { C } _ { 1 9 }$ : Uh hmm. I would consider staying there an extra day til Sunday. $mathbf { A } _ { 2 0 }$ : OK. . . OK. On Sunday I have . . \nThe question in $mathbf { C } _ { 1 7 }$ interrupts the prior discourse, in which the agent was looking for a May 15 return flight. The agent must answer the question and also realize that ‘’I would consider staying...til Sunday” means that the client would probably like to change their plan, and now go back to finding return flights, but for the 17th. \nAnother side sequence is the clarification question, which can form a subdialogue between a REQUEST and a RESPONSE. This is especially common in dialogue systems where speech recognition errors causes the system to have to ask for clarifications or repetitions like the following: \npresequence \nUser: What do you have going to UNKNOWN WORD on the 5th? System: Let’s see, going where on the 5th? User: Going to Hong Kong.   \nSystem: OK, here are some flights... \nIn addition to side-sequences, questions often have presequences, like the following example where a user starts with a question about the system’s capabilities (“Can you make train reservations”) before making a request. \nUser: Can you make train reservations?   \nSystem: Yes I can.   \nUser: Great, I’d like to reserve a seat on the 4pm train to New York. \nInitiative \ninitiative \nSometimes a conversation is completely controlled by one participant. For example a reporter interviewing a chef might ask questions, and the chef responds. We say that the reporter in this case has the conversational initiative (Walker and Whittaker, 1990). In normal human-human dialogue, however, it’s more common for initiative to shift back and forth between the participants, as they sometimes answer questions, sometimes ask them, sometimes take the conversations in new directions, sometimes not. You may ask me a question, and then I respond asking you to clarify something you said, which leads the conversation in all sorts of ways. We call such interactions mixed initiative (Walker and Whittaker, 1990). \n\nMixed initiative, while the norm for human-human conversations, is very difficult for dialogue systems to achieve. It’s much easier to design dialogue systems to be passive responders. In the question answering systems we saw in Chapter 23, or in simple search engines, the initiative lies completely with the user. In such userinitiative systems, the user specifies a query, and the systems responds. Then the user can specify another query. Alternatively, you may have had the experience of being stuck in a bad dialogue system that asks a question and gives you no opportunity to do anything until you answer it. Such system-initiative architectures can be very frustrating. \nInference and Implicature \nInference is also important in dialogue understanding. Consider the client’s response $mathbf { C } _ { 2 }$ , repeated here: \n$mathbf { A } _ { 2 }$ : And, what day in May did you want to travel? $mathbf { C } _ { 3 }$ : OK uh I need to be there for a meeting that’s from the 12th to the 15th. \nimplicature \nNotice that the client does not in fact answer the agent’s question. The client merely mentions a meeting at a certain time. What is it that licenses the agent to infer that the client is mentioning this meeting so as to inform the agent of the travel dates? \nrelevance \nThe speaker seems to expect the hearer to draw certain inferences; in other words, the speaker is communicating more information than seems to be present in the uttered words. This kind of example was pointed out by Grice (1975, 1978) as part of his theory of conversational implicature. Implicature means a particular class of licensed inferences. Grice proposed that what enables hearers to draw these inferences is that conversation is guided by a set of maxims, general heuristics that play a guiding role in the interpretation of conversational utterances. One such maxim is the maxim of relevance which says that speakers attempt to be relevant, they don’t just utter random speech acts. When the client mentions a meeting on the 12th, the agent reasons ‘There must be some relevance for mentioning this meeting. What could it be?’. The agent knows that one precondition for having a meeting (at least before Web conferencing) is being at the place where the meeting is held, and therefore that maybe the meeting is a reason for the travel, and if so, then since people like to arrive the day before a meeting, the agent should infer that the flight should be on the 11th. \nThese subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. \n24.2 Chatbots \nchatbot \nThe simplest kinds of dialogue systems are chatbots, systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or ‘chats’ characteristic of informal human-human interaction.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Properties of Human Conversation",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "counseling. \nLike practically everything else in language processing, chatbot architectures fall into two classes: rule-based systems and corpus-based systems. Rule-based systems include the early influential ELIZA and PARRY systems. Corpus-based systems mine large datasets of human-human conversations, which can be done by using information retrieval to copy a human response from a previous conversation, or using an encoder-decoder system to generate a response from a user utterance. \n24.2.1 Rule-based chatbots: ELIZA and PARRY \nELIZA (Weizenbaum, 1966) is the most important chatbot dialogue system in the history of the field, and so we take some time here to describe its algorithm in detail. ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient’s statements back at them. Rogerian psychology is the rare type of conversation in which, as Weizenbaum points out, one can “assume the pose of knowing almost nothing of the real world”. If a patient says “I went for a long boat ride” and the psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what a boat is, but rather assume she had some conversational goal. Most chatbots trying to pass the Turing test choose a domain with similar properties. \nFig. 24.4 shows the most famous ELIZA conversation, in which ELIZA seems to give very sophisticated therapeutic advice, excerpted (deleting some material with ...) from a full interaction given in the original paper (Weizenbaum, 1966). \nAs we summarized briefly in Chapter 2, ELIZA worked by pattern/transform rules like the following one: \n( $mathbf { widetilde { mathbf { Gamma } } } _ { mathbb { 0 } }$ YOU 0 ME) [pattern] $mathrel { - } >$ \n(WHAT MAKES YOU THINK I 3 YOU) [transform] \nIn the ELIZA pattern, 0 means Kleene*, and in the transform rules the numbers are the index of the constituent in the pattern. Thus the number 3 refers to the second 0 in the first pattern. This rule would transfer \nYou hate me into: \nWHAT MAKES YOU THINK I HATE YOU \nEach ELIZA pattern/rule is linked to a keyword that might occur in a user sentence. The architecture is summarized in Fig. 24.5. \nKeywords are associated with a rank, with specific words being more highly ranked, and more general words ranking lower. Consider the following user sentence: \nI know everybody laughed at me \nBecause it has the word “I”, this sentence could match the following rule whose keyword is $I$ : \n$( mathrm { I } ~ ddot { ast } ) ~  ~ ( mathrm { Y o u } ~ mathsf { s a y } ~ mathrm { y o u } ~ 2 )$ producing: \nYOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU \nBut as Weizenbaum points out, “I” is a very general word and its keywords lead to very general responses. Instead the keyword “everybody” is much more interesting, since someone using universals like everybody or always is probably “referring \nMen are all alike.   \nIN WHAT WAY   \nThey’re always bugging us about something or other.   \nCAN YOU THINK OF A SPECIFIC EXAMPLE   \nWell my boyfriend made me come here.   \nYOUR BOYFRIEND MADE YOU COME HERE   \nHe says I’m depressed much of the time.   \nI AM SORRY TO HEAR YOU ARE DEPRESSED   \n：   \n..   \nWHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?   \nMy father   \nYOUR FATHER   \nYou are like my father in some ways.   \nWHAT RESEMBLANCE DO YOU SEE   \nYou are not very aggressive but I think you don’t want me to notice that. WHAT MAKES YOU THINK I AM NOT AGGRESSIVE   \nYou don’t argue with me.   \nWHY DO YOU THINK I DON’T ARGUE WITH YOU   \nYou are afraid of me.   \nDOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU   \nMy father is afraid of everybody.   \nWHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER Bullies.   \nDOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YOU COME HERE   \nfunction ELIZA GENERATOR(user sentence) returns response   \nFind the word $w$ in sentence that has the highest keyword rank if $w$ exists Choose the highest ranked rule $r$ for $w$ that matches sentence response $$ Apply the transform in $r$ to sentence if $w = mathrm { cdot } { mathrm { m y } } ^ { mathrm { , } }$ future $$ Apply a transformation from the ‘memory’ rule list to sentence Push future onto memory queue else (no keyword applies) either response $$ Apply the transform for the NONE keyword to sentence or response $$ Pop the oldest response from the memory queue return(response) \n\nto some quite specific event or person”. Therefore, ELIZA prefers to respond with the pattern associated with the more specific keyword everybody (implementing by just assigning “everybody” rank 5 and “I” rank 0 in the lexicon), whose rule thus \ntransforms the sentence to: \nWHO IN PARTICULAR ARE YOU THINKING OF? \nIf no keyword matches, ELIZA chooses a non-committal response like “PLEASE GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”. \nFinally, ELIZA has a clever memory trick that accounts for the very last sentence of the conversation above. Whenever the word “my” is the highest ranked keyword, ELIZA will randomly select a transform on the MEMORY list, apply it to the sentence, and store it on the queue: \n(MEMORY MY( $mathbf { widetilde { mathbf { Gamma } } } _ { mathbb { 0 } }$ MY $theta  =$ LETS DISCUSS FURTHER WHY YOUR 3)( $mathbf { widetilde { xi } } _ { pmb { mathbb { 0 } } }$ MY $theta  =$ EARLIER YOU SAID YOUR 3)$( mathbb { 0 }  mathbb { M }  mathbb { 0 }  =  mathbb { D } 0 mathbb { E } mathbb { S }$ THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3\nLater, if no keyword matches a sentence, ELIZA will return the oldest entry on the MEMORY queue instead. \nPeople became deeply emotionally involved with the program. Weizenbaum tells the story of one of his staff who would ask Weizenbaum to leave the room when she talked with ELIZA. When Weizenbaum suggested that he might want to store all the ELIZA conversations for later analysis, people immediately pointed out the privacy implications, which suggested that they were having quite private conversations with ELIZA, despite knowing that it was just software. \nELIZA’s framework is still used today; modern chatbot system tools like ALICE are based on updated versions of ELIZA’s pattern/action architecture. \nA few years after ELIZA, another chatbot with a clinical psychology focus, PARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZAlike regular expressions, the PARRY system included a model of its own mental state, with affect variables for the agent’s levels of fear and anger; certain topics of conversation might lead PARRY to become more angry or mistrustful. If PARRY’s anger variable is high, he will choose from a set of “hostile” outputs. If the input mentions his delusion topic, he will increase the value of his fear variable and then begin to express the sequence of statements related to his delusion. Parry was the first known system to pass the Turing test (in 1972!); psychiatrists couldn’t distinguish text transcripts of interviews with PARRY from transcripts of interviews with real paranoids (Colby et al., 1972). \n24.2.2 Corpus-based chatbots \nCorpus-based chatbots, instead of using hand-built rules, mine conversations of human-human conversations. These systems are enormously data-intensive, requiring hundreds of millions or even billions of words for training (Serban et al., 2018). \nAvailable datasets include transcripts of natural spoken conversational corpora, like the Switchboard corpus of American English telephone conversations (Godfrey et al., 1992) or the various CALLHOME and CALLFRIEND telephone conversational corpora in many languages. Many systems also train on movie dialogue, (Danescu-Niculescu-Mizil and Lee 2011, Lison and Tiedemann 2016, inter alia) which resembles natural conversation in many ways (Forchini, 2013). \nDatasets have also been created specifically for training dialog systems by hiring crowdworkers to have conversations, often having them take on personas or talk about knowledge provided to them. For example the Topical-Chat dataset has 11K crowdsourced conversations spanning 8 broad topics (Gopalakrishnan et al.,",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Chatbots",
        "subsection": "Rule-based chatbots: ELIZA and PARRY",
        "subsubsection": "N/A"
    },
    {
        "content": "transforms the sentence to: \nWHO IN PARTICULAR ARE YOU THINKING OF? \nIf no keyword matches, ELIZA chooses a non-committal response like “PLEASE GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”. \nFinally, ELIZA has a clever memory trick that accounts for the very last sentence of the conversation above. Whenever the word “my” is the highest ranked keyword, ELIZA will randomly select a transform on the MEMORY list, apply it to the sentence, and store it on the queue: \n(MEMORY MY( $mathbf { widetilde { mathbf { Gamma } } } _ { mathbb { 0 } }$ MY $theta  =$ LETS DISCUSS FURTHER WHY YOUR 3)( $mathbf { widetilde { xi } } _ { pmb { mathbb { 0 } } }$ MY $theta  =$ EARLIER YOU SAID YOUR 3)$( mathbb { 0 }  mathbb { M }  mathbb { 0 }  =  mathbb { D } 0 mathbb { E } mathbb { S }$ THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3\nLater, if no keyword matches a sentence, ELIZA will return the oldest entry on the MEMORY queue instead. \nPeople became deeply emotionally involved with the program. Weizenbaum tells the story of one of his staff who would ask Weizenbaum to leave the room when she talked with ELIZA. When Weizenbaum suggested that he might want to store all the ELIZA conversations for later analysis, people immediately pointed out the privacy implications, which suggested that they were having quite private conversations with ELIZA, despite knowing that it was just software. \nELIZA’s framework is still used today; modern chatbot system tools like ALICE are based on updated versions of ELIZA’s pattern/action architecture. \nA few years after ELIZA, another chatbot with a clinical psychology focus, PARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZAlike regular expressions, the PARRY system included a model of its own mental state, with affect variables for the agent’s levels of fear and anger; certain topics of conversation might lead PARRY to become more angry or mistrustful. If PARRY’s anger variable is high, he will choose from a set of “hostile” outputs. If the input mentions his delusion topic, he will increase the value of his fear variable and then begin to express the sequence of statements related to his delusion. Parry was the first known system to pass the Turing test (in 1972!); psychiatrists couldn’t distinguish text transcripts of interviews with PARRY from transcripts of interviews with real paranoids (Colby et al., 1972). \n24.2.2 Corpus-based chatbots \nCorpus-based chatbots, instead of using hand-built rules, mine conversations of human-human conversations. These systems are enormously data-intensive, requiring hundreds of millions or even billions of words for training (Serban et al., 2018). \nAvailable datasets include transcripts of natural spoken conversational corpora, like the Switchboard corpus of American English telephone conversations (Godfrey et al., 1992) or the various CALLHOME and CALLFRIEND telephone conversational corpora in many languages. Many systems also train on movie dialogue, (Danescu-Niculescu-Mizil and Lee 2011, Lison and Tiedemann 2016, inter alia) which resembles natural conversation in many ways (Forchini, 2013). \nDatasets have also been created specifically for training dialog systems by hiring crowdworkers to have conversations, often having them take on personas or talk about knowledge provided to them. For example the Topical-Chat dataset has 11K crowdsourced conversations spanning 8 broad topics (Gopalakrishnan et al., \n2019), and the EMPATHETICDIALOGUES includes 25K crowdsourced conversations grounded in a specific situation where a speaker was feeling a specific emotion (Rashkin et al., 2019). \nAll of these datasets, although large, don’t reach the size of billions of words, and so many systems first pretrain on large datasets of pseudo-conversations drawn from Twitter (Ritter et al., 2010a), Reddit (Roller et al., 2021), Weibo (微博), and other social media platforms. \nAnother common technique is to extract possible responses from knowledge sources (Wikipedia, news stories) so that a chatbot can tell stories or mention facts acquired in that way. \nFinally, once a chatbot has been put into practice, the turns that humans use to respond to the chatbot can be used as additional conversational data for training or finetuning. Here it’s important to have confidence metrics to make sure that these turns come from conversations that are going well (Hancock et al., 2019). It’s also crucial in these cases to remove personally identifiable information (PII); see Section 24.6.1. \nMost corpus based chatbots produce their responses to a user’s turn in context either by retrieval methods (using information retrieval to grab a response from some corpus that is appropriate given the dialogue context) or generation methods (using a language model or encoder-decoder to generate the response given the dialogue context). In either case, systems mostly generate a single response turn that is appropriate given the entire conversation so far (for conversations that are short enough to fit into a single model’s window). For this reason they are often called response generation systems. Corpus-based chatbot algorithms thus draw on algorithms for question answering systems, which similarly focus on single responses while ignoring longer-term conversational goals. \nResponse by retrieval The retrieval method of responding is to think of the user’s turn as a query $q$ , and our job is to retrieve and repeat some appropriate turn $r$ as the response from a corpus of conversations $C$ . Generally $C$ is the training set for the system, and we score each turn in $C$ as a potential response to the context $q$ selecting the highest-scoring one. The scoring metric is similarity: we choose the $r$ that is most similar to $q$ , using any of the IR methods we saw in Section 23.1. This can be done using classic IR techniques to compute tf-idf models for $C$ and $q$ , choosing the $r$ that has the highest tf-idf cosine with $q$ : \nAnother version of this method is to return the response to the turn resembling $q$ ; that is, we first find the most similar turn $t$ to $q$ and then return as a response the following turn $r$ . \nAlternatively, we can use the neural IR techniques of Section 23.1.5. The simplest of those is a bi-encoder model, in which we train two separate encoders, one to encode the user query and one to encode the candidate response, and use the dot product between these two vectors as the score (Fig. 24.6a). For example to implement this using BERT, we would have two encoders $mathbf { B E R T } _ { Q }$ and $mathbf { B E R T } _ { R }$ and we could represent the query and candidate response as the [CLS] token of the respec\ntive encoders: \nThe IR-based approach can be extended in various ways, such as by using more sophisticated neural architectures (Humeau et al., 2020), or by using a longer context for the query than just the user’s last turn, up to the whole preceding conversation. Information about the user or sentiment or other information can also play a role. \nResponse by generation An alternate way to use a corpus to generate dialogue is to think of response production as an encoder-decoder task— transducing from the user’s prior turn to the system’s turn. We can think of this as a machine learning version of ELIZA; the system learns from a corpus to transduce a question to an answer. Ritter et al. (2011) proposed early on to think of response generation as a kind of translation, and this idea was generalized to the encoder-decoder model roughly contemporaneously by Shang et al. (2015), Vinyals and Le (2015), and Sordoni et al. (2015). \nAs we saw in Chapter 10, encoder decoder models generate each token $r _ { t }$ of the response by conditioning on the encoding of the entire query $q$ and the response so far $r _ { 1 } . . . r _ { t - 1 }$ : \nFig. 24.6 shows the intuition of the generator and retriever methods for response generation. In the generator architecture, we normally include a longer context, forming the query not just from the user’s turn but from the entire conversation-sofar. Fig. 24.7 shows a fleshed-out example. \nA number of modifications are required to the basic encoder-decoder model to adapt it for the task of response generation. For example basic encoder-decoder models have a tendency to produce predictable but repetitive and therefore dull responses like “I’m OK” or “I don’t know” that shut down the conversation. Thus instead of greedily choosing the most likely (and most predictable) response, we can use diversity-enhanced versions of beam search (Vijayakumar et al., 2018), or diversity-focused training objectives (Li et al., 2016a). Basic models also tend to produce utterances that are too short; adding minimum length constraints is also important (Roller et al., 2021). \nAn alternative to the encoder-decoder architecture is to fine-tune a large language model on a conversational dataset and use the language model directly as a response generator. In the Chirpy Cardinal system (Paranjape et al., 2020), for example, the neural chat component generates responses from GPT-2 (Radford et al., 2019), finetuned on the EmpatheticDialogues dataset (Rashkin et al., 2019). \nFinally, encoder-decoder response generators focus on generating single responses, and so don’t tend to do a good job of continuously generating responses that cohere across multiple turns. This can be addressed by using reinforcement learning, as well as techniques like adversarial networks, to learn to choose responses that make the overall conversation more natural (Li et al. 2016b, Li et al. 2017). \nResponse by retrieving and refining knowledge \nChatbots can be much more interesting and informative if they can respond based on text knowledge sources other than dialogue. This approach was pioneered early on by the COBOT chatbot (Isbell et al., 2000), which generated responses by selecting sentences from a corpus that combined the Unabomber Manifesto by Theodore Kaczynski, articles on alien abduction, the scripts of “The Big Lebowski” and “Planet of the Apes”. XiaoIce collects sentences from public lectures and news articles and searches them using IR based on query expansion from the user’s turn to respond to turns like “Tell me something about Beijing” (Zhou et al., 2020); \nOne way to augment the encoder decoder architecture for retrieve and refine is to first use IR to retrieve potentially useful passages from Wikipedia (Yan et al., 2016), and then create multiple candidates by concatenating each retrieved Wikipedia sentence to the dialogue context with a separator token. Each candidate can be given as the encoder context to the encoder-decoder model, which learns to incorporate text from the Wikipedia sentence into its generated response (Dinan et al. 2019, Roller et al. 2021). \nThe language model approach to generation can also make use of external knowledge sources, by giving these same knowledge $+$ dialogue context candidates to a language model like GPT-2 finetuned on conversational corpora to generate a knowledgeaware response (Paranjape et al., 2020). \n24.2.3 Hybrid architectures \nChatbots can also be built with architectures that are hybrids of the rule-based and neural/corpus architectures, and even use elements of the frame-based structure we describe below. This is common, for example, for systems competing in the Alexa",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Chatbots",
        "subsection": "Corpus-based chatbots",
        "subsubsection": "N/A"
    },
    {
        "content": "An alternative to the encoder-decoder architecture is to fine-tune a large language model on a conversational dataset and use the language model directly as a response generator. In the Chirpy Cardinal system (Paranjape et al., 2020), for example, the neural chat component generates responses from GPT-2 (Radford et al., 2019), finetuned on the EmpatheticDialogues dataset (Rashkin et al., 2019). \nFinally, encoder-decoder response generators focus on generating single responses, and so don’t tend to do a good job of continuously generating responses that cohere across multiple turns. This can be addressed by using reinforcement learning, as well as techniques like adversarial networks, to learn to choose responses that make the overall conversation more natural (Li et al. 2016b, Li et al. 2017). \nResponse by retrieving and refining knowledge \nChatbots can be much more interesting and informative if they can respond based on text knowledge sources other than dialogue. This approach was pioneered early on by the COBOT chatbot (Isbell et al., 2000), which generated responses by selecting sentences from a corpus that combined the Unabomber Manifesto by Theodore Kaczynski, articles on alien abduction, the scripts of “The Big Lebowski” and “Planet of the Apes”. XiaoIce collects sentences from public lectures and news articles and searches them using IR based on query expansion from the user’s turn to respond to turns like “Tell me something about Beijing” (Zhou et al., 2020); \nOne way to augment the encoder decoder architecture for retrieve and refine is to first use IR to retrieve potentially useful passages from Wikipedia (Yan et al., 2016), and then create multiple candidates by concatenating each retrieved Wikipedia sentence to the dialogue context with a separator token. Each candidate can be given as the encoder context to the encoder-decoder model, which learns to incorporate text from the Wikipedia sentence into its generated response (Dinan et al. 2019, Roller et al. 2021). \nThe language model approach to generation can also make use of external knowledge sources, by giving these same knowledge $+$ dialogue context candidates to a language model like GPT-2 finetuned on conversational corpora to generate a knowledgeaware response (Paranjape et al., 2020). \n24.2.3 Hybrid architectures \nChatbots can also be built with architectures that are hybrids of the rule-based and neural/corpus architectures, and even use elements of the frame-based structure we describe below. This is common, for example, for systems competing in the Alexa \nPrize challenge, in which university teams build social chatbots to converse with volunteers on the Amazon Alexa platform, and are scored based on the length and user ratings of their conversations (Ram et al., 2017). \nFor example the Chirpy Cardinal system (Paranjape et al., 2020) applies an NLP pipeline that includes Wikipedia entity linking (Section 23.3), user intent classification, and dialogue act classification (to be defined below in Section 24.4.1). The intent classification is used when the user wants to change the topic, and the entity linker specifies what entity is currently being discussed. Dialogue act classification is used to detect when the user is asking a question or giving an affirmative versus negative response. \nBot responses are generated by a series of response generators. Some response generators use fine-tuned neural language models: a GPT-2 (Radford et al., 2019) language model fine-tuned on the EmpatheticDialogues dataset, and a separately fine-tuned GPT-2 language model that learns in fine-tuning to paraphrase content from Wikipedia in response to questions. \nOther response generators are closer to rule-based; the movie or music generators produce scripted conversation about a movie or a musician, such as asking the user’s opinion about a movie or an actor, giving a fun fact, and so on. These generators use regexes and sentiment classifiers to classify user responses and handwritten templates to generate bot utterances. \nEach response generator can specify a priority, and a ranker is used to pass control between response generators. Fig. 24.8 shows part of a conversation. \n24.3 GUS: Simple Frame-based Dialogue Systems \nWe turn now to task-based dialogue, in which a dialogue system has the goal of helping a user solve some task like making an airplane reservation or buying a prodGUS uct. In this section we introduce the very simple GUS architecture for task-based dialogue. Although it is quite old, (it was first introduced in 1977 in the influential GUS system for travel planning (Bobrow et al., 1977)) the GUS architecture",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Chatbots",
        "subsection": "Hybrid architectures",
        "subsubsection": "N/A"
    },
    {
        "content": "Thursday Friday Saturday)) \n24.3.1 Control structure for frame-based dialogue \nThe control architecture for frame-based dialogue systems, used in various forms in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assistant, is designed around the frame. The system’s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). \nTo do this, the system asks questions of the user (using pre-specified question templates associated with each slot of each frame, as shown in Fig. 24.10), filling any slot that the user specifies (we’ll describe how slot-filling works in the next section). If a user’s response fills multiple slots, like the following: \n(24.4) I want a flight from San Francisco to Denver one way leaving after five p.m. on Tuesday. \nthe system fills all the relevant slots, and then continues asking questions to fill the remaining slots, skipping questions associated with filled slots. The GUS architecture also has condition-action rules attached to slots. For example, a rule attached to the DESTINATION slot for the plane booking frame, once the user has specified the destination, might automatically enter that city as the default StayLocation for the related hotel booking frame. Or if the user specifies the DESTINATION DAY for a short trip the system could automatically enter the ARRIVAL DAY. \nMany domains require multiple frames. Besides frames for car or hotel reservations, we might need frames with general route information (for questions like Which airlines fly from Boston to San Francisco?), or information about airfare practices (for questions like Do I have to stay a specific number of days to get a decent airfare?). The system must be able to disambiguate which slot of which frame a given input is supposed to fill and then switch dialogue control to that frame. \nBecause of this need to dynamically switch control, the GUS architecture is a production rule system. Different types of inputs cause different productions to fire, each of which can flexibly fill in different frames. The production rules can then switch control according to factors such as the user’s input and some simple dialogue history like the last question that the system asked. \nOnce the system has enough information it performs the necessary action (like querying a database of flights) and returns the result to the user. \n24.3.2 Determining Domain, Intent, and Slot fillers in GUS \nThe goal of the next component in the frame-based architecture is to extract three things from the user’s utterance. The first task is domain classification: is this user for example talking about airlines, programming an alarm clock, or dealing with their calendar? Of course this 1-of-n classification tasks is unnecessary for single-domain systems that are focused on, say, only calendar management, but multi-domain dialogue systems are the modern standard. The second is user intent determination: what general task or goal is the user trying to accomplish? For example the task could be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment. Finally, we need to do slot filling: extract the particular slots and fillers that the user intends the system to understand from their utterance with respect to their intent. From a user utterance like this one: \nShow me morning flights from Boston to San Francisco on Tuesday a system might want to build a representation like:",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "GUS: Simple Frame-based Dialogue Systems",
        "subsection": "Control structure for frame-based dialogue",
        "subsubsection": "N/A"
    },
    {
        "content": "Thursday Friday Saturday)) \n24.3.1 Control structure for frame-based dialogue \nThe control architecture for frame-based dialogue systems, used in various forms in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assistant, is designed around the frame. The system’s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). \nTo do this, the system asks questions of the user (using pre-specified question templates associated with each slot of each frame, as shown in Fig. 24.10), filling any slot that the user specifies (we’ll describe how slot-filling works in the next section). If a user’s response fills multiple slots, like the following: \n(24.4) I want a flight from San Francisco to Denver one way leaving after five p.m. on Tuesday. \nthe system fills all the relevant slots, and then continues asking questions to fill the remaining slots, skipping questions associated with filled slots. The GUS architecture also has condition-action rules attached to slots. For example, a rule attached to the DESTINATION slot for the plane booking frame, once the user has specified the destination, might automatically enter that city as the default StayLocation for the related hotel booking frame. Or if the user specifies the DESTINATION DAY for a short trip the system could automatically enter the ARRIVAL DAY. \nMany domains require multiple frames. Besides frames for car or hotel reservations, we might need frames with general route information (for questions like Which airlines fly from Boston to San Francisco?), or information about airfare practices (for questions like Do I have to stay a specific number of days to get a decent airfare?). The system must be able to disambiguate which slot of which frame a given input is supposed to fill and then switch dialogue control to that frame. \nBecause of this need to dynamically switch control, the GUS architecture is a production rule system. Different types of inputs cause different productions to fire, each of which can flexibly fill in different frames. The production rules can then switch control according to factors such as the user’s input and some simple dialogue history like the last question that the system asked. \nOnce the system has enough information it performs the necessary action (like querying a database of flights) and returns the result to the user. \n24.3.2 Determining Domain, Intent, and Slot fillers in GUS \nThe goal of the next component in the frame-based architecture is to extract three things from the user’s utterance. The first task is domain classification: is this user for example talking about airlines, programming an alarm clock, or dealing with their calendar? Of course this 1-of-n classification tasks is unnecessary for single-domain systems that are focused on, say, only calendar management, but multi-domain dialogue systems are the modern standard. The second is user intent determination: what general task or goal is the user trying to accomplish? For example the task could be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment. Finally, we need to do slot filling: extract the particular slots and fillers that the user intends the system to understand from their utterance with respect to their intent. From a user utterance like this one: \nShow me morning flights from Boston to San Francisco on Tuesday a system might want to build a representation like: \n\nDOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS ORIGIN-CITY: Boston ORIGIN-DATE: Tuesday ORIGIN-TIME: morning DEST-CITY: San Francisco while an utterance like \nWake me tomorrow at 6 should give an intent like this: \nDOMAIN: ALARM-CLOCK INTENT: SET-ALARM TIME: 2017-07-01 0600-0800 \nsemantic grammar \nThe slot-filling method used in the original GUS system, and still quite common in industrial applications, is to use handwritten rules, often as part of the conditionaction rules attached to slots or concepts. For example we might just define a regular expression for recognizing the SET-ALARM intent: \nwake me (up) | set (the|an) alarm | get me up \nRule-based research systems like the Phoenix system (Ward and Issar, 1994) consist of large hand-designed semantic grammars with thousands of rules. A semantic grammar is a context-free grammar in which the left-hand side of each rule corresponds to the semantic entities being expressed (i.e., the slot names) as in the following fragment: \nSHOW $$ show me i want can i see .   \nDEPART TIME RANGE $$ (after around before) HOUR   \nmorning afternoon evening   \nHOUR $$ one two three four... twelve (AMPM)   \nFLIGHTS $$ (a) flight flights   \nAMPM am pm   \nORIGIN $$ from CITY   \nDESTINATION → to CITY   \nCITY → Boston San Francisco Denver Washington \nSemantic grammars can be parsed by any CFG parsing algorithm (see Chapter 13), resulting in a hierarchical labeling of the input string with semantic node labels, as shown in Fig. 24.11. \nIt remains only to put the fillers into some sort of canonical form, for example by normalizing dates as discussed in Chapter 17. \nMany industrial dialogue systems employ the GUS architecture but use supervised machine learning for slot-filling instead of these kinds of rules; see Section 24.4.2. \ntemplate-based generation \n24.3.3 Other components of frame-based dialogue \nThe ASR (automatic speech recognition) component takes audio input from a phone or other device and outputs a transcribed string of words, as discussed in Chapter 26. The ASR component can also be made dependent on the dialogue state. For example, if the system has just asked the user “What city are you departing from?”, the ASR language model component can be constrained to assign high probability to answers with city names. This can be done by training a language model on answers to this question, or by hand-writing finite-state or context-free grammars for such sentences. A language model that is completely dependent on dialogue state is called a restrictive grammar; usually real systems mix restrictive grammars with more general language models. \nThe natural language generation module of any dialogue system produces the utterances that the system says to the user. Frame-based systems tend to use template-based generation, in which all or most of the words in the sentence to be uttered to the user are prespecified by the dialogue designer. Sentences created by these templates are often called prompts. Templates might be completely fixed (like ‘Hello, how can I help you?’), or can include some variables that are filled in by the generator, as in the following: \nWhat time do you want to leave CITY-ORIG? Will you return to CITY-ORIG from CITY-DEST? \nIt’s possible to do some simple grounding even in templated generation. Consider the unnaturalness of this example from Cohen et al. (2004): \n(24.5) System: Did you want to review some more of your personal profile? Caller: No. System: What’s next? \nWithout an acknowledgment, the caller doesn’t know that the system has understood her ‘No’. The use of Okay below adds grounding to the templated response What’s next?, making (24.6) a much more natural response than (24.5): \n(24.6) System: Did you want to review some more of your personal profile? Caller: No. System: Okay, what’s next? \nThe rule-based GUS approach is very common in industrial applications. As was true with the rule-based approach to information extraction, it has the advantage of high precision, and if the domain is narrow enough and experts are available, can provide sufficient coverage as well. On the other hand, the handwritten rules or grammars can be both expensive and slow to create, and handwritten rules can suffer from recall problems. \n24.4 The Dialogue-State Architecture \nModern research systems for task-based dialogue are based on a more sophisticated version of the frame-based architecture called the dialogue-state or belief-state architecture. Figure 24.12 shows the six components of a typical dialogue-state system. The speech recognition and synthesis components deal with spoken language processing; we’ll return to them in Chapter 26.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "GUS: Simple Frame-based Dialogue Systems",
        "subsection": "Determining Domain, Intent, and Slot fillers in GUS",
        "subsubsection": "N/A"
    },
    {
        "content": "Many industrial dialogue systems employ the GUS architecture but use supervised machine learning for slot-filling instead of these kinds of rules; see Section 24.4.2. \ntemplate-based generation \n24.3.3 Other components of frame-based dialogue \nThe ASR (automatic speech recognition) component takes audio input from a phone or other device and outputs a transcribed string of words, as discussed in Chapter 26. The ASR component can also be made dependent on the dialogue state. For example, if the system has just asked the user “What city are you departing from?”, the ASR language model component can be constrained to assign high probability to answers with city names. This can be done by training a language model on answers to this question, or by hand-writing finite-state or context-free grammars for such sentences. A language model that is completely dependent on dialogue state is called a restrictive grammar; usually real systems mix restrictive grammars with more general language models. \nThe natural language generation module of any dialogue system produces the utterances that the system says to the user. Frame-based systems tend to use template-based generation, in which all or most of the words in the sentence to be uttered to the user are prespecified by the dialogue designer. Sentences created by these templates are often called prompts. Templates might be completely fixed (like ‘Hello, how can I help you?’), or can include some variables that are filled in by the generator, as in the following: \nWhat time do you want to leave CITY-ORIG? Will you return to CITY-ORIG from CITY-DEST? \nIt’s possible to do some simple grounding even in templated generation. Consider the unnaturalness of this example from Cohen et al. (2004): \n(24.5) System: Did you want to review some more of your personal profile? Caller: No. System: What’s next? \nWithout an acknowledgment, the caller doesn’t know that the system has understood her ‘No’. The use of Okay below adds grounding to the templated response What’s next?, making (24.6) a much more natural response than (24.5): \n(24.6) System: Did you want to review some more of your personal profile? Caller: No. System: Okay, what’s next? \nThe rule-based GUS approach is very common in industrial applications. As was true with the rule-based approach to information extraction, it has the advantage of high precision, and if the domain is narrow enough and experts are available, can provide sufficient coverage as well. On the other hand, the handwritten rules or grammars can be both expensive and slow to create, and handwritten rules can suffer from recall problems. \n24.4 The Dialogue-State Architecture \nModern research systems for task-based dialogue are based on a more sophisticated version of the frame-based architecture called the dialogue-state or belief-state architecture. Figure 24.12 shows the six components of a typical dialogue-state system. The speech recognition and synthesis components deal with spoken language processing; we’ll return to them in Chapter 26.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "GUS: Simple Frame-based Dialogue Systems",
        "subsection": "Other components of frame-based dialogue",
        "subsubsection": "N/A"
    },
    {
        "content": "For the rest of this chapter we therefore consider the other four components, which are part of both spoken and textual dialogue systems. These four components are more complex than in the simple GUS systems. For example, like the GUS systems, the dialogue-state architecture has a component for extracting slot fillers from the user’s utterance, but generally using machine learning rather than rules. (This component is sometimes called the NLU or SLU component, for ‘Natural Language Understanding’, or ‘Spoken Language Understanding’, using the word “understanding” loosely.) The dialogue state tracker maintains the current state of the dialogue (which include the user’s most recent dialogue act, plus the entire set of slot-filler constraints the user has expressed so far). The dialogue policy decides what the system should do or say next. The dialogue policy in GUS was simple: ask questions until the frame was full and then report back the results of some database query. But a more sophisticated dialogue policy can help a system decide when to answer the user’s questions, when to instead ask the user a clarification question, when to make a suggestion, and so on. Finally, dialogue state systems have a natural language generation component. In GUS, the sentences that the generator produced were all from pre-written templates. But a more sophisticated generation component can condition on the exact context to produce turns that seem much more natural. \nAs of the time of this writing, most commercial system are architectural hybrids, based on GUS architecture augmented with some dialogue-state components, but there are a wide variety of dialogue-state systems being developed in research labs. \n24.4.1 Dialogue Acts \nDialogue-state systems make use of dialogue acts. Dialogue acts represent the interactive function of the turn or sentence, combining the idea of speech acts and grounding into a single representation. Different types of dialogue systems require labeling different kinds of acts, and so the tagset—defining what a dialogue act is exactly— tends to be designed for particular tasks. \n\nFigure 24.13 shows a tagset for a restaurant recommendation system, and Fig. 24.14 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue acts, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable. \n24.4.2 Slot Filling \nThe task of slot-filling, and the simpler tasks of domain and intent classification, are special cases of the task of supervised semantic parsing discussed in Chapter 16, in which we have a training set that associates each sentence with the correct set of slots, domain, and intent. \nA simple method is to train a sequence model to map from input words representation to slot fillers, domain and intent. For example given the sentence:",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "The Dialogue-State Architecture",
        "subsection": "Dialogue Acts",
        "subsubsection": "N/A"
    },
    {
        "content": "Figure 24.13 shows a tagset for a restaurant recommendation system, and Fig. 24.14 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue acts, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable. \n24.4.2 Slot Filling \nThe task of slot-filling, and the simpler tasks of domain and intent classification, are special cases of the task of supervised semantic parsing discussed in Chapter 16, in which we have a training set that associates each sentence with the correct set of slots, domain, and intent. \nA simple method is to train a sequence model to map from input words representation to slot fillers, domain and intent. For example given the sentence: \nI want to fly to San Francisco on Monday afternoon please we compute a sentence representation, for example by passing the sentence through a contextual embedding network like BERT. The resulting sentence representation can be passed through a feedforward layer and then a simple 1-of-N classifier to determine that the domain is AIRLINE and and the intent is SHOWFLIGHT. \nOur training data is sentences paired with sequences of BIO labels: \nO O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O I want to fly to San Francisco on Monday afternoon please \nRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning (B) and inside (I) of each slot label, and one for tokens outside (O) any slot label. The number of tags is thus $2 n + 1$ tags, where $n$ is the number of slots. \nFig. 24.15 shows the architecture. The input is a series of words $w _ { 1 } . . . w _ { n }$ , which is passed through a contextual embedding model to get contextual word representations. This is followed by a feedforward layer and a softmax at each token position over possible BIO tags, with the output a series of BIO tags $s _ { 1 } . . . s _ { n }$ . We can also combine the domain-classification and intent-extraction tasks with slot-filling simply by adding a domain concatenated with an intent as the desired output for the final EOS token. \nOnce the sequence labeler has tagged the user utterance, a filler string can be extracted for each slot from the tags (e.g., “San Francisco”), and these word strings can then be normalized to the correct form in the ontology (perhaps the airport code ‘SFO’). This normalization can take place by using homonym dictionaries (specifying, for example, that SF, SFO, and San Francisco are the same place). \nIn industrial contexts, machine learning-based systems for slot-filling are often bootstrapped from GUS-style rule-based systems in a semi-supervised learning manner. A rule-based system is first built for the domain, and a test set is carefully labeled. As new user utterances come in, they are paired with the labeling provided by the rule-based system to create training tuples. A classifier can then be trained on these tuples, using the test set to test the performance of the classifier against the rule-based system. Some heuristics can be used to eliminate errorful training tuples, with the goal of increasing precision. As sufficient training samples become available the resulting classifier can often outperform the original rule-based system (Suendermann et al., 2009), although rule-based systems may still remain higherprecision for dealing with complex cases like negation. \n\n24.4.3 Dialogue State Tracking \nThe job of the dialogue-state tracker is to determine both the current state of the frame (the fillers of each slot), as well as the user’s most recent dialogue act. The dialogue-state thus includes more than just the slot-fillers expressed in the current sentence; it includes the entire state of the frame at this point, summarizing all of the user’s constraints. The following example from Mrkˇsic´ et al. (2017) shows the required output of the dialogue state tracker after each turn: \nUser: I’m looking for a cheaper restaurant inform(price $ O =$ cheap)   \nSystem: Sure. What kind - and where?   \nUser: Thai food, somewhere downtown inform(price $circleddash$ cheap, food=Thai, area $ c =$ centre)   \nSystem: The House serves cheap Thai food   \nUser: Where is it? inform(price $ c =$ cheap, food $ c =$ Thai, area $ c =$ centre); request(address)   \nSystem: The House is at 106 Regent Street \nSince dialogue acts place some constraints on the slots and values, the tasks of dialogue-act detection and slot-filling are often performed jointly. Consider the task of determining that \nI’d like Cantonese food near the Mission District has the structure \ninform(food $ c =$ cantonese,area=mission). \nDialogue act interpretation—in this example choosing inform from the set of dialogue acts for this task—is done by supervised classification trained on handlabeled dialog acts, predicting the dialogue act tag based on embeddings representing the current input sentence and the prior dialogue acts. \nThe simplest dialogue state tracker might just take the output of a slot-filling sequence-model (Section 24.4.2) after each sentence. Alternatively, a more complex model can make use of the reading-comprehension architectures from Chapter 23. For example the model of Gao et al. (2019) trains a classifier for each slot to decide whether its value is being changed in the current sentence or should be carried over from the previous sentences. If the slot value is being changed, a span-prediction model is used to predict the start and end of the span with the slot filler. \nA special case: detecting correction acts \nSome dialogue acts are important because of their implications for dialogue control. If a dialogue system misrecognizes or misunderstands an utterance, the user will generally correct the error by repeating or reformulating the utterance. Detecting these user correction acts is therefore quite important. Ironically, it turns out that corrections are actually harder to recognize than normal sentences! In fact, corrections in one early dialogue system (the TOOT system) had double the ASR word error rate of non-corrections (Swerts et al., 2000)! One reason for this is that speakers sometimes use a specific prosodic style for corrections called hyperarticulation, in which the utterance contains exaggerated energy, duration, or F0 contours, such as \nhyperarticulation",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "The Dialogue-State Architecture",
        "subsection": "Slot Filling",
        "subsubsection": "N/A"
    },
    {
        "content": "24.4.3 Dialogue State Tracking \nThe job of the dialogue-state tracker is to determine both the current state of the frame (the fillers of each slot), as well as the user’s most recent dialogue act. The dialogue-state thus includes more than just the slot-fillers expressed in the current sentence; it includes the entire state of the frame at this point, summarizing all of the user’s constraints. The following example from Mrkˇsic´ et al. (2017) shows the required output of the dialogue state tracker after each turn: \nUser: I’m looking for a cheaper restaurant inform(price $ O =$ cheap)   \nSystem: Sure. What kind - and where?   \nUser: Thai food, somewhere downtown inform(price $circleddash$ cheap, food=Thai, area $ c =$ centre)   \nSystem: The House serves cheap Thai food   \nUser: Where is it? inform(price $ c =$ cheap, food $ c =$ Thai, area $ c =$ centre); request(address)   \nSystem: The House is at 106 Regent Street \nSince dialogue acts place some constraints on the slots and values, the tasks of dialogue-act detection and slot-filling are often performed jointly. Consider the task of determining that \nI’d like Cantonese food near the Mission District has the structure \ninform(food $ c =$ cantonese,area=mission). \nDialogue act interpretation—in this example choosing inform from the set of dialogue acts for this task—is done by supervised classification trained on handlabeled dialog acts, predicting the dialogue act tag based on embeddings representing the current input sentence and the prior dialogue acts. \nThe simplest dialogue state tracker might just take the output of a slot-filling sequence-model (Section 24.4.2) after each sentence. Alternatively, a more complex model can make use of the reading-comprehension architectures from Chapter 23. For example the model of Gao et al. (2019) trains a classifier for each slot to decide whether its value is being changed in the current sentence or should be carried over from the previous sentences. If the slot value is being changed, a span-prediction model is used to predict the start and end of the span with the slot filler. \nA special case: detecting correction acts \nSome dialogue acts are important because of their implications for dialogue control. If a dialogue system misrecognizes or misunderstands an utterance, the user will generally correct the error by repeating or reformulating the utterance. Detecting these user correction acts is therefore quite important. Ironically, it turns out that corrections are actually harder to recognize than normal sentences! In fact, corrections in one early dialogue system (the TOOT system) had double the ASR word error rate of non-corrections (Swerts et al., 2000)! One reason for this is that speakers sometimes use a specific prosodic style for corrections called hyperarticulation, in which the utterance contains exaggerated energy, duration, or F0 contours, such as \nhyperarticulation \nI said BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Even when they are not hyperarticulating, users who are frustrated seem to speak in a way that is harder for speech recognizers (Goldberg et al., 2003). \nWhat are the characteristics of these corrections? User corrections tend to be either exact repetitions or repetitions with one or more words omitted, although they may also be paraphrases of the original utterance (Swerts et al., 2000). Detecting these reformulations or correction acts can be part of the general dialogue act detection classifier. Alternatively, because the cues to these acts tend to appear in different ways than for simple acts (like INFORM or request), we can make use of features orthogonal to simple contextual embedding features; some typical features are shown below (Levow 1998, Litman et al. 1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015). \n24.4.4 Dialogue Policy \ndialogue policy \nThe goal of the dialogue policy is to decide what action the system should take next, that is, what dialogue act to generate. \nMore formally, at turn $i$ in the conversation we want to predict which action $A _ { i }$ to take, based on the entire dialogue state. The state could mean the entire sequence of dialogue acts from the system (A) and from the user (U), in which case the task would be to compute: \nWe can simplify this by maintaining as the dialogue state mainly just the set of slot-fillers that the user has expressed, collapsing across the many different conversational paths that could lead to the same set of filled slots. \nSuch a policy might then just condition on the current dialogue state as represented just by the current state of the frame Framei (which slots are filled and with what) and the last turn by the system and user: \nThese probabilities can be estimated by a neural classifier using neural representations of the slot fillers (for example as spans) and the utterances (for example as sentence embeddings computed over contextual embeddings) \nMore sophisticated models train the policy via reinforcement learning. To decide which action to take, a reinforcement learning system gets a reward at the end of the dialogue, and uses that reward to train a policy to take actions. For example in the movie-recommendation dialogue system of Fazel-Zarandi et al. (2017), the action space has only three actions: EXECUTE, CONFIRM, and ELICIT. The EXECUTE sends a query to the database and answers the user’s question, CONFIRM clarifies the intent or slot with the users (e.g., “Do you want movies directed by Christopher Nolan?”) while ELICIT asks the user for missing information (e.g., “Which movie are you talking about?”). The system gets a large positive reward if the dialogue system terminates with the correct slot representation at the end, a large negative reward if the slots are wrong, and a small negative reward for confirmation and elicitation questions to keep the system from re-confirming everything.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "The Dialogue-State Architecture",
        "subsection": "Dialogue State Tracking",
        "subsubsection": "N/A"
    },
    {
        "content": "I said BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Even when they are not hyperarticulating, users who are frustrated seem to speak in a way that is harder for speech recognizers (Goldberg et al., 2003). \nWhat are the characteristics of these corrections? User corrections tend to be either exact repetitions or repetitions with one or more words omitted, although they may also be paraphrases of the original utterance (Swerts et al., 2000). Detecting these reformulations or correction acts can be part of the general dialogue act detection classifier. Alternatively, because the cues to these acts tend to appear in different ways than for simple acts (like INFORM or request), we can make use of features orthogonal to simple contextual embedding features; some typical features are shown below (Levow 1998, Litman et al. 1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015). \n24.4.4 Dialogue Policy \ndialogue policy \nThe goal of the dialogue policy is to decide what action the system should take next, that is, what dialogue act to generate. \nMore formally, at turn $i$ in the conversation we want to predict which action $A _ { i }$ to take, based on the entire dialogue state. The state could mean the entire sequence of dialogue acts from the system (A) and from the user (U), in which case the task would be to compute: \nWe can simplify this by maintaining as the dialogue state mainly just the set of slot-fillers that the user has expressed, collapsing across the many different conversational paths that could lead to the same set of filled slots. \nSuch a policy might then just condition on the current dialogue state as represented just by the current state of the frame Framei (which slots are filled and with what) and the last turn by the system and user: \nThese probabilities can be estimated by a neural classifier using neural representations of the slot fillers (for example as spans) and the utterances (for example as sentence embeddings computed over contextual embeddings) \nMore sophisticated models train the policy via reinforcement learning. To decide which action to take, a reinforcement learning system gets a reward at the end of the dialogue, and uses that reward to train a policy to take actions. For example in the movie-recommendation dialogue system of Fazel-Zarandi et al. (2017), the action space has only three actions: EXECUTE, CONFIRM, and ELICIT. The EXECUTE sends a query to the database and answers the user’s question, CONFIRM clarifies the intent or slot with the users (e.g., “Do you want movies directed by Christopher Nolan?”) while ELICIT asks the user for missing information (e.g., “Which movie are you talking about?”). The system gets a large positive reward if the dialogue system terminates with the correct slot representation at the end, a large negative reward if the slots are wrong, and a small negative reward for confirmation and elicitation questions to keep the system from re-confirming everything. \n\nexplicit confirmation \nPolicy Example: Confirmation and Rejection \nModern dialogue systems often make mistakes. It is therefore important for dialogue systems to make sure that they have achieved the correct interpretation of the user’s input. This is generally done by two methods: confirming understandings with the user and rejecting utterances that the system is likely to have misunderstood. \nAs we saw in the prior section, most systems introduce particular strategies and actions related to confirmation and rejection. When using the explicit confirmation strategy, a system asks the user a direct question to confirm the system’s understanding, like the two examples below in which the system asks a (boldface) yes-no confirmation question: \nimplicit confirmation \nWhen using the implicit confirmation strategy, a system instead can demonstrate its understanding as a grounding strategy, for example repeating back the system’s understanding as part of asking the next question, as in the two examples below: \nrejection \nExplicit and implicit confirmation have complementary strengths. Explicit confirmation makes it easier for users to correct the system’s misrecognitions since a user can just answer “no” to the confirmation question. But explicit confirmation is awkward and increases the length of the conversation (Danieli and Gerbino 1995, Walker et al. 1998a). The explicit confirmation dialogue fragments above sound non-natural and definitely non-human; implicit confirmation is much more conversationally natural. \nConfirmation is just one kind of conversational action by which a system can express lack of understanding. Another option is rejection, in which a system gives the user a prompt like I’m sorry, I didn’t understand that. \nSometimes utterances are rejected multiple times. This might mean that the user is using language that the system is unable to follow. Thus, when an utterance is \nprogressive prompting \nrejected, systems often follow a strategy of progressive prompting or escalating detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example from Cohen et al. (2004): \nSystem: When would you like to leave?   \nCaller: Well, um, I need to be in New York in time for the first World Series game.   \nSystem: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like to leave.   \nCaller: I wanna go on October fifteenth. \nIn this example, instead of just repeating “When would you like to leave?”, the rejection prompt gives the caller more guidance about how to formulate an utterance the system will understand. These you-can-say help messages are important in helping improve systems’ understanding performance (Bohus and Rudnicky, 2005). If the caller’s utterance gets rejected yet again, the prompt can reflect this (“I still didn’t get that”), and give the caller even more guidance. \nAn alternative strategy for error handling is rapid reprompting, in which the system rejects an utterance just by saying “I’m sorry?” or “What was that?” Only if the caller’s utterance is rejected a second time does the system start applying progressive prompting. Cohen et al. (2004) summarize experiments showing that users greatly prefer rapid reprompting as a first-level error prompt. \nIt is common to use rich features other than just the dialogue state representation to make policy decisions. For example, the confidence that the ASR system assigns to an utterance can be used by explicitly confirming low-confidence sentences. Confidence is a metric that the speech recognizer can assign to its transcription of a sentence to indicate how confident it is in that transcription. Confidence is often computed from the acoustic log-likelihood of the utterance (greater probability means higher confidence), but prosodic features can also be used in confidence prediction. For example, utterances with large F0 excursions or longer durations, or those preceded by longer pauses, are likely to be misrecognized (Litman et al., 2000). \nAnother common feature in confirmation is the cost of making an error. For example, explicit confirmation is common before a flight is actually booked or money in an account is moved. Systems might have a four-tiered level of confidence with three thresholds $alpha , beta$ , and $gamma mathrm { : }$ \n$< alpha$ low confidence reject $geq alpha$ above the threshold confirm explicitly $ge beta$ high confidence confirm implictly $geq gamma$ very high confidence don’t confirm at all \ncontent planning sentence realization \n24.4.5 Natural language generation in the dialogue-state model \nOnce a dialogue act has been decided, we need to generate the text of the response to the user. The task of natural language generation (NLG) in the information-state architecture is often modeled in two stages, content planning (what to say), and sentence realization (how to say it). \nHere we’ll assume content planning has been done by the dialogue policy, which has chosen the dialogue act to generate, and chosen some attributes (slots and values) that the planner wants to say to the user (either to give the user the answer, or as part of a confirmation strategy).",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "The Dialogue-State Architecture",
        "subsection": "Dialogue Policy",
        "subsubsection": "N/A"
    },
    {
        "content": "progressive prompting \nrejected, systems often follow a strategy of progressive prompting or escalating detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example from Cohen et al. (2004): \nSystem: When would you like to leave?   \nCaller: Well, um, I need to be in New York in time for the first World Series game.   \nSystem: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like to leave.   \nCaller: I wanna go on October fifteenth. \nIn this example, instead of just repeating “When would you like to leave?”, the rejection prompt gives the caller more guidance about how to formulate an utterance the system will understand. These you-can-say help messages are important in helping improve systems’ understanding performance (Bohus and Rudnicky, 2005). If the caller’s utterance gets rejected yet again, the prompt can reflect this (“I still didn’t get that”), and give the caller even more guidance. \nAn alternative strategy for error handling is rapid reprompting, in which the system rejects an utterance just by saying “I’m sorry?” or “What was that?” Only if the caller’s utterance is rejected a second time does the system start applying progressive prompting. Cohen et al. (2004) summarize experiments showing that users greatly prefer rapid reprompting as a first-level error prompt. \nIt is common to use rich features other than just the dialogue state representation to make policy decisions. For example, the confidence that the ASR system assigns to an utterance can be used by explicitly confirming low-confidence sentences. Confidence is a metric that the speech recognizer can assign to its transcription of a sentence to indicate how confident it is in that transcription. Confidence is often computed from the acoustic log-likelihood of the utterance (greater probability means higher confidence), but prosodic features can also be used in confidence prediction. For example, utterances with large F0 excursions or longer durations, or those preceded by longer pauses, are likely to be misrecognized (Litman et al., 2000). \nAnother common feature in confirmation is the cost of making an error. For example, explicit confirmation is common before a flight is actually booked or money in an account is moved. Systems might have a four-tiered level of confidence with three thresholds $alpha , beta$ , and $gamma mathrm { : }$ \n$< alpha$ low confidence reject $geq alpha$ above the threshold confirm explicitly $ge beta$ high confidence confirm implictly $geq gamma$ very high confidence don’t confirm at all \ncontent planning sentence realization \n24.4.5 Natural language generation in the dialogue-state model \nOnce a dialogue act has been decided, we need to generate the text of the response to the user. The task of natural language generation (NLG) in the information-state architecture is often modeled in two stages, content planning (what to say), and sentence realization (how to say it). \nHere we’ll assume content planning has been done by the dialogue policy, which has chosen the dialogue act to generate, and chosen some attributes (slots and values) that the planner wants to say to the user (either to give the user the answer, or as part of a confirmation strategy). \nFig. 24.16 shows some sample input/outputs for the sentence realization phase. In the first example, the content planner has chosen the dialogue act RECOMMEND and some particular slots (name, neighborhood, cuisine) and their fillers. The goal of the sentence realizer is to generate a sentence like lines 1 or 2 shown in the figure, by training on many such examples of representation/sentence pairs from a large corpus of labeled dialogues. \nTraining data is hard to come by; we are unlikely to see every possible restaurant with every possible attribute in many possible differently worded sentences. Therefore it is common in sentence realization to increase the generality of the training examples by delexicalization. Delexicalization is the process of replacing specific words in the training set that represent slot values with a generic placeholder token representing the slot. Fig. 24.17 shows the result of delexicalizing the training sentences in Fig. 24.16. \nMapping from frames to delexicalized sentences is generally done by encoder decoder models (Wen et al. 2015a, Wen et al. 2015b, Mrksˇic´ et al. 2017, inter alia), trained on large hand-labeled corpora of task-oriented dialogue (Budzianowski et al., 2018). The input to the encoder is a sequence of tokens $x _ { t }$ that represent the dialogue act and its arguments. Thus the dialogue act RECOMMEND and the attribute/- value pairs service:decent, cuisine:null might be represented as a flat sequence of tokens (Nayak et al., 2017), each mapped to a learned embedding $w _ { t }$ , as shown in Fig. 24.18. \nThe encoder reads all the input slot/value representations, and the decoder outputs the following delexicalized English sentence: \nrestaurant name has decent service \nrelexicalize \nWe can then use the input frame from the content planner to relexicalize (fill in the exact restaurant or neighborhood or cuisine) resulting in: \nAu Midi has decent service \nclarification questions \nGenerating Clarification Questions \nIt’s also possible to design NLG algorithms that are specific to a particular dialogue act. For example, consider the task of generating clarification questions, in cases where the speech recognition fails to understand some part of the user’s utterance. While it is possible to use the generic dialogue act REJECT (“Please repeat”, or “I don’t understand what you said”), studies of human conversations show that humans instead use targeted clarification questions that reprise elements of the misunderstanding (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013). \nFor example, in the following hypothetical example the system reprises the words “going” and “on the 5th” to make it clear which aspect of the user’s turn the system needs to be clarified: \nUser: What do you have going to UNKNOWN WORD on the 5th? System: Going where on the 5th? \nTargeted clarification questions can be created by rules (such as replacing “going to UNKNOWN WORD” with “going where”) or by building classifiers to guess which slots might have been misrecognized in the sentence (Chu-Carroll and Carpenter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015). \n24.5 Evaluating Dialogue Systems \nEvaluation is crucial in dialogue system design. Chatbots and task-based systems are generally evaluated differently, since they have different goals; task-based systems have to complete a task like booking a flight; chatbots have a different kind of goal, like being enjoyable to users. \n24.5.1 Evaluating Chatbots \nChatbots are evaluated by humans, who assign a score. This can be the human who talked to the chatbot (participant evaluation) or a third party who reads a transcript of a human/chatbot conversation (observer evaluation). \nIn the participant evaluation of See et al. (2019), the human evaluator chats with the model for six turns and rates the chatbot on 8 dimensions capturing conversational quality: avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness and engagingness. A few examples: \nEngagingness How much did you enjoy talking to this user? Not at all A little Somewhat A lot Avoiding Repetition How repetitive was this user?",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "The Dialogue-State Architecture",
        "subsection": "Natural language generation in the dialogue-state model",
        "subsubsection": "N/A"
    },
    {
        "content": "clarification questions \nGenerating Clarification Questions \nIt’s also possible to design NLG algorithms that are specific to a particular dialogue act. For example, consider the task of generating clarification questions, in cases where the speech recognition fails to understand some part of the user’s utterance. While it is possible to use the generic dialogue act REJECT (“Please repeat”, or “I don’t understand what you said”), studies of human conversations show that humans instead use targeted clarification questions that reprise elements of the misunderstanding (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013). \nFor example, in the following hypothetical example the system reprises the words “going” and “on the 5th” to make it clear which aspect of the user’s turn the system needs to be clarified: \nUser: What do you have going to UNKNOWN WORD on the 5th? System: Going where on the 5th? \nTargeted clarification questions can be created by rules (such as replacing “going to UNKNOWN WORD” with “going where”) or by building classifiers to guess which slots might have been misrecognized in the sentence (Chu-Carroll and Carpenter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015). \n24.5 Evaluating Dialogue Systems \nEvaluation is crucial in dialogue system design. Chatbots and task-based systems are generally evaluated differently, since they have different goals; task-based systems have to complete a task like booking a flight; chatbots have a different kind of goal, like being enjoyable to users. \n24.5.1 Evaluating Chatbots \nChatbots are evaluated by humans, who assign a score. This can be the human who talked to the chatbot (participant evaluation) or a third party who reads a transcript of a human/chatbot conversation (observer evaluation). \nIn the participant evaluation of See et al. (2019), the human evaluator chats with the model for six turns and rates the chatbot on 8 dimensions capturing conversational quality: avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness and engagingness. A few examples: \nEngagingness How much did you enjoy talking to this user? Not at all A little Somewhat A lot Avoiding Repetition How repetitive was this user? \nRepeated themselves over and over Sometimes said the same thing twice Always said something new \nMaking sense How often did this user say something which did NOT make sense? Never made any sense Most responses didn’t make sense Some responses didn’t make sense Everything made perfect sense \nacute-eval \nObserver evaluations use third party annotators to look at the text of a complete conversation. Sometimes we’re interested in having raters assign a score to each system turn; for example (Artstein et al., 2009) have raters mark how coherent each turn is. Often, however, we just want a single high-level score to know if system A is better than system B. The acute-eval metric (Li et al., 2019a) is such an observer evaluation in which annotators look at two separate human-computer conversations (A and B) and choose the one in which the dialogue system participant performed better (interface shown in Fig. 24.19). They answer the following 4 questions (with these particular wordings shown to lead to high agreement): \nEngagingness Who would you prefer to talk to for a long conversation? \nInterestingness If you had to say one of these speakers is interesting and one is boring, who would you say is more interesting? \nHumanness Which speaker sounds more human? \nKnowledgeable If you had to say that one speaker is more knowledgeable and one is more ignorant, who is more knowledgeable? \nHello there, howare you? Hi, I'm pretty good! Just listening to some I am doing great. How are you? aerosmith, they're my fave :) whatre you lam great,I did something crazy for me up to? and colored my hair blue! I have a daughter and a son who also love Italian music, nice! What do you do for blue colored balls. Youshould meet them work? Well that neat,l got a new car my mother gave so maybe l could see them! Ohwow!l'maconstructionworker,l've It is a beautiful city. And, I try to be... Just been doing it since l was 16. cannotaffordabiggerhouseatm. l am sorry to hear that, I feel bad going out of town for spring break now. I'm a construction worker haha, just told you! Whats your favorite type of food? Ok. I going to school in the spring for casino manager Well I turn 29 next week, Iwonder if that Yum! I love mexican food, so good :) do is a good age to apply as one. you have any hobbies? My grandmother just died from lung cancer, We're a bit different- I love watching nascar and ufc.They'reso fun!   \nWho would you prefer to talk to for a long conversation?   \n$0$ I would prefer to talk to Speaker 1 ○ I would prefer to talk to Speaker 2   \nPlease provide a brief justification for your choice (a few words or a sentence) \nAutomatic evaluations are generally not used for chatbots. That’s because computational measures of generation performance like BLEU or ROUGE or embedding dot products between a chatbot’s response and a human response correlate very poorly with human judgments (Liu et al., 2016a). These methods perform poorly because there are so many possible responses to any given turn; simple word-overlap or semantic similarity metrics work best when the space of responses is small and lexically overlapping, which is true of generation tasks like machine translation or possibly summarization, but definitely not dialogue. \nHowever, research continues in ways to do more sophisticated automatic evaluations that go beyond word similarity. One novel paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and Vinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a “Turing-like” evaluator classifier to distinguish between human-generated responses and machine-generated responses. The more successful a response generation system is at fooling this evaluator, the better the system. \n24.5.2 Evaluating Task-Based Dialogue \nFor task-based dialogue, if the task is unambiguous, we can simply measure absolute task success (did the system book the right plane flight, or put the right event on the calendar). \nTo get a more fine-grained idea of user happiness, we can compute a user satisfaction rating, having users interact with a dialogue system to perform a task and then having them complete a questionnaire. For example, Fig. 24.20 shows sample multiple-choice questions (Walker et al., 2001); responses are mapped into the range of 1 to 5, and then averaged over all questions to get a total user satisfaction rating. \nIt is often economically infeasible to run complete user satisfaction studies after every change in a system. For this reason, it is useful to have performance evaluation heuristics that correlate well with human satisfaction. A number of such factors and heuristics have been studied, often grouped into two kinds of criteria: how well the system allows users to accomplish their goals (maximizing task success) with the fewest problems (minimizing costs): \nTask completion success: Task success can be measured by evaluating the correctness of the total solution. For a frame-based architecture, this might be slot error rate, the percentage of slots that were filled with the correct values: \n# of inserted/deleted/subsituted slots Slot Error Rate for a Sentence $ b =$ # of total reference slots for sentence For example consider a system given this sentence: (24.10) Make an appointment with Chris at 10:30 in Gates 104",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Evaluating Dialogue Systems",
        "subsection": "Evaluating Chatbots",
        "subsubsection": "N/A"
    },
    {
        "content": "Automatic evaluations are generally not used for chatbots. That’s because computational measures of generation performance like BLEU or ROUGE or embedding dot products between a chatbot’s response and a human response correlate very poorly with human judgments (Liu et al., 2016a). These methods perform poorly because there are so many possible responses to any given turn; simple word-overlap or semantic similarity metrics work best when the space of responses is small and lexically overlapping, which is true of generation tasks like machine translation or possibly summarization, but definitely not dialogue. \nHowever, research continues in ways to do more sophisticated automatic evaluations that go beyond word similarity. One novel paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and Vinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a “Turing-like” evaluator classifier to distinguish between human-generated responses and machine-generated responses. The more successful a response generation system is at fooling this evaluator, the better the system. \n24.5.2 Evaluating Task-Based Dialogue \nFor task-based dialogue, if the task is unambiguous, we can simply measure absolute task success (did the system book the right plane flight, or put the right event on the calendar). \nTo get a more fine-grained idea of user happiness, we can compute a user satisfaction rating, having users interact with a dialogue system to perform a task and then having them complete a questionnaire. For example, Fig. 24.20 shows sample multiple-choice questions (Walker et al., 2001); responses are mapped into the range of 1 to 5, and then averaged over all questions to get a total user satisfaction rating. \nIt is often economically infeasible to run complete user satisfaction studies after every change in a system. For this reason, it is useful to have performance evaluation heuristics that correlate well with human satisfaction. A number of such factors and heuristics have been studied, often grouped into two kinds of criteria: how well the system allows users to accomplish their goals (maximizing task success) with the fewest problems (minimizing costs): \nTask completion success: Task success can be measured by evaluating the correctness of the total solution. For a frame-based architecture, this might be slot error rate, the percentage of slots that were filled with the correct values: \n# of inserted/deleted/subsituted slots Slot Error Rate for a Sentence $ b =$ # of total reference slots for sentence For example consider a system given this sentence: (24.10) Make an appointment with Chris at 10:30 in Gates 104 \nwhich extracted the following candidate slot structure: \nHere the slot error rate is 1/3, since the TIME is wrong. Instead of error rate, slot precision, recall, and F-score can also be used. Slot error rate is also sometimes called concept error rate. \nInterestingly, sometimes the user’s perception of whether they completed the task is a better predictor of user satisfaction than the actual task completion success. (Walker et al., 2001). \nA perhaps more important, although less fine-grained, measure of success is an extrinsic metric like task error rate. In this case, the task error rate would quantify how often the correct meeting was added to the calendar at the end of the interaction. \nEfficiency cost: Efficiency costs are measures of the system’s efficiency at helping users. This can be measured by the total elapsed time for the dialogue in seconds, the number of total turns or of system turns, or the total number of queries (Polifroni et al., 1992). Other metrics include the number of system non-responses and the “turn correction ratio”: the number of system or user turns that were used solely to correct errors divided by the total number of turns (Danieli and Gerbino 1995, Hirschman and Pao 1993). \nQuality cost: Quality cost measures other aspects of the interactions that affect user’s perception of the system. One such measure is the number of times the ASR system failed to return any sentence, or the number of ASR rejection prompts. Similar metrics include the number of times the user had to barge in (interrupt the system), or the number of time-out prompts played when the user didn’t respond quickly enough. Other quality metrics focus on how well the system understood and responded to the user. The most important is the slot error rate described above, but other components include the inappropriateness (verbose or ambiguous) of the system’s questions, answers, and error messages or the correctness of each question, answer, or error message (Zue et al. 1989, Polifroni et al. 1992). \n24.6 Dialogue System Design \nvoice user interface \nThe user plays a more important role in dialogue systems than in most other areas of speech and language processing, and thus the study of dialogue systems is closely linked with the field of Human-Computer Interaction (HCI). The design of dialogue strategies, prompts, and error messages, is often called voice user interface design, and generally follows user-centered design principles (Gould and Lewis, 1985): \n1. Study the user and task: Understand the potential users and the nature of the task by interviews with users, investigation of similar systems, and study of related human-human dialogues. \nWizard-of-Oz system \n2. Build simulations and prototypes: A crucial tool in building dialogue systems is the Wizard-of-Oz system. In wizard systems, the users interact with what they think is a software agent but is in fact a human “wizard” disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the children’s book The Wizard of $O z$ (Baum, 1900), in which the wizard turned out to be just a simulation controlled by a man behind a curtain or screen.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Evaluating Dialogue Systems",
        "subsection": "Evaluating Task-Based Dialogue",
        "subsubsection": "N/A"
    },
    {
        "content": "A Wizard-of-Oz system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, has a graphical interface to a database to run sample queries based on the user utterance, and then has a way to output sentences, either by typing them or by some combination of selecting from a menu and typing. \nThe results of a Wizard-of-Oz system can also be used as training data to train a pilot dialogue system. While Wizard-of-Oz systems are very commonly used, they are not a perfect simulation; it is difficult for the wizard to exactly simulate the errors, limitations, or time constraints of a real system; results of wizard studies are thus somewhat idealized, but still can provide a useful first idea of the domain issues. \n\n3. Iteratively test the design on users: An iterative design cycle with embedded user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). For example in a well-known incident in dialogue design history, an early dialogue system required the user to press a key to interrupt the system (Stifelman et al., 1993). But user testing showed users barged in, which led to a redesign of the system to recognize overlapped speech. The iterative method is also important for designing prompts that cause the user to respond in normative ways. It’s also important to incorporate value sensitive design, in which we carefully consider during the design process the benefits, harms and possible stakeholders of the resulting system (Friedman et al. 2017, Bender and Friedman 2018). \nThere are a number of good books on conversational interface design (Cohen et al. 2004, Harris 2005, Pearl 2017). \n24.6.1 Ethical Issues in Dialogue System Design \nEthical issues have been key to how we think about designing artificial agents since well before we had conversational agents. Mary Shelley (depicted below) centered her novel Frankenstein around the problem of creating agents without considering ethical and humanistic concerns. One issue is the safety of users. If users seek information from conversational agents in safety-critical situations like asking medical advice, or in emergency situations, or when indicating the intentions of self-harm, incorrect advice can be dangerous and even life-threatening. For example (Bickmore et al., 2018) gave participants medical problems to pose to three commercial dialogue systems (Siri, Alexa, Google Assistant) and asked them to determine an action to take based on the system responses; many of the proposed actions, if actually taken, would have led to harm or death. \n\nA system can also harm users by verbally attacking them, or creating representational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes that demean particular groups of people. Both abuse and stereotypes can cause psychological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken offline 16 hours after it went live, when it began posting messages with racial slurs, conspiracy theories, and personal attacks on its users. Tay had learned these biases and actions from its training data, including from users who seemed to be purposely teaching the system to repeat this kind of language (Neff and Nagy 2016). Henderson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and found toxic and abusive language, especially in social media corpora like Twitter and Reddit, and indeed such language then appears in the text generated by language models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which can even amplify the bias from the training data (Dinan et al., 2020). Liu et al. (2020) developed another method for investigating bias, testing how neural dialogue systems responded to pairs of simulated user turns that are identical except for mentioning different genders or race. They found, for example, that simple changes like using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more offensively and with more negative sentiment. \nAnother important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. The ubiquity of in-home dialogue agents means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. \nFinally, chatbots raise important issues of gender equality in addition to textual bias. Current chatbots are overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing language, most commercial chatbots evade or give positive responses rather than responding in clear negative ways (Fessler, 2017). \nThese ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Bender and Friedman 2018) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. And because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. \n24.7 Summary \nConversational agents are crucial speech and language processing applications that are already widely used commercially. \n• In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Dialogue System Design",
        "subsection": "Ethical Issues in Dialogue System Design",
        "subsubsection": "N/A"
    },
    {
        "content": "A system can also harm users by verbally attacking them, or creating representational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes that demean particular groups of people. Both abuse and stereotypes can cause psychological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken offline 16 hours after it went live, when it began posting messages with racial slurs, conspiracy theories, and personal attacks on its users. Tay had learned these biases and actions from its training data, including from users who seemed to be purposely teaching the system to repeat this kind of language (Neff and Nagy 2016). Henderson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and found toxic and abusive language, especially in social media corpora like Twitter and Reddit, and indeed such language then appears in the text generated by language models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which can even amplify the bias from the training data (Dinan et al., 2020). Liu et al. (2020) developed another method for investigating bias, testing how neural dialogue systems responded to pairs of simulated user turns that are identical except for mentioning different genders or race. They found, for example, that simple changes like using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more offensively and with more negative sentiment. \nAnother important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. The ubiquity of in-home dialogue agents means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. \nFinally, chatbots raise important issues of gender equality in addition to textual bias. Current chatbots are overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing language, most commercial chatbots evade or give positive responses rather than responding in clear negative ways (Fessler, 2017). \nThese ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Bender and Friedman 2018) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. And because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. \n24.7 Summary \nConversational agents are crucial speech and language processing applications that are already widely used commercially. \n• In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A system can also harm users by verbally attacking them, or creating representational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes that demean particular groups of people. Both abuse and stereotypes can cause psychological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken offline 16 hours after it went live, when it began posting messages with racial slurs, conspiracy theories, and personal attacks on its users. Tay had learned these biases and actions from its training data, including from users who seemed to be purposely teaching the system to repeat this kind of language (Neff and Nagy 2016). Henderson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and found toxic and abusive language, especially in social media corpora like Twitter and Reddit, and indeed such language then appears in the text generated by language models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which can even amplify the bias from the training data (Dinan et al., 2020). Liu et al. (2020) developed another method for investigating bias, testing how neural dialogue systems responded to pairs of simulated user turns that are identical except for mentioning different genders or race. They found, for example, that simple changes like using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more offensively and with more negative sentiment. \nAnother important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. The ubiquity of in-home dialogue agents means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. \nFinally, chatbots raise important issues of gender equality in addition to textual bias. Current chatbots are overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing language, most commercial chatbots evade or give positive responses rather than responding in clear negative ways (Fessler, 2017). \nThese ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Bender and Friedman 2018) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. And because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. \n24.7 Summary \nConversational agents are crucial speech and language processing applications that are already widely used commercially. \n• In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "A system can also harm users by verbally attacking them, or creating representational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes that demean particular groups of people. Both abuse and stereotypes can cause psychological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken offline 16 hours after it went live, when it began posting messages with racial slurs, conspiracy theories, and personal attacks on its users. Tay had learned these biases and actions from its training data, including from users who seemed to be purposely teaching the system to repeat this kind of language (Neff and Nagy 2016). Henderson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and found toxic and abusive language, especially in social media corpora like Twitter and Reddit, and indeed such language then appears in the text generated by language models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which can even amplify the bias from the training data (Dinan et al., 2020). Liu et al. (2020) developed another method for investigating bias, testing how neural dialogue systems responded to pairs of simulated user turns that are identical except for mentioning different genders or race. They found, for example, that simple changes like using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more offensively and with more negative sentiment. \nAnother important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. The ubiquity of in-home dialogue agents means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. \nFinally, chatbots raise important issues of gender equality in addition to textual bias. Current chatbots are overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing language, most commercial chatbots evade or give positive responses rather than responding in clear negative ways (Fessler, 2017). \nThese ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Bender and Friedman 2018) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. And because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. \n24.7 Summary \nConversational agents are crucial speech and language processing applications that are already widely used commercially. \n• In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. \n\n• Chatbots are conversational agents designed to mimic the appearance of informal human conversation. Rule-based chatbots like ELIZA and its modern descendants use rules to map user sentences into system responses. Corpusbased chatbots mine logs of human conversation to learn to automatically map user sentences into system responses.   \n• For task-based dialogue, most commercial dialogue systems use the GUS or frame-based architecture, in which the designer specifies frames consisting of slots that the system must fill by asking the user.   \n• The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user’s dialogue acts, policies for generating its own dialogue acts, and a natural language component.   \n• Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. \nBibliographical and Historical Notes \nThe earliest conversational systems were chatbots like ELIZA (Weizenbaum, 1966) and PARRY (Colby et al., 1971). ELIZA had a widespread influence on popular perceptions of artificial intelligence, and brought up some of the first ethical questions in natural language processing —such as the issues of privacy we discussed above as well the role of algorithms in decision-making— leading its creator Joseph Weizenbaum to fight for social responsibility in AI and computer science in general. \nAnother early system, the GUS system (Bobrow et al., 1977) had by the late 1970s established the main frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. \nIn the 1990s, stochastic models that had first been applied to natural language processing began to be applied to dialogue slot filling (Miller et al. 1994, Pieraccini et al. 1991). \nBy around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple’s SIRI (Bellegarda, 2013) and other digital assistants. \nThe rise of the web and online chatbots brought new interest in chatbots and gave rise to corpus-based chatbot architectures around the turn of the century, first using information retrieval models and then in the 2010s, after the rise of deep learning, with sequence-to-sequence models. \nThe idea that utterances in a conversation are a kind of action being performed by the speaker was due originally to the philosopher Wittgenstein (1953) but worked out more fully by Austin (1962) and his student John Searle. Various sets of speech acts have been defined over the years, and a rich linguistic and philosophical literature developed, especially focused on explaining the use of indirect speech acts. \nThe idea of dialogue acts draws also from a number of other sources, including the ideas of adjacency pairs, pre-sequences, and other aspects of the international properties of human conversation developed in the field of conversation analysis (see Levinson (1983) for an introduction to the field). \n\nThis idea that acts set up strong local dialogue expectations was also prefigured by Firth (1935, p. 70), in a famous quotation: \nMost of the give-and-take of conversation in our everyday life is stereotyped and very narrowly conditioned by our particular type of culture. It is a sort of roughly prescribed social ritual, in which you generally say what the other fellow expects you, one way or the other, to say. \nAnother important research thread modeled dialogue as a kind of collaborative behavior, including the ideas of common ground (Clark and Marshall, 1981), reference as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention (Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980). \nThe dialogue-state model was also strongly informed by analytic work on the linguistic properties of dialogue acts and on methods for their detection (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012). \nTwo important lines of research that we were unable to cover in the chapter focused on the computational properties of conversational structure. One line, first suggested by Bruce (1975), suggested that since speech acts are actions, they should be planned like other actions, and drew on the AI planning literature (Fikes and Nilsson, 1971). An agent seeking to find out some information can come up with the plan of asking the interlocutor for the information. An agent hearing an utterance can interpret a speech act by running the planner “in reverse”, using inference rules to infer from what the interlocutor said what the plan might have been. Plan-based models of dialogue are referred to as BDI models because such planners model the beliefs, desires, and intentions (BDI) of the agent and interlocutor. BDI models of dialogue were first introduced by Allen, Cohen, Perrault, and their colleagues in a number of influential papers showing how speech acts could be generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980, Allen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based models of understanding as part of the task of interpreting stories. \nAnother influential line of research focused on modeling the hierarchical structure of dialogue. Grosz’s pioneering 1977b dissertation first showed that “taskoriented dialogues have a structure that closely parallels the structure of the task being performed” (p. 27), leading to her work with Sidner and others showing how to use similar notions of intention and plans to model discourse structure and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional structure in dialogue. \nThe idea of applying reinforcement learning to dialogue first came out of AT&T and Bell Laboratories around the turn of the century with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-filling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two virtual agents, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016c), and for task-oriented dialogue (Williams et al., 2017). \nDialogue state tracking quickly became an important problem for task-oriented dialogue, and there is an influential by an annual evaluation of state-tracking algorithms (Williams et al., 2016b). Other important dialogue areas include the study of affect in dialogue (Rashkin et al. 2019, Lin et al. 2019). See Gao et al. (2019) for a survey of modern dialogue system architectures. \n\nExercises \ndispreferred response \n24.1 Write a finite-state automaton for a dialogue manager for checking your bank balance and withdrawing money at an automated teller machine. \n24.2 A dispreferred response is a response that has the potential to make a person uncomfortable or embarrassed in the conversational context; the most common example dispreferred responses is turning down a request. People signal their discomfort with having to say no with surface cues (like the word well), or via significant silence. Try to notice the next time you or someone else utters a dispreferred response, and write down the utterance. What are some other cues in the response that a system might use to detect a dispreferred response? Consider non-verbal cues like eye gaze and body gestures. \n24.3 When asked a question to which they aren’t sure they know the answer, people display their lack of confidence by cues that resemble other dispreferred responses. Try to notice some unsure answers to questions. What are some of the cues? If you have trouble doing this, read Smith and Clark (1993) and listen specifically for the cues they mention. \n24.4 Implement a small air-travel help system based on text input. Your system should get constraints from users about a particular flight that they want to take, expressed in natural language, and display possible flights on a screen. Make simplifying assumptions. You may build in a simple flight database or you may use a flight information system on the Web as your backend. \n25 Phonetics \nY \nThe characters that make up the texts we’ve been discussing in this book aren’t just random symbols. They are also an amazing scientific invention: a theoretical model of the elements that make up human speech. \nThe earliest writing systems we know of (Sumerian, Chinese, Mayan) were mainly logographic: one symbol representing a whole word. But from the earliest stages we can find, some symbols were also used to represent the sounds that made up words. The cuneiform sign to the right pronounced ba and meaning “ration” in Sumerian could also function purely as the sound /ba/. The earliest Chinese characters we have, carved into bones for divination, similarly contain phonetic elements. Purely sound-based writing systems, whether syllabic (like Japanese hiragana), alphabetic (like the Roman alphabet), or consonantal (like Semitic writing systems), trace back to these early logo-syllabic systems, often as two cultures came together. Thus, the Arabic, Aramaic, Hebrew, Greek, and Roman systems all derive from a West Semitic script that is presumed to have been modified by Western Semitic mercenaries from a cursive form of Egyptian hieroglyphs. The Japanese syllabaries were modified from a cursive form of Chinese phonetic characters, which themselves were used in Chinese to phonetically represent the Sanskrit in the Buddhist scriptures that came to China in the Tang dynasty. \n\nphonetics \nThis implicit idea that the spoken word is composed of smaller units of speech underlies algorithms for both speech recognition (transcribing waveforms into text) and text-to-speech (converting text into waveforms). In this chapter we give a computational perspective on phonetics, the study of the speech sounds used in the languages of the world, how they are produced in the human vocal tract, how they are realized acoustically, and how they can be digitized and processed. \n25.1 Speech Sounds and Phonetic Transcription \nphone \nA letter like ‘p’ or ‘a’ is already a useful model of the sounds of human speech, and indeed we’ll see in Chapter 26 how to map between letters and waveforms. Nonetheless, it is helpful to represent sounds slightly more abstractly. We’ll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapated from the Roman alphabet. \nThe standard phonetic representation for transcribing the world’s languages is the International Phonetic Alphabet (IPA), an evolving standard first developed in 1888, But in this chapter we’ll instead represent phones with the ARPAbet (Shoup, 1980), a simple phonetic alphabet (Fig. 25.1) that conveniently uses ASCII symbols to represent an American-English subset of the IPA. \nMany of the IPA and ARPAbet symbols are equivalent to familiar Roman letters. So, for example, the ARPAbet phone [p] represents the consonant sound at the beginning of platypus, puma, and plantain, the middle of leopard, or the end of antelope. In general, however, the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. The English letter $c$ corresponds to phone [k] in cougar [k uw g axr], but phone [s] in cell [s eh l]. Besides appearing as $c$ and $k$ , the phone [k] can appear as part of $x$ (fox [f aa k s]), as $c k$ (jackal [jh ae k el]) and as $c c$ (raccoon [r ae k uw n]). Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English.",
        "chapter": "Chatbots & Dialogue Systems",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "25 Phonetics \nY \nThe characters that make up the texts we’ve been discussing in this book aren’t just random symbols. They are also an amazing scientific invention: a theoretical model of the elements that make up human speech. \nThe earliest writing systems we know of (Sumerian, Chinese, Mayan) were mainly logographic: one symbol representing a whole word. But from the earliest stages we can find, some symbols were also used to represent the sounds that made up words. The cuneiform sign to the right pronounced ba and meaning “ration” in Sumerian could also function purely as the sound /ba/. The earliest Chinese characters we have, carved into bones for divination, similarly contain phonetic elements. Purely sound-based writing systems, whether syllabic (like Japanese hiragana), alphabetic (like the Roman alphabet), or consonantal (like Semitic writing systems), trace back to these early logo-syllabic systems, often as two cultures came together. Thus, the Arabic, Aramaic, Hebrew, Greek, and Roman systems all derive from a West Semitic script that is presumed to have been modified by Western Semitic mercenaries from a cursive form of Egyptian hieroglyphs. The Japanese syllabaries were modified from a cursive form of Chinese phonetic characters, which themselves were used in Chinese to phonetically represent the Sanskrit in the Buddhist scriptures that came to China in the Tang dynasty. \n\nphonetics \nThis implicit idea that the spoken word is composed of smaller units of speech underlies algorithms for both speech recognition (transcribing waveforms into text) and text-to-speech (converting text into waveforms). In this chapter we give a computational perspective on phonetics, the study of the speech sounds used in the languages of the world, how they are produced in the human vocal tract, how they are realized acoustically, and how they can be digitized and processed. \n25.1 Speech Sounds and Phonetic Transcription \nphone \nA letter like ‘p’ or ‘a’ is already a useful model of the sounds of human speech, and indeed we’ll see in Chapter 26 how to map between letters and waveforms. Nonetheless, it is helpful to represent sounds slightly more abstractly. We’ll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapated from the Roman alphabet. \nThe standard phonetic representation for transcribing the world’s languages is the International Phonetic Alphabet (IPA), an evolving standard first developed in 1888, But in this chapter we’ll instead represent phones with the ARPAbet (Shoup, 1980), a simple phonetic alphabet (Fig. 25.1) that conveniently uses ASCII symbols to represent an American-English subset of the IPA. \nMany of the IPA and ARPAbet symbols are equivalent to familiar Roman letters. So, for example, the ARPAbet phone [p] represents the consonant sound at the beginning of platypus, puma, and plantain, the middle of leopard, or the end of antelope. In general, however, the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. The English letter $c$ corresponds to phone [k] in cougar [k uw g axr], but phone [s] in cell [s eh l]. Besides appearing as $c$ and $k$ , the phone [k] can appear as part of $x$ (fox [f aa k s]), as $c k$ (jackal [jh ae k el]) and as $c c$ (raccoon [r ae k uw n]). Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. \n\n25.2 Articulatory Phonetics \narticulatory phonetics \nArticulatory phonetics is the study of how these phones are produced as the various organs in the mouth, throat, and nose modify the airflow from the lungs. \nThe Vocal Organs \nFigure 25.2 shows the organs of speech. Sound is produced by the rapid movement of air. Humans produce most sounds in spoken languages by expelling air from the lungs through the windpipe (technically, the trachea) and then out the mouth or nose. As it passes through the trachea, the air passes through the larynx, commonly known as the Adam’s apple or voice box. The larynx contains two small folds of",
        "chapter": "Phonetics",
        "section": "Speech Sounds and Phonetic Transcription",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "25.2 Articulatory Phonetics \narticulatory phonetics \nArticulatory phonetics is the study of how these phones are produced as the various organs in the mouth, throat, and nose modify the airflow from the lungs. \nThe Vocal Organs \nFigure 25.2 shows the organs of speech. Sound is produced by the rapid movement of air. Humans produce most sounds in spoken languages by expelling air from the lungs through the windpipe (technically, the trachea) and then out the mouth or nose. As it passes through the trachea, the air passes through the larynx, commonly known as the Adam’s apple or voice box. The larynx contains two small folds of \nglottis \nvoiced sound unvoiced sound \nmuscle, the vocal folds (often referred to non-technically as the vocal cords), which can be moved together or apart. The space between these two folds is called the glottis. If the folds are close together (but not tightly closed), they will vibrate as air passes through them; if they are far apart, they won’t vibrate. Sounds made with the vocal folds together and vibrating are called voiced; sounds made without this vocal cord vibration are called unvoiced or voiceless. Voiced sounds include [b], [d], [g], [v], [z], and all the English vowels, among others. Unvoiced sounds include [p], [t], [k], [f], [s], and others. \nnasal \nconsonant vowel \nThe area above the trachea is called the vocal tract; it consists of the oral tract and the nasal tract. After the air leaves the trachea, it can exit the body through the mouth or the nose. Most sounds are made by air passing through the mouth. Sounds made by air passing through the nose are called nasal sounds; nasal sounds (like English [m], [n], and [ng]) use both the oral and nasal tracts as resonating cavities. \nPhones are divided into two main classes: consonants and vowels. Both kinds of sounds are formed by the motion of air through the mouth, throat or nose. Consonants are made by restriction or blocking of the airflow in some way, and can be voiced or unvoiced. Vowels have less obstruction, are usually voiced, and are generally louder and longer-lasting than consonants. The technical use of these terms is much like the common usage; [p], [b], [t], [d], [k], [g], [f], [v], [s], [z], [r], [l], etc., are consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivowels (such as [y] and [w]) have some of the properties of both; they are voiced like vowels, but they are short and less syllabic like consonants. \nConsonants: Place of Articulation \nplace of articulation \nBecause consonants are made by restricting airflow, we can group them into classes by their point of maximum restriction, their place of articulation (Fig. 25.3). \nlabial \nLabial: Consonants whose main restriction is formed by the two lips coming together have a bilabial place of articulation. In English these include [p] as in possum, [b] as in bear, and [m] as in marmot. The English labiodental consonants [v] and [f] are made by pressing the bottom lip against the upper row of teeth and letting the air flow through the space in the upper teeth. \ndental \nDental: Sounds that are made by placing the tongue against the teeth are dentals. The main dentals in English are the [th] of thing and the [dh] of though, which are made by placing the tongue behind the teeth with the tip slightly between the teeth. \nalveolar \nAlveolar: The alveolar ridge is the portion of the roof of the mouth just behind the upper teeth. Most speakers of American English make the phones [s], [z], [t], and [d] by placing the tip of the tongue against the alveolar ridge. The word coronal is often used to refer to both dental and alveolar. \npalatal palate \nPalatal: The roof of the mouth (the palate) rises sharply from the back of the alveolar ridge. The palato-alveolar sounds [sh] (shrimp), [ch] (china), [zh] (Asian), and [jh] (jar) are made with the blade of the tongue against the rising back of the alveolar ridge. The palatal sound [y] of yak is made by placing the front of the tongue up close to the palate. \nvelar \nVelar: The velum, or soft palate, is a movable muscular flap at the very back of the roof of the mouth. The sounds [k] (cuckoo), [g] (goose), and [N] (kingfisher) are made by pressing the back of the tongue up against the velum. \nglottal \nGlottal: The glottal stop [q] is made by closing the glottis (by bringing the vocal folds together). \nmanner of articulation \nConsonants: Manner of Articulation \nConsonants are also distinguished by how the restriction in airflow is made, for example, by a complete stoppage of air or by a partial blockage. This feature is called the manner of articulation of a consonant. The combination of place and manner of articulation is usually sufficient to uniquely identify a consonant. Following are the major manners of articulation for English consonants: \nA stop is a consonant in which airflow is completely blocked for a short time. This blockage is followed by an explosive sound as the air is released. The period of blockage is called the closure, and the explosion is called the release. English \nhas voiced stops like [b], [d], and [g] as well as unvoiced stops like [p], [t], and [k].   \nStops are also called plosives. \nnasal \nThe nasal sounds [n], [m], and [ng] are made by lowering the velum and allowing air to pass into the nasal cavity. \nfricatives \nIn fricatives, airflow is constricted but not cut off completely. The turbulent airflow that results from the constriction produces a characteristic “hissing” sound. The English labiodental fricatives [f] and [v] are produced by pressing the lower lip against the upper teeth, allowing a restricted airflow between the upper teeth. The dental fricatives [th] and [dh] allow air to flow around the tongue between the teeth. The alveolar fricatives [s] and [z] are produced with the tongue against the alveolar ridge, forcing air over the edge of the teeth. In the palato-alveolar fricatives [sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a groove formed in the tongue. The higher-pitched fricatives (in English [s], [z], [sh] and [zh]) are called sibilants. Stops that are followed immediately by fricatives are called affricates; these include English [ch] (chicken) and [jh] (giraffe). \nsibilants \napproximant \nIn approximants, the two articulators are close together but not close enough to cause turbulent airflow. In English [y] (yellow), the tongue moves close to the roof of the mouth but not close enough to cause the turbulence that would characterize a fricative. In English [w] $( underline { { w o o d } } )$ , the back of the tongue comes close to the velum. American [r] can be formed in at least two ways; with just the tip of the tongue extended and close to the palate or with the whole tongue bunched up near the palate. [l] is formed with the tip of the tongue up against the alveolar ridge or the teeth, with one or both sides of the tongue lowered to allow air to flow over it. [l] is called a lateral sound because of the drop in the sides of the tongue. \nA tap or flap [dx] is a quick motion of the tongue against the alveolar ridge. The consonant in the middle of the word lotus ([l ow dx ax s]) is a tap in most dialects of American English; speakers of many U.K. dialects would use a [t] instead. \nVowels \nLike consonants, vowels can be characterized by the position of the articulators as they are made. The three most relevant parameters for vowels are what is called vowel height, which correlates roughly with the height of the highest part of the tongue, vowel frontness or backness, indicating whether this high point is toward the front or back of the oral tract and whether the shape of the lips is rounded or not. Figure 25.4 shows the position of the tongue for different vowels. \nIn the vowel [iy], for example, the highest point of the tongue is toward the front of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is located toward the back of the mouth. Vowels in which the tongue is raised toward the front are called front vowels; those in which the tongue is raised toward the \nback vowel high vowel \n\nback are called back vowels. Note that while both [ih] and [eh] are front vowels, the tongue is higher for [ih] than for [eh]. Vowels in which the highest point of the tongue is comparatively high are called high vowels; vowels with mid or low values of maximum tongue height are called mid vowels or low vowels, respectively. \ndiphthong \nFigure 25.5 shows a schematic characterization of the height of different vowels. It is schematic because the abstract property height correlates only roughly with actual tongue positions; it is, in fact, a more accurate reflection of acoustic facts. Note that the chart has two kinds of vowels: those in which tongue height is represented as a point and those in which it is represented as a path. A vowel in which the tongue position changes markedly during the production of the vowel is a diphthong. English is particularly rich in diphthongs. \nrounded vowel\nThe second important articulatory dimension for vowels is the shape of the lips. Certain vowels are pronounced with the lips rounded (the same lip shape used for whistling). These rounded vowels include [uw], [ao], and [ow]. \nsyllable \nSyllables \nConsonants and vowels combine to make a syllable. A syllable is a vowel-like (or sonorant) sound together with some of the surrounding consonants that are most closely associated with it. The word dog has one syllable, [d aa g] (in our dialect); the word catnip has two syllables, [k ae t] and [n ih p]. We call the vowel at the core of a syllable the nucleus. Initial consonants, if any, are called the onset. Onsets with more than one consonant (as in strike [s t r ay k]), are called complex onsets. The coda is the optional consonant or sequence of consonants following the nucleus. Thus [d] is the onset of dog, and [g] is the coda. The rime, or rhyme, is the nucleus plus coda. Figure 25.6 shows some sample syllable structures. \nnucleus onset coda rime syllabification phonotactics \n\nThe task of automatically breaking up a word into syllables is called syllabification. Syllable structure is also closely related to the phonotactics of a language. The term phonotactics means the constraints on which phones can follow each other in a language. For example, English has strong constraints on what kinds of consonants can appear together in an onset; the sequence [zdr], for example, cannot be a legal English syllable onset. Phonotactics can be represented by a language model or finite-state model of phone sequences. \n25.3 Prosody \nprosody \nProsody is the study of the intonational and rhythmic aspects of language, and in particular the use of F0, energy, and duration to convey pragmatic, affective, or conversation-interactional meanings.1 Prosody can be used to mark discourse structure, like the difference between statements and questions, or the way that a conversation is structured. Prosody is used to mark the saliency of a particular word or phrase. Prosody is heavily used for paralinguistic functions like conveying affective meanings like happiness, surprise, or anger. And prosody plays an important role in managing turn-taking in conversation. \nprominence \n25.3.1 Prosodic Prominence: Accent, Stress and Schwa \nIn a natural utterance of American English, some words sound more prominent than others, and certain syllables in these words are also more prominent than others. What we mean by prominence is that these words or syllables are perceptually more salient to the listener. Speakers make a word or syllable more salient in English by saying it louder, saying it slower (so it has a longer duration), or by varying F0 during the word, making it higher or more variable. \npitch accent \nAccent We represent prominence via a linguistic marker called pitch accent. Words or syllables that are prominent are said to bear (be associated with) a pitch accent. Thus this utterance might be pronounced by accenting the underlined words: \n(25.1) I’m a little surprised to hear it characterized as happy. \nlexical stress \nLexical Stress The syllables that bear pitch accent are called accented syllables. Not every syllable of a word can be accented: pitch accent has to be realized on the syllable that has lexical stress. Lexical stress is a property of the word’s pronunciation in dictionaries; the syllable that has lexical stress is the one that will be louder or longer if the word is accented. For example, the word surprised is stressed on its second syllable, not its first. (Try stressing the other syllable by saying SURprised; hopefully that sounds wrong to you). Thus, if the word surprised receives a pitch accent in a sentence, it is the second syllable that will be stronger. The following example shows underlined accented words with the stressed syllable bearing the accent (the louder, longer syllable) in boldface: \n(25.2) I’m a little surprised to hear it characterized as happy.",
        "chapter": "Phonetics",
        "section": "Articulatory Phonetics",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "25.3 Prosody \nprosody \nProsody is the study of the intonational and rhythmic aspects of language, and in particular the use of F0, energy, and duration to convey pragmatic, affective, or conversation-interactional meanings.1 Prosody can be used to mark discourse structure, like the difference between statements and questions, or the way that a conversation is structured. Prosody is used to mark the saliency of a particular word or phrase. Prosody is heavily used for paralinguistic functions like conveying affective meanings like happiness, surprise, or anger. And prosody plays an important role in managing turn-taking in conversation. \nprominence \n25.3.1 Prosodic Prominence: Accent, Stress and Schwa \nIn a natural utterance of American English, some words sound more prominent than others, and certain syllables in these words are also more prominent than others. What we mean by prominence is that these words or syllables are perceptually more salient to the listener. Speakers make a word or syllable more salient in English by saying it louder, saying it slower (so it has a longer duration), or by varying F0 during the word, making it higher or more variable. \npitch accent \nAccent We represent prominence via a linguistic marker called pitch accent. Words or syllables that are prominent are said to bear (be associated with) a pitch accent. Thus this utterance might be pronounced by accenting the underlined words: \n(25.1) I’m a little surprised to hear it characterized as happy. \nlexical stress \nLexical Stress The syllables that bear pitch accent are called accented syllables. Not every syllable of a word can be accented: pitch accent has to be realized on the syllable that has lexical stress. Lexical stress is a property of the word’s pronunciation in dictionaries; the syllable that has lexical stress is the one that will be louder or longer if the word is accented. For example, the word surprised is stressed on its second syllable, not its first. (Try stressing the other syllable by saying SURprised; hopefully that sounds wrong to you). Thus, if the word surprised receives a pitch accent in a sentence, it is the second syllable that will be stronger. The following example shows underlined accented words with the stressed syllable bearing the accent (the louder, longer syllable) in boldface: \n(25.2) I’m a little surprised to hear it characterized as happy. \nStress is marked in dictionaries. The CMU dictionary (CMU, 1993), for example, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter: [K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can affect word meaning; the noun content is pronounced [K AA1 N T EH0 N T], while the adjective is pronounced [K AA0 N T EH1 N T]. \nreduced vowel schwa \nReduced Vowels and Schwa Unstressed vowels can be weakened even further to reduced vowels, the most common of which is schwa ([ax]), as in the second vowel of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as complete as for a full vowel. Not all unstressed vowels are reduced; any vowel, and diphthongs in particular, can retain its full quality even in unstressed position. For example, the vowel [iy] can appear in stressed position as in the word eat [iy t] or in unstressed position as in the word carry [k ae r iy]. \nprominence \nIn summary, there is a continuum of prosodic prominence, for which it is often useful to represent levels like accented, stressed, full vowel, and reduced vowel. \nprosodic phrasing \nintonation phrase \n25.3.2 Prosodic Structure \nintermediate phrase \nSpoken sentences have prosodic structure: some words seem to group naturally together, while some words seem to have a noticeable break or disjuncture between them. Prosodic structure is often described in terms of prosodic phrasing, meaning that an utterance has a prosodic phrase structure in a similar way to it having a syntactic phrase structure. For example, the sentence I wanted to go to London, but could only get tickets for France seems to have two main intonation phrases, their boundary occurring at the comma. Furthermore, in the first phrase, there seems to be another set of lesser prosodic phrase boundaries (often called intermediate phrases) that split up the words as I wanted to go to London. These kinds of intonation phrases are often correlated with syntactic structure constituents (Price et al. 1991, Bennett and Elfner 2019). \nAutomatically predicting prosodic boundaries can be important for tasks like TTS. Modern approaches use sequence models that take either raw text or text annotated with features like parse trees as input, and make a break/no-break decision at each word boundary. They can be trained on data labeled for prosodic structure like the Boston University Radio News Corpus (Ostendorf et al., 1995). \ntune \nquestion rise \n25.3.3 Tune \nTwo utterances with the same prominence and phrasing patterns can still differ prosodically by having different tunes. The tune of an utterance is the rise and fall of its F0 over time. A very obvious example of tune is the difference between statements and yes-no questions in English. The same words can be said with a final F0 rise to indicate a yes-no question (called a question rise): \nfinal fall \nor a final drop in F0 (called a final fall) to indicate a declarative intonation: \nLanguages make wide use of tune to express meaning (Xu, 2005). In English,",
        "chapter": "Phonetics",
        "section": "Prosody",
        "subsection": "Prosodic Prominence: Accent, Stress and Schwa",
        "subsubsection": "N/A"
    },
    {
        "content": "Stress is marked in dictionaries. The CMU dictionary (CMU, 1993), for example, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter: [K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can affect word meaning; the noun content is pronounced [K AA1 N T EH0 N T], while the adjective is pronounced [K AA0 N T EH1 N T]. \nreduced vowel schwa \nReduced Vowels and Schwa Unstressed vowels can be weakened even further to reduced vowels, the most common of which is schwa ([ax]), as in the second vowel of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as complete as for a full vowel. Not all unstressed vowels are reduced; any vowel, and diphthongs in particular, can retain its full quality even in unstressed position. For example, the vowel [iy] can appear in stressed position as in the word eat [iy t] or in unstressed position as in the word carry [k ae r iy]. \nprominence \nIn summary, there is a continuum of prosodic prominence, for which it is often useful to represent levels like accented, stressed, full vowel, and reduced vowel. \nprosodic phrasing \nintonation phrase \n25.3.2 Prosodic Structure \nintermediate phrase \nSpoken sentences have prosodic structure: some words seem to group naturally together, while some words seem to have a noticeable break or disjuncture between them. Prosodic structure is often described in terms of prosodic phrasing, meaning that an utterance has a prosodic phrase structure in a similar way to it having a syntactic phrase structure. For example, the sentence I wanted to go to London, but could only get tickets for France seems to have two main intonation phrases, their boundary occurring at the comma. Furthermore, in the first phrase, there seems to be another set of lesser prosodic phrase boundaries (often called intermediate phrases) that split up the words as I wanted to go to London. These kinds of intonation phrases are often correlated with syntactic structure constituents (Price et al. 1991, Bennett and Elfner 2019). \nAutomatically predicting prosodic boundaries can be important for tasks like TTS. Modern approaches use sequence models that take either raw text or text annotated with features like parse trees as input, and make a break/no-break decision at each word boundary. They can be trained on data labeled for prosodic structure like the Boston University Radio News Corpus (Ostendorf et al., 1995). \ntune \nquestion rise \n25.3.3 Tune \nTwo utterances with the same prominence and phrasing patterns can still differ prosodically by having different tunes. The tune of an utterance is the rise and fall of its F0 over time. A very obvious example of tune is the difference between statements and yes-no questions in English. The same words can be said with a final F0 rise to indicate a yes-no question (called a question rise): \nfinal fall \nor a final drop in F0 (called a final fall) to indicate a declarative intonation: \nLanguages make wide use of tune to express meaning (Xu, 2005). In English,",
        "chapter": "Phonetics",
        "section": "Prosody",
        "subsection": "Prosodic Structure",
        "subsubsection": "N/A"
    },
    {
        "content": "Stress is marked in dictionaries. The CMU dictionary (CMU, 1993), for example, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter: [K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can affect word meaning; the noun content is pronounced [K AA1 N T EH0 N T], while the adjective is pronounced [K AA0 N T EH1 N T]. \nreduced vowel schwa \nReduced Vowels and Schwa Unstressed vowels can be weakened even further to reduced vowels, the most common of which is schwa ([ax]), as in the second vowel of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as complete as for a full vowel. Not all unstressed vowels are reduced; any vowel, and diphthongs in particular, can retain its full quality even in unstressed position. For example, the vowel [iy] can appear in stressed position as in the word eat [iy t] or in unstressed position as in the word carry [k ae r iy]. \nprominence \nIn summary, there is a continuum of prosodic prominence, for which it is often useful to represent levels like accented, stressed, full vowel, and reduced vowel. \nprosodic phrasing \nintonation phrase \n25.3.2 Prosodic Structure \nintermediate phrase \nSpoken sentences have prosodic structure: some words seem to group naturally together, while some words seem to have a noticeable break or disjuncture between them. Prosodic structure is often described in terms of prosodic phrasing, meaning that an utterance has a prosodic phrase structure in a similar way to it having a syntactic phrase structure. For example, the sentence I wanted to go to London, but could only get tickets for France seems to have two main intonation phrases, their boundary occurring at the comma. Furthermore, in the first phrase, there seems to be another set of lesser prosodic phrase boundaries (often called intermediate phrases) that split up the words as I wanted to go to London. These kinds of intonation phrases are often correlated with syntactic structure constituents (Price et al. 1991, Bennett and Elfner 2019). \nAutomatically predicting prosodic boundaries can be important for tasks like TTS. Modern approaches use sequence models that take either raw text or text annotated with features like parse trees as input, and make a break/no-break decision at each word boundary. They can be trained on data labeled for prosodic structure like the Boston University Radio News Corpus (Ostendorf et al., 1995). \ntune \nquestion rise \n25.3.3 Tune \nTwo utterances with the same prominence and phrasing patterns can still differ prosodically by having different tunes. The tune of an utterance is the rise and fall of its F0 over time. A very obvious example of tune is the difference between statements and yes-no questions in English. The same words can be said with a final F0 rise to indicate a yes-no question (called a question rise): \nfinal fall \nor a final drop in F0 (called a final fall) to indicate a declarative intonation: \nLanguages make wide use of tune to express meaning (Xu, 2005). In English, \ncontinuation rise \nfor example, besides this well-known rise for yes-no questions, a phrase containing a list of nouns separated by commas often has a short rise called a continuation rise after each noun. Other examples include the characteristic English contours for expressing contradiction and expressing surprise. \nLinking Prominence and Tune \nToBI \nPitch accents come in different varieties that are related to tune; high pitched accents, for example, have different functions than low pitched accents. There are many typologies of accent classes in different languages. One such typology is part of the ToBI (Tone and Break Indices) theory of intonation (Silverman et al. 1992). Each word in ToBI can be associated with one of five types of pitch accents shown in in Fig. 25.7. Each utterance in ToBI consists of a sequence of intonational phrases, each of which ends in one of four boundary tones shown in Fig. 25.7, representing the utterance final aspects of tune. There are version of ToBI for many languages. \nboundary tone \n25.4 Acoustic Phonetics and Signals \nWe begin with a very brief introduction to the acoustic waveform and its digitization and frequency analysis; the interested reader is encouraged to consult the references at the end of the chapter. \n25.4.1 Waves \nAcoustic analysis is based on the sine and cosine functions. Figure 25.8 shows a plot of a sine wave, in particular the function \nfrequency amplitude \nHertz period \n\nwhere we have set the amplitude A to 1 and the frequency $f$ to 10 cycles per second. \nRecall from basic mathematics that two important characteristics of a wave are its frequency and amplitude. The frequency is the number of times a second that a wave repeats itself, that is, the number of cycles. We usually measure frequency in cycles per second. The signal in Fig. 25.8 repeats itself 5 times in .5 seconds, hence 10 cycles per second. Cycles per second are usually called hertz (shortened to $mathbf { H } mathbf { z }$ ), so the frequency in Fig. 25.8 would be described as $1 0  : mathrm { H z }$ . The amplitude $A$ of a sine wave is the maximum value on the Y axis. The period $T$ of the wave is the time it takes for one cycle to complete, defined as",
        "chapter": "Phonetics",
        "section": "Prosody",
        "subsection": "Tune",
        "subsubsection": "N/A"
    },
    {
        "content": "continuation rise \nfor example, besides this well-known rise for yes-no questions, a phrase containing a list of nouns separated by commas often has a short rise called a continuation rise after each noun. Other examples include the characteristic English contours for expressing contradiction and expressing surprise. \nLinking Prominence and Tune \nToBI \nPitch accents come in different varieties that are related to tune; high pitched accents, for example, have different functions than low pitched accents. There are many typologies of accent classes in different languages. One such typology is part of the ToBI (Tone and Break Indices) theory of intonation (Silverman et al. 1992). Each word in ToBI can be associated with one of five types of pitch accents shown in in Fig. 25.7. Each utterance in ToBI consists of a sequence of intonational phrases, each of which ends in one of four boundary tones shown in Fig. 25.7, representing the utterance final aspects of tune. There are version of ToBI for many languages. \nboundary tone \n25.4 Acoustic Phonetics and Signals \nWe begin with a very brief introduction to the acoustic waveform and its digitization and frequency analysis; the interested reader is encouraged to consult the references at the end of the chapter. \n25.4.1 Waves \nAcoustic analysis is based on the sine and cosine functions. Figure 25.8 shows a plot of a sine wave, in particular the function \nfrequency amplitude \nHertz period \n\nwhere we have set the amplitude A to 1 and the frequency $f$ to 10 cycles per second. \nRecall from basic mathematics that two important characteristics of a wave are its frequency and amplitude. The frequency is the number of times a second that a wave repeats itself, that is, the number of cycles. We usually measure frequency in cycles per second. The signal in Fig. 25.8 repeats itself 5 times in .5 seconds, hence 10 cycles per second. Cycles per second are usually called hertz (shortened to $mathbf { H } mathbf { z }$ ), so the frequency in Fig. 25.8 would be described as $1 0  : mathrm { H z }$ . The amplitude $A$ of a sine wave is the maximum value on the Y axis. The period $T$ of the wave is the time it takes for one cycle to complete, defined as \nEach cycle in Fig. 25.8 lasts a tenth of a second; hence $T = . 1$ seconds. \n25.4.2 Speech Sound Waves \nLet’s turn from hypothetical waves to sound waves. The input to a speech recognizer, like the input to the human ear, is a complex series of changes in air pressure. These changes in air pressure obviously originate with the speaker and are caused by the specific way that air passes through the glottis and out the oral or nasal cavities. We represent sound waves by plotting the change in air pressure over time. One metaphor which sometimes helps in understanding these graphs is that of a vertical plate blocking the air pressure waves (perhaps in a microphone in front of a speaker’s mouth, or the eardrum in a hearer’s ear). The graph measures the amount of compression or rarefaction (uncompression) of the air molecules at this plate. Figure 25.9 shows a short segment of a waveform taken from the Switchboard corpus of telephone speech of the vowel [iy] from someone saying “she just had a baby”. \nsampling \nThe first step in digitizing a sound wave like Fig. 25.9 is to convert the analog representations (first air pressure and then analog electric signals in a microphone) into a digital signal. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but fewer than two samples causes the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below $1 0 { , } 0 0 0 ~ mathrm { H z }$ ; thus, a $2 0 { , } 0 0 0 ~ mathrm { H z }$ sampling rate would be necessary for com\nNyquist frequency",
        "chapter": "Phonetics",
        "section": "Acoustic Phonetics and Signals",
        "subsection": "Waves",
        "subsubsection": "N/A"
    },
    {
        "content": "Each cycle in Fig. 25.8 lasts a tenth of a second; hence $T = . 1$ seconds. \n25.4.2 Speech Sound Waves \nLet’s turn from hypothetical waves to sound waves. The input to a speech recognizer, like the input to the human ear, is a complex series of changes in air pressure. These changes in air pressure obviously originate with the speaker and are caused by the specific way that air passes through the glottis and out the oral or nasal cavities. We represent sound waves by plotting the change in air pressure over time. One metaphor which sometimes helps in understanding these graphs is that of a vertical plate blocking the air pressure waves (perhaps in a microphone in front of a speaker’s mouth, or the eardrum in a hearer’s ear). The graph measures the amount of compression or rarefaction (uncompression) of the air molecules at this plate. Figure 25.9 shows a short segment of a waveform taken from the Switchboard corpus of telephone speech of the vowel [iy] from someone saying “she just had a baby”. \nsampling \nThe first step in digitizing a sound wave like Fig. 25.9 is to convert the analog representations (first air pressure and then analog electric signals in a microphone) into a digital signal. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but fewer than two samples causes the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below $1 0 { , } 0 0 0 ~ mathrm { H z }$ ; thus, a $2 0 { , } 0 0 0 ~ mathrm { H z }$ sampling rate would be necessary for com\nNyquist frequency \nplete accuracy. But telephone speech is filtered by the switching network, and only frequencies less than $4 { , } 0 0 0 ~ mathrm { H z }$ are transmitted by telephones. Thus, an ${ 8 , 0 0 0 } mathrm { { H z } }$ sampling rate is sufficient for telephone-bandwidth speech like the Switchboard corpus, while $1 6 { , } 0 0 0 mathrm { { H z } }$ sampling is often used for microphone speech. \nquantization \nEven an ${ 8 , 0 0 0 } mathrm { { H z } }$ sampling rate requires 8000 amplitude measurements for each second of speech, so it is important to store amplitude measurements efficiently. They are usually stored as integers, either 8 bit (values from -128–127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization because the difference between two integers acts as a minimum granularity (a quantum size) and all values that are closer together than this quantum size are represented identically. \nchannel \nOnce data is quantized, it is stored in various formats. One parameter of these formats is the sample rate and sample size discussed above; telephone speech is often sampled at $8 ~ mathrm { { k H z } }$ and stored as 8-bit samples, and microphone data is often sampled at $1 6 mathrm { k H z }$ and stored as 16-bit samples. Another parameter is the number of channels. For stereo data or for two-party conversations, we can store both channels in the same file or we can store them in separate files. A final parameter is individual sample storage—linearly or compressed. One common compression format used for telephone speech is $mu$ -law (often written u-law but still pronounced mu-law). The intuition of log compression algorithms like $mu$ -law is that human hearing is more sensitive at small intensities than large ones; the log represents small values with more faithfulness at the expense of more error on large values. The linear (unlogged) values are generally referred to as linear PCM values (PCM stands for pulse code modulation, but never mind that). Here’s the equation for compressing a linear PCM sample value $x$ to 8-bit $mu$ -law, (where $scriptstyle { mu = 2 5 5 }$ for 8 bits): \nThere are a number of standard file formats for storing the resulting digitized wavefile, such as Microsoft’s .wav and Apple’s AIFF all of which have special headers; simple headerless “raw” files are also used. For example, the .wav format is a subset of Microsoft’s RIFF format for multimedia files; RIFF is a general format that can represent a series of nested chunks of data and control information. Figure 25.10 shows a simple .wav file with a single data chunk together with its format chunk. \n25.4.3 Frequency and Amplitude; Pitch and Loudness \nSound waves, like all waves, can be described in terms of frequency, amplitude, and the other characteristics that we introduced earlier for pure sine waves. In sound waves, these are not quite as simple to measure as they were for sine waves. Let’s consider frequency. Note in Fig. 25.9 that although not exactly a sine, the wave is nonetheless periodic, repeating 10 times in the 38.75 milliseconds (.03875 seconds) captured in the figure. Thus, the frequency of this segment of the wave is 10/.03875 or $2 5 8 mathrm { H z }$ .",
        "chapter": "Phonetics",
        "section": "Acoustic Phonetics and Signals",
        "subsection": "Speech Sound Waves",
        "subsubsection": "N/A"
    },
    {
        "content": "plete accuracy. But telephone speech is filtered by the switching network, and only frequencies less than $4 { , } 0 0 0 ~ mathrm { H z }$ are transmitted by telephones. Thus, an ${ 8 , 0 0 0 } mathrm { { H z } }$ sampling rate is sufficient for telephone-bandwidth speech like the Switchboard corpus, while $1 6 { , } 0 0 0 mathrm { { H z } }$ sampling is often used for microphone speech. \nquantization \nEven an ${ 8 , 0 0 0 } mathrm { { H z } }$ sampling rate requires 8000 amplitude measurements for each second of speech, so it is important to store amplitude measurements efficiently. They are usually stored as integers, either 8 bit (values from -128–127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization because the difference between two integers acts as a minimum granularity (a quantum size) and all values that are closer together than this quantum size are represented identically. \nchannel \nOnce data is quantized, it is stored in various formats. One parameter of these formats is the sample rate and sample size discussed above; telephone speech is often sampled at $8 ~ mathrm { { k H z } }$ and stored as 8-bit samples, and microphone data is often sampled at $1 6 mathrm { k H z }$ and stored as 16-bit samples. Another parameter is the number of channels. For stereo data or for two-party conversations, we can store both channels in the same file or we can store them in separate files. A final parameter is individual sample storage—linearly or compressed. One common compression format used for telephone speech is $mu$ -law (often written u-law but still pronounced mu-law). The intuition of log compression algorithms like $mu$ -law is that human hearing is more sensitive at small intensities than large ones; the log represents small values with more faithfulness at the expense of more error on large values. The linear (unlogged) values are generally referred to as linear PCM values (PCM stands for pulse code modulation, but never mind that). Here’s the equation for compressing a linear PCM sample value $x$ to 8-bit $mu$ -law, (where $scriptstyle { mu = 2 5 5 }$ for 8 bits): \nThere are a number of standard file formats for storing the resulting digitized wavefile, such as Microsoft’s .wav and Apple’s AIFF all of which have special headers; simple headerless “raw” files are also used. For example, the .wav format is a subset of Microsoft’s RIFF format for multimedia files; RIFF is a general format that can represent a series of nested chunks of data and control information. Figure 25.10 shows a simple .wav file with a single data chunk together with its format chunk. \n25.4.3 Frequency and Amplitude; Pitch and Loudness \nSound waves, like all waves, can be described in terms of frequency, amplitude, and the other characteristics that we introduced earlier for pure sine waves. In sound waves, these are not quite as simple to measure as they were for sine waves. Let’s consider frequency. Note in Fig. 25.9 that although not exactly a sine, the wave is nonetheless periodic, repeating 10 times in the 38.75 milliseconds (.03875 seconds) captured in the figure. Thus, the frequency of this segment of the wave is 10/.03875 or $2 5 8 mathrm { H z }$ . \n\nWhere does this periodic $2 5 8 ~ mathrm { H z }$ wave come from? It comes from the speed of vibration of the vocal folds; since the waveform in Fig. 25.9 is from the vowel [iy], it is voiced. Recall that voicing is caused by regular openings and closing of the vocal folds. When the vocal folds are open, air is pushing up through the lungs, creating a region of high pressure. When the folds are closed, there is no pressure from the lungs. Thus, when the vocal folds are vibrating, we expect to see regular peaks in amplitude of the kind we see in Fig. 25.9, each major peak corresponding to an opening of the vocal folds. The frequency of the vocal fold vibration, or the frequency of the complex wave, is called the fundamental frequency of the waveform, often abbreviated F0. We can plot F0 over time in a pitch track. Figure 25.11 shows the pitch track of a short question, “Three o’clock?” represented below the waveform. Note the rise in F0 at the end of the question. \nThe vertical axis in Fig. 25.9 measures the amount of air pressure variation; pressure is force per unit area, measured in Pascals $mathrm { ( P a ) }$ . A high value on the vertical axis (a high amplitude) indicates that there is more air pressure at that point in time, a zero value means there is normal (atmospheric) air pressure, and a negative value means there is lower than normal air pressure (rarefaction). \nIn addition to this value of the amplitude at any point in time, we also often need to know the average amplitude over some time range, to give us some idea of how great the average displacement of air pressure is. But we can’t just take the average of the amplitude values over a range; the positive and negative values would (mostly) cancel out, leaving us with a number close to zero. Instead, we generally use the RMS (root-mean-square) amplitude, which squares each number before averaging (making it positive), and then takes the square root at the end. \nThe power of the signal is related to the square of the amplitude. If the number \nof samples of a sound is $N$ , the power is \nRather than power, we more often refer to the intensity of the sound, which normalizes the power to the human auditory threshold and is measured in dB. If $P _ { 0 }$ is the auditory threshold pressure $= 2 times 1 0 ^ { - 5 } ~ mathrm { P a }$ , then intensity is defined as follows: \npitch \nTwo important perceptual properties, pitch and loudness, are related to frequency and intensity. The pitch of a sound is the mental sensation, or perceptual correlate, of fundamental frequency; in general, if a sound has a higher fundamental frequency we perceive it as having a higher pitch. We say “in general” because the relationship is not linear, since human hearing has different acuities for different frequencies. Roughly speaking, human pitch perception is most accurate between $1 0 0 mathrm { H z }$ and $1 0 0 0 mathrm { H z }$ and in this range pitch correlates linearly with frequency. Human hearing represents frequencies above $1 0 0 0 ~ mathrm { H z }$ less accurately, and above this range, pitch correlates logarithmically with frequency. Logarithmic representation means that the differences between high frequencies are compressed and hence not as accurately perceived. There are various psychoacoustic models of pitch perception scales. One common model is the mel scale (Stevens et al. 1937, Stevens and Volkmann 1940). A mel is a unit of pitch defined such that pairs of sounds which are perceptually equidistant in pitch are separated by an equal number of mels. The mel frequency $m$ can be computed from the raw acoustic frequency as follows: \nMel \nAs we’ll see in Chapter 26, the mel scale plays an important role in speech recognition. \nThe loudness of a sound is the perceptual correlate of the power. So sounds with higher amplitudes are perceived as louder, but again the relationship is not linear. First of all, as we mentioned above when we defined $mu$ -law compression, humans have greater resolution in the low-power range; the ear is more sensitive to small power differences. Second, it turns out that there is a complex relationship between power, frequency, and perceived loudness; sounds in certain frequency ranges are perceived as being louder than those in other frequency ranges. \npitch extraction \nVarious algorithms exist for automatically extracting F0. In a slight abuse of terminology, these are called pitch extraction algorithms. The autocorrelation method of pitch extraction, for example, correlates the signal with itself at various offsets. The offset that gives the highest correlation gives the period of the signal. There are various publicly available pitch extraction toolkits; for example, an augmented autocorrelation pitch tracker is provided with Praat (Boersma and Weenink, 2005). \n25.4.4 Interpretation of Phones from a Waveform \nMuch can be learned from a visual inspection of a waveform. For example, vowels are pretty easy to spot. Recall that vowels are voiced; another property of vowels is that they tend to be long and are relatively loud (as we can see in the intensity plot in Fig. 25.12). Length in time manifests itself directly on the x-axis, and loudness is related to (the square of) amplitude on the y-axis. We saw in the previous section that voicing is realized by regular peaks in amplitude of the kind we saw in Fig. 25.9, each major peak corresponding to an opening of the vocal folds. Figure 25.13 shows the waveform of the short sentence “she just had a baby”. We have labeled this waveform with word and phone labels. Notice that each of the six vowels in Fig. 25.13, [iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks indicating voicing. \nFor a stop consonant, which consists of a closure followed by a release, we can often see a period of silence or near silence followed by a slight burst of amplitude. We can see this for both of the [b]’s in baby in Fig. 25.13. \nAnother phone that is often quite recognizable in a waveform is a fricative. Recall that fricatives, especially very strident fricatives like [sh], are made when a narrow channel for airflow causes noisy, turbulent air. The resulting hissy sounds have a noisy, irregular waveform. This can be seen somewhat in Fig. 25.13; it’s even clearer in Fig. 25.14, where we’ve magnified just the first word she.",
        "chapter": "Phonetics",
        "section": "Acoustic Phonetics and Signals",
        "subsection": "Frequency and Amplitude; Pitch and Loudness",
        "subsubsection": "N/A"
    },
    {
        "content": "The loudness of a sound is the perceptual correlate of the power. So sounds with higher amplitudes are perceived as louder, but again the relationship is not linear. First of all, as we mentioned above when we defined $mu$ -law compression, humans have greater resolution in the low-power range; the ear is more sensitive to small power differences. Second, it turns out that there is a complex relationship between power, frequency, and perceived loudness; sounds in certain frequency ranges are perceived as being louder than those in other frequency ranges. \npitch extraction \nVarious algorithms exist for automatically extracting F0. In a slight abuse of terminology, these are called pitch extraction algorithms. The autocorrelation method of pitch extraction, for example, correlates the signal with itself at various offsets. The offset that gives the highest correlation gives the period of the signal. There are various publicly available pitch extraction toolkits; for example, an augmented autocorrelation pitch tracker is provided with Praat (Boersma and Weenink, 2005). \n25.4.4 Interpretation of Phones from a Waveform \nMuch can be learned from a visual inspection of a waveform. For example, vowels are pretty easy to spot. Recall that vowels are voiced; another property of vowels is that they tend to be long and are relatively loud (as we can see in the intensity plot in Fig. 25.12). Length in time manifests itself directly on the x-axis, and loudness is related to (the square of) amplitude on the y-axis. We saw in the previous section that voicing is realized by regular peaks in amplitude of the kind we saw in Fig. 25.9, each major peak corresponding to an opening of the vocal folds. Figure 25.13 shows the waveform of the short sentence “she just had a baby”. We have labeled this waveform with word and phone labels. Notice that each of the six vowels in Fig. 25.13, [iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks indicating voicing. \nFor a stop consonant, which consists of a closure followed by a release, we can often see a period of silence or near silence followed by a slight burst of amplitude. We can see this for both of the [b]’s in baby in Fig. 25.13. \nAnother phone that is often quite recognizable in a waveform is a fricative. Recall that fricatives, especially very strident fricatives like [sh], are made when a narrow channel for airflow causes noisy, turbulent air. The resulting hissy sounds have a noisy, irregular waveform. This can be seen somewhat in Fig. 25.13; it’s even clearer in Fig. 25.14, where we’ve magnified just the first word she.",
        "chapter": "Phonetics",
        "section": "Acoustic Phonetics and Signals",
        "subsection": "Interpretation of Phones from a Waveform",
        "subsubsection": "N/A"
    },
    {
        "content": "The loudness of a sound is the perceptual correlate of the power. So sounds with higher amplitudes are perceived as louder, but again the relationship is not linear. First of all, as we mentioned above when we defined $mu$ -law compression, humans have greater resolution in the low-power range; the ear is more sensitive to small power differences. Second, it turns out that there is a complex relationship between power, frequency, and perceived loudness; sounds in certain frequency ranges are perceived as being louder than those in other frequency ranges. \npitch extraction \nVarious algorithms exist for automatically extracting F0. In a slight abuse of terminology, these are called pitch extraction algorithms. The autocorrelation method of pitch extraction, for example, correlates the signal with itself at various offsets. The offset that gives the highest correlation gives the period of the signal. There are various publicly available pitch extraction toolkits; for example, an augmented autocorrelation pitch tracker is provided with Praat (Boersma and Weenink, 2005). \n25.4.4 Interpretation of Phones from a Waveform \nMuch can be learned from a visual inspection of a waveform. For example, vowels are pretty easy to spot. Recall that vowels are voiced; another property of vowels is that they tend to be long and are relatively loud (as we can see in the intensity plot in Fig. 25.12). Length in time manifests itself directly on the x-axis, and loudness is related to (the square of) amplitude on the y-axis. We saw in the previous section that voicing is realized by regular peaks in amplitude of the kind we saw in Fig. 25.9, each major peak corresponding to an opening of the vocal folds. Figure 25.13 shows the waveform of the short sentence “she just had a baby”. We have labeled this waveform with word and phone labels. Notice that each of the six vowels in Fig. 25.13, [iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks indicating voicing. \nFor a stop consonant, which consists of a closure followed by a release, we can often see a period of silence or near silence followed by a slight burst of amplitude. We can see this for both of the [b]’s in baby in Fig. 25.13. \nAnother phone that is often quite recognizable in a waveform is a fricative. Recall that fricatives, especially very strident fricatives like [sh], are made when a narrow channel for airflow causes noisy, turbulent air. The resulting hissy sounds have a noisy, irregular waveform. This can be seen somewhat in Fig. 25.13; it’s even clearer in Fig. 25.14, where we’ve magnified just the first word she. \n25.4.5 Spectra and the Frequency Domain \nWhile some broad phonetic features (such as energy, pitch, and the presence of voicing, stop closures, or fricatives) can be interpreted directly from the waveform, most computational applications such as speech recognition (as well as human auditory processing) are based on a different representation of the sound in terms of its component frequencies. The insight of Fourier analysis is that every complex wave can be represented as a sum of many sine waves of different frequencies. Consider the waveform in Fig. 25.15. This waveform was created (in Praat) by summing two sine waveforms, one of frequency $1 0 mathrm { H z }$ and one of frequency $1 0 0 mathrm { H z }$ . \nspectrum \nWe can represent these two component frequencies with a spectrum. The spectrum of a signal is a representation of each of its frequency components and their amplitudes. Figure 25.16 shows the spectrum of Fig. 25.15. Frequency in $mathrm { H z }$ is on the $mathbf { X }$ -axis and amplitude on the y-axis. Note the two spikes in the figure, one at $1 0  : mathrm { H z }$ and one at $1 0 0 mathrm { H z }$ . Thus, the spectrum is an alternative representation of the original waveform, and we use the spectrum as a tool to study the component frequencies of a sound wave at a particular time point. \nLet’s look now at the frequency components of a speech waveform. Figure 25.17 shows part of the waveform for the vowel [ae] of the word had, cut out from the sentence shown in Fig. 25.13. \nNote that there is a complex wave that repeats about ten times in the figure; but there is also a smaller repeated wave that repeats four times for every larger pattern (notice the four small peaks inside each repeated wave). The complex wave has a frequency of about $2 3 4 mathrm { H z }$ (we can figure this out since it repeats roughly 10 times in .0427 seconds, and 10 cycles/.0427 seconds $= 2 3 4 mathrm { H z }$ ). \n\nThe smaller wave then should have a frequency of roughly four times the frequency of the larger wave, or roughly $9 3 6 mathrm { H z }$ . Then, if you look carefully, you can see two little waves on the peak of many of the $9 3 6 mathrm { H z }$ waves. The frequency of this tiniest wave must be roughly twice that of the $9 3 6 mathrm { H z }$ wave, hence $1 8 7 2 mathrm { H z }$ . \nFigure 25.18 shows a smoothed spectrum for the waveform in Fig. 25.17, computed with a discrete Fourier transform (DFT). \nThe $x cdot$ -axis of a spectrum shows frequency, and the $y$ -axis shows some measure of the magnitude of each frequency component (in decibels (dB), a logarithmic measure of amplitude that we saw earlier). Thus, Fig. 25.18 shows significant frequency components at around $9 3 0 ~ mathrm { H z }$ , $1 8 6 0 ~ mathrm { H z }$ , and $3 0 2 0 ~ mathrm { H z }$ , along with many other lower-magnitude frequency components. These first two components are just what we noticed in the time domain by looking at the wave in Fig. 25.17! \nWhy is a spectrum useful? It turns out that these spectral peaks that are easily visible in a spectrum are characteristic of different phones; phones have characteristic spectral “signatures”. Just as chemical elements give off different wavelengths of light when they burn, allowing us to detect elements in stars by looking at the spectrum of the light, we can detect the characteristic signature of the different phones by looking at the spectrum of a waveform. This use of spectral information is essential to both human and machine speech recognition. In human audition, the function of the cochlea, or inner ear, is to compute a spectrum of the incoming waveform. Similarly, the acoustic features used in speech recognition are spectral representations. \n\ncochlea \nLet’s look at the spectrum of different vowels. Since some vowels change over time, we’ll use a different kind of plot called a spectrogram. While a spectrum shows the frequency components of a wave at one point in time, a spectrogram is a way of envisioning how the different frequencies that make up a waveform change over time. The $x mathrm { . }$ -axis shows time, as it did for the waveform, but the $y$ -axis now shows frequencies in hertz. The darkness of a point on a spectrogram corresponds to the amplitude of the frequency component. Very dark points have high amplitude, light points have low amplitude. Thus, the spectrogram is a useful way of visualizing the three dimensions (time x frequency x amplitude). \nFigure 25.19 shows spectrograms of three American English vowels, [ih], [ae], and [ah]. Note that each vowel has a set of dark bars at various frequency bands, slightly different bands for each vowel. Each of these represents the same kind of spectral peak that we saw in Fig. 25.17. \nformant \nEach dark bar (or spectral peak) is called a formant. As we discuss below, a formant is a frequency band that is particularly amplified by the vocal tract. Since different vowels are produced with the vocal tract in different positions, they will produce different kinds of amplifications or resonances. Let’s look at the first two formants, called F1 and F2. Note that F1, the dark bar closest to the bottom, is in a different position for the three vowels; it’s low for [ih] (centered at about $4 7 0 ~ mathrm { H z } mathrm { , }$ ) and somewhat higher for [ae] and [ah] (somewhere around $8 0 0 ~ mathrm { H z } mathrm { , }$ ). By contrast, F2, the second dark bar from the bottom, is highest for [ih], in the middle for [ae], and lowest for [ah]. \nWe can see the same formants in running speech, although the reduction and coarticulation processes make them somewhat harder to see. Figure 25.20 shows the spectrogram of “she just had a baby”, whose waveform was shown in Fig. 25.13. F1 and F2 (and also F3) are pretty clear for the [ax] of just, the [ae] of had, and the [ey] of baby. \nWhat specific clues can spectral representations give for phone identification? First, since different vowels have their formants at characteristic places, the spectrum can distinguish vowels from each other. We’ve seen that [ae] in the sample waveform had formants at $9 3 0 ~ mathrm { H z }$ , $1 8 6 0 ~ mathrm { H z }$ , and $3 0 2 0 ~ mathrm { H z }$ . Consider the vowel [iy] at the beginning of the utterance in Fig. 25.13. The spectrum for this vowel is shown in Fig. 25.21. The first formant of [iy] is $5 4 0 mathrm { H z }$ , much lower than the first formant for [ae], and the second formant $( 2 5 8 1  mathrm { H z } )$ is much higher than the second formant for [ae]. If you look carefully, you can see these formants as dark bars in Fig. 25.20 just around 0.5 seconds. \n\nThe location of the first two formants (called F1 and F2) plays a large role in determining vowel identity, although the formants still differ from speaker to speaker. Higher formants tend to be caused more by general characteristics of a speaker’s vocal tract rather than by individual vowels. Formants also can be used to identify the nasal phones [n], [m], and [ng] and the liquids [l] and [r]. \nsource-filter model \n25.4.6 The Source-Filter Model \nWhy do different vowels have different spectral signatures? As we briefly mentioned above, the formants are caused by the resonant cavities of the mouth. The sourcefilter model is a way of explaining the acoustics of a sound by modeling how the pulses produced by the glottis (the source) are shaped by the vocal tract (the filter). \nLet’s see how this works. Whenever we have a wave such as the vibration in air caused by the glottal pulse, the wave also has harmonics. A harmonic is another wave whose frequency is a multiple of the fundamental wave. Thus, for example, a",
        "chapter": "Phonetics",
        "section": "Acoustic Phonetics and Signals",
        "subsection": "Spectra and the Frequency Domain",
        "subsubsection": "N/A"
    },
    {
        "content": "The location of the first two formants (called F1 and F2) plays a large role in determining vowel identity, although the formants still differ from speaker to speaker. Higher formants tend to be caused more by general characteristics of a speaker’s vocal tract rather than by individual vowels. Formants also can be used to identify the nasal phones [n], [m], and [ng] and the liquids [l] and [r]. \nsource-filter model \n25.4.6 The Source-Filter Model \nWhy do different vowels have different spectral signatures? As we briefly mentioned above, the formants are caused by the resonant cavities of the mouth. The sourcefilter model is a way of explaining the acoustics of a sound by modeling how the pulses produced by the glottis (the source) are shaped by the vocal tract (the filter). \nLet’s see how this works. Whenever we have a wave such as the vibration in air caused by the glottal pulse, the wave also has harmonics. A harmonic is another wave whose frequency is a multiple of the fundamental wave. Thus, for example, a \n$1 1 5 ~ mathrm { H z }$ glottal fold vibration leads to harmonics (other waves) of $2 3 0 mathrm { H z }$ , $3 4 5 ~ mathrm { H z }$ , $4 6 0 ~ mathrm { H z }$ , and so on on. In general, each of these waves will be weaker, that is, will have much less amplitude than the wave at the fundamental frequency. \nIt turns out, however, that the vocal tract acts as a kind of filter or amplifier; indeed any cavity, such as a tube, causes waves of certain frequencies to be amplified and others to be damped. This amplification process is caused by the shape of the cavity; a given shape will cause sounds of a certain frequency to resonate and hence be amplified. Thus, by changing the shape of the cavity, we can cause different frequencies to be amplified. \nWhen we produce particular vowels, we are essentially changing the shape of the vocal tract cavity by placing the tongue and the other articulators in particular positions. The result is that different vowels cause different harmonics to be amplified. So a wave of the same fundamental frequency passed through different vocal tract positions will result in different harmonics being amplified. \nWe can see the result of this amplification by looking at the relationship between the shape of the vocal tract and the corresponding spectrum. Figure 25.22 shows the vocal tract position for three vowels and a typical resulting spectrum. The formants are places in the spectrum where the vocal tract happens to amplify particular harmonic frequencies. \n25.5 Phonetic Resources \npronunciation dictionary \nA wide variety of phonetic resources can be drawn on for computational work. Online pronunciation dictionaries give phonetic transcriptions for words. The LDC distributes pronunciation lexicons for Egyptian Arabic, Dutch, English, German, Japanese, Korean, Mandarin, and Spanish. For English, the CELEX dictionary (Baayen et al., 1995) has pronunciations for 160,595 wordforms, with syllabification, stress, and morphological and part-of-speech information. The open-source CMU Pronouncing Dictionary (CMU, 1993) has pronunciations for about 134,000",
        "chapter": "Phonetics",
        "section": "Acoustic Phonetics and Signals",
        "subsection": "The Source-Filter Model",
        "subsubsection": "N/A"
    },
    {
        "content": "$1 1 5 ~ mathrm { H z }$ glottal fold vibration leads to harmonics (other waves) of $2 3 0 mathrm { H z }$ , $3 4 5 ~ mathrm { H z }$ , $4 6 0 ~ mathrm { H z }$ , and so on on. In general, each of these waves will be weaker, that is, will have much less amplitude than the wave at the fundamental frequency. \nIt turns out, however, that the vocal tract acts as a kind of filter or amplifier; indeed any cavity, such as a tube, causes waves of certain frequencies to be amplified and others to be damped. This amplification process is caused by the shape of the cavity; a given shape will cause sounds of a certain frequency to resonate and hence be amplified. Thus, by changing the shape of the cavity, we can cause different frequencies to be amplified. \nWhen we produce particular vowels, we are essentially changing the shape of the vocal tract cavity by placing the tongue and the other articulators in particular positions. The result is that different vowels cause different harmonics to be amplified. So a wave of the same fundamental frequency passed through different vocal tract positions will result in different harmonics being amplified. \nWe can see the result of this amplification by looking at the relationship between the shape of the vocal tract and the corresponding spectrum. Figure 25.22 shows the vocal tract position for three vowels and a typical resulting spectrum. The formants are places in the spectrum where the vocal tract happens to amplify particular harmonic frequencies. \n25.5 Phonetic Resources \npronunciation dictionary \nA wide variety of phonetic resources can be drawn on for computational work. Online pronunciation dictionaries give phonetic transcriptions for words. The LDC distributes pronunciation lexicons for Egyptian Arabic, Dutch, English, German, Japanese, Korean, Mandarin, and Spanish. For English, the CELEX dictionary (Baayen et al., 1995) has pronunciations for 160,595 wordforms, with syllabification, stress, and morphological and part-of-speech information. The open-source CMU Pronouncing Dictionary (CMU, 1993) has pronunciations for about 134,000 \ntime-aligned transcription \nwordforms, while the fine-grained 110,000 word UNISYN dictionary (Fitt, 2002), freely available for research purposes, gives syllabifications, stress, and also pronunciations for dozens of dialects of English. \nAnother useful resource is a phonetically annotated corpus, in which a collection of waveforms is hand-labeled with the corresponding string of phones. The TIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments (TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from 630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some selected to have particular dialect shibboleths, others to maximize phonetic diphone coverage. Each sentence in the corpus was phonetically hand-labeled, the sequence of phones was automatically aligned with the sentence wavefile, and then the automatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988). The result is a time-aligned transcription: a transcription in which each phone is associated with a start and end time in the waveform, like the example in Fig. 25.23. \nThe Switchboard Transcription Project phonetically annotated corpus consists of 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al., 1996), together with transcriptions time-aligned at the syllable level. Figure 25.24 shows an example . \nFigure 25.24 Phonetic transcription of the Switchboard phrase they’re kind of in between right now. Note vowel reduction in they’re and of, coda deletion in kind and right, and resyllabification (the [v] of of attaches as the onset of $i n$ ). Time is given in number of seconds from the beginning of sentence to the start of each syllable. \nThe Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically transcribed corpus of spontaneous American speech, containing about 300,000 words from 40 talkers. Phonetically transcribed corpora are also available for other languages, including the Kiel corpus of German and Mandarin corpora transcribed by the Chinese Academy of Social Sciences (Li et al., 2000). \nIn addition to resources like dictionaries and corpora, there are many useful phonetic software tools. Many of the figures in this book were generated by the Praat package (Boersma and Weenink, 2005), which includes pitch, spectral, and formant analysis, as well as a scripting language. \n25.6 Summary \nThis chapter has introduced many of the important concepts of phonetics and computational phonetics. \n• We can represent the pronunciation of words in terms of units called phones. The standard system for representing phones is the International Phonetic",
        "chapter": "Phonetics",
        "section": "Phonetic Resources",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "time-aligned transcription \nwordforms, while the fine-grained 110,000 word UNISYN dictionary (Fitt, 2002), freely available for research purposes, gives syllabifications, stress, and also pronunciations for dozens of dialects of English. \nAnother useful resource is a phonetically annotated corpus, in which a collection of waveforms is hand-labeled with the corresponding string of phones. The TIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments (TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from 630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some selected to have particular dialect shibboleths, others to maximize phonetic diphone coverage. Each sentence in the corpus was phonetically hand-labeled, the sequence of phones was automatically aligned with the sentence wavefile, and then the automatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988). The result is a time-aligned transcription: a transcription in which each phone is associated with a start and end time in the waveform, like the example in Fig. 25.23. \nThe Switchboard Transcription Project phonetically annotated corpus consists of 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al., 1996), together with transcriptions time-aligned at the syllable level. Figure 25.24 shows an example . \nFigure 25.24 Phonetic transcription of the Switchboard phrase they’re kind of in between right now. Note vowel reduction in they’re and of, coda deletion in kind and right, and resyllabification (the [v] of of attaches as the onset of $i n$ ). Time is given in number of seconds from the beginning of sentence to the start of each syllable. \nThe Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically transcribed corpus of spontaneous American speech, containing about 300,000 words from 40 talkers. Phonetically transcribed corpora are also available for other languages, including the Kiel corpus of German and Mandarin corpora transcribed by the Chinese Academy of Social Sciences (Li et al., 2000). \nIn addition to resources like dictionaries and corpora, there are many useful phonetic software tools. Many of the figures in this book were generated by the Praat package (Boersma and Weenink, 2005), which includes pitch, spectral, and formant analysis, as well as a scripting language. \n25.6 Summary \nThis chapter has introduced many of the important concepts of phonetics and computational phonetics. \n• We can represent the pronunciation of words in terms of units called phones. The standard system for representing phones is the International Phonetic",
        "chapter": "Phonetics",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "time-aligned transcription \nwordforms, while the fine-grained 110,000 word UNISYN dictionary (Fitt, 2002), freely available for research purposes, gives syllabifications, stress, and also pronunciations for dozens of dialects of English. \nAnother useful resource is a phonetically annotated corpus, in which a collection of waveforms is hand-labeled with the corresponding string of phones. The TIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments (TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from 630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some selected to have particular dialect shibboleths, others to maximize phonetic diphone coverage. Each sentence in the corpus was phonetically hand-labeled, the sequence of phones was automatically aligned with the sentence wavefile, and then the automatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988). The result is a time-aligned transcription: a transcription in which each phone is associated with a start and end time in the waveform, like the example in Fig. 25.23. \nThe Switchboard Transcription Project phonetically annotated corpus consists of 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al., 1996), together with transcriptions time-aligned at the syllable level. Figure 25.24 shows an example . \nFigure 25.24 Phonetic transcription of the Switchboard phrase they’re kind of in between right now. Note vowel reduction in they’re and of, coda deletion in kind and right, and resyllabification (the [v] of of attaches as the onset of $i n$ ). Time is given in number of seconds from the beginning of sentence to the start of each syllable. \nThe Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically transcribed corpus of spontaneous American speech, containing about 300,000 words from 40 talkers. Phonetically transcribed corpora are also available for other languages, including the Kiel corpus of German and Mandarin corpora transcribed by the Chinese Academy of Social Sciences (Li et al., 2000). \nIn addition to resources like dictionaries and corpora, there are many useful phonetic software tools. Many of the figures in this book were generated by the Praat package (Boersma and Weenink, 2005), which includes pitch, spectral, and formant analysis, as well as a scripting language. \n25.6 Summary \nThis chapter has introduced many of the important concepts of phonetics and computational phonetics. \n• We can represent the pronunciation of words in terms of units called phones. The standard system for representing phones is the International Phonetic",
        "chapter": "Phonetics",
        "section": "Bibliographical and Historical Notes",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "time-aligned transcription \nwordforms, while the fine-grained 110,000 word UNISYN dictionary (Fitt, 2002), freely available for research purposes, gives syllabifications, stress, and also pronunciations for dozens of dialects of English. \nAnother useful resource is a phonetically annotated corpus, in which a collection of waveforms is hand-labeled with the corresponding string of phones. The TIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments (TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from 630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some selected to have particular dialect shibboleths, others to maximize phonetic diphone coverage. Each sentence in the corpus was phonetically hand-labeled, the sequence of phones was automatically aligned with the sentence wavefile, and then the automatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988). The result is a time-aligned transcription: a transcription in which each phone is associated with a start and end time in the waveform, like the example in Fig. 25.23. \nThe Switchboard Transcription Project phonetically annotated corpus consists of 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al., 1996), together with transcriptions time-aligned at the syllable level. Figure 25.24 shows an example . \nFigure 25.24 Phonetic transcription of the Switchboard phrase they’re kind of in between right now. Note vowel reduction in they’re and of, coda deletion in kind and right, and resyllabification (the [v] of of attaches as the onset of $i n$ ). Time is given in number of seconds from the beginning of sentence to the start of each syllable. \nThe Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically transcribed corpus of spontaneous American speech, containing about 300,000 words from 40 talkers. Phonetically transcribed corpora are also available for other languages, including the Kiel corpus of German and Mandarin corpora transcribed by the Chinese Academy of Social Sciences (Li et al., 2000). \nIn addition to resources like dictionaries and corpora, there are many useful phonetic software tools. Many of the figures in this book were generated by the Praat package (Boersma and Weenink, 2005), which includes pitch, spectral, and formant analysis, as well as a scripting language. \n25.6 Summary \nThis chapter has introduced many of the important concepts of phonetics and computational phonetics. \n• We can represent the pronunciation of words in terms of units called phones. The standard system for representing phones is the International Phonetic \nAlphabet or IPA. The most common computational system for transcription of English is the ARPAbet, which conveniently uses ASCII symbols. \n• Phones can be described by how they are produced articulatorily by the vocal organs; consonants are defined in terms of their place and manner of articulation and voicing; vowels by their height, backness, and roundness.   \n• Speech sounds can also be described acoustically. Sound waves can be described in terms of frequency, amplitude, or their perceptual correlates, pitch and loudness.   \n• The spectrum of a sound describes its different frequency components. While some phonetic properties are recognizable from the waveform, both humans and machines rely on spectral analysis for phone detection.   \n• A spectrogram is a plot of a spectrum over time. Vowels are described by characteristic harmonics called formants. \nBibliographical and Historical Notes \nThe major insights of articulatory phonetics date to the linguists of 800–150 B.C. India. They invented the concepts of place and manner of articulation, worked out the glottal mechanism of voicing, and understood the concept of assimilation. European science did not catch up with the Indian phoneticians until over 2000 years later, in the late 19th century. The Greeks did have some rudimentary phonetic knowledge; by the time of Plato’s Theaetetus and Cratylus, for example, they distinguished vowels from consonants, and stop consonants from continuants. The Stoics developed the idea of the syllable and were aware of phonotactic constraints on possible words. An unknown Icelandic scholar of the 12th century exploited the concept of the phoneme and proposed a phonemic writing system for Icelandic, including diacritics for length and nasality. But his text remained unpublished until 1818 and even then was largely unknown outside Scandinavia (Robins, 1967). The modern era of phonetics is usually said to have begun with Sweet, who proposed what is essentially the phoneme in his Handbook of Phonetics 1877. He also devised an alphabet for transcription and distinguished between broad and narrow transcription, proposing many ideas that were eventually incorporated into the IPA. Sweet was considered the best practicing phonetician of his time; he made the first scientific recordings of languages for phonetic purposes and advanced the state of the art of articulatory description. He was also infamously difficult to get along with, a trait that is well captured in Henry Higgins, the stage character that George Bernard Shaw modeled after him. The phoneme was first named by the Polish scholar Baudouin de Courtenay, who published his theories in 1894. \nIntroductory phonetics textbooks include Ladefoged (1993) and Clark and Yallop (1995). Wells (1982) is the definitive three-volume source on dialects of English. \nMany of the classic insights in acoustic phonetics had been developed by the late 1950s or early 1960s; just a few highlights include techniques like the sound spectrograph (Koenig et al., 1946), theoretical insights like the working out of the source-filter theory and other issues in the mapping between articulation and acoustics ((Fant, 1960), Stevens et al. 1953, Stevens and House 1955, Heinz and Stevens 1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and Barney, 1952), the understanding of the phonetic nature of stress and the use of duration and intensity as cues (Fry, 1955), and a basic understanding of issues in phone perception (Miller and Nicely 1955,Liberman et al. 1952). Lehiste (1967) is a collection of classic papers on acoustic phonetics. Many of the seminal papers of Gunnar Fant have been collected in Fant (2004). \n\nExcellent textbooks on acoustic phonetics include Johnson (2003) and Ladefoged (1996). Coleman (2005) includes an introduction to computational processing of acoustics and speech from a linguistic perspective. Stevens (1998) lays out an influential theory of speech sound production. There are a number of software packages for acoustic phonetic analysis. Probably the most widely used one is Praat (Boersma and Weenink, 2005). \nExercises \n25.1 Find the mistakes in the ARPAbet transcriptions of the following words: \na. “three” [dh r i] d. “study” [s t uh d i] g. “slight” [s l iy t] b. “sing” [s ih n g] e. “though” [th ow] c. “eyes” [ay s] f. “planning” [p pl aa n ih ng] \n25.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pronunciations (each) of the words “tomato”, “potato”, and “either”. Transcribe into the ARPAbet both pronunciations of each of these three words. \n25.3 Transcribe the following words in the ARPAbet: \n1. dark   \n2. suit   \n3. greasy   \n4. wash   \n5. water \n25.4 Take a wavefile of your choice. Some examples are on the textbook website. Download the Praat software, and use it to transcribe the wavefiles at the word level and into ARPAbet phones, using Praat to help you play pieces of each wavefile and to look at the wavefile and the spectrogram. \n25.5 Record yourself saying five of the English vowels: [aa], [eh], [ae], [iy], [uw]. Find F1 and F2 for each of your vowels. \n26 aAnudtoTmexati-tcoS-SpepecehchRecognition \nI KNOW not whether I see your meaning: if I do, it lies Upon the wordy wavelets of your voice, Dim as an evening shadow in a brook, Thomas Lovell Beddoes, 1851 \nUnderstanding spoken language, or at least transcribing the words into writing, is one of the earliest goals of computer language processing. In fact, speech processing \npredates the computer by many decades! The first machine that recognized speech was a toy from the 1920s. “Radio Rex”, shown to the right, was a celluloid dog that moved (by means of a spring) when the spring was released by ${ 5 0 0 } mathrm { H z }$ acoustic energy. Since $5 0 0 ~ mathrm { H z }$ is roughly the first formant of the vowel [eh] in “Rex”, Rex seemed to come when he was called (David, Jr. and Selfridge, 1962). \nIn modern times, we expect more of our automatic systems. The task of automatic speech recognition (ASR) is to map any waveform like this: \nto the appropriate string of words: \nAutomatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home appliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (“Accounting, please”) or in sophisticated dialogue applications (“I’d like to change the return date of my flight”). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcription is important in fields like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between computers and humans with some disability resulting in difficulties or inabilities in typing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury. \nWhat about the opposite problem, going from text to speech? This is a problem with an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box filled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Europe and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artificial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box.",
        "chapter": "Phonetics",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "What is less well known is that von Kempelen, an extraordinarily \nprolific inventor, also built between 1769 and 1790 what was definitely not a hoax: the first full-sentence speech synthesizer, shown partially to the right. His device consisted of a bellows to simulate the lungs, a rubber mouthpiece and a nose aperture, a reed to simulate the vocal folds, various whistles for the fricatives, and a \nspeech synthesis text-to-speech \nsmall auxiliary bellows to provide the puff of air for plosives. By moving levers with both hands to open and close apertures, and adjusting the flexible leather “vocal tract”, an operator could produce different consonants and vowels. \nMore than two centuries later, we no longer build our synthesizers out of wood and leather, nor do we need human operators. The modern task of speech synthesis, also called text-to-speech or TTS, is exactly the reverse of ASR; to map text: \nto an acoustic waveform: \nModern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. \nIn the next sections we’ll show how to do ASR with encoder-decoders, introduce the CTC loss functions, the standard word error rate evaluation metric, and describe how acoustic features are extracted. We’ll then see how TTS can be modeled with almost the same algorithm in reverse, and conclude with a brief mention of other speech tasks. \n26.1 The Automatic Speech Recognition Task \ndigit recognition \nBefore describing algorithms for ASR, let’s talk about how the task itself varies. One dimension of variation is vocabulary size. Some ASR tasks can be solved with extremely high accuracy, like those with a 2-word vocabulary (yes versus no) or an 11 word vocabulary like digit recognition (recognizing sequences of digits including zero to nine plus oh). Open-ended tasks like transcribing videos or human conversations, with large vocabularies of up to 60,000 words, are much harder. \nread speech conversational speech \nA second dimension of variation is who the speaker is talking to. Humans speaking to machines (either dictating or talking to a dialogue system) are easier to recognize than humans speaking to humans. Read speech, in which humans are reading out loud, for example in audio books, is also relatively easy to recognize. Recognizing the speech of two humans talking to each other in conversational speech, for example, for transcribing a business meeting, is the hardest. It seems that when humans talk to machines, or read without an audience present, they simplify their speech quite a bit, talking more slowly and more clearly. \nA third dimension of variation is channel and noise. Speech is easier to recognize if it’s recorded in a quiet room with head-mounted microphones than if it’s recorded by a distant microphone on a noisy city street, or in a car with the window open. \nA final dimension of variation is accent or speaker-class characteristics. Speech is easier to recognize if the speaker is speaking the same dialect or variety that the system was trained on. Speech by speakers of regional or ethnic dialects, or speech by children can be quite difficult to recognize if the system is only trained on speakers of standard dialects, or only adult speakers. \nLibriSpeech \nA number of publicly available corpora with human-created transcripts are used to create ASR test and training sets to explore this variation; we mention a few of them here since you will encounter them in the literature. LibriSpeech is a large open-source read-speech $1 6 mathrm { k H z }$ dataset with over 1000 hours of audio books from the LibriVox project, with transcripts aligned at the sentence level (Panayotov et al., 2015). It is divided into an easier (“clean”) and a more difficult portion (“other”) with the clean portion of higher recording quality and with accents closer to US English. This was done by running a speech recognizer (trained on read speech from the Wall Street Journal) on all the audio, computing the WER for each speaker based on the gold transcripts, and dividing the speakers roughly in half, with recordings from lower-WER speakers called “clean” and recordings from higher-WER speakers “other”. \nSwitchboard \nCALLHOME \nThe Switchboard corpus of prompted telephone conversations between strangers was collected in the early 1990s; it contains 2430 conversations averaging 6 minutes each, totaling 240 hours of $8 mathrm {  k H z }$ speech and about 3 million words (Godfrey et al., 1992). Switchboard has the singular advantage of an enormous amount of auxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic and prosodic labeling, and discourse and information structure. The CALLHOME corpus was collected in the late 1990s and consists of 120 unscripted 30-minute telephone conversations between native speakers of English who were usually close friends or family (Canavan et al., 1997). \nCORAAL \nThe Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is a large corpus of naturally occurring everyday spoken interactions from all over the United States, mostly face-to-face conversation, but also town-hall meetings, food preparation, on-the-job talk, and classroom lectures. The corpus was anonymized by removing personal names and other identifying information (replaced by pseudonyms in the transcripts, and masked in the audio). \nCHiME \nCORAAL is a collection of over 150 sociolinguistic interviews with African American speakers, with the goal of studying African American Language (AAL), the many variations of language used in African American communities (Kendall and Farrington, 2020). The interviews are anonymized with transcripts aligned at the utterance level. The CHiME Challenge is a series of difficult shared tasks with corpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of conversational speech in real home environments (specifically dinner parties). The \n\ncorpus contains recordings of twenty different dinner parties in real homes, each with four participants, and in three locations (kitchen, dining area, living room), recorded both with distant room microphones and with body-worn mikes. The   \nHKUST HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone conversations between speakers of Mandarin across China, including transcripts of the conversations, which are between either friends or strangers (Liu et al., 2006). The   \nISHELL-1 AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken from various domains, read by different speakers mainly from northern China (Bu et al., 2017). \nFigure 26.1 shows the rough percentage of incorrect words (the word error rate, or WER, defined on page 591) from state-of-the-art systems on some of these tasks. Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is around $2 %$ ; this is a solved task, although these numbers come from systems that require enormous computational resources. By contrast, the error rate for transcribing conversations between humans is much higher; 5.8 to $1 1 %$ for the Switchboard and CALLHOME corpora. The error rate is higher yet again for speakers of varieties like African American Vernacular English, and yet again for difficult conversational tasks like transcription of 4-speaker dinner party speech, which can have error rates as high as $8 1 . 3 %$ . Character error rates (CER) are also much lower for read Mandarin speech than for natural conversation. \n26.2 Feature Extraction for ASR: Log Mel Spectrum \nfeature vector \nThe first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, each vector representing the information in a small time window of the signal. Let’s see how to convert a raw wavefile to the most commonly used features, sequences of log mel spectrum vectors. A speech signal processing course is recommended for more details. \n26.2.1 Sampling and Quantization \nRecall from Section 25.4.2 that the first step is to convert the analog representations (first air pressure and then analog electric signals in a microphone) into a digital sig",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "The Automatic Speech Recognition Task",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "corpus contains recordings of twenty different dinner parties in real homes, each with four participants, and in three locations (kitchen, dining area, living room), recorded both with distant room microphones and with body-worn mikes. The   \nHKUST HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone conversations between speakers of Mandarin across China, including transcripts of the conversations, which are between either friends or strangers (Liu et al., 2006). The   \nISHELL-1 AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken from various domains, read by different speakers mainly from northern China (Bu et al., 2017). \nFigure 26.1 shows the rough percentage of incorrect words (the word error rate, or WER, defined on page 591) from state-of-the-art systems on some of these tasks. Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is around $2 %$ ; this is a solved task, although these numbers come from systems that require enormous computational resources. By contrast, the error rate for transcribing conversations between humans is much higher; 5.8 to $1 1 %$ for the Switchboard and CALLHOME corpora. The error rate is higher yet again for speakers of varieties like African American Vernacular English, and yet again for difficult conversational tasks like transcription of 4-speaker dinner party speech, which can have error rates as high as $8 1 . 3 %$ . Character error rates (CER) are also much lower for read Mandarin speech than for natural conversation. \n26.2 Feature Extraction for ASR: Log Mel Spectrum \nfeature vector \nThe first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, each vector representing the information in a small time window of the signal. Let’s see how to convert a raw wavefile to the most commonly used features, sequences of log mel spectrum vectors. A speech signal processing course is recommended for more details. \n26.2.1 Sampling and Quantization \nRecall from Section 25.4.2 that the first step is to convert the analog representations (first air pressure and then analog electric signals in a microphone) into a digital sig\nsampling \nsampling rate \nNyquist frequency \nnal. This analog-to-digital conversion has two steps: sampling and quantization. A signal is sampled by measuring its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but less than two samples will cause the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below $1 0 { , } 0 0 0 mathrm { H z }$ , so a $2 0 { , } 0 0 0 mathrm { H z }$ sampling rate would be necessary for complete accuracy. But telephone speech is filtered by the switching network, and only frequencies less than $4 { , } 0 0 0 ~ mathrm { H z }$ are transmitted by telephones. Thus, an $8 { , } 0 0 0 ~ mathrm { H z }$ sampling rate is sufficient for telephone-bandwidth speech, and $1 6 { , } 0 0 0 mathrm { { H z } }$ for microphone speech. \ntelephonebandwidth \nquantization \nAlthough using higher sampling rates produces higher ASR accuracy, we can’t combine different sampling rates for training and testing ASR systems. Thus if we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must downsample our training corpus to $8 ~ mathrm { K H z }$ . Similarly, if we are training on multiple corpora and one of them includes telephone speech, we downsample all the wideband corpora to 8Khz. \nAmplitude measurements are stored as integers, either 8 bit (values from $- 1 2 8 -$ 127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization; all values that are closer together than the minimum granularity (the quantum size) are represented identically. We refer to each sample at time index $n$ in the digitized, quantized waveform as $x [ n ]$ . \n26.2.2 Windowing \nstationary non-stationary \nFrom the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. Inside this small window, we can roughly think of the signal as stationary (that is, its statistical properties are constant within this region). (By contrast, in general, speech is a non-stationary signal, meaning that its statistical properties are not constant over time). We extract this roughly stationary portion of speech by using a window which is non-zero inside a region and zero elsewhere, running this window across the speech signal and multiplying it by the input waveform to produce a windowed waveform. \nframe \nstride \nThe speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. \nTo extract the signal we multiply the value of the signal at time $n , s [ n ]$ by the value of the window at time $n , w [ n ]$ : \nrectangular \nThe window shape sketched in Fig. 26.2 is rectangular; you can see the extracted windowed signal looks just like the original signal. The rectangular window, however, abruptly cuts off the signal at its boundaries, which creates problems when we do Fourier analysis. For this reason, for acoustic feature creation we more commonly use the Hamming window, which shrinks the values of the signal toward \nHamming",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Feature Extraction for ASR: Log Mel Spectrum",
        "subsection": "Sampling and Quantization",
        "subsubsection": "N/A"
    },
    {
        "content": "sampling \nsampling rate \nNyquist frequency \nnal. This analog-to-digital conversion has two steps: sampling and quantization. A signal is sampled by measuring its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but less than two samples will cause the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below $1 0 { , } 0 0 0 mathrm { H z }$ , so a $2 0 { , } 0 0 0 mathrm { H z }$ sampling rate would be necessary for complete accuracy. But telephone speech is filtered by the switching network, and only frequencies less than $4 { , } 0 0 0 ~ mathrm { H z }$ are transmitted by telephones. Thus, an $8 { , } 0 0 0 ~ mathrm { H z }$ sampling rate is sufficient for telephone-bandwidth speech, and $1 6 { , } 0 0 0 mathrm { { H z } }$ for microphone speech. \ntelephonebandwidth \nquantization \nAlthough using higher sampling rates produces higher ASR accuracy, we can’t combine different sampling rates for training and testing ASR systems. Thus if we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must downsample our training corpus to $8 ~ mathrm { K H z }$ . Similarly, if we are training on multiple corpora and one of them includes telephone speech, we downsample all the wideband corpora to 8Khz. \nAmplitude measurements are stored as integers, either 8 bit (values from $- 1 2 8 -$ 127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization; all values that are closer together than the minimum granularity (the quantum size) are represented identically. We refer to each sample at time index $n$ in the digitized, quantized waveform as $x [ n ]$ . \n26.2.2 Windowing \nstationary non-stationary \nFrom the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. Inside this small window, we can roughly think of the signal as stationary (that is, its statistical properties are constant within this region). (By contrast, in general, speech is a non-stationary signal, meaning that its statistical properties are not constant over time). We extract this roughly stationary portion of speech by using a window which is non-zero inside a region and zero elsewhere, running this window across the speech signal and multiplying it by the input waveform to produce a windowed waveform. \nframe \nstride \nThe speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. \nTo extract the signal we multiply the value of the signal at time $n , s [ n ]$ by the value of the window at time $n , w [ n ]$ : \nrectangular \nThe window shape sketched in Fig. 26.2 is rectangular; you can see the extracted windowed signal looks just like the original signal. The rectangular window, however, abruptly cuts off the signal at its boundaries, which creates problems when we do Fourier analysis. For this reason, for acoustic feature creation we more commonly use the Hamming window, which shrinks the values of the signal toward \nHamming \nzero at the window boundaries, avoiding discontinuities. Figure 26.3 shows both; the equations are as follows (assuming a window that is $L$ frames long): \n26.2.3 Discrete Fourier Transform \nThe next step is to extract spectral information for our windowed signal; we need to know how much energy the signal contains at different frequency bands. The tool for extracting spectral information for discrete frequency bands for a discrete-time (sampled) signal is the discrete Fourier transform or DFT.",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Feature Extraction for ASR: Log Mel Spectrum",
        "subsection": "Windowing",
        "subsubsection": "N/A"
    },
    {
        "content": "zero at the window boundaries, avoiding discontinuities. Figure 26.3 shows both; the equations are as follows (assuming a window that is $L$ frames long): \n26.2.3 Discrete Fourier Transform \nThe next step is to extract spectral information for our windowed signal; we need to know how much energy the signal contains at different frequency bands. The tool for extracting spectral information for discrete frequency bands for a discrete-time (sampled) signal is the discrete Fourier transform or DFT. \nThe input to the DFT is a windowed signal $x [ n ] . . . x [ m ]$ , and the output, for each of $N$ discrete frequency bands, is a complex number $X [ k ]$ representing the magnitude and phase of that frequency component in the original signal. If we plot the magnitude against the frequency, we can visualize the spectrum that we introduced in Chapter 25. For example, Fig. 26.4 shows a $2 5 ~ mathrm { m s }$ Hamming-windowed portion of a signal and its spectrum as computed by a DFT (with some additional smoothing). \nEuler’s formula \nWe do not introduce the mathematical details of the DFT here, except to note that Fourier analysis relies on Euler’s formula, with $j$ as the imaginary unit: \nAs a brief reminder for those students who have already studied signal processing, the DFT is defined as follows: \nfast Fourier transform FFT \nA commonly used algorithm for computing the DFT is the fast Fourier transform or FFT. This implementation of the DFT is very efficient but only works for values of $N$ that are powers of 2. \n26.2.4 Mel Filter Bank and Log \nThe results of the FFT tell us the energy at each frequency band. Human hearing, however, is not equally sensitive at all frequency bands; it is less sensitive at higher frequencies. This bias toward low frequencies helps human recognition, since information in low frequencies like formants is crucial for distinguishing values or nasals, while information in high frequencies like stop bursts or fricative noise is less crucial for successful recognition. Modeling this human perceptual property improves speech recognition performance in the same way. \nmel \nWe implement this intuition by by collecting energies, not equally at each frequency band, but according to the mel scale, an auditory frequency scale (Chapter 25). A mel (Stevens et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of sounds that are perceptually equidistant in pitch are separated by an equal number of mels. The mel frequency $m$ can be computed from the raw acoustic frequency by a log transformation:",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Feature Extraction for ASR: Log Mel Spectrum",
        "subsection": "Discrete Fourier Transform",
        "subsubsection": "N/A"
    },
    {
        "content": "The input to the DFT is a windowed signal $x [ n ] . . . x [ m ]$ , and the output, for each of $N$ discrete frequency bands, is a complex number $X [ k ]$ representing the magnitude and phase of that frequency component in the original signal. If we plot the magnitude against the frequency, we can visualize the spectrum that we introduced in Chapter 25. For example, Fig. 26.4 shows a $2 5 ~ mathrm { m s }$ Hamming-windowed portion of a signal and its spectrum as computed by a DFT (with some additional smoothing). \nEuler’s formula \nWe do not introduce the mathematical details of the DFT here, except to note that Fourier analysis relies on Euler’s formula, with $j$ as the imaginary unit: \nAs a brief reminder for those students who have already studied signal processing, the DFT is defined as follows: \nfast Fourier transform FFT \nA commonly used algorithm for computing the DFT is the fast Fourier transform or FFT. This implementation of the DFT is very efficient but only works for values of $N$ that are powers of 2. \n26.2.4 Mel Filter Bank and Log \nThe results of the FFT tell us the energy at each frequency band. Human hearing, however, is not equally sensitive at all frequency bands; it is less sensitive at higher frequencies. This bias toward low frequencies helps human recognition, since information in low frequencies like formants is crucial for distinguishing values or nasals, while information in high frequencies like stop bursts or fricative noise is less crucial for successful recognition. Modeling this human perceptual property improves speech recognition performance in the same way. \nmel \nWe implement this intuition by by collecting energies, not equally at each frequency band, but according to the mel scale, an auditory frequency scale (Chapter 25). A mel (Stevens et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of sounds that are perceptually equidistant in pitch are separated by an equal number of mels. The mel frequency $m$ can be computed from the raw acoustic frequency by a log transformation: \nWe implement this intuition by creating a bank of filters that collect energy from each frequency band, spread logarithmically so that we have very fine resolution at low frequencies, and less resolution at high frequencies. Figure 26.5 shows a sample bank of triangular filters that implement this idea, that can be multiplied by the spectrum to get a mel spectrum. \nFinally, we take the log of each of the mel spectrum values. The human response to signal level is logarithmic (like the human response to frequency). Humans are less sensitive to slight differences in amplitude at high amplitudes than at low amplitudes. In addition, using a log makes the feature estimates less sensitive to variations in input such as power variations due to the speaker’s mouth moving closer or further from the microphone. \n26.3 Speech Recognition Architecture \nAED listen attend and spell \nThe basic architecture for ASR is the encoder-decoder (implemented with either RNNs or Transformers), exactly the same architecture introduced for MT in Chapter 10. Generally we start from the log mel spectral features described in the previous section, and map to letters, although it’s also possible to map to induced morphemelike chunks like wordpieces or BPE. \nFig. 26.6 sketches the standard encoder-decoder architecture, which is commonly referred to as the attention-based encoder decoder or AED, or listen attend and spell (LAS) after the two papers which first applied it to speech (Chorowski et al. 2014, Chan et al. 2016). The input is a sequence of $t$ acoustic feature vectors $F = f _ { 1 } , f _ { 2 } , . . . , f _ { t }$ , one vector per $1 0 ~ mathrm { { m s } }$ frame. The output can be letters or wordpieces; we’ll assume letters here. Thus the output sequence $Y = ( langle mathrm { S O S } rangle , y _ { 1 } , . . . , y _ { m } langle mathrm { E O S } rangle )$ , assuming special start of sequence and end of sequence tokens $langle cos rangle$ and $left. cos right.$ and each $y _ { i }$ is a character; for English we might choose the set: \nOf course the encoder-decoder architecture is particularly appropriate when input and output sequences have stark length differences, as they do for speech, with very long acoustic feature sequences mapping to much shorter sequences of letters or words. A single word might be 5 letters long but, supposing it lasts about 2 seconds, would take 200 acoustic frames (of 10ms each). \nBecause this length difference is so extreme for speech, encoder-decoder architectures for speech need to have a special compression stage that shortens the acoustic feature sequence before the encoder stage. (Alternatively, we can use a loss function that is designed to deal well with compression, like the CTC loss function we’ll introduce in the next section.)",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Feature Extraction for ASR: Log Mel Spectrum",
        "subsection": "Mel Filter Bank and Log",
        "subsubsection": "N/A"
    },
    {
        "content": "26.3.1 Learning \nEncoder-decoders for speech are trained with the normal cross-entropy loss generally used for conditional language models. At timestep $i$ of decoding, the loss is the log probability of the correct token (letter) $y _ { i }$ : \nThe loss for the entire sentence is the sum of these losses: \nThis loss is then backpropagated through the entire end-to-end model to train the entire encoder-decoder. \nAs we described in Chapter 10, we normally use teacher forcing, in which the decoder history is forced to be the correct gold $y _ { i }$ rather than the predicted $hat { y } _ { i }$ . It’s also possible to use a mixture of the gold and decoder output, for example using the gold output $90 %$ of the time, but with probability .1 taking the decoder output instead: \n26.4 CTC \nWe pointed out in the previous section that speech recognition has two particular properties that make it very appropriate for the encoder-decoder architecture, where the encoder produces an encoding of the input that the decoder uses attention to explore. First, in speech we have a very long acoustic input sequence $X$ mapping to a much shorter sequence of letters $Y$ , and second, it’s hard to know exactly which part of $X$ maps to which part of $Y$ . \nIn this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that the output is the same length as the input, and then to apply a collapsing function that combines sequences of identical letters, resulting in a shorter sequence. \nLet’s imagine inference on someone saying the word dinner, and let’s suppose we had a function that chooses the most probable letter for each input spectral frame representation $x _ { i }$ . We’ll call the sequence of letters corresponding to each input",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Speech Recognition Architecture",
        "subsection": "Learning",
        "subsubsection": "N/A"
    },
    {
        "content": "Y . We’ll use the inverse of our $B$ function, called $B ^ { - 1 }$ , and represent that set as $B ^ { - 1 } ( Y )$ . \n26.4.1 CTC Inference \nBefore we see how to compute $P _ { mathrm { C T C } } ( Y | X )$ let’s first see how CTC assigns a probability to one particular alignment $hat { A } = { hat { a } _ { 1 } , dots , hat { a } _ { n } }$ . CTC makes a strong conditional independence assumption: it assumes that, given the input $X$ , the CTC model output $a _ { t }$ at time $t$ is independent of the output labels at any other time $a _ { i }$ . Thus: \nThus to find the best alignment $hat { A } = { hat { a } _ { 1 } , dots , hat { a } _ { T } }$ we can greedily choose the character with the max probability at each time step $t$ : \nWe then pass the resulting sequence $A$ to the CTC collapsing function $B$ to get the output sequence $Y$ . \nLet’s talk about how this simple inference algorithm for finding the best alignment A would be implemented. Because we are making a decision at each time point, we can treat CTC as a sequence-modeling task, where we output one letter $hat { y } _ { t }$ at time $t$ corresponding to each input token $x _ { t }$ , eliminating the need for a full decoder. Fig. 26.10 sketches this architecture, where we take an encoder, produce a hidden state $h _ { t }$ at each timestep, and decode by taking a softmax over the character vocabulary at each time step. \nAlas, there is a potential flaw with the inference algorithm sketched in (Eq. 26.14) and Fig. 26.9. The problem is that we chose the most likely alignment $A$ , but the most likely alignment may not correspond to the most likely final collapsed output string Y . That’s because there are many possible alignments that lead to the same output string, and hence the most likely output string might not correspond to the most probable alignment. For example, imagine the most probable alignment $A$ for an input $X = left[ x _ { 1 } x _ { 2 } x _ { 3 } right]$ is the string [a b $epsilon ]$ but the next two most probable alignments are $[ mathsf { b } epsilon mathsf { b } ]$ and $[ epsilon  b  b ]$ . The output $Y = [ { mathsf { b } }  { mathsf { b } } ]$ , summing over those two alignments, might be more probable than $Y = [ mathrm { a }  b ]$ . \n\nFor this reason, the most probable output sequence $Y$ is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments: \nAlas, summing over all alignments is very expensive (there are a lot of alignments), so we approximate this sum by using a version of Viterbi beam search that cleverly keeps in the beam the high-probability alignments that map to the same output string, and sums those as an approximation of (Eq. 26.15). See Hannun (2017) for a clear explanation of this extension of beam search for CTC. \nBecause of the strong conditional independence assumption mentioned earlier (that the output at time $t$ is independent of the output at time $t - 1$ , given the input), CTC does not implicitly learn a language model over the data (unlike the attentionbased encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model (and some sort of length factor $L ( Y ) _ { cdot } ^ { cdot }$ ) using interpolation weights that are trained on a dev set: \n26.4.2 CTC Training \nTo train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset $D$ is the sum of the negative log-likelihoods of the correct output $Y$ for each input $X$ : \nTo compute CTC loss function for a single input pair $( X , Y )$ , we need the probability of the output $Y$ given the input $X$ . As we saw in Eq. 26.15, to compute the probability of a given output $Y$ we need to sum over all the possible alignments that would collapse to $Y$ . In other words: \nNaively summing over all possible alignments is not feasible (there are too many alignments). However, we can efficiently compute the sum by using dynamic programming to merge alignments, with a version of the forward-backward algorithm also used to train HMMs (Appendix A) and CRFs. The original dynamic programming algorithms for both training and inference are laid out in (Graves et al., 2006); see (Hannun, 2017) for a detailed explanation of both.",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "CTC",
        "subsection": "CTC Inference",
        "subsubsection": "N/A"
    },
    {
        "content": "For this reason, the most probable output sequence $Y$ is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments: \nAlas, summing over all alignments is very expensive (there are a lot of alignments), so we approximate this sum by using a version of Viterbi beam search that cleverly keeps in the beam the high-probability alignments that map to the same output string, and sums those as an approximation of (Eq. 26.15). See Hannun (2017) for a clear explanation of this extension of beam search for CTC. \nBecause of the strong conditional independence assumption mentioned earlier (that the output at time $t$ is independent of the output at time $t - 1$ , given the input), CTC does not implicitly learn a language model over the data (unlike the attentionbased encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model (and some sort of length factor $L ( Y ) _ { cdot } ^ { cdot }$ ) using interpolation weights that are trained on a dev set: \n26.4.2 CTC Training \nTo train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset $D$ is the sum of the negative log-likelihoods of the correct output $Y$ for each input $X$ : \nTo compute CTC loss function for a single input pair $( X , Y )$ , we need the probability of the output $Y$ given the input $X$ . As we saw in Eq. 26.15, to compute the probability of a given output $Y$ we need to sum over all the possible alignments that would collapse to $Y$ . In other words: \nNaively summing over all possible alignments is not feasible (there are too many alignments). However, we can efficiently compute the sum by using dynamic programming to merge alignments, with a version of the forward-backward algorithm also used to train HMMs (Appendix A) and CRFs. The original dynamic programming algorithms for both training and inference are laid out in (Graves et al., 2006); see (Hannun, 2017) for a detailed explanation of both. \n26.4.3 Combining CTC and Encoder-Decoder \nIt’s also possible to combine the two architectures/loss functions we’ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 26.11 shows a sketch. For training, we can can simply weight the two losses with a $lambda$ tuned on a dev set: \nFor inference, we can combine the two with the language model (or the length penalty), again with learned weights: \nstreaming \n26.4.4 Streaming Models: RNN-T for improving CTC \nBecause of the strong independence assumption in CTC (assuming that the output at time $t$ is independent of the output at time $t - 1$ ), recognizers based on CTC don’t achieve as high an accuracy as the attention-based encoder-decoder recognizers. CTC recognizers have the advantage, however, that they can be used for streaming. Streaming means recognizing words on-line rather than waiting until the end of the sentence to recognize them. Streaming is crucial for many applications, from commands to dictation, where we want to start recognition while the user is still talking. Algorithms that use attention need to compute the hidden state sequence over the entire input first in order to provide the attention distribution context, before the decoder can start decoding. By contrast, a CTC algorithm can input letters from left to right immediately. \nIf we want to do streaming, we need a way to improve CTC recognition to remove the conditional independent assumption, enabling it to know about output history. The RNN-Transducer (RNN-T), shown in Fig. 26.12, is just such a model (Graves 2012, Graves et al. 2013a). The RNN-T has two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history. At each time step $t$ , the CTC encoder outputs a hidden state $h _ { t } ^ { mathrm { e n c } }$ given the input $x _ { 1 } . . . x _ { t }$ . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpured. The two are passed through another network whose output is then passed through a",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "CTC",
        "subsection": "CTC Training",
        "subsubsection": "N/A"
    },
    {
        "content": "26.4.3 Combining CTC and Encoder-Decoder \nIt’s also possible to combine the two architectures/loss functions we’ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 26.11 shows a sketch. For training, we can can simply weight the two losses with a $lambda$ tuned on a dev set: \nFor inference, we can combine the two with the language model (or the length penalty), again with learned weights: \nstreaming \n26.4.4 Streaming Models: RNN-T for improving CTC \nBecause of the strong independence assumption in CTC (assuming that the output at time $t$ is independent of the output at time $t - 1$ ), recognizers based on CTC don’t achieve as high an accuracy as the attention-based encoder-decoder recognizers. CTC recognizers have the advantage, however, that they can be used for streaming. Streaming means recognizing words on-line rather than waiting until the end of the sentence to recognize them. Streaming is crucial for many applications, from commands to dictation, where we want to start recognition while the user is still talking. Algorithms that use attention need to compute the hidden state sequence over the entire input first in order to provide the attention distribution context, before the decoder can start decoding. By contrast, a CTC algorithm can input letters from left to right immediately. \nIf we want to do streaming, we need a way to improve CTC recognition to remove the conditional independent assumption, enabling it to know about output history. The RNN-Transducer (RNN-T), shown in Fig. 26.12, is just such a model (Graves 2012, Graves et al. 2013a). The RNN-T has two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history. At each time step $t$ , the CTC encoder outputs a hidden state $h _ { t } ^ { mathrm { e n c } }$ given the input $x _ { 1 } . . . x _ { t }$ . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpured. The two are passed through another network whose output is then passed through a",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "CTC",
        "subsection": "Combining CTC and Encoder-Decoder",
        "subsubsection": "N/A"
    },
    {
        "content": "26.4.3 Combining CTC and Encoder-Decoder \nIt’s also possible to combine the two architectures/loss functions we’ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 26.11 shows a sketch. For training, we can can simply weight the two losses with a $lambda$ tuned on a dev set: \nFor inference, we can combine the two with the language model (or the length penalty), again with learned weights: \nstreaming \n26.4.4 Streaming Models: RNN-T for improving CTC \nBecause of the strong independence assumption in CTC (assuming that the output at time $t$ is independent of the output at time $t - 1$ ), recognizers based on CTC don’t achieve as high an accuracy as the attention-based encoder-decoder recognizers. CTC recognizers have the advantage, however, that they can be used for streaming. Streaming means recognizing words on-line rather than waiting until the end of the sentence to recognize them. Streaming is crucial for many applications, from commands to dictation, where we want to start recognition while the user is still talking. Algorithms that use attention need to compute the hidden state sequence over the entire input first in order to provide the attention distribution context, before the decoder can start decoding. By contrast, a CTC algorithm can input letters from left to right immediately. \nIf we want to do streaming, we need a way to improve CTC recognition to remove the conditional independent assumption, enabling it to know about output history. The RNN-Transducer (RNN-T), shown in Fig. 26.12, is just such a model (Graves 2012, Graves et al. 2013a). The RNN-T has two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history. At each time step $t$ , the CTC encoder outputs a hidden state $h _ { t } ^ { mathrm { e n c } }$ given the input $x _ { 1 } . . . x _ { t }$ . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpured. The two are passed through another network whose output is then passed through a \nsoftmax to predict the next character. \n26.5 ASR Evaluation: Word Error Rate \nword error \nThe standard evaluation metric for speech recognition systems is the word error rate. The word error rate is based on how much the word string returned by the recognizer (the hypothesized word string) differs from a reference transcription. The first step in computing word error is to compute the minimum edit distance in words between the hypothesized and correct strings, giving us the minimum number of word substitutions, word insertions, and word deletions necessary to map between the correct and hypothesized strings. The word error rate (WER) is then defined as follows (note that because the equation includes insertions, the error rate can be greater than $100 %$ ): \nHere is a sample alignment between a reference and a hypothesis utterance from the CallHome corpus, showing the counts used to compute the error rate: \nThis utterance has six substitutions, three insertions, and one deletion: \nThe standard method for computing word error rates is a free script called sclite, available from the National Institute of Standards and Technologies (NIST) (NIST,",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "CTC",
        "subsection": "Streaming Models: RNN-T for improving CTC",
        "subsubsection": "N/A"
    },
    {
        "content": "softmax to predict the next character. \n26.5 ASR Evaluation: Word Error Rate \nword error \nThe standard evaluation metric for speech recognition systems is the word error rate. The word error rate is based on how much the word string returned by the recognizer (the hypothesized word string) differs from a reference transcription. The first step in computing word error is to compute the minimum edit distance in words between the hypothesized and correct strings, giving us the minimum number of word substitutions, word insertions, and word deletions necessary to map between the correct and hypothesized strings. The word error rate (WER) is then defined as follows (note that because the equation includes insertions, the error rate can be greater than $100 %$ ): \nHere is a sample alignment between a reference and a hypothesis utterance from the CallHome corpus, showing the counts used to compute the error rate: \nThis utterance has six substitutions, three insertions, and one deletion: \nThe standard method for computing word error rates is a free script called sclite, available from the National Institute of Standards and Technologies (NIST) (NIST, \n2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sentences and a matching set of hypothesis sentences. Besides performing alignments, and computing word error rate, sclite performs a number of other useful tasks. For example, for error analysis it gives useful information such as confusion matrices showing which words are often misrecognized for others, and summarizes statistics of words that are often inserted or deleted. sclite also gives error rates by speaker (if sentences are labeled for speaker ID), as well as useful statistics like the sentence error rate, the percentage of sentences with at least one word error. \nStatistical significance for ASR: MAPSSWE or MacNemar \nAs with other language processing algorithms, we need to know whether a particular improvement in word error rate is significant or not. \nThe standard statistical tests for determining if two word error rates are different is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989). \nThe MAPSSWE test is a parametric test that looks at the difference between the number of word errors the two systems produce, averaged across a number of segments. The segments may be quite short or as long as an entire utterance; in general, we want to have the largest number of (short) segments in order to justify the normality assumption and to maximize power. The test requires that the errors in one segment be statistically independent of the errors in another segment. Since ASR systems tend to use trigram LMs, we can approximate this requirement by defining a segment as a region bounded on both sides by words that both recognizers get correct (or by turn/utterance boundaries). Here’s an example from NIST (2007) with four regions: \nI II III IV REF: |it was|the best|of|times it|was the worst|of times| |it was SYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was SYS B:|it was|the best| |times it|WON the TEST |of times| |it was \nIn region I, system A has two errors (a deletion and an insertion) and system B has zero; in region III, system A has one error (a substitution) and system B has two. Let’s define a sequence of variables $Z$ representing the difference between the errors in the two systems as follows: \n${ N } _ { A } ^ { i }$ the number of errors made on segment $i$ by system A $N _ { B } ^ { i }$ the number of errors made on segment $i$ by system $B$ $Z$ $N _ { A } ^ { i } - N _ { B } ^ { i } , i = 1 , 2 , cdots , n$ where $n$ is the number of segments \nIn the example above, the sequence of $Z$ values is ${ 2 , - 1 , - 1 , 1 }$ . Intuitively, if the two systems are identical, we would expect the average difference, that is, the average of the $Z$ values, to be zero. If we call the true average of the differences $m u _ { z }$ , we would thus like to know whether $m u _ { z } = 0$ . Following closely the original proposal and notation of Gillick and Cox (1989), we can estimate the true average from our limited sample as $textstyle { hat { mu } } _ { z } = sum _ { i = 1 } ^ { n } Z _ { i } / n$ . The estimate of the variance of the $Z _ { i }$ ’s is \nLet \nMcNemar’s test \nFor a large enough n $( > 5 0 )$ , $W$ will approximately have a normal distribution with unit variance. The null hypothesis is $H _ { 0 } : mu _ { z } = 0$ , and it can thus be rejected if $2 * P ( Z geq | w | ) leq 0 . 0 5$ (two-tailed) or $P ( Z geq | w | ) leq 0 . 0 5$ (one-tailed), where $Z$ is standard normal and $w$ is the realized value $W$ ; these probabilities can be looked up in the standard tables of the normal distribution. \nEarlier work sometimes used McNemar’s test for significance, but McNemar’s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words. \nCould we improve on word error rate as a metric? It would be nice, for example, to have something that didn’t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 24 on page 548. \n26.6 TTS \nThe goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that’s important for a variety of applications from dialogue systems to games to education. \nLike ASR systems, TTS systems are generally based on the encoder-decoder architecture, either using LSTMs or Transformers. There is a general difference in training. The default condition for ASR systems is to be speaker-independent: they are trained on large corpora with thousands of hours of speech from many speakers because they must generalize well to an unseen test speaker. By contrast, in TTS, it’s less crucial to use multiple voices, and so basic TTS systems are speaker-dependent: trained to have a consistent voice, on much less data, but all from one speaker. For example, one commonly used public domain dataset, the LJ speech corpus, consists of 24 hours of one speaker, Linda Johnson, reading audio books in the LibriVox project (Ito and Johnson, 2017), much smaller than standard ASR corpora which are hundreds or thousands of hours.2 \nWe generally break up the TTS task into two components. The first component is an encoder-decoder model for spectrogram prediction: it maps from strings of letters to mel spectrographs: sequences of mel spectral values over time. Thus we",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "ASR Evaluation: Word Error Rate",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "might map from this string: \nIt’s time for lunch! \nto the following mel spectrogram: \nvocoding vocoder \nThe second component maps from mel spectrograms to waveforms. Generating waveforms from intermediate representations like spectrograms is called vocoding and this second component is called a vocoder: \nThese standard encoder-decoder algorithms for TTS are still quite computationally intensive, so a significant focus of modern research is on ways to speed them up. \nnon-standard words \n26.6.1 TTS Preprocessing: Text normalization \nBefore either of these two steps, however, TTS systems require text normalization preprocessing for handling non-standard words: numbers, monetary amounts, dates, and other concepts that are verbalized differently than they are spelled. A TTS system seeing a number like 151 needs to know to verbalize it as one hundred fifty one if it occurs as $$ 151$ but as one fifty one if it occurs in the context 151 Chapultepec Ave.. The number 1750 can be spoken in at least four different ways, depending on the context: \nseventeen fifty: (in “The European economy in 1750”) one seven five zero: (in “The password is 1750”) seventeen hundred and fifty: (in “1750 dollars”) one thousand, seven hundred, and fifty: (in “1750 dollars”) \nOften the verbalization of a non-standard word depends on its meaning (what Taylor (2009) calls its semiotic class). Fig. 26.13 lays out some English nonstandard word types. \nMany classes have preferred realizations. A year is generally read as paired digits (e.g., seventeen fifty for 1750). $$ 3.2$ billion must be read out with the word dollars at the end, as three point two billion dollars. Some abbreviations like N.Y. are expanded (to New York), while other acronyms like GPU are pronounced as letter sequences. In languages with grammatical gender, normalization may depend on morphological properties. In French, the phrase 1 mangue (‘one mangue’) is normalized to une mangue, but 1 ananas (‘one pineapple’) is normalized to un ananas. In German, Heinrich IV (‘Henry IV’) can be normalized to Heinrich der Vierte, Heinrich des Vierten, Heinrich dem Vierten, or Heinrich den Vierten depending on the grammatical case of the noun (Demberg, 2006). \nModern end-to-end TTS systems can learn to do some normalization themselves, but TTS systems are only trained on a limited amount of data (like the 220,000 words we mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate normalization step is important. \nNormalization can be done by rule or by an encoder-decoder model. Rule-based normalization is done in two stages: tokenization and verbalization. In the tokenization stage we hand-write write rules to detect non-standard words. These can be regular expressions, like the following for detecting years: \n/(1[89][0-9][0-9]) (20[0-9][0-9]/ \nA second pass of rules express how to verbalize each semiotic class. Larger TTS systems instead use more complex rule-systems, like the Kestral system of (Ebden and Sproat, 2015), which first classifies and parses each input into a normal form and then produces text using a verbalization grammar. Rules have the advantage that they don’t require training data, and they can be designed for high precision, but can be brittle, and require expert rule-writers so are hard to maintain. \nThe alternative model is to use encoder-decoder models, which have been shown to work better than rules for such transduction tasks, but do require expert-labeled training sets in which non-standard words have been replaced with the appropriate verbalization; such training sets for some languages are available (Sproat and Gorman 2018, Zhang et al. 2019). \nIn the simplest encoder-decoder setting, we simply treat the problem like machine translation, training a system to map from: \nThey live at 224 Mission St. \nto \nThey live at two twenty four Mission Street \nWhile encoder-decoder algorithms are highly accurate, they occasionally produce errors that are egregious; for example normalizing 45 minutes as forty five millimeters. To address this, more complex systems use mechanisms like lightweight covering grammars, which enumerate a large set of possible verbalizations but don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang et al., 2019). \n26.6.2 TTS: Spectrogram prediction \nThe exact same architecture we described for ASR—the encoder-decoder with attention– can be used for the first component of TTS. Here we’ll give a simplified overview",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "TTS",
        "subsection": "TTS Preprocessing: Text normalization",
        "subsubsection": "N/A"
    },
    {
        "content": "Modern end-to-end TTS systems can learn to do some normalization themselves, but TTS systems are only trained on a limited amount of data (like the 220,000 words we mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate normalization step is important. \nNormalization can be done by rule or by an encoder-decoder model. Rule-based normalization is done in two stages: tokenization and verbalization. In the tokenization stage we hand-write write rules to detect non-standard words. These can be regular expressions, like the following for detecting years: \n/(1[89][0-9][0-9]) (20[0-9][0-9]/ \nA second pass of rules express how to verbalize each semiotic class. Larger TTS systems instead use more complex rule-systems, like the Kestral system of (Ebden and Sproat, 2015), which first classifies and parses each input into a normal form and then produces text using a verbalization grammar. Rules have the advantage that they don’t require training data, and they can be designed for high precision, but can be brittle, and require expert rule-writers so are hard to maintain. \nThe alternative model is to use encoder-decoder models, which have been shown to work better than rules for such transduction tasks, but do require expert-labeled training sets in which non-standard words have been replaced with the appropriate verbalization; such training sets for some languages are available (Sproat and Gorman 2018, Zhang et al. 2019). \nIn the simplest encoder-decoder setting, we simply treat the problem like machine translation, training a system to map from: \nThey live at 224 Mission St. \nto \nThey live at two twenty four Mission Street \nWhile encoder-decoder algorithms are highly accurate, they occasionally produce errors that are egregious; for example normalizing 45 minutes as forty five millimeters. To address this, more complex systems use mechanisms like lightweight covering grammars, which enumerate a large set of possible verbalizations but don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang et al., 2019). \n26.6.2 TTS: Spectrogram prediction \nThe exact same architecture we described for ASR—the encoder-decoder with attention– can be used for the first component of TTS. Here we’ll give a simplified overview \nTacotron2 Wavenet \nlocation-based attention \nof the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron (Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., 2016). Fig. 26.14 sketches out the entire architecture. \nThe encoder’s job is to take a sequence of letters and produce a hidden representation representing the letter sequence, which is then used by the attention mechanism in the decoder. The Tacotron2 encoder first maps every input grapheme to a 512-dimensional character embedding. These are then passed through a stack of 3 convolutional layers, each containing 512 filters with shape $5 times 1$ , i.e. each filter spanning 5 characters, to model the larger letter context. The output of the final convolutional layer is passed through a biLSTM to produce the final encoding. It’s common to use a slightly higher quality (but slower) version of attention called location-based attention, in which the computation of the $alpha$ values (Eq. 10.16 in Chapter 10) makes use of the $alpha$ values from the prior time-state. \nIn the decoder, the predicted mel spectrum from the prior time slot is passed through a small pre-net as a bottleneck. This prior output is then concatenated with the encoder’s attention vector context and passed through 2 LSTM layers. The output of this LSTM is used in two ways. First, it is passed through a linear layer, and some output processing, to autoregressively predict one 80-dimensional log-mel filterbank vector frame $5 0 mathrm { m s }$ , with a $1 2 . 5 mathrm { m s }$ stride) at each step. Second, it is passed through another linear layer to a sigmoid to make a “stop token prediction” decision about whether to stop producing output. \n,Tt heefiltseyrbsatnekm is traWienemidniomnizegthoel sdulmomge-dmealnfisqltuaerebdaernrokr (fMeaStE)ufrreosm, buefsoirneg teacher forcing, nt.hat is the decodewr tihsafleodg ltihkel hcordrleocsts blyo gm-odmeloindgetlhespouetcpturtadlisftreibauttuiornewaitheach decoder step idnesctoedeardwiothf attheen preadiMcitxetudredDeecnosidtyerNeotuwtorpku[t2f3,ro24m]  tohaevoipdriaossrusmtiengp.a \n26.6.3 ivoTonlaulT- S:deVcodecroLdSTinMgo \noWwaevdebNyetb \nnTohrmealvizoactiond[e1r8]for Tpraocbaobtirliotyn h2a itsheaonu paudtasepqtuaetnicoenhaos fcotmhpeleteWd.aTvheisN“settopvtokceon”der (van den Oord oentvoallu.t,io2na0l1la6y)e. Herperedwiceti’olnl sguisved daursinogminfe rwenhcaet osiallmowpltihfie emdo deletsocdryinpatmiiocanl yof vocoding using ayWeraisvepaNssetd.i \n5R12eucnailtsl (t2h5a6t thferagmoeaflorowfhtiche hvisopcrobdaibnilgityperxocecedssa therersehowldilolf 0b.e5 to invert a log mel specttironu nme rweorpkrewshiecnhtatidornospobuta[c2k5]iwnithoparotbiambileit-yd0o.5,manidnLSwTaMvelafyoersmarererepgruelasrieznetdation. WaveNet is eangn hacuotntoerxet vgercetosrsiveusninegt zwoneroku, l[i2k6]ewtitheprloabnabgiluitayg0e.1.mInodrdelrs owinet iodnutcreoodutpcuet d in Chapter 9. It takes spectrograms as input and produces audio output represented as sequences of 8-bit mu-law (page 565). The probability of a waveform , a sequence of 8-bit mulaw values $Y = y _ { 1 } , . . . , y _ { t }$ , given an intermediate input mel spectrogram $h$ is computed as:",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "TTS",
        "subsection": "TTS: Spectrogram prediction",
        "subsubsection": "N/A"
    },
    {
        "content": "Tacotron2 Wavenet \nlocation-based attention \nof the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron (Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., 2016). Fig. 26.14 sketches out the entire architecture. \nThe encoder’s job is to take a sequence of letters and produce a hidden representation representing the letter sequence, which is then used by the attention mechanism in the decoder. The Tacotron2 encoder first maps every input grapheme to a 512-dimensional character embedding. These are then passed through a stack of 3 convolutional layers, each containing 512 filters with shape $5 times 1$ , i.e. each filter spanning 5 characters, to model the larger letter context. The output of the final convolutional layer is passed through a biLSTM to produce the final encoding. It’s common to use a slightly higher quality (but slower) version of attention called location-based attention, in which the computation of the $alpha$ values (Eq. 10.16 in Chapter 10) makes use of the $alpha$ values from the prior time-state. \nIn the decoder, the predicted mel spectrum from the prior time slot is passed through a small pre-net as a bottleneck. This prior output is then concatenated with the encoder’s attention vector context and passed through 2 LSTM layers. The output of this LSTM is used in two ways. First, it is passed through a linear layer, and some output processing, to autoregressively predict one 80-dimensional log-mel filterbank vector frame $5 0 mathrm { m s }$ , with a $1 2 . 5 mathrm { m s }$ stride) at each step. Second, it is passed through another linear layer to a sigmoid to make a “stop token prediction” decision about whether to stop producing output. \n,Tt heefiltseyrbsatnekm is traWienemidniomnizegthoel sdulmomge-dmealnfisqltuaerebdaernrokr (fMeaStE)ufrreosm, buefsoirneg teacher forcing, nt.hat is the decodewr tihsafleodg ltihkel hcordrleocsts blyo gm-odmeloindgetlhespouetcpturtadlisftreibauttuiornewaitheach decoder step idnesctoedeardwiothf attheen preadiMcitxetudredDeecnosidtyerNeotuwtorpku[t2f3,ro24m]  tohaevoipdriaossrusmtiengp.a \n26.6.3 ivoTonlaulT- S:deVcodecroLdSTinMgo \noWwaevdebNyetb \nnTohrmealvizoactiond[e1r8]for Tpraocbaobtirliotyn h2a itsheaonu paudtasepqtuaetnicoenhaos fcotmhpeleteWd.aTvheisN“settopvtokceon”der (van den Oord oentvoallu.t,io2na0l1la6y)e. Herperedwiceti’olnl sguisved daursinogminfe rwenhcaet osiallmowpltihfie emdo deletsocdryinpatmiiocanl yof vocoding using ayWeraisvepaNssetd.i \n5R12eucnailtsl (t2h5a6t thferagmoeaflorowfhtiche hvisopcrobdaibnilgityperxocecedssa therersehowldilolf 0b.e5 to invert a log mel specttironu nme rweorpkrewshiecnhtatidornospobuta[c2k5]iwnithoparotbiambileit-yd0o.5,manidnLSwTaMvelafyoersmarererepgruelasrieznetdation. WaveNet is eangn hacuotntoerxet vgercetosrsiveusninegt zwoneroku, l[i2k6]ewtitheprloabnabgiluitayg0e.1.mInodrdelrs owinet iodnutcreoodutpcuet d in Chapter 9. It takes spectrograms as input and produces audio output represented as sequences of 8-bit mu-law (page 565). The probability of a waveform , a sequence of 8-bit mulaw values $Y = y _ { 1 } , . . . , y _ { t }$ , given an intermediate input mel spectrogram $h$ is computed as: \n\ninThFiisg.pr2otbhaebirleitcyepdtisvter bfiueltidoins osnlmyo5de(l=ed#lbay ears a+ckfiltoefrslpencgitahl -co1n).voIlnutihoisn lpaypers, which include a specific convolutional structure called dilated convolutions, and a specific non-linearity function. \ncAondviollatuetidonco(anlvso uctailolend `a ta osusb, yoprec onfvoclautisoanl cwoitnhvohloulteiso)niasl alacyoenr. lCutaiuosnalwohre smaapspklied covnevroalnutairoenas larogoekr tohnalny tast tehnegtphasbty isnkpiputp, nrgatihnepruthvanlutehsewfiutthuarec; trthaeinpsrtediction of $y _ { t + 1 }$ can only depend on $y _ { 1 } , . . . , y _ { t }$ , useful for autoregressive left-to-right processing. In dilated convolutions, at each successive layer we apply the convoluhteionuatlpfiutl ehrasotvheer saasmpeansilzoenagsert htheainpitust. eAnsg ah sbpyecsikailpcpaisneg, idnilpauttedvaclounesv.olTuthiuosnatwittihmde $t$ swthitehsatadnidlatridoncovnavlouleutoifon1., Ficgo.n3vodleuptioctnsaldifillaterdocfaluesnalg cho2nvwoloutliodnseefoirnpdiultatviaolnuses1 $x _ { t }$ ialnatd $x _ { t - 1 }$ .nvBoultu aiofinlstehravwei phreavdiiosutisllyatbioeennvualsueed ionf 2varwioulsdcsoknitpexatns  en.pg.uts, sgonawl opurlodc shene idneprutetval.u,es1 $x _ { t }$ 9a;ndD $x _ { t - 1 }$ e.uFxi,g.19286.91),5asnhdowims athge csoegmpeuntatatition o(fCthenouetpault. at2t0i1m5;e $t$ 2w0it1h6)4. dilated convolution layers with dilation values, 1, 2, 4, and 8. \nThe Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a dilileaptiroesnecryvicnleg tsihzeeinopfu6t,remseoaluntiinognthartotuhgehofiurts th6elnaeytewrsorhkaavse wdiellaltiaosncsoomfp1u,ta2t,io4n,a8l,e1f6fi, apnadpe3r,2.thaendi tahtieoneixstd6oulablyedrsfoargaeivnerhyalvaeyedri luaptitonsa loifm1i,t a2,nd4,th8e,n1r6e,paenatded3:2.e.gD.ilated convolutions allow the vocoder to grow the receptive field exponentially with depth. \nWaveNet predicts mu-law audio samples. Recall from page 565 that this is a standard compression for audio in which the values at each sampling timestep are compressed into 8-bits. This means that we can predict the value of each sample awtitvhe (ansoinm-lpilne a2r5)6c-owuantyercpaatertgofr iaca1l cl1a0s2s4ificeor.nvTolhuetiount.pSutecofntdh,estdaiclaktiendg tchoensveoblluoticoknss siesstthuesmpaosdseldctahpraocuitgyhaandsotfhtemraexcewpthiivcehfiemladkseiszet.his 256-way decision. \nThe spectrogram prediction encoder-decoder and the WaveNet vocoder are trained SseOpFaTraMtAelXy. IASfTtRerIBtUheT sOpNeSctrogram predictor is trained, the spectrogram prediction network is run in teacher-forcing mode, with each predicted spectral frame condiptipornoeadchotno hmeoednecliondgedthtexctoindpiutitoanald tdhisetrpirbeuvtionus fpr(axmte|fxro1,m. .t.h,exgtr−o1u)nodvterrutth espiencd-i trogram. This sequence of ground truth-aligned spectral features and gold audio output is then used to train the vocoder. \npTlihcits yhacsonbtieneunoousnl(yasa shitgheh-claesve l osrkietmcahgeofpitxhel iTntTeSnspitrioecseosrs. Tdihoe rseamarpelenvuamluersroeuas oinmspiosrthaant adectatielgsotrhiactalthdeisrtreiabduetirointiesremsotered iflnexgiobilenganfudrtchaenrmwoirteh eTaTsiSlymaoydewlaanrt to look into. For example WaveNet uses a special kind of a gated activation function as its non-linearity, and contains residual and skip connections. In practice, predicting 8-bit audio values doesn’t as work as well as 16-bit, for which a simple softmax is insufficient, so decoders use fancier ways as the last step of predicting audio sample values, like mixtures of distributions. Finally, the WaveNet vocoder as we have described it would be so slow as to be useless; many different kinds of efficiency improvements are necessary in practice, for example by finding ways to do non-autoregressive generation, avoiding the latency of having to wait to generate each frame until the prior frame has been generated, and instead making predictions in parallel. We encourage the interested reader to consult the original papers and various version of the code. \n\nAB tests \n26.6.4 TTS Evaluation \nSpeech synthesis systems are evaluated by human listeners. (The development of a good automatic metric for synthesis evaluation, one that would eliminate the need for expensive and time-consuming human listening experiments, remains an open and exciting research topic.) \nWe evaluate the quality of synthesized utterances by playing a sentence to listeners and ask them to give a mean opinion score (MOS), a rating of how good the synthesized utterances are, usually on a scale from 1–5. We can then compare systems by comparing their MOS scores on the same sentences (using, e.g., paired t-tests to test for significant differences). \nIf we are comparing exactly two systems (perhaps to see if a particular change actually improved the system), we can use AB tests. In AB tests, we play the same sentence synthesized by two different systems (an A and a B system). The human listeners choose which of the two utterances they like better. We do this for say 50 sentences (presented in random order) and compare the number of sentences preferred for each system. \n26.7 Other Speech Tasks \nwake word \nWhile we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. \nspeaker diarization \nThe task of wake word detection is to detect a word or short phrase, usually in order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant. The goal with wake words is build the detection into small devices at the computing edge, to maintain privacy by transmitting the least amount of user speech to a cloudbased server. Thus wake word detectors need to be fast, small footprint software that can fit into embedded devices. Wake word detectors usually use the same frontend feature extraction we saw for ASR, often followed by a whole-word classifier. \nSpeaker diarization is the task of determining ‘who spoke when’ in a long multi-speaker audio recording, marking the start and end of each speaker’s turns in the interaction. This can be useful for transcribing meetings, classroom speech, or medical interactions. Often diarization systems use voice activity detection (VAD) to find segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker. More recent work is investigating end-to-end algorithms to map directly from input speech to a sequence of speaker labels for each frame.",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "TTS",
        "subsection": "TTS: Vocoding",
        "subsubsection": "N/A"
    },
    {
        "content": "AB tests \n26.6.4 TTS Evaluation \nSpeech synthesis systems are evaluated by human listeners. (The development of a good automatic metric for synthesis evaluation, one that would eliminate the need for expensive and time-consuming human listening experiments, remains an open and exciting research topic.) \nWe evaluate the quality of synthesized utterances by playing a sentence to listeners and ask them to give a mean opinion score (MOS), a rating of how good the synthesized utterances are, usually on a scale from 1–5. We can then compare systems by comparing their MOS scores on the same sentences (using, e.g., paired t-tests to test for significant differences). \nIf we are comparing exactly two systems (perhaps to see if a particular change actually improved the system), we can use AB tests. In AB tests, we play the same sentence synthesized by two different systems (an A and a B system). The human listeners choose which of the two utterances they like better. We do this for say 50 sentences (presented in random order) and compare the number of sentences preferred for each system. \n26.7 Other Speech Tasks \nwake word \nWhile we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. \nspeaker diarization \nThe task of wake word detection is to detect a word or short phrase, usually in order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant. The goal with wake words is build the detection into small devices at the computing edge, to maintain privacy by transmitting the least amount of user speech to a cloudbased server. Thus wake word detectors need to be fast, small footprint software that can fit into embedded devices. Wake word detectors usually use the same frontend feature extraction we saw for ASR, often followed by a whole-word classifier. \nSpeaker diarization is the task of determining ‘who spoke when’ in a long multi-speaker audio recording, marking the start and end of each speaker’s turns in the interaction. This can be useful for transcribing meetings, classroom speech, or medical interactions. Often diarization systems use voice activity detection (VAD) to find segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker. More recent work is investigating end-to-end algorithms to map directly from input speech to a sequence of speaker labels for each frame.",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "TTS",
        "subsection": "TTS Evaluation",
        "subsubsection": "N/A"
    },
    {
        "content": "AB tests \n26.6.4 TTS Evaluation \nSpeech synthesis systems are evaluated by human listeners. (The development of a good automatic metric for synthesis evaluation, one that would eliminate the need for expensive and time-consuming human listening experiments, remains an open and exciting research topic.) \nWe evaluate the quality of synthesized utterances by playing a sentence to listeners and ask them to give a mean opinion score (MOS), a rating of how good the synthesized utterances are, usually on a scale from 1–5. We can then compare systems by comparing their MOS scores on the same sentences (using, e.g., paired t-tests to test for significant differences). \nIf we are comparing exactly two systems (perhaps to see if a particular change actually improved the system), we can use AB tests. In AB tests, we play the same sentence synthesized by two different systems (an A and a B system). The human listeners choose which of the two utterances they like better. We do this for say 50 sentences (presented in random order) and compare the number of sentences preferred for each system. \n26.7 Other Speech Tasks \nwake word \nWhile we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. \nspeaker diarization \nThe task of wake word detection is to detect a word or short phrase, usually in order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant. The goal with wake words is build the detection into small devices at the computing edge, to maintain privacy by transmitting the least amount of user speech to a cloudbased server. Thus wake word detectors need to be fast, small footprint software that can fit into embedded devices. Wake word detectors usually use the same frontend feature extraction we saw for ASR, often followed by a whole-word classifier. \nSpeaker diarization is the task of determining ‘who spoke when’ in a long multi-speaker audio recording, marking the start and end of each speaker’s turns in the interaction. This can be useful for transcribing meetings, classroom speech, or medical interactions. Often diarization systems use voice activity detection (VAD) to find segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker. More recent work is investigating end-to-end algorithms to map directly from input speech to a sequence of speaker labels for each frame. \nspeaker recognition \nSpeaker recognition, is the task of identifying a speaker. We generally distinguish the subtasks of speaker verification, where we make a binary decision (is this speaker $X$ or not?), such as for security when accessing personal information over the telephone, and speaker identification, where we make a one of $N$ decision trying to match a speaker’s voice against a database of many speakers . These tasks are related to language identification, in which we are given a wavefile and must identify which language is being spoken; this is useful for example for automatically directing callers to human operators that speak appropriate languages. \n26.8 Summary \nThis chapter introduced the fundamental algorithms of automatic speech recognition (ASR) and text-to-speech (TTS). \n• The task of speech recognition (or speech-to-text) is to map acoustic waveforms to sequences of graphemes.   \n• The input to a speech recognizer is a series of acoustic waves. that are sampled, quantized, and converted to a spectral representation like the log mel spectrum.   \n• Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete.   \n• ASR is evaluated using the Word Error Rate; the edit distance between the hypothesis and the gold transcription.   \n• TTS systems are also based on the encoder-decoder architecture. The encoder maps letters to an encoding, which is consumed by the decoder which generates mel spectrogram output. A neural vocoder then reads the spectrogram and generates waveforms.   \n• TTS systems require a first pass of text normalization to deal with numbers and abbreviations and other non-standard words.   \n• TTS is evaluated by playing a sentence to human listeners and having them give a mean opinion score (MOS) or by doing AB tests. \nBibliographical and Historical Notes \nASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the first two vowel formants in the digit. They achieved $9 7 % - 9 9 %$ accuracy by choosing the pattern that had the highest relative correlation coefficient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes’s system was the first to use phoneme transition probabilities to constrain the recognizer.",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Other Speech Tasks",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "speaker recognition \nSpeaker recognition, is the task of identifying a speaker. We generally distinguish the subtasks of speaker verification, where we make a binary decision (is this speaker $X$ or not?), such as for security when accessing personal information over the telephone, and speaker identification, where we make a one of $N$ decision trying to match a speaker’s voice against a database of many speakers . These tasks are related to language identification, in which we are given a wavefile and must identify which language is being spoken; this is useful for example for automatically directing callers to human operators that speak appropriate languages. \n26.8 Summary \nThis chapter introduced the fundamental algorithms of automatic speech recognition (ASR) and text-to-speech (TTS). \n• The task of speech recognition (or speech-to-text) is to map acoustic waveforms to sequences of graphemes.   \n• The input to a speech recognizer is a series of acoustic waves. that are sampled, quantized, and converted to a spectral representation like the log mel spectrum.   \n• Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete.   \n• ASR is evaluated using the Word Error Rate; the edit distance between the hypothesis and the gold transcription.   \n• TTS systems are also based on the encoder-decoder architecture. The encoder maps letters to an encoding, which is consumed by the decoder which generates mel spectrogram output. A neural vocoder then reads the spectrogram and generates waveforms.   \n• TTS systems require a first pass of text normalization to deal with numbers and abbreviations and other non-standard words.   \n• TTS is evaluated by playing a sentence to human listeners and having them give a mean opinion score (MOS) or by doing AB tests. \nBibliographical and Historical Notes \nASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the first two vowel formants in the digit. They achieved $9 7 % - 9 9 %$ accuracy by choosing the pattern that had the highest relative correlation coefficient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes’s system was the first to use phoneme transition probabilities to constrain the recognizer.",
        "chapter": "Automatic Speech Recognition and Text-to-Speech",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A"
    }
]