[
    {
        "content": "1 Introduction \n1.1 What is machine learning? \nA popular definition of machine learning or ML, due to Tom Mitchell [Mit97], is as follows: \nA computer program is said to learn from experience E with respect to some class of tasks T, and performance measure P, if its performance at tasks in T, as measured by $mathrm { P }$ , improves with experience E. \nThus there are many different kinds of machine learning, depending on the nature of the tasks $T$ we wish the system to learn, the nature of the performance measure $P$ we use to evaluate the system, and the nature of the training signal or experience $E$ we give it. \nIn this book, we will cover the most common types of ML, but from a probabilistic perspective. Roughly speaking, this means that we treat all unknown quantities (e.g., predictions about the future value of some quantity of interest, such as tomorrow’s temperature, or the parameters of some model) as random variables, that are endowed with probability distributions which describe a weighted set of possible values the variable may have. (See Chapter 2 for a quick refresher on the basics of probability, if necessary.) \nThere are two main reasons we adopt a probabilistic approach. First, it is the optimal approach to decision making under uncertainty, as we explain in Section 5.1. Second, probabilistic modeling is the language used by most other areas of science and engineering, and thus provides a unifying framework between these fields. As Shakir Mohamed, a researcher at DeepMind, put it:1 \nAlmost all of machine learning can be viewed in probabilistic terms, making probabilistic thinking fundamental. It is, of course, not the only view. But it is through this view that we can connect what we do in machine learning to every other computational science, whether that be in stochastic optimisation, control theory, operations research, econometrics, information theory, statistical physics or bio-statistics. For this reason alone, mastery of probabilistic thinking is essential. \n1.2 Supervised learning \nThe most common form of ML is supervised learning. In this problem, the task $T$ is to learn a mapping $f$ from inputs $mathbf { boldsymbol { x } } in mathcal { X }$ to outputs $pmb { y } in mathcal { V }$ . The inputs $_ { x }$ are also called the features,",
        "chapter": "Introduction",
        "section": "What is machine learning?",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "Table 1.1: A subset of the Iris design matrix. The features are: sepal length, sepal width, petal length, petal width. There are 50 examples of each class. \ncovariates, or predictors; this is often a fixed-dimensional vector of numbers, such as the height and weight of a person, or the pixels in an image. In this case, $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , where $D$ is the dimensionality response.2 The experience of the vector (i.e., the number of input features). The output $E$ is given in the form of a set of $N$ $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ input-output pairs is also known as the label, target, or $mathcal { D } = { ( boldsymbol { x } _ { n } , boldsymbol { y } _ { n } ) } _ { n = 1 } ^ { N }$ known as the training set. ( $N$ is called the sample size.) The performance measure $P$ depends on the type of output we are predicting, as we discuss below. \n1.2.1 Classification \nIn classification problems, the output space is a set of $C$ unordered and mutually exclusive labels known as classes, $mathcal { V } = { 1 , 2 , ldots , C }$ . The problem of predicting the class label given an input is also called pattern recognition. (If there are just two classes, often denoted by $y in { 0 , 1 }$ or $y in { - 1 , + 1 }$ , it is called binary classification.) \n1.2.1.1 Example: classifying Iris flowers \nAs an example, consider the problem of classifying Iris flowers into their 3 subspecies, Setosa, Versicolor and Virginica. Figure 1.1 shows one example of each of these classes. \nIn image classification, the input space $mathcal { X }$ is the set of images, which is a very high-dimensional space: for a color image with $C = 3$ channels (e.g., RGB) and $D _ { 1 } times D _ { 2 }$ pixels, we have $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , where $D = C times D _ { 1 } times D _ { 2 }$ . (In practice we represent each pixel intensity with an integer, typically from the range ${ 0 , 1 , ldots , 2 5 5 }$ , but we assume real valued inputs for notational simplicity.) Learning a mapping $f : mathcal { X }  mathcal { Y }$ from images to labels is quite challenging, as illustrated in Figure 1.2. However, it can be tackled using certain kinds of functions, such as a convolutional neural network or CNN, which we discuss in Section 14.1. \nFortunately for us, some botanists have already identified 4 simple, but highly informative, numeric features — sepal length, sepal width, petal length, petal width — which can be used to distinguish the three kinds of Iris flowers. In this section, we will use this much lower-dimensional input space, $mathcal { X } = mathbb { R } ^ { 4 }$ , for simplicity. The Iris dataset is a collection of 150 labeled examples of Iris flowers, 50 of each type, described by these 4 features. It is widely used as an example, because it is small and simple to understand. (We will discuss larger and more complex datasets later in the book.) \nWhen we have small datasets of features, it is common to store them in an $N times D$ matrix, in which each row represents an example, and each column represents a feature. This is known as a design matrix; see Table 1.1 for an example.3 \nThe Iris dataset is an example of tabular data. When the inputs are of variable size (e.g., sequences of words, or social networks), rather than fixed-length vectors, the data is usually stored in some other format rather than in a design matrix. However, such data is often converted to a fixed-sized feature representation (a process known as featurization), thus implicitly creating a design matrix for further processing. We give an example of this in Section 1.5.4.1, where we discuss the “bag of words” representation for sequence data. \n\n1.2.1.2 Exploratory data analysis \nBefore tackling a problem with ML, it is usually a good idea to perform exploratory data analysis, to see if there are any obvious patterns (which might give hints on what method to choose), or any obvious problems with the data (e.g., label noise or outliers). \nFor tabular data with a small number of features, it is common to make a pair plot, in which panel $( i , j )$ shows a scatter plot of variables $i$ and $j$ , and the diagonal entries $( i , i )$ show the marginal density of variable $i$ ; all plots are optionally color coded by class label — see Figure 1.3 for an example. \nFor higher-dimensional data, it is common to first perform dimensionality reduction, and then \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 to visualize the data in 2d or 3d. We discuss methods for dimensionality reduction in Chapter 20. \n\n1.2.1.3 Learning a classifier \nFrom Figure 1.3, we can see that the Setosa class is easy to distinguish from the other two classes. For example, suppose we create the following decision rule: \nThis is a very simple example of a classifier, in which we have partitioned the input space into two regions, defined by the one-dimensional (1d) decision boundary at $x _ { mathrm { p e t a l ~ l e n g t h } } = 2 . 4 5$ . Points lying to the left of this boundary are classified as Setosa; points to the right are either Versicolor or Virginica. \nWe see that this rule perfectly classifies the Setosa examples, but not the Virginica and Versicolor ones. To improve performance, we can recursively partition the space, by splitting regions in which the classifier makes errors. For example, we can add another decision rule, to be applied to inputs that fail the first test, to check if the petal width is below 1.75cm (in which case we predict Versicolor) or above (in which case we predict Virginica). We can arrange these nested rules into a tree structure, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license called a decision tree, as shown in Figure 1.4a This induces the 2d decision surface shown in Figure 1.4b. \n\nWe can represent the tree by storing, for each internal node, the feature index that is used, as well as the corresponding threshold value. We denote all these parameters by $pmb theta$ . We discuss how to learn these parameters in Section 18.1. \n1.2.1.4 Empirical risk minimization \nThe goal of supervised learning is to automatically come up with classification models such as the one shown in Figure 1.4a, so as to reliably predict the labels for any given input. A common way to measure performance on this task is in terms of the misclassification rate on the training set: \nwhere $mathbb { I } left( e right)$ is the binary indicator function, which returns 1 iff (if and only if) the condition $e$ is true, and returns 0 otherwise, i.e., \nThis assumes all errors are equal. However it may be the case that some errors are more costly than others. For example, suppose we are foraging in the wilderness and we find some Iris flowers. Furthermore, suppose that Setosa and Versicolor are tasty, but Virginica is poisonous. In this case, we might use the asymmetric loss function $ell ( y , hat { y } )$ shown in Table 1.2. \nWe can then define empirical risk to be the average loss of the predictor on the training set: \nWe see that the misclassification rate Equation (1.2) is equal to the empirical risk when we use zero-one loss for comparing the true label with the prediction: \nSee Section 5.1 for more details. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nOne way to define the problem of model fitting or training is to find a setting of the parameters that minimizes the empirical risk on the training set: \nThis is called empirical risk minimization. \nHowever, our true goal is to minimize the expected loss on future data that we have not yet seen. That is, we want to generalize, rather than just do well on the training set. We discuss this important point in Section 1.2.3. \n1.2.1.5 Uncertainty \n[We must avoid] false confidence bred from an ignorance of the probabilistic nature of the world, from a desire to see black and white where we should rightly see gray. — Immanuel Kant, as paraphrased by Maria Konnikova [Kon20]. \nIn many cases, we will not be able to perfectly predict the exact output given the input, due to lack of knowledge of the input-output mapping (this is called epistemic uncertainty or model uncertainty), and/or due to intrinsic (irreducible) stochasticity in the mapping (this is called aleatoric uncertainty or data uncertainty). \nRepresenting uncertainty in our prediction can be important for various applications. For example, let us return to our poisonous flower example, whose loss matrix is shown in Table 1.2. If we predict the flower is Virginica with high probability, then we should not eat the flower. Alternatively, we may be able to perform an information gathering action, such as performing a diagnostic test, to reduce our uncertainty. For more information about how to make optimal decisions in the presence of uncertainty, see Section 5.1. \nWe can capture our uncertainty using the following conditional probability distribution: \nwhere $f : mathcal { X }  [ 0 , 1 ] ^ { C }$ maps inputs to a probability distribution over the $C$ possible output labels. Since $f _ { c } ( { pmb x } ; { pmb theta } )$ returns the probability of class label $c$ , we require $0 leq f _ { c } leq 1$ for each $c$ , and $textstyle sum _ { c = 1 } ^ { C } f _ { c } = 1$ To avoid this restriction, it is common to instead require the model to return unnormalized logprobabilities. We can then convert these to probabilities using the softmax function, which is defined as follows \nThis maps $mathbb { R } ^ { C }$ to $[ 0 , 1 ] ^ { C }$ , and satisfies the constraints that $0 leq mathrm { s o f t m a x } ( pmb { a } ) _ { c } leq 1$ and $textstyle sum _ { c = 1 } ^ { C }$ $mathrm { s o f t m a x } ( a ) _ { c } =$ 1. The inputs to the softmax, $pmb { a } = f ( pmb { x } ; pmb { theta } )$ , are called logits. See Section 2.5.2 for details. We thus define the overall model as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nA common special case of this arises when $f$ is an affine function of the form \nwhere $pmb theta = ( b , pmb w )$ are the parameters of the model. This model is called logistic regression, and will be discussed in more detail in Chapter 10. \nIn statistics, the $mathbf { boldsymbol { w } }$ parameters are usually called regression coefficients (and are typically denoted by $beta$ ) and $b$ is called the intercept. In ML, the parameters $mathbf { boldsymbol { w } }$ are called the weights and $b$ is called the bias. This terminology arises from electrical engineering, where we view the function $f$ as a circuit which takes in $_ { x }$ and returns $f ( { pmb x } )$ . Each input is fed to the circuit on “wires”, which have weights $mathbf { boldsymbol { w } }$ . The circuit computes the weighted sum of its inputs, and adds a constant bias or offset term $b$ . (This use of the term “bias” should not be confused with the statistical concept of bias discussed in Section 4.7.6.1.) \nTo reduce notational clutter, it is common to absorb the bias term $b$ into the weights $mathbf { boldsymbol { w } }$ by defining $tilde { pmb { w } } = [ b , w _ { 1 } , dotsc , w _ { D } ]$ and defining $tilde { boldsymbol { x } } = [ 1 , x _ { 1 } , dots , x _ { D } ]$ , so that \nThis converts the affine function into a linear function. We will usually assume that this has been done, so we can just write the prediction function as follows: \n1.2.1.6 Maximum likelihood estimation \nWhen fitting probabilistic models, it is common to use the negative log probability as our loss function: \nThe reasons for this are explained in Section 5.1.6.1, but the intuition is that a good model (with low loss) is one that assigns a high probability to the true output $y$ for each corresponding input $_ { x }$ . The average negative log probability of the training set is given by \nThis is called the negative log likelihood. If we minimize this, we can compute the maximum likelihood estimate or MLE: \nThis is a very common way to fit models to data, as we will see. \n1.2.2 Regression \nNow suppose that we want to predict a real-valued quantity $y in mathbb { R }$ instead of a class label $y in$ ${ 1 , ldots , C }$ ; this is known as regression. For example, in the case of Iris flowers, $y$ might be the degree of toxicity if the flower is eaten, or the average height of the plant. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "Introduction",
        "section": "Supervised learning",
        "subsection": "Classification",
        "subsubsection": "N/A"
    },
    {
        "content": "A common special case of this arises when $f$ is an affine function of the form \nwhere $pmb theta = ( b , pmb w )$ are the parameters of the model. This model is called logistic regression, and will be discussed in more detail in Chapter 10. \nIn statistics, the $mathbf { boldsymbol { w } }$ parameters are usually called regression coefficients (and are typically denoted by $beta$ ) and $b$ is called the intercept. In ML, the parameters $mathbf { boldsymbol { w } }$ are called the weights and $b$ is called the bias. This terminology arises from electrical engineering, where we view the function $f$ as a circuit which takes in $_ { x }$ and returns $f ( { pmb x } )$ . Each input is fed to the circuit on “wires”, which have weights $mathbf { boldsymbol { w } }$ . The circuit computes the weighted sum of its inputs, and adds a constant bias or offset term $b$ . (This use of the term “bias” should not be confused with the statistical concept of bias discussed in Section 4.7.6.1.) \nTo reduce notational clutter, it is common to absorb the bias term $b$ into the weights $mathbf { boldsymbol { w } }$ by defining $tilde { pmb { w } } = [ b , w _ { 1 } , dotsc , w _ { D } ]$ and defining $tilde { boldsymbol { x } } = [ 1 , x _ { 1 } , dots , x _ { D } ]$ , so that \nThis converts the affine function into a linear function. We will usually assume that this has been done, so we can just write the prediction function as follows: \n1.2.1.6 Maximum likelihood estimation \nWhen fitting probabilistic models, it is common to use the negative log probability as our loss function: \nThe reasons for this are explained in Section 5.1.6.1, but the intuition is that a good model (with low loss) is one that assigns a high probability to the true output $y$ for each corresponding input $_ { x }$ . The average negative log probability of the training set is given by \nThis is called the negative log likelihood. If we minimize this, we can compute the maximum likelihood estimate or MLE: \nThis is a very common way to fit models to data, as we will see. \n1.2.2 Regression \nNow suppose that we want to predict a real-valued quantity $y in mathbb { R }$ instead of a class label $y in$ ${ 1 , ldots , C }$ ; this is known as regression. For example, in the case of Iris flowers, $y$ might be the degree of toxicity if the flower is eaten, or the average height of the plant. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nRegression is very similar to classification. However, since the output is real-valued, we need to use a different loss function. For regression, the most common choice is to use quadratic loss, or $ell _ { 2 }$ loss: \nThis penalizes large residuals $y - { hat { y } }$ more than small ones.4 The empirical risk when using quadratic loss is equal to the mean squared error or MSE: \nBased on the discussion in Section 1.2.1.5, we should also model the uncertainty in our prediction. In regression problems, it is common to assume the output distribution is a Gaussian or normal. As we explain in Section 2.6, this distribution is defined by \nwhere $mu$ is the mean, $sigma ^ { 2 }$ is the variance, and $scriptstyle { sqrt { 2 pi sigma ^ { 2 } } }$ is the normalization constant needed to ensure the density integrates to 1. In the context of regression, we can make the mean depend on the inputs by defining $mu = f ( pmb { x } _ { n } ; pmb { theta } )$ . We therefore get the following conditional probability distribution: \nIf we assume that the variance $sigma ^ { 2 }$ is fixed (for simplicity), the corresponding negative log likelihood becomes \nWe see that the NLL is proportional to the MSE. Hence computing the maximum likelihood estimate of the parameters will result in minimizing the squared error, which seems like a sensible approach to model fitting. \n1.2.2.1 Linear regression \nAs an example of a regression model, consider the 1d data in Figure 1.5a. We can fit this data using a simple linear regression model of the form \nwhere $w$ is the slope, $b$ is the offset, and $pmb theta = ( w , b )$ are all the parameters of the model. By adjusting $pmb theta$ , we can minimize the sum of squared errors, shown by the vertical lines in Figure 1.5b. until we find the least squares solution \nSee Section 11.2.2.1 for details. \nIf we have multiple input features, we can write \nwhere $pmb theta = ( pmb w , b )$ . This is called multiple linear regression. \nFor example, consider the task of predicting temperature as a function of 2d location in a room. Figure 1.6(a) plots the results of a linear model of the following form: \nWe can extend this model to use $D > 2$ input features (such as time of day), but then it becomes harder to visualize. \n1.2.2.2 Polynomial regression \nThe linear model in Figure 1.5a is obviously not a very good fit to the data. We can improve the fit by using a polynomial regression model of degree $D$ . This has the form $f ( x ; { pmb w } ) = { pmb w } ^ { 1 } phi ( x )$ , where $phi ( x )$ is a feature vector derived from the input, which has the following form: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis is a simple example of feature preprocessing, also called feature engineering. \nIn Figure 1.7a, we see that using $D = 2$ results in a much better fit. We can keep increasing $D$ , and hence the number of parameters in the model, until $D = N - 1$ ; in this case, we have one parameter per data point, so we can perfectly interpolate the data. The resulting model will have 0 MSE, as shown in Figure 1.7c. However, intuitively the resulting function will not be a good predictor for future inputs, since it is too “wiggly”. We discuss this in more detail in Section 1.2.3. \nWe can also apply polynomial regression to multi-dimensional inputs. For example, Figure 1.6(b) plots the predictions for the temperature model after performing a quadratic expansion of the inputs \nThe quadratic shape is a better fit to the data than the linear model in Figure 1.6(a), since it captures the fact that the middle of the room is hotter. We can also add cross terms, such as $x _ { 1 } x _ { 2 }$ , to capture interaction effects. See Section 1.5.3.2 for details. \nNote that the above models still use a prediction function that is a linear function of the parameters $mathbf { boldsymbol { w } }$ , even though it is a nonlinear function of the original input $_ { x }$ . The reason this is important is that a linear model induces an MSE loss function MSE(θ) that has a unique global optimum, as we explain in Section 11.2.2.1. \n1.2.2.3 Deep neural networks \nIn Section 1.2.2.2, we manually specified the transformation of the input features, namely polynomial expansion, $phi ( pmb { x } ) = [ 1 , x _ { 1 } , x _ { 2 } , x _ { 1 } ^ { 2 } , x _ { 2 } ^ { 2 } , . . . ]$ . We can create much more powerful models by learning to do such nonlinear feature extraction automatically. If we let $phi ( { pmb x } )$ have its own set of parameters, say $mathbf { V }$ , then the overall model has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can recursively decompose the feature extractor $phi ( pmb { x } ; mathbf { V } )$ into a composition of simpler functions. The resulting model then becomes a stack of $L$ nested functions: \nwhere $f _ { ell } ( { pmb x } ) ~ = ~ f ( { pmb x } ; { pmb theta } _ { ell } )$ is the function at layer $ell$ . The final layer is linear and has the form $f _ { L } ( pmb { x } ) = pmb { w } ^ { mathsf { T } } f _ { 1 : L - 1 } ( pmb { x } )$ , where $f _ { 1 : L - 1 } ( { pmb x } )$ is the learned feature extractor. This is the key idea behind deep neural networks or DNNs, which includes common variants such as convolutional neural networks (CNNs) for images, and recurrent neural networks (RNNs) for sequences. See Part III for details. \n1.2.3 Overfitting and generalization \nWe can rewrite the empirical risk in Equation (1.4) in the following equivalent way: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $| mathcal { D } _ { mathrm { t r a i n } } |$ is the size of the training set $mathscr { D } _ { mathrm { t r a i n } }$ . This formulation is useful because it makes explicit which dataset the loss is being evaluated on.",
        "chapter": "Introduction",
        "section": "Supervised learning",
        "subsection": "Regression",
        "subsubsection": "N/A"
    },
    {
        "content": "We can recursively decompose the feature extractor $phi ( pmb { x } ; mathbf { V } )$ into a composition of simpler functions. The resulting model then becomes a stack of $L$ nested functions: \nwhere $f _ { ell } ( { pmb x } ) ~ = ~ f ( { pmb x } ; { pmb theta } _ { ell } )$ is the function at layer $ell$ . The final layer is linear and has the form $f _ { L } ( pmb { x } ) = pmb { w } ^ { mathsf { T } } f _ { 1 : L - 1 } ( pmb { x } )$ , where $f _ { 1 : L - 1 } ( { pmb x } )$ is the learned feature extractor. This is the key idea behind deep neural networks or DNNs, which includes common variants such as convolutional neural networks (CNNs) for images, and recurrent neural networks (RNNs) for sequences. See Part III for details. \n1.2.3 Overfitting and generalization \nWe can rewrite the empirical risk in Equation (1.4) in the following equivalent way: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $| mathcal { D } _ { mathrm { t r a i n } } |$ is the size of the training set $mathscr { D } _ { mathrm { t r a i n } }$ . This formulation is useful because it makes explicit which dataset the loss is being evaluated on. \n\nWith a suitably flexible model, we can drive the training loss to zero (assuming no label noise), by simply memorizing the correct output for each input. For example, Figure 1.7(c) perfectly interpolates the training data (modulo the last point on the right). But what we care about is prediction accuracy on new data, which may not be part of the training set. A model that perfectly fits the training data, but which is too complex, is said to suffer from overfitting. \nTo detect if a model is overfitting, let us assume (for now) that we have access to the true (but unknown) distribution $p ^ { * } ( { pmb x } , { pmb y } )$ used to generate the training set. Then, instead of computing the empirical risk we compute the theoretical expected loss or population risk \nThe difference $mathcal { L } ( pmb { theta } ; p ^ { * } ) - mathcal { L } ( pmb { theta } ; mathcal { D } _ { mathrm { t r a i n } } )$ is called the generalization gap. If a model has a large generalization gap (i.e., low empirical risk but high population risk), it is a sign that it is overfitting. In practice we don’t know $p ^ { * }$ . However, we can partition the data we do have into two subsets, known as the training set and the test set. Then we can approximate the population risk using the test risk: \nAs an example, in Figure 1.7d, we plot the training error and test error for polynomial regression as a function of degree $D$ . We see that the training error goes to 0 as the model becomes more complex. However, the test error has a characteristic U-shaped curve: on the left, where $D = 1$ , the model is underfitting; on the right, where $D gg 1$ , the model is overfitting; and when $D = 2$ , the model complexity is “just right”. \nHow can we pick a model of the right complexity? If we use the training set to evaluate different models, we will always pick the most complex model, since that will have the most degrees of freedom, and hence will have minimum loss. So instead we should pick the model with minimum test loss. \nIn practice, we need to partition the data into three sets, namely the training set, the test set and a validation set; the latter is used for model selection, and we just use the test set to estimate future performance (the population risk), i.e., the test set is not used for model fitting or model selection. See Section 4.5.4 for further details. \n1.2.4 No free lunch theorem \nAll models are wrong, but some models are useful. — George Box [BD87, p424].5 \nGiven the large variety of models in the literature, it is natural to wonder which one is best. Unfortunately, there is no single best model that works optimally for all kinds of problems — this is sometimes called the no free lunch theorem [Wol96]. The reason is that a set of assumptions (also called inductive bias) that works well in one domain may work poorly in another. The best way to pick a suitable model is based on domain knowledge, and/or trial and error (i.e., using model selection techniques such as cross validation (Section 4.5.4) or Bayesian methods (Section 5.2.2 and Section 5.2.6). For this reason, it is important to have many models and algorithmic techniques in one’s toolbox to choose from.",
        "chapter": "Introduction",
        "section": "Supervised learning",
        "subsection": "Overfitting and generalization",
        "subsubsection": "N/A"
    },
    {
        "content": "With a suitably flexible model, we can drive the training loss to zero (assuming no label noise), by simply memorizing the correct output for each input. For example, Figure 1.7(c) perfectly interpolates the training data (modulo the last point on the right). But what we care about is prediction accuracy on new data, which may not be part of the training set. A model that perfectly fits the training data, but which is too complex, is said to suffer from overfitting. \nTo detect if a model is overfitting, let us assume (for now) that we have access to the true (but unknown) distribution $p ^ { * } ( { pmb x } , { pmb y } )$ used to generate the training set. Then, instead of computing the empirical risk we compute the theoretical expected loss or population risk \nThe difference $mathcal { L } ( pmb { theta } ; p ^ { * } ) - mathcal { L } ( pmb { theta } ; mathcal { D } _ { mathrm { t r a i n } } )$ is called the generalization gap. If a model has a large generalization gap (i.e., low empirical risk but high population risk), it is a sign that it is overfitting. In practice we don’t know $p ^ { * }$ . However, we can partition the data we do have into two subsets, known as the training set and the test set. Then we can approximate the population risk using the test risk: \nAs an example, in Figure 1.7d, we plot the training error and test error for polynomial regression as a function of degree $D$ . We see that the training error goes to 0 as the model becomes more complex. However, the test error has a characteristic U-shaped curve: on the left, where $D = 1$ , the model is underfitting; on the right, where $D gg 1$ , the model is overfitting; and when $D = 2$ , the model complexity is “just right”. \nHow can we pick a model of the right complexity? If we use the training set to evaluate different models, we will always pick the most complex model, since that will have the most degrees of freedom, and hence will have minimum loss. So instead we should pick the model with minimum test loss. \nIn practice, we need to partition the data into three sets, namely the training set, the test set and a validation set; the latter is used for model selection, and we just use the test set to estimate future performance (the population risk), i.e., the test set is not used for model fitting or model selection. See Section 4.5.4 for further details. \n1.2.4 No free lunch theorem \nAll models are wrong, but some models are useful. — George Box [BD87, p424].5 \nGiven the large variety of models in the literature, it is natural to wonder which one is best. Unfortunately, there is no single best model that works optimally for all kinds of problems — this is sometimes called the no free lunch theorem [Wol96]. The reason is that a set of assumptions (also called inductive bias) that works well in one domain may work poorly in another. The best way to pick a suitable model is based on domain knowledge, and/or trial and error (i.e., using model selection techniques such as cross validation (Section 4.5.4) or Bayesian methods (Section 5.2.2 and Section 5.2.6). For this reason, it is important to have many models and algorithmic techniques in one’s toolbox to choose from. \n\n1.3 Unsupervised learning \nIn supervised learning, we assume that each input example $_ { x }$ in the training set has an associated set of output targets $pmb { y }$ , and our goal is to learn the input-output mapping. Although this is useful, and can be difficult, supervised learning is essentially just “glorified curve fitting” [Pea18]. \nAn arguably much more interesting task is to try to “make sense of” data, as opposed to just learning a mapping. That is, we just get observed “inputs” $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ without any corresponding “outputs” ${ bf { y } } _ { n }$ . This is called unsupervised learning. \nFrom a probabilistic perspective, we can view the task of unsupervised learning as fitting an unconditional model of the form $p ( { pmb x } )$ , which can generate new data $_ { x }$ , whereas supervised learning involves fitting a conditional model, $p ( pmb { y } | pmb { x } )$ , which specifies (a distribution over) outputs given inputs.6 \nUnsupervised learning avoids the need to collect large labeled datasets for training, which can often be time consuming and expensive (think of asking doctors to label medical images). \nUnsupervised learning also avoids the need to learn how to partition the world into often arbitrary categories. For example, consider the task of labeling when an action, such as “drinking” or “sipping”, occurs in a video. Is it when the person picks up the glass, or when the glass first touches the mouth, or when the liquid pours out? What if they pour out some liquid, then pause, then pour again — is that two actions or one? Humans will often disagree on such issues [Idr+17], which means the task is not well defined. It is therefore not reasonable to expect machines to learn such mappings.7 \nFinally, unsupervised learning forces the model to “explain” the high-dimensional inputs, rather than just the low-dimensional outputs. This allows us to learn richer models of “how the world works”. As Geoff Hinton, who is a famous professor of ML at the University of Toronto, has said: \nWhen we’re learning to see, nobody’s telling us what the right answers are — we just look. Every so often, your mother says “that’s a dog”, but that’s very little information. You’d be lucky if you got a few bits of information — even one bit per second — that way. The brain’s visual system has $1 0 ^ { 1 4 }$ neural connections. And you only live for $1 0 ^ { 9 }$ seconds. So it’s no use learning one bit per second. You need more like $1 0 ^ { 5 }$ bits per second. And there’s only one place you can get that much information: from the input itself. — Geoffrey Hinton, 1996 (quoted in [Gor06]). \n1.3.1 Clustering \nA simple example of unsupervised learning is the problem of finding clusters in data. The goal is to partition the input into regions that contain “similar” points. As an example, consider a 2d version of the Iris dataset. In Figure 1.8a, we show the points without any class labels. Intuitively there are at least two clusters in the data, one in the bottom left and one in the top right. Furthermore, if we assume that a “good” set of clusters should be fairly compact, then we might want to split the top right into (at least) two subclusters. The resulting partition into three clusters is shown in Figure 1.8b. (Note that there is no correct number of clusters; instead, we need to consider the tradeoff between model complexity and fit to the data. We discuss ways to make this tradeoff in Section 21.3.7.)",
        "chapter": "Introduction",
        "section": "Supervised learning",
        "subsection": "No free lunch theorem",
        "subsubsection": "N/A"
    },
    {
        "content": "1.3 Unsupervised learning \nIn supervised learning, we assume that each input example $_ { x }$ in the training set has an associated set of output targets $pmb { y }$ , and our goal is to learn the input-output mapping. Although this is useful, and can be difficult, supervised learning is essentially just “glorified curve fitting” [Pea18]. \nAn arguably much more interesting task is to try to “make sense of” data, as opposed to just learning a mapping. That is, we just get observed “inputs” $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ without any corresponding “outputs” ${ bf { y } } _ { n }$ . This is called unsupervised learning. \nFrom a probabilistic perspective, we can view the task of unsupervised learning as fitting an unconditional model of the form $p ( { pmb x } )$ , which can generate new data $_ { x }$ , whereas supervised learning involves fitting a conditional model, $p ( pmb { y } | pmb { x } )$ , which specifies (a distribution over) outputs given inputs.6 \nUnsupervised learning avoids the need to collect large labeled datasets for training, which can often be time consuming and expensive (think of asking doctors to label medical images). \nUnsupervised learning also avoids the need to learn how to partition the world into often arbitrary categories. For example, consider the task of labeling when an action, such as “drinking” or “sipping”, occurs in a video. Is it when the person picks up the glass, or when the glass first touches the mouth, or when the liquid pours out? What if they pour out some liquid, then pause, then pour again — is that two actions or one? Humans will often disagree on such issues [Idr+17], which means the task is not well defined. It is therefore not reasonable to expect machines to learn such mappings.7 \nFinally, unsupervised learning forces the model to “explain” the high-dimensional inputs, rather than just the low-dimensional outputs. This allows us to learn richer models of “how the world works”. As Geoff Hinton, who is a famous professor of ML at the University of Toronto, has said: \nWhen we’re learning to see, nobody’s telling us what the right answers are — we just look. Every so often, your mother says “that’s a dog”, but that’s very little information. You’d be lucky if you got a few bits of information — even one bit per second — that way. The brain’s visual system has $1 0 ^ { 1 4 }$ neural connections. And you only live for $1 0 ^ { 9 }$ seconds. So it’s no use learning one bit per second. You need more like $1 0 ^ { 5 }$ bits per second. And there’s only one place you can get that much information: from the input itself. — Geoffrey Hinton, 1996 (quoted in [Gor06]). \n1.3.1 Clustering \nA simple example of unsupervised learning is the problem of finding clusters in data. The goal is to partition the input into regions that contain “similar” points. As an example, consider a 2d version of the Iris dataset. In Figure 1.8a, we show the points without any class labels. Intuitively there are at least two clusters in the data, one in the bottom left and one in the top right. Furthermore, if we assume that a “good” set of clusters should be fairly compact, then we might want to split the top right into (at least) two subclusters. The resulting partition into three clusters is shown in Figure 1.8b. (Note that there is no correct number of clusters; instead, we need to consider the tradeoff between model complexity and fit to the data. We discuss ways to make this tradeoff in Section 21.3.7.) \n\n1.3.2 Discovering latent “factors of variation” \nWhen dealing with high-dimensional data, it is often useful to reduce the dimensionality by projecting it to a lower dimensional subspace which captures the “essence” of the data. One approach to this problem is to assume that each observed high-dimensional output $pmb { x } _ { n } in mathbb { R } ^ { D }$ was generated by a set \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license of hidden or unobserved low-dimensional latent factors $z _ { n } in mathbb { R } ^ { K }$ . We can represent the model diagrammatically as follows: $z _ { n } to x _ { n }$ , where the arrow represents causation. Since we don’t know the latent factors $z _ { n }$ , we often assume a simple prior probability model for $p ( z _ { n } )$ such as a Gaussian, which says that each factor is a random $K$ -dimensional vector. If the data is real-valued, we can use a Gaussian likelihood as well.",
        "chapter": "Introduction",
        "section": "Unsupervised learning",
        "subsection": "Clustering",
        "subsubsection": "N/A"
    },
    {
        "content": "1.3.2 Discovering latent “factors of variation” \nWhen dealing with high-dimensional data, it is often useful to reduce the dimensionality by projecting it to a lower dimensional subspace which captures the “essence” of the data. One approach to this problem is to assume that each observed high-dimensional output $pmb { x } _ { n } in mathbb { R } ^ { D }$ was generated by a set \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license of hidden or unobserved low-dimensional latent factors $z _ { n } in mathbb { R } ^ { K }$ . We can represent the model diagrammatically as follows: $z _ { n } to x _ { n }$ , where the arrow represents causation. Since we don’t know the latent factors $z _ { n }$ , we often assume a simple prior probability model for $p ( z _ { n } )$ such as a Gaussian, which says that each factor is a random $K$ -dimensional vector. If the data is real-valued, we can use a Gaussian likelihood as well. \n\nThe simplest example is when we use a linear model, $p ( pmb { x } _ { n } | mathbf { tilde { z } } _ { n } ; pmb { theta } ) = mathcal { N } ( pmb { x } _ { n } | mathbf { W } boldsymbol { z } _ { n } + pmb { mu } , pmb { Sigma } )$ . The resulting model is called factor analysis (FA). It is similar to linear regression, except we only observe the outputs ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , and not the inputs $z _ { n }$ . In the special case that $pmb { Sigma } = sigma ^ { 2 } pmb { mathrm { I } }$ , this reduces to a model called probabilistic principal components analysis (PCA), which we will explain in Section 20.1. In Figure 1.9, we give an illustration of how this method can find a 2d linear subspace when applied to some simple 3d data. \nOf course, assuming a linear mapping from $z _ { n }$ to ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ is very restrictive. However, we can create nonlinear extensions by defining $p ( pmb { x } _ { n } | mathbf { boldsymbol { z } } _ { n } ; pmb { theta } ) = mathcal { N } ( pmb { x } _ { n } | f ( mathbf { boldsymbol { z } } _ { n } ; pmb { theta } ) , sigma ^ { 2 } mathbf { I } )$ , where $f ( z ; theta )$ is a nonlinear model, such as a deep neural network. It becomes much harder to fit such a model (i.e., to estimate the parameters $pmb theta$ ), because the inputs to the neural net have to be inferred, as well as the parameters of the model. However, there are various approximate methods, such as the variational autoencoder which can be applied (see Section 20.3.5). \n1.3.3 Self-supervised learning \nA recently popular approach to unsupervised learning is known as self-supervised learning. In this approach, we create proxy supervised tasks from unlabeled data. For example, we might try to learn to predict a color image from a grayscale image, or to mask out words in a sentence and then try to predict them given the surrounding context. The hope is that the resulting predictor $hat { pmb x } _ { 1 } = f ( pmb x _ { 2 } ; pmb theta )$ , where $mathbf { boldsymbol { x } } _ { 2 }$ is the observed input and $scriptstyle { hat { mathbf { x } } } _ { 1 }$ is the predicted output, will learn useful features from the data, that can then be used in standard, downstream supervised tasks. This avoids the hard problem of trying to infer the “true latent factors” $mathscr { z }$ behind the observed data, and instead relies on standard supervised learning methods. We discuss this approach in more detail in Section 19.2. \n1.3.4 Evaluating unsupervised learning \nAlthough unsupervised learning is appealing, it is very hard to evaluate the quality of the output of an unsupervised learning method, because there is no ground truth to compare to [TOB16]. \nA common method for evaluating unsupervised models is to measure the probability assigned by the model to unseen test examples. We can do this by computing the (unconditional) negative log likelihood of the data: \nThis treats the problem of unsupervised learning as one of density estimation. The idea is that a good model will not be “surprised” by actual data samples (i.e., will assign them high probability). Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of data space where the data samples come from, it implicitly assigns low probability to the regions where the data does not come from. Thus the model has learned to capture the typical patterns in the data. This can be used inside of a data compression algorithm. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "Introduction",
        "section": "Unsupervised learning",
        "subsection": "Discovering latent ``factors of variation''",
        "subsubsection": "N/A"
    },
    {
        "content": "The simplest example is when we use a linear model, $p ( pmb { x } _ { n } | mathbf { tilde { z } } _ { n } ; pmb { theta } ) = mathcal { N } ( pmb { x } _ { n } | mathbf { W } boldsymbol { z } _ { n } + pmb { mu } , pmb { Sigma } )$ . The resulting model is called factor analysis (FA). It is similar to linear regression, except we only observe the outputs ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , and not the inputs $z _ { n }$ . In the special case that $pmb { Sigma } = sigma ^ { 2 } pmb { mathrm { I } }$ , this reduces to a model called probabilistic principal components analysis (PCA), which we will explain in Section 20.1. In Figure 1.9, we give an illustration of how this method can find a 2d linear subspace when applied to some simple 3d data. \nOf course, assuming a linear mapping from $z _ { n }$ to ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ is very restrictive. However, we can create nonlinear extensions by defining $p ( pmb { x } _ { n } | mathbf { boldsymbol { z } } _ { n } ; pmb { theta } ) = mathcal { N } ( pmb { x } _ { n } | f ( mathbf { boldsymbol { z } } _ { n } ; pmb { theta } ) , sigma ^ { 2 } mathbf { I } )$ , where $f ( z ; theta )$ is a nonlinear model, such as a deep neural network. It becomes much harder to fit such a model (i.e., to estimate the parameters $pmb theta$ ), because the inputs to the neural net have to be inferred, as well as the parameters of the model. However, there are various approximate methods, such as the variational autoencoder which can be applied (see Section 20.3.5). \n1.3.3 Self-supervised learning \nA recently popular approach to unsupervised learning is known as self-supervised learning. In this approach, we create proxy supervised tasks from unlabeled data. For example, we might try to learn to predict a color image from a grayscale image, or to mask out words in a sentence and then try to predict them given the surrounding context. The hope is that the resulting predictor $hat { pmb x } _ { 1 } = f ( pmb x _ { 2 } ; pmb theta )$ , where $mathbf { boldsymbol { x } } _ { 2 }$ is the observed input and $scriptstyle { hat { mathbf { x } } } _ { 1 }$ is the predicted output, will learn useful features from the data, that can then be used in standard, downstream supervised tasks. This avoids the hard problem of trying to infer the “true latent factors” $mathscr { z }$ behind the observed data, and instead relies on standard supervised learning methods. We discuss this approach in more detail in Section 19.2. \n1.3.4 Evaluating unsupervised learning \nAlthough unsupervised learning is appealing, it is very hard to evaluate the quality of the output of an unsupervised learning method, because there is no ground truth to compare to [TOB16]. \nA common method for evaluating unsupervised models is to measure the probability assigned by the model to unseen test examples. We can do this by computing the (unconditional) negative log likelihood of the data: \nThis treats the problem of unsupervised learning as one of density estimation. The idea is that a good model will not be “surprised” by actual data samples (i.e., will assign them high probability). Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of data space where the data samples come from, it implicitly assigns low probability to the regions where the data does not come from. Thus the model has learned to capture the typical patterns in the data. This can be used inside of a data compression algorithm. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "Introduction",
        "section": "Unsupervised learning",
        "subsection": "Self-supervised learning",
        "subsubsection": "N/A"
    },
    {
        "content": "The simplest example is when we use a linear model, $p ( pmb { x } _ { n } | mathbf { tilde { z } } _ { n } ; pmb { theta } ) = mathcal { N } ( pmb { x } _ { n } | mathbf { W } boldsymbol { z } _ { n } + pmb { mu } , pmb { Sigma } )$ . The resulting model is called factor analysis (FA). It is similar to linear regression, except we only observe the outputs ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , and not the inputs $z _ { n }$ . In the special case that $pmb { Sigma } = sigma ^ { 2 } pmb { mathrm { I } }$ , this reduces to a model called probabilistic principal components analysis (PCA), which we will explain in Section 20.1. In Figure 1.9, we give an illustration of how this method can find a 2d linear subspace when applied to some simple 3d data. \nOf course, assuming a linear mapping from $z _ { n }$ to ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ is very restrictive. However, we can create nonlinear extensions by defining $p ( pmb { x } _ { n } | mathbf { boldsymbol { z } } _ { n } ; pmb { theta } ) = mathcal { N } ( pmb { x } _ { n } | f ( mathbf { boldsymbol { z } } _ { n } ; pmb { theta } ) , sigma ^ { 2 } mathbf { I } )$ , where $f ( z ; theta )$ is a nonlinear model, such as a deep neural network. It becomes much harder to fit such a model (i.e., to estimate the parameters $pmb theta$ ), because the inputs to the neural net have to be inferred, as well as the parameters of the model. However, there are various approximate methods, such as the variational autoencoder which can be applied (see Section 20.3.5). \n1.3.3 Self-supervised learning \nA recently popular approach to unsupervised learning is known as self-supervised learning. In this approach, we create proxy supervised tasks from unlabeled data. For example, we might try to learn to predict a color image from a grayscale image, or to mask out words in a sentence and then try to predict them given the surrounding context. The hope is that the resulting predictor $hat { pmb x } _ { 1 } = f ( pmb x _ { 2 } ; pmb theta )$ , where $mathbf { boldsymbol { x } } _ { 2 }$ is the observed input and $scriptstyle { hat { mathbf { x } } } _ { 1 }$ is the predicted output, will learn useful features from the data, that can then be used in standard, downstream supervised tasks. This avoids the hard problem of trying to infer the “true latent factors” $mathscr { z }$ behind the observed data, and instead relies on standard supervised learning methods. We discuss this approach in more detail in Section 19.2. \n1.3.4 Evaluating unsupervised learning \nAlthough unsupervised learning is appealing, it is very hard to evaluate the quality of the output of an unsupervised learning method, because there is no ground truth to compare to [TOB16]. \nA common method for evaluating unsupervised models is to measure the probability assigned by the model to unseen test examples. We can do this by computing the (unconditional) negative log likelihood of the data: \nThis treats the problem of unsupervised learning as one of density estimation. The idea is that a good model will not be “surprised” by actual data samples (i.e., will assign them high probability). Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of data space where the data samples come from, it implicitly assigns low probability to the regions where the data does not come from. Thus the model has learned to capture the typical patterns in the data. This can be used inside of a data compression algorithm. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nUnfortunately, density estimation is difficult, especially in high dimensions. Furthermore, a model that assigns high probability to the data may not have learned useful high-level patterns (after all, the model could just memorize all the training examples). \nAn alternative evaluation metric is to use the learned unsupervised representation as features or input to a downstream supervised learning method. If the unsupervised method has discovered useful patterns, then it should be possible to use these patterns to perform supervised learning using much less labeled data than when working with the original features. For example, in Section 1.2.1.1, we saw how the 4 manually defined features of iris flowers contained most of the information needed to perform classification. We were thus able to train a classifier with nearly perfect performance using just 150 examples. If the input was raw pixels, we would need many more examples to achieve comparable performance (see Section 14.1). That is, we can increase the sample efficiency of learning (i.e., reduce the number of labeled examples needed to get good performance) by first learning a good representation. \nIncreased sample efficiency is a useful evaluation metric, but in many applications, especially in science, the goal of unsupervised learning is to gain understanding, not to improve performance on some prediction task. This requires the use of models that are interpretable, but which can also generate or “explain” most of the observed patterns in the data. To paraphrase Plato, the goal is to discover how to “carve nature at its joints”. Of course, evaluating whether we have successfully discovered the true underlying structure behind some dataset often requires performing experiments and thus interacting with the world. We discuss this topic further in Section 1.4. \n1.4 Reinforcement learning \nIn addition to supervised and unsupervised learning, there is a third kind of ML known as reinforcement learning (RL). In this class of problems, the system or agent has to learn how to interact with its environment. This can be encoded by means of a policy $pmb { a } = pi ( pmb { x } )$ , which specifies which action to take in response to each possible input $_ { x }$ (derived from the environment state). \nFor example, consider an agent that learns to play a video game, such as Atari Space Invaders (see \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "Introduction",
        "section": "Unsupervised learning",
        "subsection": "Evaluating unsupervised learning",
        "subsubsection": "N/A"
    },
    {
        "content": "Unfortunately, density estimation is difficult, especially in high dimensions. Furthermore, a model that assigns high probability to the data may not have learned useful high-level patterns (after all, the model could just memorize all the training examples). \nAn alternative evaluation metric is to use the learned unsupervised representation as features or input to a downstream supervised learning method. If the unsupervised method has discovered useful patterns, then it should be possible to use these patterns to perform supervised learning using much less labeled data than when working with the original features. For example, in Section 1.2.1.1, we saw how the 4 manually defined features of iris flowers contained most of the information needed to perform classification. We were thus able to train a classifier with nearly perfect performance using just 150 examples. If the input was raw pixels, we would need many more examples to achieve comparable performance (see Section 14.1). That is, we can increase the sample efficiency of learning (i.e., reduce the number of labeled examples needed to get good performance) by first learning a good representation. \nIncreased sample efficiency is a useful evaluation metric, but in many applications, especially in science, the goal of unsupervised learning is to gain understanding, not to improve performance on some prediction task. This requires the use of models that are interpretable, but which can also generate or “explain” most of the observed patterns in the data. To paraphrase Plato, the goal is to discover how to “carve nature at its joints”. Of course, evaluating whether we have successfully discovered the true underlying structure behind some dataset often requires performing experiments and thus interacting with the world. We discuss this topic further in Section 1.4. \n1.4 Reinforcement learning \nIn addition to supervised and unsupervised learning, there is a third kind of ML known as reinforcement learning (RL). In this class of problems, the system or agent has to learn how to interact with its environment. This can be encoded by means of a policy $pmb { a } = pi ( pmb { x } )$ , which specifies which action to take in response to each possible input $_ { x }$ (derived from the environment state). \nFor example, consider an agent that learns to play a video game, such as Atari Space Invaders (see \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n\"Pure\" Reinforcement Learning (cherry) >The machine predicts a scalar reward given once ina while A fewbits forsome samples   \nSupervised Learning (icing) The machine predicts a category orafewnumbers foreach input ■Predicting human-supplied data >10→10,000 bits per sample   \nUnsupervised/Predictive Learning (cake) $blacktriangleright$ The machine predicts any part of its input for any observed part. H ?Predicts future frames in videos Millionsofbits persample \nFigure 1.10a). In this case, the input $_ { x }$ is the image (or sequence of past images), and the output $textbf { em a }$ is the direction to move in (left or right) and whether to fire a missile or not. As a more complex example, consider the problem of a robot learning to walk (see Figure 1.10b). In this case, the input $_ { x }$ is the set of joint positions and angles for all the limbs, and the output $textbf { em a }$ is a set of actuation or motor control signals. \nThe difference from supervised learning (SL) is that the system is not told which action is the best one to take (i.e., which output to produce for a given input). Instead, the system just receives an occasional reward (or punishment) signal in response to the actions that it takes. This is like learning with a critic, who gives an occasional thumbs up or thumbs down, as opposed to learning with a teacher, who tells you what to do at each step. \nRL has grown in popularity recently, due to its broad applicability (since the reward signal that the agent is trying to optimize can be any metric of interest). However, it can be harder to make RL work than it is for supervised or unsupervised learning, for a variety of reasons. A key difficulty is that the reward signal may only be given occasionally (e.g., if the agent eventually reaches a desired state), and even then it may be unclear to the agent which of its many actions were responsible for getting the reward. (Think of playing a game like chess, where there is a single win or lose signal at the end of the game.) \nTo compensate for the minimal amount of information coming from the reward signal, it is common to use other information sources, such as expert demonstrations, which can be used in a supervised way, or unlabeled data, which can be used by an unsupervised learning system to discover the underlying structure of the environment. This can make it feasible to learn from a limited number of trials (interactions with the environment). As Yann LeCun put it, in an invited talk at the NIPS8 conference in 2016: “If intelligence was a cake, unsupervised learning would be the chocolate sponge, supervised learning would be the icing, and reinforcement learning would be the cherry.” This is illustrated in Figure 1.11. \nMore information on RL can be found in the sequel to this book, [Mur23]. \n1.5 Data \nMachine learning is concerned with fitting models to data using various algorithms. Although we focus on the modeling and algorithm aspects, it is important to mention that the nature and quality of the training data also plays a vital role in the success of any learned model. \nIn this section, we briefly describe some common image and text datasets that we will use in this book. We also briefly discuss the topic of data preprocessing. \n1.5.1 Some common image datasets \nIn this section, we briefly discuss some image datasets that we will use in this book. \n1.5.1.1 Small image datasets \nOne of the simplest and most widely used is known as MNIST [LeC+98; YB19].9 This is a dataset of 60k training images and 10k test images, each of size $2 8 times 2 8$ (grayscale), illustrating handwritten digits from 10 categories. Each pixel is an integer in the range ${ 0 , 1 , ldots , 2 5 5 }$ ; these are usually rescaled to $[ 0 , 1 ]$ , to represent pixel intensity. We can optionally convert this to a binary image by thresholding. See Figure 1.12a for an illustration.",
        "chapter": "Introduction",
        "section": "Reinforcement learning",
        "subsection": "N/A",
        "subsubsection": "N/A"
    },
    {
        "content": "More information on RL can be found in the sequel to this book, [Mur23]. \n1.5 Data \nMachine learning is concerned with fitting models to data using various algorithms. Although we focus on the modeling and algorithm aspects, it is important to mention that the nature and quality of the training data also plays a vital role in the success of any learned model. \nIn this section, we briefly describe some common image and text datasets that we will use in this book. We also briefly discuss the topic of data preprocessing. \n1.5.1 Some common image datasets \nIn this section, we briefly discuss some image datasets that we will use in this book. \n1.5.1.1 Small image datasets \nOne of the simplest and most widely used is known as MNIST [LeC+98; YB19].9 This is a dataset of 60k training images and 10k test images, each of size $2 8 times 2 8$ (grayscale), illustrating handwritten digits from 10 categories. Each pixel is an integer in the range ${ 0 , 1 , ldots , 2 5 5 }$ ; these are usually rescaled to $[ 0 , 1 ]$ , to represent pixel intensity. We can optionally convert this to a binary image by thresholding. See Figure 1.12a for an illustration. \nMNIST is so widely used in the ML community that Geoff Hinton, a famous ML researcher, has called it the “drosophila of machine learning”, since if we cannot make a method work well on MNIST, it will likely not work well on harder datasets. However, nowadays MNIST classification is considered “too easy”, since it is possible to distinguish most pairs of digits by looking at just a single pixel. Various extensions have been proposed. \nIn [Coh+17], they proposed EMNIST (extended MNIST), that also includes lower and upper case letters. See Figure 1.12b for a visualization. This dataset is much harder than MNIST, since there are 62 classes, several of which are quite ambiguous (e.g., the digit 1 vs the lower case letter l). \nIn [XRV17], they proposed Fashion-MNIST, which has exactly the same size and shape as MNIST, but where each image is the picture of a piece of clothing instead of a handwritten digit. See Figure 1.13a for a visualization. \nFor small color images, the most common dataset is CIFAR [KH09].10 This is a dataset of $6 0 k$ images, each of size $3 2 times 3 2 times 3$ , representing everyday objects from 10 or 100 classes; see Figure 1.13b for an illustration.11 \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n1.5.1.2 ImageNet \nSmall datasets are useful for prototyping ideas, but it is also important to test methods on larger datasets, both in terms of image size and number of labeled examples. The most widely used dataset of this type is called ImageNet [Rus+15]. This is a dataset of $sim 1 4 M$ images of size $2 5 6 times 2 5 6 times 3$ illustrating various objects from 20,000 classes; see Figure 1.14a for some examples. \nThe ImageNet dataset was used as the basis of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which ran from 2010 to 2018. This used a subset of 1.3M images from 1000 classes. During the course of the competition, significant progress was made by the community, as shown in Figure 1.14b. In particular, 2015 marked the first year in which CNNs could outperform humans (or at least one human, namely Andrej Karpathy) at the task of classifying images from ImageNet. Note that this does not mean that CNNs are better at vision than humans (see e.g., [YL21] for some common failure modes). Instead, it mostly likely reflects the fact that the dataset makes many fine-grained classification distinctions — such as between a “tiger” and a “tiger cat” — that humans find difficult to understand; by contrast, sufficiently flexible CNNs can learn arbitrary patterns, including random labels [Zha+17a]. \nTable 1.3: We show snippets of the first two sentences from the IMDB movie review dataset. The first example is labeled positive and the second negative. (<UNK> refers to an unknown token.) \nAlthough ImageNet is much harder than MNIST and CIFAR as a classification benchmark, it too is almost “saturated” [Bey+20]. Nevertheless, relative performance of methods on ImageNet is often a surprisingly good predictor of performance on other, unrelated image classification tasks (see e.g., [Rec+19]), so it remains very widely used. \n1.5.2 Some common text datasets \nMachine learning is often applied to text to solve a variety of tasks. This is known as natural language processing or NLP (see e.g., [JM20] for details). Below we briefly mention a few text datasets that we will use in this book. \n1.5.2.1 Text classification \nA simple NLP task is text classification, which can be used for email spam classification, sentiment analysis (e.g., is a movie or product review positive or negative), etc. A common dataset for evaluating such methods is the IMDB movie review dataset from [Maa+11]. (IMDB stands for “Internet Movie Database”.) This contains 25k labeled examples for training, and 25k for testing. Each example has a binary label, representing a positive or negative rating. See Table 1.3 for some example sentences. \n1.5.2.2 Machine translation \nA more difficult NLP task is to learn to map a sentence $_ { x }$ in one language to a “semantically equivalent” sentence $textbf {  { y } }$ in another language; this is called machine translation. Training such models requires aligned $( { pmb x } , { pmb y } )$ pairs. Fortunately, several such datasets exist, e.g., from the Canadian parliament (English-French pairs), and the European Union (Europarl). A subset of the latter, known as the WMT dataset (Workshop on Machine Translation), consists of English-German pairs, and is widely used as a benchmark dataset. \n1.5.2.3 Other seq2seq tasks \nA generalization of machine translation is to learn a mapping from one sequence $_ { x }$ to any other sequence $mathbf { Delta } _ { mathbf { mathcal { S } } }$ . This is called a seq2seq model, and can be viewed as a form of high-dimensional classification (see Section 15.2.3 for details). This framing of the problem is very general, and includes many tasks, such as document summarization, question answering, etc. For example, Table 1.4 shows how to formulate question answering as a seq2seq problem: the input is the text T \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "Introduction",
        "section": "Data",
        "subsection": "Some common image datasets",
        "subsubsection": "N/A"
    },
    {
        "content": "Table 1.3: We show snippets of the first two sentences from the IMDB movie review dataset. The first example is labeled positive and the second negative. (<UNK> refers to an unknown token.) \nAlthough ImageNet is much harder than MNIST and CIFAR as a classification benchmark, it too is almost “saturated” [Bey+20]. Nevertheless, relative performance of methods on ImageNet is often a surprisingly good predictor of performance on other, unrelated image classification tasks (see e.g., [Rec+19]), so it remains very widely used. \n1.5.2 Some common text datasets \nMachine learning is often applied to text to solve a variety of tasks. This is known as natural language processing or NLP (see e.g., [JM20] for details). Below we briefly mention a few text datasets that we will use in this book. \n1.5.2.1 Text classification \nA simple NLP task is text classification, which can be used for email spam classification, sentiment analysis (e.g., is a movie or product review positive or negative), etc. A common dataset for evaluating such methods is the IMDB movie review dataset from [Maa+11]. (IMDB stands for “Internet Movie Database”.) This contains 25k labeled examples for training, and 25k for testing. Each example has a binary label, representing a positive or negative rating. See Table 1.3 for some example sentences. \n1.5.2.2 Machine translation \nA more difficult NLP task is to learn to map a sentence $_ { x }$ in one language to a “semantically equivalent” sentence $textbf {  { y } }$ in another language; this is called machine translation. Training such models requires aligned $( { pmb x } , { pmb y } )$ pairs. Fortunately, several such datasets exist, e.g., from the Canadian parliament (English-French pairs), and the European Union (Europarl). A subset of the latter, known as the WMT dataset (Workshop on Machine Translation), consists of English-German pairs, and is widely used as a benchmark dataset. \n1.5.2.3 Other seq2seq tasks \nA generalization of machine translation is to learn a mapping from one sequence $_ { x }$ to any other sequence $mathbf { Delta } _ { mathbf { mathcal { S } } }$ . This is called a seq2seq model, and can be viewed as a form of high-dimensional classification (see Section 15.2.3 for details). This framing of the problem is very general, and includes many tasks, such as document summarization, question answering, etc. For example, Table 1.4 shows how to formulate question answering as a seq2seq problem: the input is the text T \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nT: In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”. \nQ1: What causes precipitation to fall? A1: gravity Q2: What is another main form of precipitation besides drizzle, rain, snow, sleet and hail? A2: graupel Q3: Where do water droplets collide with ice crystals to form precipitation? A3: within a cloud \nTable 1.4: Question-answer pairs for a sample passage in the SQuAD dataset. Each of the answers is a segment of text from the passage. This can be solved using sentence pair tagging. The input is the paragraph text $T$ and the question Q. The output is a tagging of the relevant words in T that answer the question in $Q$ . From Figure 1 of [Raj+16]. Used with kind permission of Percy Liang. \nand question Q, and the output is the answer A, which is a set of words, possibly extracted from the input. \n1.5.2.4 Language modeling \nThe rather grandiose term “language modeling” refers to the task of creating unconditional generative models of text sequences, $p ( x _ { 1 } , dots , x _ { T } )$ . This only requires input sentences $_ { x }$ , without any corresponding “labels” $pmb { y }$ . We can therefore think of this as a form of unsupervised learning, which we discuss in Section 1.3. If the language model generates output in response to an input, as in seq2seq, we can regard it as a conditional generative model. \n1.5.3 Preprocessing discrete input data \nMany ML models assume that the data consists of real-valued feature vectors, $pmb { x } in mathbb { R } ^ { D }$ . However, sometimes the input may have discrete input features, such as categorical variables like race and gender, or words from some vocabulary. In the sections below, we discuss some ways to preprocess such data to convert it to vector form. This is a common operation that is used for many different kinds of models. \n1.5.3.1 One-hot encoding \nWhen we have categorical features, we need to convert them to a numerical scale, so that computing weighted combinations of the inputs makes sense. The standard way to preprocess such categorical variables is to use a one-hot encoding, also called a dummy encoding. If a variable $x$ has $K$ values, we will denote its dummy encoding as follows: o $operatorname { n e - h o t } ( x ) = [ mathbb { I } left( x = 1 right) , dots , mathbb { I } left( x = K right) ]$ . For example, if there are 3 colors (say red, green and blue), the corresponding one-hot vectors will be one- $mathrm { h o t } ( mathrm { r e d } ) = [ 1 , 0 , 0 ]$ , one- $mathrm { h o t } ( mathrm { g r e e n } ) = [ 0 , 1 , 0 ]$ , and one- $mathrm { h o t } ( mathrm { b l u e } ) = [ 0 , 0 , 1 ]$ . \n1.5.3.2 Feature crosses \nA linear model using a dummy encoding for each categorical variable can capture the main effects of each variable, but cannot capture interaction effects between them. For example, suppose we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "Introduction",
        "section": "Data",
        "subsection": "Some common text datasets",
        "subsubsection": "N/A"
    },
    {
        "content": "T: In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”. \nQ1: What causes precipitation to fall? A1: gravity Q2: What is another main form of precipitation besides drizzle, rain, snow, sleet and hail? A2: graupel Q3: Where do water droplets collide with ice crystals to form precipitation? A3: within a cloud \nTable 1.4: Question-answer pairs for a sample passage in the SQuAD dataset. Each of the answers is a segment of text from the passage. This can be solved using sentence pair tagging. The input is the paragraph text $T$ and the question Q. The output is a tagging of the relevant words in T that answer the question in $Q$ . From Figure 1 of [Raj+16]. Used with kind permission of Percy Liang. \nand question Q, and the output is the answer A, which is a set of words, possibly extracted from the input. \n1.5.2.4 Language modeling \nThe rather grandiose term “language modeling” refers to the task of creating unconditional generative models of text sequences, $p ( x _ { 1 } , dots , x _ { T } )$ . This only requires input sentences $_ { x }$ , without any corresponding “labels” $pmb { y }$ . We can therefore think of this as a form of unsupervised learning, which we discuss in Section 1.3. If the language model generates output in response to an input, as in seq2seq, we can regard it as a conditional generative model. \n1.5.3 Preprocessing discrete input data \nMany ML models assume that the data consists of real-valued feature vectors, $pmb { x } in mathbb { R } ^ { D }$ . However, sometimes the input may have discrete input features, such as categorical variables like race and gender, or words from some vocabulary. In the sections below, we discuss some ways to preprocess such data to convert it to vector form. This is a common operation that is used for many different kinds of models. \n1.5.3.1 One-hot encoding \nWhen we have categorical features, we need to convert them to a numerical scale, so that computing weighted combinations of the inputs makes sense. The standard way to preprocess such categorical variables is to use a one-hot encoding, also called a dummy encoding. If a variable $x$ has $K$ values, we will denote its dummy encoding as follows: o $operatorname { n e - h o t } ( x ) = [ mathbb { I } left( x = 1 right) , dots , mathbb { I } left( x = K right) ]$ . For example, if there are 3 colors (say red, green and blue), the corresponding one-hot vectors will be one- $mathrm { h o t } ( mathrm { r e d } ) = [ 1 , 0 , 0 ]$ , one- $mathrm { h o t } ( mathrm { g r e e n } ) = [ 0 , 1 , 0 ]$ , and one- $mathrm { h o t } ( mathrm { b l u e } ) = [ 0 , 0 , 1 ]$ . \n1.5.3.2 Feature crosses \nA linear model using a dummy encoding for each categorical variable can capture the main effects of each variable, but cannot capture interaction effects between them. For example, suppose we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwant to predict the fuel efficiency of a vehicle given two categorical input variables: the type (say SUV, Truck, or Family car), and the country of origin (say USA or Japan). If we concatenate the one-hot encodings for the ternary and binary features, we get the following input encoding: \nwhere $x _ { 1 }$ is the type and $x _ { 2 }$ is the country of origin. \nThis model cannot capture dependencies between the features. For example, we expect trucks to be less fuel efficient, but perhaps trucks from the USA are even less efficient than trucks from Japan. This cannot be captured using the linear model in Equation (1.34) since the contribution from the country of origin is independent of the car type. \nWe can fix this by computing explicit feature crosses. For example, we can define a new composite feature with $3 times 2$ possible values, to capture the interaction of type and country of origin. The new model becomes \nWe can see that the use of feature crosses converts the original dataset into a wide format, with many more columns. \n1.5.4 Preprocessing text data \nIn Section 1.5.2, we briefly discussed text classification and other NLP tasks. To feed text data into a classifier, we need to tackle various issues. First, documents have a variable length, and are thus not fixed-length feature vectors, as assumed by many kinds of models. Second, words are categorical variables with many possible values (equal to the size of the vocabulary), so the corresponding one-hot encodings will be very high-dimensional, with no natural notion of similarity. Third, we may encounter words at test time that have not been seen during training (so-called out-of-vocabulary or OOV words). We discuss some solutions to these problems below. More details can be found in e.g., [BKL10; MRS08; JM20]. \n1.5.4.1 Bag of words model \nA simple approach to dealing with variable-length text documents is to interpret them as a bag of words, in which we ignore word order. To convert this to a vector from a fixed input space, we first map each word to a token from some vocabulary. \nTo reduce the number of tokens, we often use various pre-processing techniques such as the following: dropping punctuation, converting all words to lower case; dropping common but uninformative words, such as “and” and “the” (this is called stop word removal); replacing words with their base form, such as replacing “running” and “runs” with “run” (this is called word stemming); etc. For details, see e.g., [BL12], and for some sample code, see text_preproc_jax.ipynb. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "Introduction",
        "section": "Data",
        "subsection": "Preprocessing discrete input data",
        "subsubsection": "N/A"
    },
    {
        "content": "want to predict the fuel efficiency of a vehicle given two categorical input variables: the type (say SUV, Truck, or Family car), and the country of origin (say USA or Japan). If we concatenate the one-hot encodings for the ternary and binary features, we get the following input encoding: \nwhere $x _ { 1 }$ is the type and $x _ { 2 }$ is the country of origin. \nThis model cannot capture dependencies between the features. For example, we expect trucks to be less fuel efficient, but perhaps trucks from the USA are even less efficient than trucks from Japan. This cannot be captured using the linear model in Equation (1.34) since the contribution from the country of origin is independent of the car type. \nWe can fix this by computing explicit feature crosses. For example, we can define a new composite feature with $3 times 2$ possible values, to capture the interaction of type and country of origin. The new model becomes \nWe can see that the use of feature crosses converts the original dataset into a wide format, with many more columns. \n1.5.4 Preprocessing text data \nIn Section 1.5.2, we briefly discussed text classification and other NLP tasks. To feed text data into a classifier, we need to tackle various issues. First, documents have a variable length, and are thus not fixed-length feature vectors, as assumed by many kinds of models. Second, words are categorical variables with many possible values (equal to the size of the vocabulary), so the corresponding one-hot encodings will be very high-dimensional, with no natural notion of similarity. Third, we may encounter words at test time that have not been seen during training (so-called out-of-vocabulary or OOV words). We discuss some solutions to these problems below. More details can be found in e.g., [BKL10; MRS08; JM20]. \n1.5.4.1 Bag of words model \nA simple approach to dealing with variable-length text documents is to interpret them as a bag of words, in which we ignore word order. To convert this to a vector from a fixed input space, we first map each word to a token from some vocabulary. \nTo reduce the number of tokens, we often use various pre-processing techniques such as the following: dropping punctuation, converting all words to lower case; dropping common but uninformative words, such as “and” and “the” (this is called stop word removal); replacing words with their base form, such as replacing “running” and “runs” with “run” (this is called word stemming); etc. For details, see e.g., [BL12], and for some sample code, see text_preproc_jax.ipynb. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nLet $x _ { n t }$ be the token at location $t$ in the $n$ ’th document. If there are $D$ unique tokens in the vocabulary, then we can represent the $n$ ’th document as a $D$ -dimensional vector $scriptstyle { dot { mathbf { x } } } _ { n }$ , where $widetilde { x } _ { n v }$ is the number of times that word $v$ occurs in document $n$ : \nwhere $T$ is the length of document $n$ . We can now interpret documents as vectors in $mathbb { R } ^ { D }$ . This is called the vector space model of text [SWY75; TP10]. \nWe traditionally store input data in an $N times D$ design matrix denoted by $mathbf { X }$ , where $D$ is the number of features. In the context of vector space models, it is more common to represent the input data as a $D times N$ term frequency matrix, where $mathrm { T F } _ { i j }$ is the frequency of term $i$ in document $j$ . See Figure 1.15 for an illustration. \n1.5.4.2 TF-IDF \nOne problem with representing documents as word count vectors is that frequent words may have undue influence, just because the magnitude of their word count is higher, even if they do not carry much semantic content. A common solution to this is to transform the counts by taking logs, which reduces the impact of words that occur many times within a single document. \nTo reduce the impact of words that occur many times in general (across all documents), we compute a quantity called the inverse document frequency, defined as follows: $begin{array} { r } { { mathrm { I D F } } _ { i } triangleq log frac { N } { 1 + { mathrm { D F } } _ { i } } } end{array}$ , where DF is the number of documents with term $i$ . We can combine these transformations to compute the $_ i$ TF-IDF matrix as follows: \n(We often normalize each row as well.) This provides a more meaningful representation of documents, and can be used as input to many ML algorithms. See tfidf_demo.ipynb for an example. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n1.5.4.3 Word embeddings \nAlthough the TF-IDF transformation improves on raw count vectors by placing more weight on “informative” words and less on “uninformative” words, it does not solve the fundamental problem with the one-hot encoding (from which count vectors are derived), which is that that semantically similar words, such as “man” and “woman”, may be not be any closer (in vector space) than semantically dissimilar words, such as “man” and “banana”. Thus the assumption that points that are close in input space should have similar outputs, which is implicitly made by most prediction models, is invalid. \nThe standard way to solve this problem is to use word embeddings, in which we map each sparse one-hot vector, ${ pmb x } _ { n t } in { 0 , 1 } ^ { V }$ , to a lower-dimensional dense vector, $boldsymbol { e } _ { n t } in mathbb { R } ^ { K }$ using $e _ { n t } = mathbf { E } x _ { n t }$ , where $mathbf { E } in mathbb { R } ^ { K times V }$ is learned such that semantically similar words are placed close by. There are many ways to learn such embeddings, as we discuss in Section 20.5. \nOnce we have an embedding matrix, we can represent a variable-length text document as a bag of word embeddings. We can then convert this to a fixed length vector by summing (or averaging) the embeddings: \nwhere $tilde { mathbf { x } } _ { n }$ is the bag of words representation from Equation (1.37). We can then use this inside of a logistic regression classifier, which we briefly introduced in Section 1.2.1.5. The overall model has the form \nWe often use a pre-trained word embedding matrix $mathbf { E }$ , in which case the model is linear in $mathbf { W }$ , which simplifies parameter estimation (see Chapter 10). See also Section 15.7 for a discussion of contextual word embeddings. \n1.5.4.4 Dealing with novel words \nAt test time, the model may encounter a completely novel word that it has not seen before. This is known as the out of vocabulary or OOV problem. Such novel words are bound to occur, because the set of words is an open class. For example, the set of proper nouns (names of people and places) is unbounded. \nA standard heuristic to solve this problem is to replace all novel words with the special symbol UNK, which stands for “unknown”. However, this loses information. For example, if we encounter the word “athazagoraphobia”, we may guess it means “fear of something”, since phobia is a common suffix in English (derived from Greek) to mean “fear of”. (It turns out that athazagoraphobia means “fear of being forgotten about or ignored”.) \nWe could work at the character level, but this would require the model to learn how to group common letter combinations together into words. It is better to leverage the fact that words have substructure, and then to take as input subword units or wordpieces [SHB16; Wu+16]; these are often created using a method called byte-pair encoding [Gag94], which is a form of data compression that creates new symbols to represent common substrings. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n1.5.5 Handling missing data \nSometimes we may have missing data, in which parts of the input $_ { x }$ or output $y$ may be unknown. If the output is unknown during training, the example is unlabeled; we consider such semi-supervised learning scenarios in Section 19.3. We therefore focus on the case where some of the input features may be missing, either at training or testing time, or both. \nTo model this, let M be an $N times D$ matrix of binary variables, where $M _ { n d } = 1$ if feature $d$ in example $n$ is missing, and $M _ { n d } = 0$ otherwise. Let $mathbf { X } _ { v }$ be the visible parts of the input feature matrix, corresponding to $M _ { n d } = 0$ , and $mathbf { X } _ { h }$ be the missing parts, corresponding to $M _ { n d } = 1$ . Let $mathbf { Y }$ be the output label matrix, which we assume is fully observed. If we assume $p ( mathbf { M } | mathbf { X } _ { v } , mathbf { X } _ { h } , mathbf { Y } ) = p ( mathbf { M } )$ , we say the data is missing completely at random or MCAR, since the missingness does not depend on the hidden or observed features. If we assume $p ( mathbf { M } | mathbf { X } _ { v } , mathbf { X } _ { h } , mathbf { Y } ) = p ( mathbf { M } | mathbf { X } _ { v } , mathbf { Y } )$ , we say the data is missing at random or MAR, since the missingness does not depend on the hidden features, but may depend on the visible features. If neither of these assumptions hold, we say the data is not missing at random or NMAR. \nIn the MCAR and MAR cases, we can ignore the missingness mechanism, since it tells us nothing about the hidden features. However, in the NMAR case, we need to model the missing data mechanism, since the lack of information may be informative. For example, the fact that someone did not fill out an answer to a sensitive question on a survey (e.g., “Do you have COVID?”) could be informative about the underlying value. See e.g., [LR87; Mar08] for more information on missing data models. \nIn this book, we will always make the MAR assumption. However, even with this assumption, we cannot directly use a discriminative model, such as a DNN, when we have missing input features, since the input $_ { x }$ will have some unknown values. \nA common heuristic is called mean value imputation, in which missing values are replaced by their empirical mean. More generally, we can fit a generative model to the input, and use that to fill in the missing values. We briefly discuss some suitable generative models for this task in Chapter 20, and in more detail in the sequel to this book, [Mur23]. \n1.6 Discussion \nIn this section, we situate ML and this book into a larger context. \n1.6.1 The relationship between ML and other fields \nThere are several subcommunities that work on ML-related topics, each of which have different names. The field of predictive analytics is similar to supervised learning (in particular, classification and regression), but focuses more on business applications. Data mining covers both supervised and unsupervised machine learning, but focuses more on structured data, usually stored in large commercial databases. Data science uses techniques from machine learning and statistics, but also emphasizes other topics, such as data integration, data visualization, and working with domain experts, often in an iterative feedback loop (see e.g., [BS17]). The difference between these areas is often just one of terminology.12",
        "chapter": "Introduction",
        "section": "Data",
        "subsection": "Preprocessing text data",
        "subsubsection": "N/A"
    },
    {
        "content": "1.5.5 Handling missing data \nSometimes we may have missing data, in which parts of the input $_ { x }$ or output $y$ may be unknown. If the output is unknown during training, the example is unlabeled; we consider such semi-supervised learning scenarios in Section 19.3. We therefore focus on the case where some of the input features may be missing, either at training or testing time, or both. \nTo model this, let M be an $N times D$ matrix of binary variables, where $M _ { n d } = 1$ if feature $d$ in example $n$ is missing, and $M _ { n d } = 0$ otherwise. Let $mathbf { X } _ { v }$ be the visible parts of the input feature matrix, corresponding to $M _ { n d } = 0$ , and $mathbf { X } _ { h }$ be the missing parts, corresponding to $M _ { n d } = 1$ . Let $mathbf { Y }$ be the output label matrix, which we assume is fully observed. If we assume $p ( mathbf { M } | mathbf { X } _ { v } , mathbf { X } _ { h } , mathbf { Y } ) = p ( mathbf { M } )$ , we say the data is missing completely at random or MCAR, since the missingness does not depend on the hidden or observed features. If we assume $p ( mathbf { M } | mathbf { X } _ { v } , mathbf { X } _ { h } , mathbf { Y } ) = p ( mathbf { M } | mathbf { X } _ { v } , mathbf { Y } )$ , we say the data is missing at random or MAR, since the missingness does not depend on the hidden features, but may depend on the visible features. If neither of these assumptions hold, we say the data is not missing at random or NMAR. \nIn the MCAR and MAR cases, we can ignore the missingness mechanism, since it tells us nothing about the hidden features. However, in the NMAR case, we need to model the missing data mechanism, since the lack of information may be informative. For example, the fact that someone did not fill out an answer to a sensitive question on a survey (e.g., “Do you have COVID?”) could be informative about the underlying value. See e.g., [LR87; Mar08] for more information on missing data models. \nIn this book, we will always make the MAR assumption. However, even with this assumption, we cannot directly use a discriminative model, such as a DNN, when we have missing input features, since the input $_ { x }$ will have some unknown values. \nA common heuristic is called mean value imputation, in which missing values are replaced by their empirical mean. More generally, we can fit a generative model to the input, and use that to fill in the missing values. We briefly discuss some suitable generative models for this task in Chapter 20, and in more detail in the sequel to this book, [Mur23]. \n1.6 Discussion \nIn this section, we situate ML and this book into a larger context. \n1.6.1 The relationship between ML and other fields \nThere are several subcommunities that work on ML-related topics, each of which have different names. The field of predictive analytics is similar to supervised learning (in particular, classification and regression), but focuses more on business applications. Data mining covers both supervised and unsupervised machine learning, but focuses more on structured data, usually stored in large commercial databases. Data science uses techniques from machine learning and statistics, but also emphasizes other topics, such as data integration, data visualization, and working with domain experts, often in an iterative feedback loop (see e.g., [BS17]). The difference between these areas is often just one of terminology.12",
        "chapter": "Introduction",
        "section": "Data",
        "subsection": "Handling missing data",
        "subsubsection": "N/A"
    },
    {
        "content": "1.5.5 Handling missing data \nSometimes we may have missing data, in which parts of the input $_ { x }$ or output $y$ may be unknown. If the output is unknown during training, the example is unlabeled; we consider such semi-supervised learning scenarios in Section 19.3. We therefore focus on the case where some of the input features may be missing, either at training or testing time, or both. \nTo model this, let M be an $N times D$ matrix of binary variables, where $M _ { n d } = 1$ if feature $d$ in example $n$ is missing, and $M _ { n d } = 0$ otherwise. Let $mathbf { X } _ { v }$ be the visible parts of the input feature matrix, corresponding to $M _ { n d } = 0$ , and $mathbf { X } _ { h }$ be the missing parts, corresponding to $M _ { n d } = 1$ . Let $mathbf { Y }$ be the output label matrix, which we assume is fully observed. If we assume $p ( mathbf { M } | mathbf { X } _ { v } , mathbf { X } _ { h } , mathbf { Y } ) = p ( mathbf { M } )$ , we say the data is missing completely at random or MCAR, since the missingness does not depend on the hidden or observed features. If we assume $p ( mathbf { M } | mathbf { X } _ { v } , mathbf { X } _ { h } , mathbf { Y } ) = p ( mathbf { M } | mathbf { X } _ { v } , mathbf { Y } )$ , we say the data is missing at random or MAR, since the missingness does not depend on the hidden features, but may depend on the visible features. If neither of these assumptions hold, we say the data is not missing at random or NMAR. \nIn the MCAR and MAR cases, we can ignore the missingness mechanism, since it tells us nothing about the hidden features. However, in the NMAR case, we need to model the missing data mechanism, since the lack of information may be informative. For example, the fact that someone did not fill out an answer to a sensitive question on a survey (e.g., “Do you have COVID?”) could be informative about the underlying value. See e.g., [LR87; Mar08] for more information on missing data models. \nIn this book, we will always make the MAR assumption. However, even with this assumption, we cannot directly use a discriminative model, such as a DNN, when we have missing input features, since the input $_ { x }$ will have some unknown values. \nA common heuristic is called mean value imputation, in which missing values are replaced by their empirical mean. More generally, we can fit a generative model to the input, and use that to fill in the missing values. We briefly discuss some suitable generative models for this task in Chapter 20, and in more detail in the sequel to this book, [Mur23]. \n1.6 Discussion \nIn this section, we situate ML and this book into a larger context. \n1.6.1 The relationship between ML and other fields \nThere are several subcommunities that work on ML-related topics, each of which have different names. The field of predictive analytics is similar to supervised learning (in particular, classification and regression), but focuses more on business applications. Data mining covers both supervised and unsupervised machine learning, but focuses more on structured data, usually stored in large commercial databases. Data science uses techniques from machine learning and statistics, but also emphasizes other topics, such as data integration, data visualization, and working with domain experts, often in an iterative feedback loop (see e.g., [BS17]). The difference between these areas is often just one of terminology.12 \nML is also very closely related to the field of statistics. Indeed, Jerry Friedman, a famous statistics professor at Stanford, said $^ { 1 3 }$ \n[If the statistics field had] incorporated computing methodology from its inception as a fundamental tool, as opposed to simply a convenient way to apply our existing tools, many of the other data related fields [such as ML] would not have needed to exist — they would have been part of statistics. — Jerry Friedman [Fri97b] \nMachine learning is also related to artificial intelligence (AI). Historically, the field of AI assumed that we could program “intelligence” by hand (see e.g., [RN10; PM17]), but this approach has largely failed to live up to expectations, mostly because it proved to be too hard to explicitly encode all the knowledge such systems need. Consequently, there is renewed interest in using ML to help an AI system acquire its own knowledge. (Indeed the connections are so close that sometimes the terms “ML” and “AI” are used interchangeably, although this is arguably misleading [Pre21].) \n1.6.2 Structure of the book \nWe have seen that ML is closely related to many other subjects in mathematics, statistics, computer science, etc. It can be hard to know where to start. \nIn this book, we take one particular path through this interconnected landscape, using probability theory as our unifying lens. We cover statistical foundations in Part I, supervised learning in Part II–Part IV, and unsupervised learning in Part V. For more information on these (and other) topics, please see the sequel to this book, [Mur23], \nIn addition to the book, you may find the online Python notebooks that accompany this book helpful. See probml.github.io/book1 for details. \n1.6.3 Caveats \nIn this book, we will see how machine learning can be used to create systems that can (attempt to) predict outputs given inputs. These predictions can then be used to choose actions so as to minimize expected loss. When designing such systems, it can be hard to design a loss function that correctly specifies all of our preferences; this can result in “reward hacking” in which the machine optimizes the reward function we give it, but then we realize that the function did not capture various constraints or preferences that we forgot to specify [Wei76; Amo+16; D’A+20]. (This is particularly important when tradeoffs need to be made between multiple objectives.) \nReward hacking is an example of a larger problem known as the “alignment problem” [Chr20], which refers to the potential discrepancy between what we ask our algorithms to optimize and what we actually want them to do for us; this has raised various concerns in the context of AI ethics and AI safety (see e.g., [KR19; Lia20]). Russell [Rus19] proposes to solve this problem by not explicitly specifying a reward function, but instead forcing the machine to infer the reward by observing human behavior, an approach known as inverse reinforcement learning. However, emulating current or past human behavior too closely may be undesirable, and can be biased by the data that is available for training (see e.g., [Pau+20]). \nThe above view of AI, in which an “intelligent” system makes decisions on its own, without a human in the loop, is believed by many to be the path towards “artificial general intelligence”",
        "chapter": "Introduction",
        "section": "Discussion",
        "subsection": "The relationship between ML and other fields",
        "subsubsection": "N/A"
    },
    {
        "content": "ML is also very closely related to the field of statistics. Indeed, Jerry Friedman, a famous statistics professor at Stanford, said $^ { 1 3 }$ \n[If the statistics field had] incorporated computing methodology from its inception as a fundamental tool, as opposed to simply a convenient way to apply our existing tools, many of the other data related fields [such as ML] would not have needed to exist — they would have been part of statistics. — Jerry Friedman [Fri97b] \nMachine learning is also related to artificial intelligence (AI). Historically, the field of AI assumed that we could program “intelligence” by hand (see e.g., [RN10; PM17]), but this approach has largely failed to live up to expectations, mostly because it proved to be too hard to explicitly encode all the knowledge such systems need. Consequently, there is renewed interest in using ML to help an AI system acquire its own knowledge. (Indeed the connections are so close that sometimes the terms “ML” and “AI” are used interchangeably, although this is arguably misleading [Pre21].) \n1.6.2 Structure of the book \nWe have seen that ML is closely related to many other subjects in mathematics, statistics, computer science, etc. It can be hard to know where to start. \nIn this book, we take one particular path through this interconnected landscape, using probability theory as our unifying lens. We cover statistical foundations in Part I, supervised learning in Part II–Part IV, and unsupervised learning in Part V. For more information on these (and other) topics, please see the sequel to this book, [Mur23], \nIn addition to the book, you may find the online Python notebooks that accompany this book helpful. See probml.github.io/book1 for details. \n1.6.3 Caveats \nIn this book, we will see how machine learning can be used to create systems that can (attempt to) predict outputs given inputs. These predictions can then be used to choose actions so as to minimize expected loss. When designing such systems, it can be hard to design a loss function that correctly specifies all of our preferences; this can result in “reward hacking” in which the machine optimizes the reward function we give it, but then we realize that the function did not capture various constraints or preferences that we forgot to specify [Wei76; Amo+16; D’A+20]. (This is particularly important when tradeoffs need to be made between multiple objectives.) \nReward hacking is an example of a larger problem known as the “alignment problem” [Chr20], which refers to the potential discrepancy between what we ask our algorithms to optimize and what we actually want them to do for us; this has raised various concerns in the context of AI ethics and AI safety (see e.g., [KR19; Lia20]). Russell [Rus19] proposes to solve this problem by not explicitly specifying a reward function, but instead forcing the machine to infer the reward by observing human behavior, an approach known as inverse reinforcement learning. However, emulating current or past human behavior too closely may be undesirable, and can be biased by the data that is available for training (see e.g., [Pau+20]). \nThe above view of AI, in which an “intelligent” system makes decisions on its own, without a human in the loop, is believed by many to be the path towards “artificial general intelligence”",
        "chapter": "Introduction",
        "section": "Discussion",
        "subsection": "Structure of the book",
        "subsubsection": "N/A"
    },
    {
        "content": "ML is also very closely related to the field of statistics. Indeed, Jerry Friedman, a famous statistics professor at Stanford, said $^ { 1 3 }$ \n[If the statistics field had] incorporated computing methodology from its inception as a fundamental tool, as opposed to simply a convenient way to apply our existing tools, many of the other data related fields [such as ML] would not have needed to exist — they would have been part of statistics. — Jerry Friedman [Fri97b] \nMachine learning is also related to artificial intelligence (AI). Historically, the field of AI assumed that we could program “intelligence” by hand (see e.g., [RN10; PM17]), but this approach has largely failed to live up to expectations, mostly because it proved to be too hard to explicitly encode all the knowledge such systems need. Consequently, there is renewed interest in using ML to help an AI system acquire its own knowledge. (Indeed the connections are so close that sometimes the terms “ML” and “AI” are used interchangeably, although this is arguably misleading [Pre21].) \n1.6.2 Structure of the book \nWe have seen that ML is closely related to many other subjects in mathematics, statistics, computer science, etc. It can be hard to know where to start. \nIn this book, we take one particular path through this interconnected landscape, using probability theory as our unifying lens. We cover statistical foundations in Part I, supervised learning in Part II–Part IV, and unsupervised learning in Part V. For more information on these (and other) topics, please see the sequel to this book, [Mur23], \nIn addition to the book, you may find the online Python notebooks that accompany this book helpful. See probml.github.io/book1 for details. \n1.6.3 Caveats \nIn this book, we will see how machine learning can be used to create systems that can (attempt to) predict outputs given inputs. These predictions can then be used to choose actions so as to minimize expected loss. When designing such systems, it can be hard to design a loss function that correctly specifies all of our preferences; this can result in “reward hacking” in which the machine optimizes the reward function we give it, but then we realize that the function did not capture various constraints or preferences that we forgot to specify [Wei76; Amo+16; D’A+20]. (This is particularly important when tradeoffs need to be made between multiple objectives.) \nReward hacking is an example of a larger problem known as the “alignment problem” [Chr20], which refers to the potential discrepancy between what we ask our algorithms to optimize and what we actually want them to do for us; this has raised various concerns in the context of AI ethics and AI safety (see e.g., [KR19; Lia20]). Russell [Rus19] proposes to solve this problem by not explicitly specifying a reward function, but instead forcing the machine to infer the reward by observing human behavior, an approach known as inverse reinforcement learning. However, emulating current or past human behavior too closely may be undesirable, and can be biased by the data that is available for training (see e.g., [Pau+20]). \nThe above view of AI, in which an “intelligent” system makes decisions on its own, without a human in the loop, is believed by many to be the path towards “artificial general intelligence” \nor AGI. An alternative approach is to view AI as “augmented intelligence” (sometimes called intelligence augmentation or IA). In this paradigm, AI is a process for creating “smart tools”, like adaptive cruise control or auto-complete in search engines; such tools maintain a human in the decision-making loop. In this framing, systems which have AI/ML components in them are not that different from other complex, semi-autonomous human artefacts, such as aeroplanes with autopilot, online trading platforms or medical diagnostic systems (c.f. [Jor19; Ace]). Of course, as the AI tools becomes more powerful, they can end up doing more and more on their own, making this approach similar to AGI. However, in augmented intelligence, the goal is not to emulate or exceed human behavior at certain tasks, but instead to help humans get stuff done more easily; this is how we treat most other technologies [Kap16]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nPart I \nFoundations",
        "chapter": "Introduction",
        "section": "Discussion",
        "subsection": "Caveats",
        "subsubsection": "N/A"
    },
    {
        "content": "2 Probability: Univariate Models \n2.1 Introduction \nIn this chapter, we give a brief introduction to the basics of probability theory. There are many good books that go into more detail, e.g., [GS97; BT08]. \n2.1.1 What is probability? \nProbability theory is nothing but common sense reduced to calculation. — Pierre Laplace, 1812 \nWe are all comfortable saying that the probability that a (fair) coin will land heads is $5 0 %$ . But what does this mean? There are actually two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events that can happen multiple times. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time.1 \nThe other interpretation is called the Bayesian interpretation of probability. In this view, probability is used to quantify our uncertainty or ignorance about something; hence it is fundamentally related to information rather than repeated trials [Jay03; Lin06]. In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss. \nOne big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about one-off events that do not have long term frequencies. For example, we might want to compute the probability that the polar ice cap will melt by 2030 CE. This event will happen zero or one times, but cannot happen repeatedly. Nevertheless, we ought to be able to quantify our uncertainty about this event; based on how probable we think this event is, we can decide how to take the optimal action, as discussed in Chapter 5. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted. \n2.1.2 Types of uncertainty \nThe uncertainty in our predictions can arise for two fundamentally different reasons. The first is due to our ignorance of the underlying hidden causes or mechanism generating our data. This is called epistemic uncertainty, since epistemology is the philosophical term used to describe the study of knowledge. However, a simpler term for this is model uncertainty. The second kind of uncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], derived from the Latin word for “dice”, although a simpler term would be data uncertainty. As a concrete example, consider tossing a fair coin. We might know for sure that the probability of heads is $p = 0 . 5$ , so there is no epistemic uncertainty, but we still cannot perfectly predict the outcome.",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Introduction",
        "subsubsection": "What is probability?"
    },
    {
        "content": "2 Probability: Univariate Models \n2.1 Introduction \nIn this chapter, we give a brief introduction to the basics of probability theory. There are many good books that go into more detail, e.g., [GS97; BT08]. \n2.1.1 What is probability? \nProbability theory is nothing but common sense reduced to calculation. — Pierre Laplace, 1812 \nWe are all comfortable saying that the probability that a (fair) coin will land heads is $5 0 %$ . But what does this mean? There are actually two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events that can happen multiple times. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time.1 \nThe other interpretation is called the Bayesian interpretation of probability. In this view, probability is used to quantify our uncertainty or ignorance about something; hence it is fundamentally related to information rather than repeated trials [Jay03; Lin06]. In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss. \nOne big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about one-off events that do not have long term frequencies. For example, we might want to compute the probability that the polar ice cap will melt by 2030 CE. This event will happen zero or one times, but cannot happen repeatedly. Nevertheless, we ought to be able to quantify our uncertainty about this event; based on how probable we think this event is, we can decide how to take the optimal action, as discussed in Chapter 5. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted. \n2.1.2 Types of uncertainty \nThe uncertainty in our predictions can arise for two fundamentally different reasons. The first is due to our ignorance of the underlying hidden causes or mechanism generating our data. This is called epistemic uncertainty, since epistemology is the philosophical term used to describe the study of knowledge. However, a simpler term for this is model uncertainty. The second kind of uncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], derived from the Latin word for “dice”, although a simpler term would be data uncertainty. As a concrete example, consider tossing a fair coin. We might know for sure that the probability of heads is $p = 0 . 5$ , so there is no epistemic uncertainty, but we still cannot perfectly predict the outcome. \n\nThis distinction can be important for applications such as active learning. A typical strategy is to query examples for which $mathbb { H } ( p ( boldsymbol { y } | boldsymbol { x } , mathcal { D } ) )$ is large (where $mathbb H ( p )$ is the entropy, discussed in Section 6.1). However, this could be due to uncertainty about the parameters, i.e., large $mathbb { H } ( p ( pmb { theta } | mathcal { D } ) )$ , or just due to inherent variability of the outcome, corresponding to large entropy of $p ( boldsymbol { y } | boldsymbol { mathbf { x } } , boldsymbol { theta } )$ . In the latter case, there would not be much use collecting more samples, since our epistemic uncertainty would not be reduced. See [Osb16] for further discussion of this point. \n2.1.3 Probability as an extension of logic \nIn this section, we review the basic rules of probability, following the presentation of [Jay03], in which we view probability as an extension of Boolean logic. \n2.1.3.1 Probability of an event \nWe define an event, denoted by the binary variable $A$ , as some state of the world that either holds or does not hold. For example, $A$ might be event “it will rain tomorrow”, or “it rained yesterday”, or “the label is $y = 1 ^ { gamma }$ , or “the parameter $theta$ is between 1.5 and 2.0”, etc. The expression $operatorname* { P r } ( A )$ denotes the probability with which you believe event $A$ is true (or the long run fraction of times that $A$ will occur). We require that $0 leq operatorname* { P r } ( A ) leq 1$ , where $operatorname* { P r } ( A ) = 0$ means the event definitely will not happen, and $operatorname* { P r } ( A ) = 1$ means the event definitely will happen. We write $operatorname* { P r } ( { overline { { A } } } )$ to denote the probability of event $A$ not happening; this is defined to be $operatorname* { P r } (  b { A } ) = 1 - operatorname* { P r } (  b { A } )$ . \n2.1.3.2 Probability of a conjunction of two events \nWe denote the joint probability of events $A$ and $B$ both happening as follows: \nIf $A$ and $B$ are independent events, we have \nFor example, suppose $X$ and $Y$ are chosen uniformly at random from the set $mathcal { X } = { 1 , 2 , 3 , 4 }$ . Let $A$ be the event that $X in { 1 , 2 }$ , and $B$ be the event that $Y in { 3 }$ . Then we have $operatorname* { P r } ( A , B ) =$ $operatorname* { P r } ( A ) operatorname* { P r } ( B ) = { frac { 1 } { 2 } } cdot { frac { 1 } { 4 } } $ . \n2.1.3.3 Probability of a union of two events \nThe probability of event $A$ or $B$ happening is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Introduction",
        "subsubsection": "Types of uncertainty"
    },
    {
        "content": "This distinction can be important for applications such as active learning. A typical strategy is to query examples for which $mathbb { H } ( p ( boldsymbol { y } | boldsymbol { x } , mathcal { D } ) )$ is large (where $mathbb H ( p )$ is the entropy, discussed in Section 6.1). However, this could be due to uncertainty about the parameters, i.e., large $mathbb { H } ( p ( pmb { theta } | mathcal { D } ) )$ , or just due to inherent variability of the outcome, corresponding to large entropy of $p ( boldsymbol { y } | boldsymbol { mathbf { x } } , boldsymbol { theta } )$ . In the latter case, there would not be much use collecting more samples, since our epistemic uncertainty would not be reduced. See [Osb16] for further discussion of this point. \n2.1.3 Probability as an extension of logic \nIn this section, we review the basic rules of probability, following the presentation of [Jay03], in which we view probability as an extension of Boolean logic. \n2.1.3.1 Probability of an event \nWe define an event, denoted by the binary variable $A$ , as some state of the world that either holds or does not hold. For example, $A$ might be event “it will rain tomorrow”, or “it rained yesterday”, or “the label is $y = 1 ^ { gamma }$ , or “the parameter $theta$ is between 1.5 and 2.0”, etc. The expression $operatorname* { P r } ( A )$ denotes the probability with which you believe event $A$ is true (or the long run fraction of times that $A$ will occur). We require that $0 leq operatorname* { P r } ( A ) leq 1$ , where $operatorname* { P r } ( A ) = 0$ means the event definitely will not happen, and $operatorname* { P r } ( A ) = 1$ means the event definitely will happen. We write $operatorname* { P r } ( { overline { { A } } } )$ to denote the probability of event $A$ not happening; this is defined to be $operatorname* { P r } (  b { A } ) = 1 - operatorname* { P r } (  b { A } )$ . \n2.1.3.2 Probability of a conjunction of two events \nWe denote the joint probability of events $A$ and $B$ both happening as follows: \nIf $A$ and $B$ are independent events, we have \nFor example, suppose $X$ and $Y$ are chosen uniformly at random from the set $mathcal { X } = { 1 , 2 , 3 , 4 }$ . Let $A$ be the event that $X in { 1 , 2 }$ , and $B$ be the event that $Y in { 3 }$ . Then we have $operatorname* { P r } ( A , B ) =$ $operatorname* { P r } ( A ) operatorname* { P r } ( B ) = { frac { 1 } { 2 } } cdot { frac { 1 } { 4 } } $ . \n2.1.3.3 Probability of a union of two events \nThe probability of event $A$ or $B$ happening is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIf the events are mutually exclusive (so they cannot happen at the same time), we get \nFor example, suppose $X$ is chosen uniformly at random from the set $mathcal { X } = { 1 , 2 , 3 , 4 }$ . Let $A$ be the event that $X in { 1 , 2 }$ and $B$ be the event that $X in { 3 }$ . Then we have $textstyle operatorname* { P r } ( A vee B ) = { frac { 2 } { 4 } } + { frac { 1 } { 4 } }$ . \n2.1.3.4 Conditional probability of one event given another \nWe define the conditional probability of event $B$ happening given that $A$ has occurred as follows: \nThis is not defined if $Pr ( A ) = 0$ , since we cannot condition on an impossible event. \n2.1.3.5 Independence of events \nWe say that event $A$ is independent of event $B$ if \n2.1.3.6 Conditional independence of events \nWe say that events $A$ and $B$ are conditionally independent given event $C$ if \nThis is written as $A perp B | C$ . Events are often dependent on each other, but may be rendered independent if we condition on the relevant intermediate variables, as we discuss in more detail later in this chapter. \n2.2 Random variables \nSuppose $X$ represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of $X$ is unknown and/or could change, we call it a random variable or rv. The set of possible values, denoted $mathcal { X }$ , is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if $X$ represents the face of a dice that is rolled, so $mathcal { X } = { 1 , 2 , ldots , 6 }$ , the event of “seeing a $1 ^ { circ }$ is denoted $X = 1$ , the event of “seeing an odd number” is denoted $X in { 1 , 3 , 5 }$ , the event of “seeing a number between 1 and $3 ^ { circ }$ is denoted $1 leq X leq 3$ , etc. \n2.2.1 Discrete random variables \nIf the sample space $mathcal { X }$ is finite or countably infinite, then $X$ is called a discrete random variable. In this case, we denote the probability of the event that $X$ has value $x$ by $operatorname* { P r } ( X = x )$ . We define the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Introduction",
        "subsubsection": "Probability as an extension of logic"
    },
    {
        "content": "If the events are mutually exclusive (so they cannot happen at the same time), we get \nFor example, suppose $X$ is chosen uniformly at random from the set $mathcal { X } = { 1 , 2 , 3 , 4 }$ . Let $A$ be the event that $X in { 1 , 2 }$ and $B$ be the event that $X in { 3 }$ . Then we have $textstyle operatorname* { P r } ( A vee B ) = { frac { 2 } { 4 } } + { frac { 1 } { 4 } }$ . \n2.1.3.4 Conditional probability of one event given another \nWe define the conditional probability of event $B$ happening given that $A$ has occurred as follows: \nThis is not defined if $Pr ( A ) = 0$ , since we cannot condition on an impossible event. \n2.1.3.5 Independence of events \nWe say that event $A$ is independent of event $B$ if \n2.1.3.6 Conditional independence of events \nWe say that events $A$ and $B$ are conditionally independent given event $C$ if \nThis is written as $A perp B | C$ . Events are often dependent on each other, but may be rendered independent if we condition on the relevant intermediate variables, as we discuss in more detail later in this chapter. \n2.2 Random variables \nSuppose $X$ represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of $X$ is unknown and/or could change, we call it a random variable or rv. The set of possible values, denoted $mathcal { X }$ , is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if $X$ represents the face of a dice that is rolled, so $mathcal { X } = { 1 , 2 , ldots , 6 }$ , the event of “seeing a $1 ^ { circ }$ is denoted $X = 1$ , the event of “seeing an odd number” is denoted $X in { 1 , 3 , 5 }$ , the event of “seeing a number between 1 and $3 ^ { circ }$ is denoted $1 leq X leq 3$ , etc. \n2.2.1 Discrete random variables \nIf the sample space $mathcal { X }$ is finite or countably infinite, then $X$ is called a discrete random variable. In this case, we denote the probability of the event that $X$ has value $x$ by $operatorname* { P r } ( X = x )$ . We define the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nprobability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value: \nThe pmf satisfies the properties $0 leq p ( x ) leq 1$ and $begin{array} { r } { sum _ { x in mathcal { X } } p ( x ) = 1 } end{array}$ . \nIf $X$ has a finite number of values, say $K$ , the pmf can be represented as a list of $K$ numbers, which we can plot as a histogram. For example, Figure 2.1 shows two pmf’s defined on $mathcal { X } = { 1 , 2 , 3 , 4 }$ . On the left we have a uniform distribution, $p ( x ) = 1 / 4$ , and on the right, we have a degenerate distribution, $p ( x ) = mathbb { I } left( x = 1 right)$ , where $mathbb { I } left( right)$ is the binary indicator function. Thus the distribution in Figure 2.1(b) represents the fact that $X$ is always equal to the value 1. (Thus we see that random variables can also be constant.) \n2.2.2 Continuous random variables \nIf $X in mathbb R$ is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. If we associate events with $X$ being in each one of these intervals, we can use the methods discussed above for discrete random variables. Informally speaking, we can represent the probability of $X$ taking on a specific real value by allowing the size of the intervals to shrink to zero, as we show below. \n2.2.2.1 Cumulative distribution function (cdf) \nDefine the events $A = ( X leq a )$ , $B = ( X leq b )$ and $C = ( a < X leq b )$ , where $a < b$ . We have that $B = A lor C$ , and since $A$ and $C$ are mutually exclusive, the sum rules gives \nand hence the probability of being in interval $C$ is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Random variables",
        "subsubsection": "Discrete random variables"
    },
    {
        "content": "probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value: \nThe pmf satisfies the properties $0 leq p ( x ) leq 1$ and $begin{array} { r } { sum _ { x in mathcal { X } } p ( x ) = 1 } end{array}$ . \nIf $X$ has a finite number of values, say $K$ , the pmf can be represented as a list of $K$ numbers, which we can plot as a histogram. For example, Figure 2.1 shows two pmf’s defined on $mathcal { X } = { 1 , 2 , 3 , 4 }$ . On the left we have a uniform distribution, $p ( x ) = 1 / 4$ , and on the right, we have a degenerate distribution, $p ( x ) = mathbb { I } left( x = 1 right)$ , where $mathbb { I } left( right)$ is the binary indicator function. Thus the distribution in Figure 2.1(b) represents the fact that $X$ is always equal to the value 1. (Thus we see that random variables can also be constant.) \n2.2.2 Continuous random variables \nIf $X in mathbb R$ is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. If we associate events with $X$ being in each one of these intervals, we can use the methods discussed above for discrete random variables. Informally speaking, we can represent the probability of $X$ taking on a specific real value by allowing the size of the intervals to shrink to zero, as we show below. \n2.2.2.1 Cumulative distribution function (cdf) \nDefine the events $A = ( X leq a )$ , $B = ( X leq b )$ and $C = ( a < X leq b )$ , where $a < b$ . We have that $B = A lor C$ , and since $A$ and $C$ are mutually exclusive, the sum rules gives \nand hence the probability of being in interval $C$ is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn general, we define the cumulative distribution function or cdf of the rv $X$ as follows: \n(Note that we use a capital $P$ to represent the cdf.) Using this, we can compute the probability of being in any interval as follows: \nCdf’s are monotonically non-decreasing functions. See Figure 2.2a for an example, where we illustrate the cdf of a standard normal distribution, $mathcal { N } ( x | 0 , 1 )$ ; see Section 2.6 for details. \n2.2.2.2 Probability density function (pdf) \nWe define the probability density function or pdf as the derivative of the cdf: \n(Note that this derivative does not always exist, in which case the pdf is not defined.) See Figure 2.2b for an example, where we illustrate the pdf of a univariate Gaussian (see Section 2.6 for details). \nGiven a pdf, we can compute the probability of a continuous variable being in a finite interval as follows: \nAs the size of the interval gets smaller, we can write \nIntuitively, this says the probability of $X$ being in a small interval around $x$ is the density at $x$ times the width of the interval. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n2.2.2.3 Quantiles \nIf the cdf $P$ is strictly monotonically increasing, it has an inverse, called the inverse cdf, or percent point function $mathbf { Pi } ( mathbf { p p f } )$ , or quantile function. \nIf $P$ is the cdf of $X$ , then $P ^ { - 1 } ( q )$ is the value $x _ { q }$ such that $Pr ( X leq x _ { q } ) = q$ ; this is called the $q$ ’th quantile of $P$ . The value $P ^ { - 1 } ( 0 . 5 )$ is the median of the distribution, with half of the probability mass on the left, and half on the right. The values $P ^ { - 1 } ( 0 . 2 5 )$ and $P ^ { - 1 } ( 0 . 7 5 )$ are the lower and upper quartiles. \nFor example, let $Phi$ be the cdf of the Gaussian distribution $mathcal { N } ( 0 , 1 )$ , and $Phi ^ { - 1 }$ be the inverse cdf. Then points to the left of $Phi ^ { - 1 } ( alpha / 2 )$ contain $alpha / 2$ of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of $Phi ^ { - 1 } ( 1 - alpha / 2 )$ also contain $alpha / 2$ of the mass. Hence the central interval $( Phi ^ { - 1 } ( alpha / 2 ) , Phi ^ { - 1 } ( 1 - alpha / 2 ) )$ contains $1 - alpha$ of the mass. If we set $alpha = 0 . 0 5$ , the central $9 5 %$ interval is covered by the range \nIf the distribution is ${ mathcal { N } } ( mu , sigma ^ { 2 } )$ , then the $9 5 %$ interval becomes $( mu - 1 . 9 6 sigma , mu + 1 . 9 6 sigma )$ . This is often approximated by writing $mu pm 2 sigma$ . \n2.2.3 Sets of related random variables \nIn this section, we discuss distributions over sets of related random variables. \nSuppose, to start, that we have two random variables, $X$ and $Y$ . We can define the joint distribution of two random variables using $p ( x , y ) = p ( X = x , Y = y )$ for all possible values of $X$ and $Y$ . If both variables have finite cardinality, we can represent the joint distribution as a 2d table, all of whose entries sum to one. For example, consider the following example with two binary variables: \nIf two variables are independent, we can represent the joint as the product of the two marginals. If both variables have finite cardinality, we can factorize the 2d joint table into a product of two 1d vectors, as shown in Figure 2.3. \nGiven a joint distribution, we define the marginal distribution of an rv as follows: \nwhere we are summing over all possible states of $Y$ . This is sometimes called the sum rule or the rule of total probability. We define $p ( Y = y )$ similarly. For example, from the above 2d table, we see $p ( X = 0 ) = 0 . 2 + 0 . 3 = 0 . 5$ and $p ( Y = 0 ) = 0 . 2 + 0 . 3 = 0 . 5$ . (The term “marginal” comes from the accounting practice of writing the sums of rows and columns on the side, or margin, of a table.) We define the conditional distribution of an rv using \nWe can rearrange this equation to get \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Random variables",
        "subsubsection": "Continuous random variables"
    },
    {
        "content": "2.2.2.3 Quantiles \nIf the cdf $P$ is strictly monotonically increasing, it has an inverse, called the inverse cdf, or percent point function $mathbf { Pi } ( mathbf { p p f } )$ , or quantile function. \nIf $P$ is the cdf of $X$ , then $P ^ { - 1 } ( q )$ is the value $x _ { q }$ such that $Pr ( X leq x _ { q } ) = q$ ; this is called the $q$ ’th quantile of $P$ . The value $P ^ { - 1 } ( 0 . 5 )$ is the median of the distribution, with half of the probability mass on the left, and half on the right. The values $P ^ { - 1 } ( 0 . 2 5 )$ and $P ^ { - 1 } ( 0 . 7 5 )$ are the lower and upper quartiles. \nFor example, let $Phi$ be the cdf of the Gaussian distribution $mathcal { N } ( 0 , 1 )$ , and $Phi ^ { - 1 }$ be the inverse cdf. Then points to the left of $Phi ^ { - 1 } ( alpha / 2 )$ contain $alpha / 2$ of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of $Phi ^ { - 1 } ( 1 - alpha / 2 )$ also contain $alpha / 2$ of the mass. Hence the central interval $( Phi ^ { - 1 } ( alpha / 2 ) , Phi ^ { - 1 } ( 1 - alpha / 2 ) )$ contains $1 - alpha$ of the mass. If we set $alpha = 0 . 0 5$ , the central $9 5 %$ interval is covered by the range \nIf the distribution is ${ mathcal { N } } ( mu , sigma ^ { 2 } )$ , then the $9 5 %$ interval becomes $( mu - 1 . 9 6 sigma , mu + 1 . 9 6 sigma )$ . This is often approximated by writing $mu pm 2 sigma$ . \n2.2.3 Sets of related random variables \nIn this section, we discuss distributions over sets of related random variables. \nSuppose, to start, that we have two random variables, $X$ and $Y$ . We can define the joint distribution of two random variables using $p ( x , y ) = p ( X = x , Y = y )$ for all possible values of $X$ and $Y$ . If both variables have finite cardinality, we can represent the joint distribution as a 2d table, all of whose entries sum to one. For example, consider the following example with two binary variables: \nIf two variables are independent, we can represent the joint as the product of the two marginals. If both variables have finite cardinality, we can factorize the 2d joint table into a product of two 1d vectors, as shown in Figure 2.3. \nGiven a joint distribution, we define the marginal distribution of an rv as follows: \nwhere we are summing over all possible states of $Y$ . This is sometimes called the sum rule or the rule of total probability. We define $p ( Y = y )$ similarly. For example, from the above 2d table, we see $p ( X = 0 ) = 0 . 2 + 0 . 3 = 0 . 5$ and $p ( Y = 0 ) = 0 . 2 + 0 . 3 = 0 . 5$ . (The term “marginal” comes from the accounting practice of writing the sums of rows and columns on the side, or margin, of a table.) We define the conditional distribution of an rv using \nWe can rearrange this equation to get \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis is called the product rule. By extending the product rule to $D$ variables, we get the chain rule of probability: \nThis provides a way to create a high dimensional joint distribution from a set of conditional distributions. We discuss this in more detail in Section 3.6. \n2.2.4 Independence and conditional independence \nWe say $X$ and $Y$ are unconditionally independent or marginally independent, denoted $X perp Y$ , if we can represent the joint as the product of the two marginals (see Figure 2.3), i.e., \nIn general, we say a set of variables $X _ { 1 } , ldots , X _ { n }$ is (mutually) independent if the joint can be written as a product of marginals for all subsets ${ X _ { 1 } , ldots , X _ { m } } subseteq { X _ { 1 } , ldots , X _ { n } }$ : i.e., \nFor example, we say $X _ { 1 } , X _ { 2 } , X _ { 3 }$ are mutually independent if the following conditions hold: $p ( X _ { 1 } , X _ { 2 } , X _ { 3 } ) =$ $p ( X _ { 1 } ) p ( X _ { 2 } ) p ( X _ { 3 } ) , p ( X _ { 1 } , X _ { 2 } ) = p ( X _ { 1 } ) p ( X _ { 2 } ) , p ( X _ { 2 } , X _ { 3 } ) = p ( X _ { 2 } ) p ( X _ { 3 } )$ , and $p ( X _ { 1 } , X _ { 3 } ) = p ( X _ { 1 } ) p ( X _ { 3 } )$ .2 \nUnfortunately, unconditional independence is rare, because most variables can influence most other variables. However, usually this influence is mediated via other variables rather than being direct. We therefore say $X$ and $Y$ are conditionally independent (CI) given $Z$ iff the conditional joint can be written as a product of conditional marginals:",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Random variables",
        "subsubsection": "Sets of related random variables"
    },
    {
        "content": "This is called the product rule. By extending the product rule to $D$ variables, we get the chain rule of probability: \nThis provides a way to create a high dimensional joint distribution from a set of conditional distributions. We discuss this in more detail in Section 3.6. \n2.2.4 Independence and conditional independence \nWe say $X$ and $Y$ are unconditionally independent or marginally independent, denoted $X perp Y$ , if we can represent the joint as the product of the two marginals (see Figure 2.3), i.e., \nIn general, we say a set of variables $X _ { 1 } , ldots , X _ { n }$ is (mutually) independent if the joint can be written as a product of marginals for all subsets ${ X _ { 1 } , ldots , X _ { m } } subseteq { X _ { 1 } , ldots , X _ { n } }$ : i.e., \nFor example, we say $X _ { 1 } , X _ { 2 } , X _ { 3 }$ are mutually independent if the following conditions hold: $p ( X _ { 1 } , X _ { 2 } , X _ { 3 } ) =$ $p ( X _ { 1 } ) p ( X _ { 2 } ) p ( X _ { 3 } ) , p ( X _ { 1 } , X _ { 2 } ) = p ( X _ { 1 } ) p ( X _ { 2 } ) , p ( X _ { 2 } , X _ { 3 } ) = p ( X _ { 2 } ) p ( X _ { 3 } )$ , and $p ( X _ { 1 } , X _ { 3 } ) = p ( X _ { 1 } ) p ( X _ { 3 } )$ .2 \nUnfortunately, unconditional independence is rare, because most variables can influence most other variables. However, usually this influence is mediated via other variables rather than being direct. We therefore say $X$ and $Y$ are conditionally independent (CI) given $Z$ iff the conditional joint can be written as a product of conditional marginals: \nWe can write this assumption as a graph $X mathrm { ~ - ~ } Z mathrm { ~ - ~ } Y$ , which captures the intuition that all the dependencies between $X$ and $Y$ are mediated via $Z$ . By using larger graphs, we can define complex joint distributions; these are known as graphical models, and are discussed in Section 3.6. \n2.2.5 Moments of a distribution \nIn this section, we describe various summary statistics that can be derived from a probability distribution (either a pdf or pmf). \n2.2.5.1 Mean of a distribution \nThe most familiar property of a distribution is its mean, or expected value, often denoted by $mu$ . For continuous rv’s, the mean is defined as follows: \nIf the integral is not finite, the mean is not defined; we will see some examples of this later. For discrete rv’s, the mean is defined as follows: \nHowever, this is only meaningful if the values of $x$ are ordered in some way (e.g., if they represent integer counts). \nSince the mean is a linear operator, we have \nThis is called the linearity of expectation. \nFor a set of $n$ random variables, one can show that the expectation of their sum is as follows: \nIf they are independent, the expectation of their product is given by \n2.2.5.2 Variance of a distribution \nThe variance is a measure of the “spread” of a distribution, often denoted by $sigma ^ { 2 }$ . This is defined as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Random variables",
        "subsubsection": "Independence and conditional independence"
    },
    {
        "content": "We can write this assumption as a graph $X mathrm { ~ - ~ } Z mathrm { ~ - ~ } Y$ , which captures the intuition that all the dependencies between $X$ and $Y$ are mediated via $Z$ . By using larger graphs, we can define complex joint distributions; these are known as graphical models, and are discussed in Section 3.6. \n2.2.5 Moments of a distribution \nIn this section, we describe various summary statistics that can be derived from a probability distribution (either a pdf or pmf). \n2.2.5.1 Mean of a distribution \nThe most familiar property of a distribution is its mean, or expected value, often denoted by $mu$ . For continuous rv’s, the mean is defined as follows: \nIf the integral is not finite, the mean is not defined; we will see some examples of this later. For discrete rv’s, the mean is defined as follows: \nHowever, this is only meaningful if the values of $x$ are ordered in some way (e.g., if they represent integer counts). \nSince the mean is a linear operator, we have \nThis is called the linearity of expectation. \nFor a set of $n$ random variables, one can show that the expectation of their sum is as follows: \nIf they are independent, the expectation of their product is given by \n2.2.5.2 Variance of a distribution \nThe variance is a measure of the “spread” of a distribution, often denoted by $sigma ^ { 2 }$ . This is defined as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nfrom which we derive the useful result \nThe standard deviation is defined as \nThis is useful since it has the same units as $X$ itself. \nThe variance of a shifted and scaled version of a random variable is given by \nIf we have a set of $n$ independent random variables, the variance of their sum is given by the sum of their variances: \nThe variance of their product can also be derived, as follows: \n2.2.5.3 Mode of a distribution \nThe mode of a distribution is the value with the highest probability mass or probability density: \nIf the distribution is multimodal, this may not be unique, as illustrated in Figure 2.4. Furthermore, even if there is a unique mode, this point may not be a good summary of the distribution. \n2.2.5.4 Conditional moments \nWhen we have two or more dependent random variables, we can compute the moments of one given knowledge of the other. For example, the law of iterated expectations, also called the law of total expectation, tells us that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTo prove this, let us suppose, for simplicity, that $X$ and $Y$ are both discrete rv’s. Then we have \nTo give a more intuitive explanation, consider the following simple example.3 Let $X$ be the lifetime duration of a lightbulb, and let $Y$ be the factory the lightbulb was produced in. Suppose $mathbb { E } left[ X | Y = 1 right] = 5 0 0 0$ and $mathbb { E } left[ X | Y = 2 right] = 4 0 0 0$ , indicating that factory 1 produces longer lasting bulbs. Suppose factory 1 supplies $6 0 %$ of the lightbulbs, so $p ( Y = 1 ) = 0 . 6$ and $p ( Y = 2 ) = 0 . 4$ . Then the expected duration of a random lightbulb is given by \nThere is a similar formula for the variance. In particular, the law of total variance, also called the conditional variance formula, tells us that \nTo see this, let us define the conditional moments, $mu _ { X | Y } = operatorname { mathbb { E } } left[ X | Y right]$ , $s _ { X | Y } = mathbb { E } leftlfloor X ^ { 2 } | Y rightrfloor$ , and $sigma _ { X | Y } ^ { 2 } = Psi left[ X | Y right] = s _ { X | Y } - mu _ { X | Y } ^ { 2 }$ , which are functions of $Y$ (and therefore are random\u0002 quan\u0003tities). Then we have \nTo get some intuition for these formulas, consider a mixture of $K$ univariate Gaussians. Let $Y$ be the hidden indicator variable that specifies which mixture component we are using, and let \n$begin{array} { r } { X = sum _ { y = 1 } ^ { K } pi _ { y } mathcal { N } ( X | mu _ { y } , sigma _ { y } ) } end{array}$ . In Figure 2.4, we have $pi _ { 1 } = pi _ { 2 } = 0 . 5$ , $mu _ { 1 } = 0$ , $mu _ { 2 } = 2$ , $sigma _ { 1 } = sigma _ { 2 } = 0 . 5$ . Thus \nSo we get the intuitive result that the variance of $X$ is dominated by which centroid it is drawn from (i.e., difference in the means), rather than the local variance around each centroid. \n2.2.6 Limitations of summary statistics * \nAlthough it is common to summarize a probability distribution (or points sampled from a distribution) using simple statistics such as the mean and variance, this can lose a lot of information. A striking example of this is known as Anscombe’s quartet [Ans73], which is illustrated in Figure 2.5. This shows 4 different datasets of $( x , y )$ pairs, all of which have identical mean, variance and correlation coefficient $rho$ (defined in Section 3.1.2): $mathbb { E } left[ x right] = 9$ , $mathbb { V } left[ x right] = 1 1 , mathbb { E } left[ y right] = 7 . 5 0 , mathbb { V } left[ y right] = 4 . 1 2$ , and $rho = 0 . 8 1 6$ .4 However, the joint distributions $p ( x , y )$ from which these points were sampled are clearly very different. Anscombe invented these datasets, each consisting of 10 data points, to counter the impression among statisticians that numerical summaries are superior to data visualization [Ans73]. \nAn even more striking example of this phenomenon is shown in Figure 2.6. This consists of a dataset that looks like a dinosaur5, plus 11 other datasets, all of which have identical low order statistics. This collection of datasets is called the Datasaurus Dozen [MF17]. The exact values of the $( x , y )$ points are available online.6 They were computed using simulated annealing, a derivative free optimization method which we discuss in the sequel to this book, [Mur23]. (The objective function being optimized measures deviation from the target summary statistics of the original dinosaur, plus distance from a particular target shape.)",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Random variables",
        "subsubsection": "Moments of a distribution"
    },
    {
        "content": "$begin{array} { r } { X = sum _ { y = 1 } ^ { K } pi _ { y } mathcal { N } ( X | mu _ { y } , sigma _ { y } ) } end{array}$ . In Figure 2.4, we have $pi _ { 1 } = pi _ { 2 } = 0 . 5$ , $mu _ { 1 } = 0$ , $mu _ { 2 } = 2$ , $sigma _ { 1 } = sigma _ { 2 } = 0 . 5$ . Thus \nSo we get the intuitive result that the variance of $X$ is dominated by which centroid it is drawn from (i.e., difference in the means), rather than the local variance around each centroid. \n2.2.6 Limitations of summary statistics * \nAlthough it is common to summarize a probability distribution (or points sampled from a distribution) using simple statistics such as the mean and variance, this can lose a lot of information. A striking example of this is known as Anscombe’s quartet [Ans73], which is illustrated in Figure 2.5. This shows 4 different datasets of $( x , y )$ pairs, all of which have identical mean, variance and correlation coefficient $rho$ (defined in Section 3.1.2): $mathbb { E } left[ x right] = 9$ , $mathbb { V } left[ x right] = 1 1 , mathbb { E } left[ y right] = 7 . 5 0 , mathbb { V } left[ y right] = 4 . 1 2$ , and $rho = 0 . 8 1 6$ .4 However, the joint distributions $p ( x , y )$ from which these points were sampled are clearly very different. Anscombe invented these datasets, each consisting of 10 data points, to counter the impression among statisticians that numerical summaries are superior to data visualization [Ans73]. \nAn even more striking example of this phenomenon is shown in Figure 2.6. This consists of a dataset that looks like a dinosaur5, plus 11 other datasets, all of which have identical low order statistics. This collection of datasets is called the Datasaurus Dozen [MF17]. The exact values of the $( x , y )$ points are available online.6 They were computed using simulated annealing, a derivative free optimization method which we discuss in the sequel to this book, [Mur23]. (The objective function being optimized measures deviation from the target summary statistics of the original dinosaur, plus distance from a particular target shape.) \n\nThe same simulated annealing approach can be applied to 1d datasets, as shown in Figure 2.7. We see that all the datasets are quite different, but they all have the same median and inter-quartile range as shown by the central shaded part of the box plots in the middle. A better visualization is known as a violin plot, shown on the right. This shows (two copies of) the 1d kernel density estimate (Section 16.3) of the distribution on the vertical axis, in addition to the median and IQR markers. This visualization is better able to distinguish differences in the distributions. However, the technique is limited to 1d data. \n2.3 Bayes’ rule \nBayes’s theorem is to the theory of probability what Pythagoras’s theorem is to geometry. Sir Harold Jeffreys, 1973 [Jef73]. \nIn this section, we discuss the basics of Bayesian inference. According to the Merriam-Webster dictionary, the term “inference” means “the act of passing from sample data to generalizations, usually with calculated degrees of certainty”. The term “Bayesian” is used to refer to inference methods \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Random variables",
        "subsubsection": "Limitations of summary statistics *"
    },
    {
        "content": "Normalizing the joint distribution by computing $p ( H = h , Y = y ) / p ( Y = y )$ for each $h$ gives the posterior distribution $p ( H = h | Y = y ) $ ; this represents our new belief state about the possible values of $H$ . \nWe can summarize Bayes rule in words as follows: \nposterior $propto$ prior $times$ likelihood \nHere we use the symbol $propto$ to denote “proportional to”, since we are ignoring the denominator, which is just a constant, independent of $H$ . Using Bayes rule to update a distribution over unknown values of some quantity of interest, given relevant observed data, is called Bayesian inference, or posterior inference. It can also just be called probabilistic inference. \nBelow we give some simple examples of Bayesian inference in action. We will see many more interesting examples later in this book. \n2.3.1 Example: Testing for COVID-19 \nSuppose you think you may have contracted COVID-19, which is an infectious disease caused by the SARS-CoV-2 virus. You decide to take a diagnostic test, and you want to use its result to determine if you are infected or not. \nLet $H = 1$ be the event that you are infected, and $H = 0$ be the event you are not infected. Let $Y = 1$ if the test is positive, and $Y = 0$ if the test is negative. We want to compute $p ( H = h | Y = y ,$ ), for $h in { 0 , 1 }$ , where $y$ is the observed test outcome. (We will write the distribution of values, $[ p ( H = 0 | Y = y ) , p ( H = 1 | Y = y ) ]$ ] as $p ( H | y )$ , for brevity.) We can think of this as a form of binary classification, where $H$ is the unknown class label, and $y$ is the feature vector. \nFirst we must specify the likelihood. This quantity obviously depends on how reliable the test is. There are two key parameters. The sensitivity (aka true positive rate) is defined as $p ( Y = 1 | H = 1 )$ , i.e., the probability of a positive test given that the truth is positive. The false negative rate is defined as one minus the sensitivity. The specificity (aka true negative rate) is defined as $p ( Y = 0 | H = 0 )$ , i.e., the probability of a negative test given that the truth is negative. The false positive rate is defined as one minus the specificity. We summarize all these quantities in Table 2.1. (See Section 5.1.3.1 for more details.) Following https://nyti.ms/31MTZgV, we set the sensitivity to $8 7 . 5 %$ and the specificity to $9 7 . 5 %$ . \nNext we must specify the prior. The quantity $p ( H = 1 )$ represents the prevalence of the disease in the area in which you live. We set this to $p ( H = 1 ) = 0 . 1$ (i.e., $1 0 %$ ), which was the prevalence in New York City in Spring 2020. (This example was chosen to match the numbers in https://nyti.ms/31MTZgV.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nNow suppose you test positive. We have \nSo there is a $7 9 . 5 %$ chance you are infected. \nNow suppose you test negative. The probability you are infected is given by \nSo there is just a $1 . 4 %$ chance you are infected. \nNowadays COVID-19 prevalence is much lower. Suppose we repeat these calculations using a base rate of $1 %$ ; now the posteriors reduce to $2 6 %$ and $0 . 1 3 %$ respectively. \nThe fact that you only have a $2 6 %$ chance of being infected with COVID-19, even after a positive test, is very counter-intuitive. The reason is that a single positive test is more likely to be a false positive than due to the disease, since the disease is rare. To see this, suppose we have a population of 100,000 people, of whom 1000 are infected. Of those who are infected, $8 7 5 = 0 . 8 7 5 times 1 0 0 0$ test positive, and of those who are uninfected, $2 4 7 5 = 0 . 0 2 5 times 9 9 , 0 0 0$ test positive. Thus the total number of positives is $3 3 5 0 = 8 7 5 + 2 4 7 5$ , so the posterior probability of being infected given a positive test is $8 7 5 / 3 3 5 0 = 0 . 2 6$ . \nOf course, the above calculations assume we know the sensitivity and specificity of the test. See [GC20] for how to apply Bayes rule for diagnostic testing when there is uncertainty about these parameters. \n2.3.2 Example: The Monty Hall problem \nIn this section, we consider a more “frivolous” application of Bayes rule. In particular, we apply it to the famous Monty Hall problem. \nImagine a game show with the following rules: There are three doors, labeled 1, 2, 3. A single prize (e.g., a car) has been hidden behind one of them. You get to select one door. Then the gameshow host opens one of the other two doors (not the one you picked), in such a way as to not reveal the prize location. At this point, you will be given a fresh choice of door: you can either stick with your first choice, or you can switch to the other closed door. All the doors will then be opened and you will receive whatever is behind your final choice of door. \nFor example, suppose you choose door 1, and the gameshow host opens door 3, revealing nothing behind the door, as promised. Should you (a) stick with door 1, or (b) switch to door 2, or (c) does it make no difference? \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Bayes' rule",
        "subsubsection": "Example: Testing for COVID-19"
    },
    {
        "content": "Now suppose you test positive. We have \nSo there is a $7 9 . 5 %$ chance you are infected. \nNow suppose you test negative. The probability you are infected is given by \nSo there is just a $1 . 4 %$ chance you are infected. \nNowadays COVID-19 prevalence is much lower. Suppose we repeat these calculations using a base rate of $1 %$ ; now the posteriors reduce to $2 6 %$ and $0 . 1 3 %$ respectively. \nThe fact that you only have a $2 6 %$ chance of being infected with COVID-19, even after a positive test, is very counter-intuitive. The reason is that a single positive test is more likely to be a false positive than due to the disease, since the disease is rare. To see this, suppose we have a population of 100,000 people, of whom 1000 are infected. Of those who are infected, $8 7 5 = 0 . 8 7 5 times 1 0 0 0$ test positive, and of those who are uninfected, $2 4 7 5 = 0 . 0 2 5 times 9 9 , 0 0 0$ test positive. Thus the total number of positives is $3 3 5 0 = 8 7 5 + 2 4 7 5$ , so the posterior probability of being infected given a positive test is $8 7 5 / 3 3 5 0 = 0 . 2 6$ . \nOf course, the above calculations assume we know the sensitivity and specificity of the test. See [GC20] for how to apply Bayes rule for diagnostic testing when there is uncertainty about these parameters. \n2.3.2 Example: The Monty Hall problem \nIn this section, we consider a more “frivolous” application of Bayes rule. In particular, we apply it to the famous Monty Hall problem. \nImagine a game show with the following rules: There are three doors, labeled 1, 2, 3. A single prize (e.g., a car) has been hidden behind one of them. You get to select one door. Then the gameshow host opens one of the other two doors (not the one you picked), in such a way as to not reveal the prize location. At this point, you will be given a fresh choice of door: you can either stick with your first choice, or you can switch to the other closed door. All the doors will then be opened and you will receive whatever is behind your final choice of door. \nFor example, suppose you choose door 1, and the gameshow host opens door 3, revealing nothing behind the door, as promised. Should you (a) stick with door 1, or (b) switch to door 2, or (c) does it make no difference? \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTable 2.2: 3 possible states for the Monty Hall game, showing that switching doors is two times better (on average) than staying with your original choice. Adapted from Table 6.1 of [PM18]. \nIntuitively, it seems it should make no difference, since your initial choice of door cannot influence the location of the prize. However, the fact that the host opened door 3 tells us something about the location of the prize, since he made his choice conditioned on the knowledge of the true location and on your choice. As we show below, you are in fact twice as likely to win the prize if you switch to door 2. \nTo show this, we will use Bayes’ rule. Let $H _ { i }$ denote the hypothesis that the prize is behind door $i$ . We make the following assumptions: the three hypotheses $H _ { 1 }$ , $H _ { 2 }$ and $H _ { 3 }$ are equiprobable a priori, i.e., \nThe datum we receive, after choosing door 1, is either $Y = 3$ and $Y = 2$ (meaning door 3 or 2 is opened, respectively). We assume that these two possible outcomes have the following probabilities. If the prize is behind door 1, then the host selects at random between $Y = 2$ and $Y = 3$ . Otherwise the choice of the host is forced and the probabilities are 0 and $^ { 1 }$ . \nNow, using Bayes’ theorem, we evaluate the posterior probabilities of the hypotheses: \nThe denominator $P ( Y = 3 )$ is $begin{array} { r } { P ( Y = 3 ) = frac { 1 } { 6 } + frac { 1 } { 3 } = frac { 1 } { 2 } } end{array}$ . So \nSo the contestant should switch to door 2 in order to have the biggest chance of getting the prize.   \nSee Table 2.2 for a worked example. \nMany people find this outcome surprising. One way to make it more intuitive is to perform a thought experiment in which the game is played with a million doors. The rules are now that the contestant chooses one door, then the game show host opens 999,998 doors in such a way as not to reveal the prize, leaving the contestant’s selected door and one other door closed. The contestant may now stick or switch. Imagine the contestant confronted by a million doors, of which doors 1 and 234,598 have not been opened, door 1 having been the contestant’s initial guess. Where do you think the prize is? \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n2.3.3 Inverse problems * \nProbability theory is concerned with predicting a distribution over outcomes $y$ given knowledge (or assumptions) about the state of the world, $h$ . By contrast, inverse probability is concerned with inferring the state of the world from observations of outcomes. We can think of this as inverting the $h  y$ mapping. \nFor example, consider trying to infer a 3d shape $h$ from a 2d image $y$ , which is a classic problem in visual scene understanding. Unfortunately, this is a fundamentally ill-posed problem, as illustrated in Figure 2.8, since there are multiple possible hidden $h$ ’s consistent with the same observed $y$ (see e.g., [Piz01]). Similarly, we can view natural language understanding as an ill-posed problem, in which the listener must infer the intention $h$ from the (often ambiguous) words spoken by the speaker (see e.g., [Sab21]). \nTo tackle such inverse problems, we can use Bayes’ rule to compute the posterior, $p ( h | y )$ , which gives a distribution over possible states of the world. This requires specifying the forwards model, $p ( y | h )$ , as well as a prior $p ( h )$ , which can be used to rule out (or downweight) implausible world states. We discuss this topic in more detail in the sequel to this book, [Mur23]. \n2.4 Bernoulli and binomial distributions \nPerhaps the simplest probability distribution is the Bernoulli distribution, which can be used to model binary events, as we discuss below. \n2.4.1 Definition \nConsider tossing a coin, where the probability of event that it lands heads is given by $0 leq theta leq 1$ . Let $Y = 1$ denote this event, and let $Y = 0$ denote the event that the coin lands tails. Thus we are assuming that $p ( Y = 1 ) = theta$ and $p ( Y = 0 ) = 1 - theta$ . This is called the Bernoulli distribution, and can be written as follows \n$Y sim operatorname { B e r } ( theta )$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where the symbol $sim$ means “is sampled from” or “is distributed as”, and Ber refers to Bernoulli. The probability mass function (pmf) of this distribution is defined as follows:",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Bayes' rule",
        "subsubsection": "Example: The Monty Hall problem"
    },
    {
        "content": "2.3.3 Inverse problems * \nProbability theory is concerned with predicting a distribution over outcomes $y$ given knowledge (or assumptions) about the state of the world, $h$ . By contrast, inverse probability is concerned with inferring the state of the world from observations of outcomes. We can think of this as inverting the $h  y$ mapping. \nFor example, consider trying to infer a 3d shape $h$ from a 2d image $y$ , which is a classic problem in visual scene understanding. Unfortunately, this is a fundamentally ill-posed problem, as illustrated in Figure 2.8, since there are multiple possible hidden $h$ ’s consistent with the same observed $y$ (see e.g., [Piz01]). Similarly, we can view natural language understanding as an ill-posed problem, in which the listener must infer the intention $h$ from the (often ambiguous) words spoken by the speaker (see e.g., [Sab21]). \nTo tackle such inverse problems, we can use Bayes’ rule to compute the posterior, $p ( h | y )$ , which gives a distribution over possible states of the world. This requires specifying the forwards model, $p ( y | h )$ , as well as a prior $p ( h )$ , which can be used to rule out (or downweight) implausible world states. We discuss this topic in more detail in the sequel to this book, [Mur23]. \n2.4 Bernoulli and binomial distributions \nPerhaps the simplest probability distribution is the Bernoulli distribution, which can be used to model binary events, as we discuss below. \n2.4.1 Definition \nConsider tossing a coin, where the probability of event that it lands heads is given by $0 leq theta leq 1$ . Let $Y = 1$ denote this event, and let $Y = 0$ denote the event that the coin lands tails. Thus we are assuming that $p ( Y = 1 ) = theta$ and $p ( Y = 0 ) = 1 - theta$ . This is called the Bernoulli distribution, and can be written as follows \n$Y sim operatorname { B e r } ( theta )$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where the symbol $sim$ means “is sampled from” or “is distributed as”, and Ber refers to Bernoulli. The probability mass function (pmf) of this distribution is defined as follows:",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Bayes' rule",
        "subsubsection": "Inverse problems *"
    },
    {
        "content": "2.3.3 Inverse problems * \nProbability theory is concerned with predicting a distribution over outcomes $y$ given knowledge (or assumptions) about the state of the world, $h$ . By contrast, inverse probability is concerned with inferring the state of the world from observations of outcomes. We can think of this as inverting the $h  y$ mapping. \nFor example, consider trying to infer a 3d shape $h$ from a 2d image $y$ , which is a classic problem in visual scene understanding. Unfortunately, this is a fundamentally ill-posed problem, as illustrated in Figure 2.8, since there are multiple possible hidden $h$ ’s consistent with the same observed $y$ (see e.g., [Piz01]). Similarly, we can view natural language understanding as an ill-posed problem, in which the listener must infer the intention $h$ from the (often ambiguous) words spoken by the speaker (see e.g., [Sab21]). \nTo tackle such inverse problems, we can use Bayes’ rule to compute the posterior, $p ( h | y )$ , which gives a distribution over possible states of the world. This requires specifying the forwards model, $p ( y | h )$ , as well as a prior $p ( h )$ , which can be used to rule out (or downweight) implausible world states. We discuss this topic in more detail in the sequel to this book, [Mur23]. \n2.4 Bernoulli and binomial distributions \nPerhaps the simplest probability distribution is the Bernoulli distribution, which can be used to model binary events, as we discuss below. \n2.4.1 Definition \nConsider tossing a coin, where the probability of event that it lands heads is given by $0 leq theta leq 1$ . Let $Y = 1$ denote this event, and let $Y = 0$ denote the event that the coin lands tails. Thus we are assuming that $p ( Y = 1 ) = theta$ and $p ( Y = 0 ) = 1 - theta$ . This is called the Bernoulli distribution, and can be written as follows \n$Y sim operatorname { B e r } ( theta )$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where the symbol $sim$ means “is sampled from” or “is distributed as”, and Ber refers to Bernoulli. The probability mass function (pmf) of this distribution is defined as follows: \n\n(See Section 2.2.1 for details on pmf’s.) We can write this in a more concise manner as follows: \nThe Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of $N$ Bernoulli trials, denoted $y _ { n } sim mathrm { B e r } ( cdot | theta )$ , for $n = 1 : N$ . Concretely, think of tossing a coin $N$ times. Let us define $s$ to be the total number of heads, $begin{array} { r } { s triangleq sum _ { n = 1 } ^ { N } mathbb { I } left( y _ { n } = 1 right) } end{array}$ . The distribution of $s$ is given by the binomial distribution: \nwhere \nis the number of ways to choose $k$ items from $N$ (this is known as the binomial coefficient, and is pronounced “N choose k”). See Figure 2.9 for some examples of the binomial distribution. If $N = 1$ , the binomial distribution reduces to the Bernoulli distribution. \n2.4.2 Sigmoid (logistic) function \nWhen we want to predict a binary variable $y in { 0 , 1 }$ given some inputs $pmb { x } in mathcal { X }$ , we need to use a conditional probability distribution of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Bernoulli and binomial distributions",
        "subsubsection": "Definition"
    },
    {
        "content": "(See Section 2.2.1 for details on pmf’s.) We can write this in a more concise manner as follows: \nThe Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of $N$ Bernoulli trials, denoted $y _ { n } sim mathrm { B e r } ( cdot | theta )$ , for $n = 1 : N$ . Concretely, think of tossing a coin $N$ times. Let us define $s$ to be the total number of heads, $begin{array} { r } { s triangleq sum _ { n = 1 } ^ { N } mathbb { I } left( y _ { n } = 1 right) } end{array}$ . The distribution of $s$ is given by the binomial distribution: \nwhere \nis the number of ways to choose $k$ items from $N$ (this is known as the binomial coefficient, and is pronounced “N choose k”). See Figure 2.9 for some examples of the binomial distribution. If $N = 1$ , the binomial distribution reduces to the Bernoulli distribution. \n2.4.2 Sigmoid (logistic) function \nWhen we want to predict a binary variable $y in { 0 , 1 }$ given some inputs $pmb { x } in mathcal { X }$ , we need to use a conditional probability distribution of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTable 2.3: Some useful properties of the sigmoid (logistic) and related functions. Note that the logit function is the inverse of the sigmoid function, and has a domain of [0, 1]. \nwhere $f ( { pmb x } ; { pmb theta } )$ is some function that predicts the mean parameter of the output distribution. We will consider many different kinds of function $f$ in Part II–Part IV. \nTo avoid the requirement that $0 leq f ( pmb { x } ; pmb { theta } ) leq 1$ , we can let $f$ be an unconstrained function, and use the following model: \nHere $sigma ( )$ is the sigmoid or logistic function, defined as follows: \nwhere $a = f ( pmb { x } ; pmb { theta } )$ . The term “sigmoid” means S-shaped: see Figure 2.10a for a plot. We see that it \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nmaps the whole real line to $[ 0 , 1 ]$ , which is necessary for the output to be interpreted as a probability (and hence a valid value for the Bernoulli parameter $theta$ ). The sigmoid function can be thought of as a “soft” version of the heaviside step function, defined by \nas shown in Figure 2.10b. \nPlugging the definition of the sigmoid function into Equation (2.78) we get \nThe quantity $a$ is equal to the log odds, $textstyle log ( { frac { p } { 1 - p } } )$ , where $p = p ( y = 1 | mathbf { x } ; pmb { theta } )$ . To see this, note that \nThe logistic function or sigmoid function maps the log-odds $a$ to $p$ : \nThe inverse of this is called the logit function, and maps $p$ to the log-odds $a$ : \nSee Table 2.3 for some useful properties of these functions. \n2.4.3 Binary logistic regression \nIn this section, we use a conditional Bernoulli model, where we use a linear predictor of the form $f ( pmb { x } ; pmb { theta } ) = pmb { w } ^ { 1 } pmb { x } + b$ . Thus the model has the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Bernoulli and binomial distributions",
        "subsubsection": "Sigmoid (logistic) function"
    },
    {
        "content": "maps the whole real line to $[ 0 , 1 ]$ , which is necessary for the output to be interpreted as a probability (and hence a valid value for the Bernoulli parameter $theta$ ). The sigmoid function can be thought of as a “soft” version of the heaviside step function, defined by \nas shown in Figure 2.10b. \nPlugging the definition of the sigmoid function into Equation (2.78) we get \nThe quantity $a$ is equal to the log odds, $textstyle log ( { frac { p } { 1 - p } } )$ , where $p = p ( y = 1 | mathbf { x } ; pmb { theta } )$ . To see this, note that \nThe logistic function or sigmoid function maps the log-odds $a$ to $p$ : \nThe inverse of this is called the logit function, and maps $p$ to the log-odds $a$ : \nSee Table 2.3 for some useful properties of these functions. \n2.4.3 Binary logistic regression \nIn this section, we use a conditional Bernoulli model, where we use a linear predictor of the form $f ( pmb { x } ; pmb { theta } ) = pmb { w } ^ { 1 } pmb { x } + b$ . Thus the model has the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn other words, \nThis is called logistic regression. \nFor example consider a 1-dimensional, 2-class version of the iris dataset, where the positive class is “Virginica” and the negative class is “not Virginica”, and the feature $x$ we use is the petal width. We fit a logistic regression model to this and show the results in Figure 2.11. The decision boundary corresponds to the value $x ^ { * }$ where $p ( y = 1 | x = x ^ { * } , pmb { theta } ) = 0 . 5$ . We see that, in this example, $x ^ { * } approx 1 . 7$ . As $x$ moves away from this boundary, the classifier becomes more confident in its prediction about the class label. \nIt should be clear from this example why it would be inappropriate to use linear regression for a (binary) classification problem. In such a model, the probabilities would increase above 1 as we move far enough to the right, and below 0 as we move far enough to the left. \nFor more detail on logistic regression, see Chapter 10. \n2.5 Categorical and multinomial distributions \nTo represent a distribution over a finite set of labels, $y in { 1 , ldots , C }$ , we can use the categorical distribution, which generalizes the Bernoulli to $C > 2$ values. \n2.5.1 Definition \nThe categorical distribution is a discrete probability distribution with one parameter per class: \nIn other words, $p ( y = c | pmb { theta } ) = theta _ { c }$ . Note that the parameters are constrained so that $0 leq theta _ { c } leq 1$ and $textstyle sum _ { c = 1 } ^ { C } theta _ { c } = 1$ ; thus there are only $C - 1$ independent parameters. \nWe can write the categorical distribution in another way by converting the discrete variable $y$ into a one-hot vector with $C$ elements, all of which are 0 except for the entry corresponding to the class label. (The term “one-hot” arises from electrical engineering, where binary vectors are encoded as electrical current on a set of wires, which can be active (“hot”) or not (“cold”).) For example, if $C = 3$ , we encode the classes 1, 2 and 3 as $( 1 , 0 , 0 )$ , $( 0 , 1 , 0 )$ , and $( 0 , 0 , 1 )$ . More generally, we can encode the classes using unit vectors, where $e _ { c }$ is all 0s except for dimension $c$ . (This is also called a dummy encoding.) Using one-hot encodings, we can write the categorical distribution as follows: \nThe categorical distribution is a special case of the multinomial distribution. To explain this, suppose we observe $N$ categorical trials, $y _ { n } sim operatorname { C a t } ( cdot | pmb theta )$ , for $n = 1 : N$ . Concretely, think of rolling a $C$ -sided dice $N$ times. Let us define $pmb { y }$ to be a vector that counts the number of times each face \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Bernoulli and binomial distributions",
        "subsubsection": "Binary logistic regression"
    },
    {
        "content": "In other words, \nThis is called logistic regression. \nFor example consider a 1-dimensional, 2-class version of the iris dataset, where the positive class is “Virginica” and the negative class is “not Virginica”, and the feature $x$ we use is the petal width. We fit a logistic regression model to this and show the results in Figure 2.11. The decision boundary corresponds to the value $x ^ { * }$ where $p ( y = 1 | x = x ^ { * } , pmb { theta } ) = 0 . 5$ . We see that, in this example, $x ^ { * } approx 1 . 7$ . As $x$ moves away from this boundary, the classifier becomes more confident in its prediction about the class label. \nIt should be clear from this example why it would be inappropriate to use linear regression for a (binary) classification problem. In such a model, the probabilities would increase above 1 as we move far enough to the right, and below 0 as we move far enough to the left. \nFor more detail on logistic regression, see Chapter 10. \n2.5 Categorical and multinomial distributions \nTo represent a distribution over a finite set of labels, $y in { 1 , ldots , C }$ , we can use the categorical distribution, which generalizes the Bernoulli to $C > 2$ values. \n2.5.1 Definition \nThe categorical distribution is a discrete probability distribution with one parameter per class: \nIn other words, $p ( y = c | pmb { theta } ) = theta _ { c }$ . Note that the parameters are constrained so that $0 leq theta _ { c } leq 1$ and $textstyle sum _ { c = 1 } ^ { C } theta _ { c } = 1$ ; thus there are only $C - 1$ independent parameters. \nWe can write the categorical distribution in another way by converting the discrete variable $y$ into a one-hot vector with $C$ elements, all of which are 0 except for the entry corresponding to the class label. (The term “one-hot” arises from electrical engineering, where binary vectors are encoded as electrical current on a set of wires, which can be active (“hot”) or not (“cold”).) For example, if $C = 3$ , we encode the classes 1, 2 and 3 as $( 1 , 0 , 0 )$ , $( 0 , 1 , 0 )$ , and $( 0 , 0 , 1 )$ . More generally, we can encode the classes using unit vectors, where $e _ { c }$ is all 0s except for dimension $c$ . (This is also called a dummy encoding.) Using one-hot encodings, we can write the categorical distribution as follows: \nThe categorical distribution is a special case of the multinomial distribution. To explain this, suppose we observe $N$ categorical trials, $y _ { n } sim operatorname { C a t } ( cdot | pmb theta )$ , for $n = 1 : N$ . Concretely, think of rolling a $C$ -sided dice $N$ times. Let us define $pmb { y }$ to be a vector that counts the number of times each face \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nshows up, i.e., $begin{array} { r } { y _ { c } = N _ { c } triangleq sum _ { n = 1 } ^ { N } mathbb { I } left( y _ { n } = c right) } end{array}$ . Now $textbf {  { y } }$ is no longer one-hot, but is “multi-hot”, since it has a non-zero entry for every value of $c$ that was observed across all $N$ trials. The distribution of $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ is given by the multinomial distribution: \nwhere $theta _ { c }$ is the probability that side $c$ shows up, and \nis the multinomial coefficient, which is the number of ways to divide a set of size $begin{array} { r } { N = sum _ { c = 1 } ^ { C } N _ { c } } end{array}$ into subsets with sizes $N _ { 1 }$ up to $N _ { C }$ . If $N = 1$ , the multinomial distribution becomes the categorical distribution. \n2.5.2 Softmax function \nIn the conditional case, we can define \nwhich we can also write as \nWe require that $0 leq f _ { c } ( boldsymbol { x } ; boldsymbol { theta } ) leq 1$ and $begin{array} { r } { sum _ { c = 1 } ^ { C } f _ { c } ( { pmb x } ; { pmb theta } ) = 1 } end{array}$ . \nTo avoid the requirement that $f$ directly predict a probability vector, it is common to pass the output from $f$ into the softmax function [Bri90], also called the multinomial logit. This is defined as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Categorical and multinomial distributions",
        "subsubsection": "Definition"
    },
    {
        "content": "shows up, i.e., $begin{array} { r } { y _ { c } = N _ { c } triangleq sum _ { n = 1 } ^ { N } mathbb { I } left( y _ { n } = c right) } end{array}$ . Now $textbf {  { y } }$ is no longer one-hot, but is “multi-hot”, since it has a non-zero entry for every value of $c$ that was observed across all $N$ trials. The distribution of $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ is given by the multinomial distribution: \nwhere $theta _ { c }$ is the probability that side $c$ shows up, and \nis the multinomial coefficient, which is the number of ways to divide a set of size $begin{array} { r } { N = sum _ { c = 1 } ^ { C } N _ { c } } end{array}$ into subsets with sizes $N _ { 1 }$ up to $N _ { C }$ . If $N = 1$ , the multinomial distribution becomes the categorical distribution. \n2.5.2 Softmax function \nIn the conditional case, we can define \nwhich we can also write as \nWe require that $0 leq f _ { c } ( boldsymbol { x } ; boldsymbol { theta } ) leq 1$ and $begin{array} { r } { sum _ { c = 1 } ^ { C } f _ { c } ( { pmb x } ; { pmb theta } ) = 1 } end{array}$ . \nTo avoid the requirement that $f$ directly predict a probability vector, it is common to pass the output from $f$ into the softmax function [Bri90], also called the multinomial logit. This is defined as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis maps $mathbb { R } ^ { C }$ to $[ 0 , 1 ] ^ { C }$ , and satisfies the constraints that $0 leq mathrm { s o f t m a x } ( pmb { a } ) _ { c } leq 1$ and $textstyle sum _ { c = 1 } ^ { C }$ $mathrm { s o f t m a x } ( pmb { a } ) _ { c } =$ 1. The inputs to the softmax, $pmb { a } = f ( pmb { x } ; pmb { theta } )$ , are called logits, and are a generalization of the log odds. The softmax function is so-called since it acts a bit like the argmax function. To see this, let us divide each $boldsymbol { a } _ { c }$ by a constant $T$ called the temperature.8 Then as $T  0$ , we find \nIn other words, at low temperatures, the distribution puts most of its probability mass in the most probable state (this is called winner takes all), whereas at high temperatures, it spreads the mass uniformly. See Figure 2.12 for an illustration. \n2.5.3 Multiclass logistic regression \nIf we use a linear predictor of the form $f ( { pmb x } ; { pmb theta } ) = mathbf { W } { pmb x } + { pmb b }$ , where $mathbf { W }$ is a $C times D$ matrix, and $^ { b }$ is a $C$ -dimensional bias vector, the final model becomes \nLet $pmb { a } = mathbf { W } pmb { x } + pmb { b }$ be the $C$ -dimensional vector of logits. Then we can rewrite the above as follows: \nThis is known as multinomial logistic regression. \nIf we have just two classes, this reduces to binary logistic regression. To see this, note that \nso we can just train the model to predict $a = a _ { 1 } - a _ { 0 }$ . This can be done with a single weight vector $pmb { w }$ ; if we use the multi-class formulation, we will have two weight vectors, ${ pmb w } _ { 0 }$ and ${ pmb w } _ { 1 }$ . Such a model is over-parameterized, which can hurt interpretability, but the predictions will be the same.",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Categorical and multinomial distributions",
        "subsubsection": "Softmax function"
    },
    {
        "content": "This maps $mathbb { R } ^ { C }$ to $[ 0 , 1 ] ^ { C }$ , and satisfies the constraints that $0 leq mathrm { s o f t m a x } ( pmb { a } ) _ { c } leq 1$ and $textstyle sum _ { c = 1 } ^ { C }$ $mathrm { s o f t m a x } ( pmb { a } ) _ { c } =$ 1. The inputs to the softmax, $pmb { a } = f ( pmb { x } ; pmb { theta } )$ , are called logits, and are a generalization of the log odds. The softmax function is so-called since it acts a bit like the argmax function. To see this, let us divide each $boldsymbol { a } _ { c }$ by a constant $T$ called the temperature.8 Then as $T  0$ , we find \nIn other words, at low temperatures, the distribution puts most of its probability mass in the most probable state (this is called winner takes all), whereas at high temperatures, it spreads the mass uniformly. See Figure 2.12 for an illustration. \n2.5.3 Multiclass logistic regression \nIf we use a linear predictor of the form $f ( { pmb x } ; { pmb theta } ) = mathbf { W } { pmb x } + { pmb b }$ , where $mathbf { W }$ is a $C times D$ matrix, and $^ { b }$ is a $C$ -dimensional bias vector, the final model becomes \nLet $pmb { a } = mathbf { W } pmb { x } + pmb { b }$ be the $C$ -dimensional vector of logits. Then we can rewrite the above as follows: \nThis is known as multinomial logistic regression. \nIf we have just two classes, this reduces to binary logistic regression. To see this, note that \nso we can just train the model to predict $a = a _ { 1 } - a _ { 0 }$ . This can be done with a single weight vector $pmb { w }$ ; if we use the multi-class formulation, we will have two weight vectors, ${ pmb w } _ { 0 }$ and ${ pmb w } _ { 1 }$ . Such a model is over-parameterized, which can hurt interpretability, but the predictions will be the same. \nWe discuss this in more detail in Section 10.3. For now, we just give an example. Figure 2.13 shows what happens when we fit this model to the 3-class iris dataset, using just 2 features. We see that the decision boundaries between each class are linear. We can create nonlinear boundaries by transforming the features (e.g., using polynomials), as we discuss in Section 10.3.1. \n2.5.4 Log-sum-exp trick \nIn this section, we discuss one important practical detail to pay attention to when working with the softmax distribution. Suppose we want to compute the normalized probability $p _ { c } = p ( y = c | mathbf { x } )$ , which is given by \nwhere $pmb { a } = f ( pmb { x } ; pmb { theta } )$ are the logits. We might encounter numerical problems when computing the partition function $Z$ . For example, suppose we have 3 classes, with logits ${ pmb a } = ( 0 , 1 , 0 )$ . Then we find $Z = e ^ { 0 } + e ^ { 1 } + e ^ { 0 } = 4 . 7 1$ . But now suppose $pmb { a } = ( 1 0 0 0 , 1 0 0 1 , 1 0 0 0 )$ ; we find $Z = infty$ , since on a computer, even using 64 bit precision, $mathtt { n p . e x p ( 1 0 0 0 ) = i n f }$ . Similarly, suppose $pmb { a } = ( - 1 0 0 0 , - 9 9 9 , - 1 0 0 0 )$ ; now we find $Z = 0$ , since $mathtt { n p . e x p } ( - 1 0 0 0 ) { = } 0$ . To avoid numerical problems, we can use the following identity: \nThis holds for any $m$ . It is common to use $m = operatorname* { m a x } _ { c } a _ { c }$ which ensures that the largest value you exponentiate will be zero, so you will definitely not overflow, and even if you underflow, the answer will be sensible. This is known as the log-sum-exp trick. We use this trick when implementing the lse function: \nWe can use this to compute the probabilities from the logits: \nWe can then pass this to the cross-entropy loss, defined in Equation (5.41). \nHowever, to save computational effort, and for numerical stability, it is quite common to modify the cross-entropy loss so that it takes the logits $textbf { em a }$ as inputs, instead of the probability vector $mathbf { nabla } _ { mathbf { p } }$ . For example, consider the binary case. The CE loss for one example is \nwhere \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Categorical and multinomial distributions",
        "subsubsection": "Multiclass logistic regression"
    },
    {
        "content": "We discuss this in more detail in Section 10.3. For now, we just give an example. Figure 2.13 shows what happens when we fit this model to the 3-class iris dataset, using just 2 features. We see that the decision boundaries between each class are linear. We can create nonlinear boundaries by transforming the features (e.g., using polynomials), as we discuss in Section 10.3.1. \n2.5.4 Log-sum-exp trick \nIn this section, we discuss one important practical detail to pay attention to when working with the softmax distribution. Suppose we want to compute the normalized probability $p _ { c } = p ( y = c | mathbf { x } )$ , which is given by \nwhere $pmb { a } = f ( pmb { x } ; pmb { theta } )$ are the logits. We might encounter numerical problems when computing the partition function $Z$ . For example, suppose we have 3 classes, with logits ${ pmb a } = ( 0 , 1 , 0 )$ . Then we find $Z = e ^ { 0 } + e ^ { 1 } + e ^ { 0 } = 4 . 7 1$ . But now suppose $pmb { a } = ( 1 0 0 0 , 1 0 0 1 , 1 0 0 0 )$ ; we find $Z = infty$ , since on a computer, even using 64 bit precision, $mathtt { n p . e x p ( 1 0 0 0 ) = i n f }$ . Similarly, suppose $pmb { a } = ( - 1 0 0 0 , - 9 9 9 , - 1 0 0 0 )$ ; now we find $Z = 0$ , since $mathtt { n p . e x p } ( - 1 0 0 0 ) { = } 0$ . To avoid numerical problems, we can use the following identity: \nThis holds for any $m$ . It is common to use $m = operatorname* { m a x } _ { c } a _ { c }$ which ensures that the largest value you exponentiate will be zero, so you will definitely not overflow, and even if you underflow, the answer will be sensible. This is known as the log-sum-exp trick. We use this trick when implementing the lse function: \nWe can use this to compute the probabilities from the logits: \nWe can then pass this to the cross-entropy loss, defined in Equation (5.41). \nHowever, to save computational effort, and for numerical stability, it is quite common to modify the cross-entropy loss so that it takes the logits $textbf { em a }$ as inputs, instead of the probability vector $mathbf { nabla } _ { mathbf { p } }$ . For example, consider the binary case. The CE loss for one example is \nwhere \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n2.6 Univariate Gaussian (normal) distribution \nThe most widely used distribution of real-valued random variables $y in mathbb { R }$ is the Gaussian distribution, also called the normal distribution (see Section 2.6.4 for a discussion of these names). \n2.6.1 Cumulative distribution function \nWe define the cumulative distribution function or cdf of a continuous random variable $Y$ as follows: \n(Note that we use a capital $P$ to represent the cdf.) Using this, we can compute the probability of being in any interval as follows: \nCdf’s are monotonically non-decreasing functions. \nThe cdf of the Gaussian is defined by \nSee Figure 2.2a for a plot. Note that the cdf of the Gaussian is often implemented using $Phi ( y ; mu , sigma ^ { 2 } ) =$ ${ textstyle frac { 1 } { 2 } } [ 1 + mathrm { e r f } ( z / sqrt { 2 } ) ]$ , where $z = ( y - mu ) / sigma$ and $mathrm { e r f } ( u )$ is the error function, defined as \nThe parameter $mu$ encodes the mean of the distribution, which is the same as the mode, since the distribution is unimodal. The parameter $sigma ^ { 2 }$ encodes the variance. (Sometimes we talk about the precision of a Gaussian, which is the inverse variance, denoted $lambda = 1 / sigma ^ { 2 }$ .) When $mu = 0$ and $sigma = 1$ , the Gaussian is called the standard normal distribution. \nIf $P$ is the cdf of $Y$ , then $P ^ { - 1 } ( q )$ is the value such that $p ( Y leq y _ { q } ) = q$ ; this is called the ’th $y _ { q }$ $q$ quantile of $P$ . The value $P ^ { - 1 } ( 0 . 5 )$ is the median of the distribution, with half of the probability mass on the left, and half on the right. The values $P ^ { - 1 } ( 0 . 2 5 )$ and $P ^ { - 1 } ( 0 . 7 5 )$ are the lower and upper quartiles. \nFor example, let $Phi$ be the cdf of the Gaussian distribution $mathcal { N } ( 0 , 1 )$ , and $Phi ^ { - 1 }$ be the inverse cdf (also known as the probit function). Then points to the left of $Phi ^ { - 1 } ( alpha / 2 )$ contain $alpha / 2$ of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of $Phi ^ { - 1 } ( 1 - alpha / 2 )$ also contain $alpha / 2$ of the mass. Hence the central interval $( Phi ^ { - 1 } ( alpha / 2 ) , Phi ^ { - 1 } ( 1 - alpha / 2 ) )$ contains $1 - alpha$ of the mass. If we set $alpha = 0 . 0 5$ , the central $9 5 %$ interval is covered by the range \nIf the distribution is ${ mathcal { N } } ( mu , sigma ^ { 2 } )$ , then the $9 5 %$ interval becomes $( mu - 1 . 9 6 sigma , mu + 1 . 9 6 sigma )$ . This is often approximated by writing $mu pm 2 sigma$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Categorical and multinomial distributions",
        "subsubsection": "Log-sum-exp trick"
    },
    {
        "content": "2.6 Univariate Gaussian (normal) distribution \nThe most widely used distribution of real-valued random variables $y in mathbb { R }$ is the Gaussian distribution, also called the normal distribution (see Section 2.6.4 for a discussion of these names). \n2.6.1 Cumulative distribution function \nWe define the cumulative distribution function or cdf of a continuous random variable $Y$ as follows: \n(Note that we use a capital $P$ to represent the cdf.) Using this, we can compute the probability of being in any interval as follows: \nCdf’s are monotonically non-decreasing functions. \nThe cdf of the Gaussian is defined by \nSee Figure 2.2a for a plot. Note that the cdf of the Gaussian is often implemented using $Phi ( y ; mu , sigma ^ { 2 } ) =$ ${ textstyle frac { 1 } { 2 } } [ 1 + mathrm { e r f } ( z / sqrt { 2 } ) ]$ , where $z = ( y - mu ) / sigma$ and $mathrm { e r f } ( u )$ is the error function, defined as \nThe parameter $mu$ encodes the mean of the distribution, which is the same as the mode, since the distribution is unimodal. The parameter $sigma ^ { 2 }$ encodes the variance. (Sometimes we talk about the precision of a Gaussian, which is the inverse variance, denoted $lambda = 1 / sigma ^ { 2 }$ .) When $mu = 0$ and $sigma = 1$ , the Gaussian is called the standard normal distribution. \nIf $P$ is the cdf of $Y$ , then $P ^ { - 1 } ( q )$ is the value such that $p ( Y leq y _ { q } ) = q$ ; this is called the ’th $y _ { q }$ $q$ quantile of $P$ . The value $P ^ { - 1 } ( 0 . 5 )$ is the median of the distribution, with half of the probability mass on the left, and half on the right. The values $P ^ { - 1 } ( 0 . 2 5 )$ and $P ^ { - 1 } ( 0 . 7 5 )$ are the lower and upper quartiles. \nFor example, let $Phi$ be the cdf of the Gaussian distribution $mathcal { N } ( 0 , 1 )$ , and $Phi ^ { - 1 }$ be the inverse cdf (also known as the probit function). Then points to the left of $Phi ^ { - 1 } ( alpha / 2 )$ contain $alpha / 2$ of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of $Phi ^ { - 1 } ( 1 - alpha / 2 )$ also contain $alpha / 2$ of the mass. Hence the central interval $( Phi ^ { - 1 } ( alpha / 2 ) , Phi ^ { - 1 } ( 1 - alpha / 2 ) )$ contains $1 - alpha$ of the mass. If we set $alpha = 0 . 0 5$ , the central $9 5 %$ interval is covered by the range \nIf the distribution is ${ mathcal { N } } ( mu , sigma ^ { 2 } )$ , then the $9 5 %$ interval becomes $( mu - 1 . 9 6 sigma , mu + 1 . 9 6 sigma )$ . This is often approximated by writing $mu pm 2 sigma$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n2.6.2 Probability density function \nWe define the probability density function or pdf as the derivative of the cdf: \nThe pdf of the Gaussian is given by \nwhere $scriptstyle { sqrt { 2 pi sigma ^ { 2 } } }$ is the normalization constant needed to ensure the density integrates to 1 (see Exercise 2.12). See Figure 2.2b for a plot. \nGiven a pdf, we can compute the probability of a continuous variable being in a finite interval as follows: \nAs the size of the interval gets smaller, we can write \nIntuitively, this says the probability of $Y$ being in a small interval around $y$ is the density at $y$ times the width of the interval. One important consequence of the above result is that the pdf at a point can be larger than 1. For example, $mathcal { N } ( 0 | 0 , 0 . 1 ) = 3 . 9 9$ . \nWe can use the pdf to compute the mean, or expected value, of the distribution: \nFor a Gaussian, we have the familiar result that $mathbb { E } left[ mathcal { N } ( cdot | mu , sigma ^ { 2 } ) right] = mu$ . (Note, however, that for some distributions, this integral is not finite, so the mean is not defined.) \nWe can also use the pdf to compute the variance of a distribution. This is a measure of the “spread”, and is often denoted by $sigma ^ { 2 }$ . The variance is defined as follows: \nfrom which we derive the useful result \nThe standard deviation is defined as \n(The standard deviation can be more intepretable than the variance since it has the same units as $Y$ itself.) For a Gaussian, we have the familiar result that std $leftlfloor mathcal { N } ( cdot | mu , sigma ^ { 2 } ) rightrfloor = sigma$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Univariate Gaussian (normal) distribution",
        "subsubsection": "Cumulative distribution function"
    },
    {
        "content": "2.6.2 Probability density function \nWe define the probability density function or pdf as the derivative of the cdf: \nThe pdf of the Gaussian is given by \nwhere $scriptstyle { sqrt { 2 pi sigma ^ { 2 } } }$ is the normalization constant needed to ensure the density integrates to 1 (see Exercise 2.12). See Figure 2.2b for a plot. \nGiven a pdf, we can compute the probability of a continuous variable being in a finite interval as follows: \nAs the size of the interval gets smaller, we can write \nIntuitively, this says the probability of $Y$ being in a small interval around $y$ is the density at $y$ times the width of the interval. One important consequence of the above result is that the pdf at a point can be larger than 1. For example, $mathcal { N } ( 0 | 0 , 0 . 1 ) = 3 . 9 9$ . \nWe can use the pdf to compute the mean, or expected value, of the distribution: \nFor a Gaussian, we have the familiar result that $mathbb { E } left[ mathcal { N } ( cdot | mu , sigma ^ { 2 } ) right] = mu$ . (Note, however, that for some distributions, this integral is not finite, so the mean is not defined.) \nWe can also use the pdf to compute the variance of a distribution. This is a measure of the “spread”, and is often denoted by $sigma ^ { 2 }$ . The variance is defined as follows: \nfrom which we derive the useful result \nThe standard deviation is defined as \n(The standard deviation can be more intepretable than the variance since it has the same units as $Y$ itself.) For a Gaussian, we have the familiar result that std $leftlfloor mathcal { N } ( cdot | mu , sigma ^ { 2 } ) rightrfloor = sigma$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n2.6.3 Regression \nSo far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form \nwhere $f _ { mu } ( pmb { x } ; pmb { theta } ) in mathbb { R }$ predicts the mean, and $f _ { sigma } ( pmb { x } ; pmb { theta } ) ^ { 2 } in mathbb { R } _ { + }$ predicts the variance. \nIt is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression: \nwhere $pmb theta = ( pmb w , b , sigma ^ { 2 } )$ . See Figure 2.14(a) for an illustration of this model in 1d. and Section 11.2 for more details on this model. \nHowever, we can also make the variance depend on the input; this is called heteroskedastic regression. In the linear regression setting, we have \nwhere $pmb theta = ( pmb w _ { mu } , pmb w _ { sigma } )$ are the two forms of regression weights, and \nis the softplus function, that maps from $mathbb { R }$ to $mathbb { R } _ { + }$ , to ensure the predicted standard deviation is non-negative. See Figure 2.14(b) for an illustration of this model in 1d. \nNote that Figure 2.14 plots the $9 5 %$ predictive interval, $[ mu ( x ) - 2 sigma ( x ) , mu ( x ) + 2 sigma ( x ) ]$ . This is the uncertainty in the predicted observation $y$ given $_ { x }$ , and captures the variability in the blue dots. By contrast, the uncertainty in the underlying (noise-free) function is represented by $sqrt { mathbb { V } left[ f _ { mu } ( pmb { x } ; pmb { theta } ) right] }$ , which does not involve the $sigma$ term; now the uncertainty is over the parameters $pmb theta$ , rather than the output $y$ . See Section 11.7 for details on how to model parameter uncertainty. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Univariate Gaussian (normal) distribution",
        "subsubsection": "Probability density function"
    },
    {
        "content": "2.6.3 Regression \nSo far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form \nwhere $f _ { mu } ( pmb { x } ; pmb { theta } ) in mathbb { R }$ predicts the mean, and $f _ { sigma } ( pmb { x } ; pmb { theta } ) ^ { 2 } in mathbb { R } _ { + }$ predicts the variance. \nIt is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression: \nwhere $pmb theta = ( pmb w , b , sigma ^ { 2 } )$ . See Figure 2.14(a) for an illustration of this model in 1d. and Section 11.2 for more details on this model. \nHowever, we can also make the variance depend on the input; this is called heteroskedastic regression. In the linear regression setting, we have \nwhere $pmb theta = ( pmb w _ { mu } , pmb w _ { sigma } )$ are the two forms of regression weights, and \nis the softplus function, that maps from $mathbb { R }$ to $mathbb { R } _ { + }$ , to ensure the predicted standard deviation is non-negative. See Figure 2.14(b) for an illustration of this model in 1d. \nNote that Figure 2.14 plots the $9 5 %$ predictive interval, $[ mu ( x ) - 2 sigma ( x ) , mu ( x ) + 2 sigma ( x ) ]$ . This is the uncertainty in the predicted observation $y$ given $_ { x }$ , and captures the variability in the blue dots. By contrast, the uncertainty in the underlying (noise-free) function is represented by $sqrt { mathbb { V } left[ f _ { mu } ( pmb { x } ; pmb { theta } ) right] }$ , which does not involve the $sigma$ term; now the uncertainty is over the parameters $pmb theta$ , rather than the output $y$ . See Section 11.7 for details on how to model parameter uncertainty. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n2.6.4 Why is the Gaussian distribution so widely used? \nThe Gaussian distribution is the most widely used distribution in statistics and machine learning. There are several reasons for this. First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance. Second, the central limit theorem (Section 2.8.6) tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”. Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy), subject to the constraint of having a specified mean and variance, as we show in Section 3.4.4; this makes it a good default choice in many cases. Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see in Section 3.2. \nFrom a historical perspective, it’s worth remarking that the term “Gaussian distribution” is a bit misleading, since, as Jaynes [Jay03, p241] notes: “The fundamental nature of this distribution and its main properties were noted by Laplace when Gauss was six years old; and the distribution itself had been found by de Moivre before Laplace was born”. However, Gauss popularized the use of the distribution in the 1800s, and the term “Gaussian” is now widely used in science and engineering. \nThe name “normal distribution” seems to have arisen in connection with the normal equations in linear regression (see Section 11.2.2.2). However, we prefer to avoid the term “normal”, since it suggests other distributions are “abnormal”, whereas, as Jaynes [Jay03] points out, it is the Gaussian that is abnormal in the sense that it has many special properties that are untypical of general distributions. \n2.6.5 Dirac delta function as a limiting case \nAs the variance of a Gaussian goes to 0, the distribution approaches an infinitely narrow, but infinitely tall, “spike” at the mean. We can write this as follows: \nwhere $delta$ is the Dirac delta function, defined by \nwhere \nA slight variant of this is to define \nNote that we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Univariate Gaussian (normal) distribution",
        "subsubsection": "Regression"
    },
    {
        "content": "2.6.4 Why is the Gaussian distribution so widely used? \nThe Gaussian distribution is the most widely used distribution in statistics and machine learning. There are several reasons for this. First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance. Second, the central limit theorem (Section 2.8.6) tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”. Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy), subject to the constraint of having a specified mean and variance, as we show in Section 3.4.4; this makes it a good default choice in many cases. Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see in Section 3.2. \nFrom a historical perspective, it’s worth remarking that the term “Gaussian distribution” is a bit misleading, since, as Jaynes [Jay03, p241] notes: “The fundamental nature of this distribution and its main properties were noted by Laplace when Gauss was six years old; and the distribution itself had been found by de Moivre before Laplace was born”. However, Gauss popularized the use of the distribution in the 1800s, and the term “Gaussian” is now widely used in science and engineering. \nThe name “normal distribution” seems to have arisen in connection with the normal equations in linear regression (see Section 11.2.2.2). However, we prefer to avoid the term “normal”, since it suggests other distributions are “abnormal”, whereas, as Jaynes [Jay03] points out, it is the Gaussian that is abnormal in the sense that it has many special properties that are untypical of general distributions. \n2.6.5 Dirac delta function as a limiting case \nAs the variance of a Gaussian goes to 0, the distribution approaches an infinitely narrow, but infinitely tall, “spike” at the mean. We can write this as follows: \nwhere $delta$ is the Dirac delta function, defined by \nwhere \nA slight variant of this is to define \nNote that we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Univariate Gaussian (normal) distribution",
        "subsubsection": "Why is the Gaussian distribution so widely used?"
    },
    {
        "content": "2.6.4 Why is the Gaussian distribution so widely used? \nThe Gaussian distribution is the most widely used distribution in statistics and machine learning. There are several reasons for this. First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance. Second, the central limit theorem (Section 2.8.6) tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”. Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy), subject to the constraint of having a specified mean and variance, as we show in Section 3.4.4; this makes it a good default choice in many cases. Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see in Section 3.2. \nFrom a historical perspective, it’s worth remarking that the term “Gaussian distribution” is a bit misleading, since, as Jaynes [Jay03, p241] notes: “The fundamental nature of this distribution and its main properties were noted by Laplace when Gauss was six years old; and the distribution itself had been found by de Moivre before Laplace was born”. However, Gauss popularized the use of the distribution in the 1800s, and the term “Gaussian” is now widely used in science and engineering. \nThe name “normal distribution” seems to have arisen in connection with the normal equations in linear regression (see Section 11.2.2.2). However, we prefer to avoid the term “normal”, since it suggests other distributions are “abnormal”, whereas, as Jaynes [Jay03] points out, it is the Gaussian that is abnormal in the sense that it has many special properties that are untypical of general distributions. \n2.6.5 Dirac delta function as a limiting case \nAs the variance of a Gaussian goes to 0, the distribution approaches an infinitely narrow, but infinitely tall, “spike” at the mean. We can write this as follows: \nwhere $delta$ is the Dirac delta function, defined by \nwhere \nA slight variant of this is to define \nNote that we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThe delta function distribution satisfies the following sifting property, which we will use later on: \n2.7 Some other common univariate distributions * \nIn this section, we briefly introduce some other univariate distributions that we will use in this book. \n2.7.1 Student $scriptstyle t$ distribution \nThe Gaussian distribution is quite sensitive to outliers. A robust alternative to the Gaussian is the Student $t$ -distribution, which we shall call the Student distribution for short.9 Its pdf is as follows: \nwhere $mu$ is the mean, $sigma > 0$ is the scale parameter (not the standard deviation), and $nu > 0$ is called the degrees of freedom (although a better term would be the degree of normality [Kru13], since large values of $nu$ make the distribution act like a Gaussian).",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Univariate Gaussian (normal) distribution",
        "subsubsection": "Dirac delta function as a limiting case"
    },
    {
        "content": "The delta function distribution satisfies the following sifting property, which we will use later on: \n2.7 Some other common univariate distributions * \nIn this section, we briefly introduce some other univariate distributions that we will use in this book. \n2.7.1 Student $scriptstyle t$ distribution \nThe Gaussian distribution is quite sensitive to outliers. A robust alternative to the Gaussian is the Student $t$ -distribution, which we shall call the Student distribution for short.9 Its pdf is as follows: \nwhere $mu$ is the mean, $sigma > 0$ is the scale parameter (not the standard deviation), and $nu > 0$ is called the degrees of freedom (although a better term would be the degree of normality [Kru13], since large values of $nu$ make the distribution act like a Gaussian). \nWe see that the probability density decays as a polynomial function of the squared distance from the center, as opposed to an exponential function, so there is more probability mass in the tail than with a Gaussian distribution, as shown in Figure 2.15. We say that the Student distribution has heavy tails, which makes it robust to outliers. \nTo illustrate the robustness of the Student distribution, consider Figure 2.16. On the left, we show a Gaussian and a Student distribution fit to some data with no outliers. On the right, we add some outliers. We see that the Gaussian is affected a lot, whereas the Student hardly changes. We discuss how to use the Student distribution for robust linear regression in Section 11.6.2. \nFor later reference, we note that the Student distribution has the following properties: \nThe mean is only defined if $nu > 1$ . The variance is only defined if $nu > 2$ . For $nu gg 5$ , the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties. It is common to use $nu = 4$ , which gives good performance in a range of problems [LLT89]. \n2.7.2 Cauchy distribution \nIf $nu = 1$ , the Student distribution is known as the Cauchy or Lorentz distribution. Its pdf is defined by \nThis distribution has very heavy tails compared to a Gaussian. For example, $9 5 %$ of the values from a standard normal are between -1.96 and 1.96, but for a standard Cauchy they are between -12.7 and 12.7. In fact the tails are so heavy that the integral that defines the mean does not converge. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Some other common univariate distributions *",
        "subsubsection": "Student t distribution"
    },
    {
        "content": "We see that the probability density decays as a polynomial function of the squared distance from the center, as opposed to an exponential function, so there is more probability mass in the tail than with a Gaussian distribution, as shown in Figure 2.15. We say that the Student distribution has heavy tails, which makes it robust to outliers. \nTo illustrate the robustness of the Student distribution, consider Figure 2.16. On the left, we show a Gaussian and a Student distribution fit to some data with no outliers. On the right, we add some outliers. We see that the Gaussian is affected a lot, whereas the Student hardly changes. We discuss how to use the Student distribution for robust linear regression in Section 11.6.2. \nFor later reference, we note that the Student distribution has the following properties: \nThe mean is only defined if $nu > 1$ . The variance is only defined if $nu > 2$ . For $nu gg 5$ , the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties. It is common to use $nu = 4$ , which gives good performance in a range of problems [LLT89]. \n2.7.2 Cauchy distribution \nIf $nu = 1$ , the Student distribution is known as the Cauchy or Lorentz distribution. Its pdf is defined by \nThis distribution has very heavy tails compared to a Gaussian. For example, $9 5 %$ of the values from a standard normal are between -1.96 and 1.96, but for a standard Cauchy they are between -12.7 and 12.7. In fact the tails are so heavy that the integral that defines the mean does not converge. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThe half Cauchy distribution is a version of the Cauchy (with $mu = 0$ ) that is “folded over” on itself, so all its probability density is on the positive reals. Thus it has the form \nThis is useful in Bayesian modeling, where we want to use a distribution over positive reals with heavy tails, but finite density at the origin. \n2.7.3 Laplace distribution \nAnother distribution with heavy tails is the Laplace distribution $cdot ^ { 1 0 }$ , also known as the double sided exponential distribution. This has the following pdf: \nSee Figure 2.15 for a plot. Here $mu$ is a location parameter and $b > 0$ is a scale parameter. This distribution has the following properties: \nIn Section 11.6.1, we discuss how to use the Laplace distribution for robust linear regression, and in Section 11.4, we discuss how to use the Laplace distribution for sparse linear regression. \n2.7.4 Beta distribution \nThe beta distribution has support over the interval $[ 0 , 1 ]$ and is defined as follows: \nwhere $B ( a , b )$ is the beta function, defined by \nwhere $Gamma ( a )$ is the Gamma function defined by \nSee Figure 2.17a for plots of some beta distributions. \nWe require $a , b > 0$ to ensure the distribution is integrable (i.e., to ensure $B ( a , b )$ exists). If $a = b = 1$ , we get the uniform distribution. If $a$ and $b$ are both less than 1, we get a bimodal distribution with “spikes” at 0 and $1$ ; if $a$ and $b$ are both greater than 1, the distribution is unimodal. For later reference, we note that the distribution has the following properties (Exercise 2.8):",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Some other common univariate distributions *",
        "subsubsection": "Cauchy distribution"
    },
    {
        "content": "The half Cauchy distribution is a version of the Cauchy (with $mu = 0$ ) that is “folded over” on itself, so all its probability density is on the positive reals. Thus it has the form \nThis is useful in Bayesian modeling, where we want to use a distribution over positive reals with heavy tails, but finite density at the origin. \n2.7.3 Laplace distribution \nAnother distribution with heavy tails is the Laplace distribution $cdot ^ { 1 0 }$ , also known as the double sided exponential distribution. This has the following pdf: \nSee Figure 2.15 for a plot. Here $mu$ is a location parameter and $b > 0$ is a scale parameter. This distribution has the following properties: \nIn Section 11.6.1, we discuss how to use the Laplace distribution for robust linear regression, and in Section 11.4, we discuss how to use the Laplace distribution for sparse linear regression. \n2.7.4 Beta distribution \nThe beta distribution has support over the interval $[ 0 , 1 ]$ and is defined as follows: \nwhere $B ( a , b )$ is the beta function, defined by \nwhere $Gamma ( a )$ is the Gamma function defined by \nSee Figure 2.17a for plots of some beta distributions. \nWe require $a , b > 0$ to ensure the distribution is integrable (i.e., to ensure $B ( a , b )$ exists). If $a = b = 1$ , we get the uniform distribution. If $a$ and $b$ are both less than 1, we get a bimodal distribution with “spikes” at 0 and $1$ ; if $a$ and $b$ are both greater than 1, the distribution is unimodal. For later reference, we note that the distribution has the following properties (Exercise 2.8):",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Some other common univariate distributions *",
        "subsubsection": "Laplace distribution"
    },
    {
        "content": "The half Cauchy distribution is a version of the Cauchy (with $mu = 0$ ) that is “folded over” on itself, so all its probability density is on the positive reals. Thus it has the form \nThis is useful in Bayesian modeling, where we want to use a distribution over positive reals with heavy tails, but finite density at the origin. \n2.7.3 Laplace distribution \nAnother distribution with heavy tails is the Laplace distribution $cdot ^ { 1 0 }$ , also known as the double sided exponential distribution. This has the following pdf: \nSee Figure 2.15 for a plot. Here $mu$ is a location parameter and $b > 0$ is a scale parameter. This distribution has the following properties: \nIn Section 11.6.1, we discuss how to use the Laplace distribution for robust linear regression, and in Section 11.4, we discuss how to use the Laplace distribution for sparse linear regression. \n2.7.4 Beta distribution \nThe beta distribution has support over the interval $[ 0 , 1 ]$ and is defined as follows: \nwhere $B ( a , b )$ is the beta function, defined by \nwhere $Gamma ( a )$ is the Gamma function defined by \nSee Figure 2.17a for plots of some beta distributions. \nWe require $a , b > 0$ to ensure the distribution is integrable (i.e., to ensure $B ( a , b )$ exists). If $a = b = 1$ , we get the uniform distribution. If $a$ and $b$ are both less than 1, we get a bimodal distribution with “spikes” at 0 and $1$ ; if $a$ and $b$ are both greater than 1, the distribution is unimodal. For later reference, we note that the distribution has the following properties (Exercise 2.8): \n2.7.5 Gamma distribution \nThe gamma distribution is a flexible distribution for positive real valued rv’s, $x > 0$ . It is defined in terms of two parameters, called the shape $a > 0$ and the rate $b > 0$ : \nSometimes the distribution is parameterized in terms of the shape $a$ and the scale $s = 1 / b$ : \nSee Figure 2.17b for some plots of the gamma pdf. For reference, we note that the distribution has the following properties: \nThere are several distributions which are just special cases of the Gamma, which we discuss below. \n• Exponential distribution. This is defined by \nThis distribution describes the times between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate $lambda$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Some other common univariate distributions *",
        "subsubsection": "Beta distribution"
    },
    {
        "content": "2.7.5 Gamma distribution \nThe gamma distribution is a flexible distribution for positive real valued rv’s, $x > 0$ . It is defined in terms of two parameters, called the shape $a > 0$ and the rate $b > 0$ : \nSometimes the distribution is parameterized in terms of the shape $a$ and the scale $s = 1 / b$ : \nSee Figure 2.17b for some plots of the gamma pdf. For reference, we note that the distribution has the following properties: \nThere are several distributions which are just special cases of the Gamma, which we discuss below. \n• Exponential distribution. This is defined by \nThis distribution describes the times between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate $lambda$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n• Chi-squared distribution. This is defined by \nwhere $nu$ is called the degrees of freedom. This is the distribution of the sum of squared Gaussian random variables. More precisely, if $Z _ { i } sim mathcal { N } ( 0 , 1 )$ , and $begin{array} { r } { S = sum _ { i = 1 } ^ { nu } Z _ { i } ^ { 2 } } end{array}$ , then $S sim chi _ { nu } ^ { 2 }$ . \n• The inverse Gamma distribution is defined as follows: \nThe distribution has these properties \nThe mean only exists if $a > 1$ . The variance only exists if $a > 2$ . Note: if $X sim { mathrm { G a } } ( { mathrm { s h a p e } } =$ $a , { mathrm { r a t e } } = b$ ), then $1 / X sim mathrm { I G } ( { mathrm { s h a p e } } = a , { mathrm { s c a l e } } = b$ ). (Note that $b$ plays two different roles in this case.) \n2.7.6 Empirical distribution \nSuppose we have a set of $N$ samples $mathcal { D } = { x ^ { ( 1 ) } , . . . , x ^ { ( N ) } }$ , derived from a distribution $p ( X )$ , where $X in mathbb R$ . We can approximate the pdf using a set of delta functions (Section 2.6.5) or “spikes”, centered on these samples: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Some other common univariate distributions *",
        "subsubsection": "Gamma distribution"
    },
    {
        "content": "• Chi-squared distribution. This is defined by \nwhere $nu$ is called the degrees of freedom. This is the distribution of the sum of squared Gaussian random variables. More precisely, if $Z _ { i } sim mathcal { N } ( 0 , 1 )$ , and $begin{array} { r } { S = sum _ { i = 1 } ^ { nu } Z _ { i } ^ { 2 } } end{array}$ , then $S sim chi _ { nu } ^ { 2 }$ . \n• The inverse Gamma distribution is defined as follows: \nThe distribution has these properties \nThe mean only exists if $a > 1$ . The variance only exists if $a > 2$ . Note: if $X sim { mathrm { G a } } ( { mathrm { s h a p e } } =$ $a , { mathrm { r a t e } } = b$ ), then $1 / X sim mathrm { I G } ( { mathrm { s h a p e } } = a , { mathrm { s c a l e } } = b$ ). (Note that $b$ plays two different roles in this case.) \n2.7.6 Empirical distribution \nSuppose we have a set of $N$ samples $mathcal { D } = { x ^ { ( 1 ) } , . . . , x ^ { ( N ) } }$ , derived from a distribution $p ( X )$ , where $X in mathbb R$ . We can approximate the pdf using a set of delta functions (Section 2.6.5) or “spikes”, centered on these samples: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis is called the empirical distribution of the dataset $mathcal { D }$ . An example of this, with $N = 5$ , is shown in Figure 2.18(a). \nThe corresponding cdf is given by \nwhere $u _ { y } ( x )$ is a step function at $y$ defined by \nThis can be visualized as a “stair case”, as in Figure 2.18(b), where the jumps of height $1 / N$ occur at every sample. \n2.8 Transformations of random variables * \nSuppose $mathbf { boldsymbol { x } } sim p ( mathbf { boldsymbol { mathbf { mathit { rho } } } } )$ is some random variable, and $boldsymbol { y } = f ( boldsymbol { x } )$ is some deterministic transformation of it.   \nIn this section, we discuss how to compute $p ( pmb { y } )$ . \n2.8.1 Discrete case \nIf $X$ is a discrete rv, we can derive the pmf for $Y$ by simply summing up the probability mass for all the $x$ ’s such that $f ( x ) = y$ : \nFor example, if $f ( X ) = 1$ if $X$ is even and $f ( X ) = 0$ otherwise, and $p _ { x } ( X )$ is uniform on the set ${ 1 , ldots , 1 0 }$ , then $begin{array} { r } { p _ { y } ( 1 ) = sum _ { x in { 2 , 4 , 6 , 8 , 1 0 } } p _ { x } ( x ) = 0 . 5 } end{array}$ , and hence $p _ { y } ( 0 ) = 0 . 5$ also. Note that in this example, $f$ is a many-to-one function. \n2.8.2 Continuous case \nIf $X$ is continuous, we cannot use Equation (2.150) since $p _ { x } ( x )$ is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdf’s, as follows: \nIf $f$ is invertible, we can derive the pdf of $y$ by differentiating the cdf, as we show below. If $f$ is not invertible, we can use numerical integration, or a Monte Carlo approximation. \n2.8.3 Invertible transformations (bijections) \nIn this section, we consider the case of monotonic and hence invertible functions. (Note a function is invertible iff it is a bijector). With this assumption, there is a simple formula for the pdf of $y$ , as we will see. (This can be generalized to invertible, but non-monotonic, functions, but we ignore this case.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Some other common univariate distributions *",
        "subsubsection": "Empirical distribution"
    },
    {
        "content": "This is called the empirical distribution of the dataset $mathcal { D }$ . An example of this, with $N = 5$ , is shown in Figure 2.18(a). \nThe corresponding cdf is given by \nwhere $u _ { y } ( x )$ is a step function at $y$ defined by \nThis can be visualized as a “stair case”, as in Figure 2.18(b), where the jumps of height $1 / N$ occur at every sample. \n2.8 Transformations of random variables * \nSuppose $mathbf { boldsymbol { x } } sim p ( mathbf { boldsymbol { mathbf { mathit { rho } } } } )$ is some random variable, and $boldsymbol { y } = f ( boldsymbol { x } )$ is some deterministic transformation of it.   \nIn this section, we discuss how to compute $p ( pmb { y } )$ . \n2.8.1 Discrete case \nIf $X$ is a discrete rv, we can derive the pmf for $Y$ by simply summing up the probability mass for all the $x$ ’s such that $f ( x ) = y$ : \nFor example, if $f ( X ) = 1$ if $X$ is even and $f ( X ) = 0$ otherwise, and $p _ { x } ( X )$ is uniform on the set ${ 1 , ldots , 1 0 }$ , then $begin{array} { r } { p _ { y } ( 1 ) = sum _ { x in { 2 , 4 , 6 , 8 , 1 0 } } p _ { x } ( x ) = 0 . 5 } end{array}$ , and hence $p _ { y } ( 0 ) = 0 . 5$ also. Note that in this example, $f$ is a many-to-one function. \n2.8.2 Continuous case \nIf $X$ is continuous, we cannot use Equation (2.150) since $p _ { x } ( x )$ is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdf’s, as follows: \nIf $f$ is invertible, we can derive the pdf of $y$ by differentiating the cdf, as we show below. If $f$ is not invertible, we can use numerical integration, or a Monte Carlo approximation. \n2.8.3 Invertible transformations (bijections) \nIn this section, we consider the case of monotonic and hence invertible functions. (Note a function is invertible iff it is a bijector). With this assumption, there is a simple formula for the pdf of $y$ , as we will see. (This can be generalized to invertible, but non-monotonic, functions, but we ignore this case.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "Discrete case"
    },
    {
        "content": "This is called the empirical distribution of the dataset $mathcal { D }$ . An example of this, with $N = 5$ , is shown in Figure 2.18(a). \nThe corresponding cdf is given by \nwhere $u _ { y } ( x )$ is a step function at $y$ defined by \nThis can be visualized as a “stair case”, as in Figure 2.18(b), where the jumps of height $1 / N$ occur at every sample. \n2.8 Transformations of random variables * \nSuppose $mathbf { boldsymbol { x } } sim p ( mathbf { boldsymbol { mathbf { mathit { rho } } } } )$ is some random variable, and $boldsymbol { y } = f ( boldsymbol { x } )$ is some deterministic transformation of it.   \nIn this section, we discuss how to compute $p ( pmb { y } )$ . \n2.8.1 Discrete case \nIf $X$ is a discrete rv, we can derive the pmf for $Y$ by simply summing up the probability mass for all the $x$ ’s such that $f ( x ) = y$ : \nFor example, if $f ( X ) = 1$ if $X$ is even and $f ( X ) = 0$ otherwise, and $p _ { x } ( X )$ is uniform on the set ${ 1 , ldots , 1 0 }$ , then $begin{array} { r } { p _ { y } ( 1 ) = sum _ { x in { 2 , 4 , 6 , 8 , 1 0 } } p _ { x } ( x ) = 0 . 5 } end{array}$ , and hence $p _ { y } ( 0 ) = 0 . 5$ also. Note that in this example, $f$ is a many-to-one function. \n2.8.2 Continuous case \nIf $X$ is continuous, we cannot use Equation (2.150) since $p _ { x } ( x )$ is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdf’s, as follows: \nIf $f$ is invertible, we can derive the pdf of $y$ by differentiating the cdf, as we show below. If $f$ is not invertible, we can use numerical integration, or a Monte Carlo approximation. \n2.8.3 Invertible transformations (bijections) \nIn this section, we consider the case of monotonic and hence invertible functions. (Note a function is invertible iff it is a bijector). With this assumption, there is a simple formula for the pdf of $y$ , as we will see. (This can be generalized to invertible, but non-monotonic, functions, but we ignore this case.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "Continuous case"
    },
    {
        "content": "This is called the empirical distribution of the dataset $mathcal { D }$ . An example of this, with $N = 5$ , is shown in Figure 2.18(a). \nThe corresponding cdf is given by \nwhere $u _ { y } ( x )$ is a step function at $y$ defined by \nThis can be visualized as a “stair case”, as in Figure 2.18(b), where the jumps of height $1 / N$ occur at every sample. \n2.8 Transformations of random variables * \nSuppose $mathbf { boldsymbol { x } } sim p ( mathbf { boldsymbol { mathbf { mathit { rho } } } } )$ is some random variable, and $boldsymbol { y } = f ( boldsymbol { x } )$ is some deterministic transformation of it.   \nIn this section, we discuss how to compute $p ( pmb { y } )$ . \n2.8.1 Discrete case \nIf $X$ is a discrete rv, we can derive the pmf for $Y$ by simply summing up the probability mass for all the $x$ ’s such that $f ( x ) = y$ : \nFor example, if $f ( X ) = 1$ if $X$ is even and $f ( X ) = 0$ otherwise, and $p _ { x } ( X )$ is uniform on the set ${ 1 , ldots , 1 0 }$ , then $begin{array} { r } { p _ { y } ( 1 ) = sum _ { x in { 2 , 4 , 6 , 8 , 1 0 } } p _ { x } ( x ) = 0 . 5 } end{array}$ , and hence $p _ { y } ( 0 ) = 0 . 5$ also. Note that in this example, $f$ is a many-to-one function. \n2.8.2 Continuous case \nIf $X$ is continuous, we cannot use Equation (2.150) since $p _ { x } ( x )$ is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdf’s, as follows: \nIf $f$ is invertible, we can derive the pdf of $y$ by differentiating the cdf, as we show below. If $f$ is not invertible, we can use numerical integration, or a Monte Carlo approximation. \n2.8.3 Invertible transformations (bijections) \nIn this section, we consider the case of monotonic and hence invertible functions. (Note a function is invertible iff it is a bijector). With this assumption, there is a simple formula for the pdf of $y$ , as we will see. (This can be generalized to invertible, but non-monotonic, functions, but we ignore this case.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n2.8.3.1 Change of variables: scalar case \nWe start with an example. Suppose $x sim mathrm { U n i f } ( 0 , 1 )$ , and $y = f ( x ) = 2 x + 1$ . This function stretches and shifts the probability distribution, as shown in Figure 2.19(a). Now let us zoom in on a point $x$ and another point that is infinitesimally close, namely $x + d x$ . We see this interval gets mapped to $( y , y + d y )$ . The probability mass in these intervals must be the same, hence $p ( x ) d x = p ( y ) d y$ , and so $p ( y ) = p ( x ) d x / d y$ . However, since it does not matter (in terms of probability preservation) whether $d x / d y > 0$ or $d x / d y < 0$ , we get \nNow consider the general case for any $p _ { x } ( x )$ and any monotonic function $f : mathbb { R } to mathbb { R }$ . Let $g = f ^ { - 1 }$ , so $y = f ( x )$ and $x = g ( y )$ . If we assume that $f : mathbb { R } to mathbb { R }$ is monotonically increasing we get \nTaking derivatives we get \nWe can derive a similar expression (but with opposite signs) for the case where $f$ is monotonically decreasing. To handle the general case we take the absolute value to get \nThis is called change of variables formula. \n2.8.3.2 Change of variables: multivariate case \nWe can extend the previous results to multivariate distributions as follows. Let $f$ be an invertible function that maps $mathbb { R } ^ { n }$ to $mathbb { R } ^ { n }$ , with inverse $pmb { g }$ . Suppose we want to compute the pdf of $boldsymbol { y } = boldsymbol { f } ( boldsymbol { x } )$ . By \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nanalogy with the scalar case, we have \nwhere $begin{array} { r } { mathbf { J } _ { g } = frac { d g ( pmb { y } ) } { d pmb { y } ^ { top } } } end{array}$ is the Jacobian of $pmb { g }$ , and $| operatorname* { d e t } mathbf { J } ( pmb { y } ) |$ is the absolute value of the determinant of $mathbf { J }$ evaluated at $textbf {  { y } }$ . (See Section 7.8.5 for a discussion of Jacobians.) In Exercise 3.6 you will use this formula to derive the normalization constant for a multivariate Gaussian. \nFigure 2.20 illustrates this result in 2d, for the case where $f ( { pmb x } ) = { bf A } { pmb x } + { pmb b }$ , where $mathbf { A } = { binom { a } { b } } mathbf { binom { c } { d } }$ We see that the area of the unit square changes by a factor of $operatorname* { d e t } ( mathbf { A } ) = a d - b c$ , which is the area of the parallelogram. \nAs another example, consider transforming a density from Cartesian coordinates ${ pmb x } = ( x _ { 1 } , x _ { 2 } )$ to polar coordinates $pmb { y } = pmb { f } ( x _ { 1 } , x _ { 2 } )$ , so $pmb { g } ( r , theta ) = ( r cos theta , r sin theta )$ . Then \nHence \nTo see this geometrically, notice that the area of the shaded patch in Figure 2.21 is given by \nIn the limit, this is equal to the density at the center of the patch times the size of the patch, which is given by $r$ dr $d theta$ . Hence \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n2.8.4 Moments of a linear transformation \nSuppose $f$ is an affine function, so $pmb { y } = mathbf { A } pmb { x } + pmb { b }$ . In this case, we can easily derive the mean and covariance of $textbf {  { y } }$ as follows. First, for the mean, we have \nwhere $pmb { mu } = mathbb { E } left[ pmb { x } right]$ . If $f$ is a scalar-valued function, $f ( { pmb x } ) = { pmb a } ^ { 1 } { pmb x } + b$ , the corresponding result is \nFor the covariance, we have \nwhere $pmb { Sigma } = mathrm { C o v } left[ pmb { x } right]$ . We leave the proof of this as an exercise. \nAs a special case, if $y = a ^ { mathsf { T } } x + b$ , we get \nFor example, to compute the variance of the sum of two scalar random variables, we can set $pmb { a } = lfloor 1 , 1 rfloor$ to get \nNote, however, that although some distributions (such as the Gaussian) are completely characterized by their mean and covariance, in general we must use the techniques described above to derive the full distribution of $textbf {  { y } }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "Invertible transformations (bijections)"
    },
    {
        "content": "2.8.4 Moments of a linear transformation \nSuppose $f$ is an affine function, so $pmb { y } = mathbf { A } pmb { x } + pmb { b }$ . In this case, we can easily derive the mean and covariance of $textbf {  { y } }$ as follows. First, for the mean, we have \nwhere $pmb { mu } = mathbb { E } left[ pmb { x } right]$ . If $f$ is a scalar-valued function, $f ( { pmb x } ) = { pmb a } ^ { 1 } { pmb x } + b$ , the corresponding result is \nFor the covariance, we have \nwhere $pmb { Sigma } = mathrm { C o v } left[ pmb { x } right]$ . We leave the proof of this as an exercise. \nAs a special case, if $y = a ^ { mathsf { T } } x + b$ , we get \nFor example, to compute the variance of the sum of two scalar random variables, we can set $pmb { a } = lfloor 1 , 1 rfloor$ to get \nNote, however, that although some distributions (such as the Gaussian) are completely characterized by their mean and covariance, in general we must use the techniques described above to derive the full distribution of $textbf {  { y } }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTable 2.4: Discrete convolution of $pmb { x } = [ 1 , 2 , 3 , 4 ]$ with $pmb { y } = [ 5 , 6 , 7 ]$ to yield $boldsymbol { z } = [ 5 , 1 6 , 3 4 , 5 2 , 4 5 , 2 8 ]$ . In general, $begin{array} { r } { z _ { n } = sum _ { k = - infty } ^ { infty } x _ { k } y _ { n - k } } end{array}$ . We see that this operation consists of “flipping” $_ y$ and then “dragging” it over $_ { pmb { x } }$ , multiplying elementwise, and adding up the results. \n2.8.5 The convolution theorem \nLet $y = x _ { 1 } + x _ { 2 }$ , where $x _ { 1 }$ and $x _ { 2 }$ are independent rv’s. If these are discrete random variables, we can compute the pmf for the sum as follows: \nfor $j = ldots , - 2 , - 1 , 0 , 1 , 2 , ldots$ \nIf $x _ { 1 }$ and $x _ { 2 }$ have pdf’s $p _ { 1 } ( x _ { 1 } )$ and $p _ { 2 } ( x _ { 2 } )$ , what is the distribution of $y$ ? The cdf for $y$ is given by \nwhere we integrate over the region $R$ defined by $x _ { 1 } + x _ { 2 } < y ^ { * }$ . Thus the pdf for $y$ is \nwhere we used the rule of differentiating under the integral sign: \nWe can write Equation (2.170) as follows: \nwhere $circledast$ represents the convolution operator. For finite length vectors, the integrals become sums, and convolution can be thought of as a “flip and drag” operation, as illustrated in Table 2.4. Consequently, Equation (2.170) is called the convolution theorem. \nFor example, suppose we roll two dice, so $p _ { 1 }$ and $p _ { 2 }$ are both the discrete uniform distributions \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "Moments of a linear transformation"
    },
    {
        "content": "Table 2.4: Discrete convolution of $pmb { x } = [ 1 , 2 , 3 , 4 ]$ with $pmb { y } = [ 5 , 6 , 7 ]$ to yield $boldsymbol { z } = [ 5 , 1 6 , 3 4 , 5 2 , 4 5 , 2 8 ]$ . In general, $begin{array} { r } { z _ { n } = sum _ { k = - infty } ^ { infty } x _ { k } y _ { n - k } } end{array}$ . We see that this operation consists of “flipping” $_ y$ and then “dragging” it over $_ { pmb { x } }$ , multiplying elementwise, and adding up the results. \n2.8.5 The convolution theorem \nLet $y = x _ { 1 } + x _ { 2 }$ , where $x _ { 1 }$ and $x _ { 2 }$ are independent rv’s. If these are discrete random variables, we can compute the pmf for the sum as follows: \nfor $j = ldots , - 2 , - 1 , 0 , 1 , 2 , ldots$ \nIf $x _ { 1 }$ and $x _ { 2 }$ have pdf’s $p _ { 1 } ( x _ { 1 } )$ and $p _ { 2 } ( x _ { 2 } )$ , what is the distribution of $y$ ? The cdf for $y$ is given by \nwhere we integrate over the region $R$ defined by $x _ { 1 } + x _ { 2 } < y ^ { * }$ . Thus the pdf for $y$ is \nwhere we used the rule of differentiating under the integral sign: \nWe can write Equation (2.170) as follows: \nwhere $circledast$ represents the convolution operator. For finite length vectors, the integrals become sums, and convolution can be thought of as a “flip and drag” operation, as illustrated in Table 2.4. Consequently, Equation (2.170) is called the convolution theorem. \nFor example, suppose we roll two dice, so $p _ { 1 }$ and $p _ { 2 }$ are both the discrete uniform distributions \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nover ${ 1 , 2 , ldots , 6 }$ . Let $y = x _ { 1 } + x _ { 2 }$ be the sum of the dice. We have \nContinuing in this way, we find $p ( y = 4 ) = 3 / 3 6$ , $p ( y = 5 ) = 4 / 3 6$ , $p ( y = 6 ) = 5 / 3 6$ , $p ( y = 7 ) = 6 / 3 6$ , $p ( y = 8 ) = 5 / 3 6$ , $p ( y = 9 ) = 4 / 3 6$ , $p ( y = 1 0 ) = 3 / 3 6$ , $p ( y = 1 1 ) = 2 / 3 6$ and $p ( y = 1 2 ) = 1 / 3 6$ . See Figure 2.22 for a plot. We see that the distribution looks like a Gaussian; we explain the reasons for this in Section 2.8.6. \nWe can also compute the pdf of the sum of two continuous rv’s. For example, in the case of Gaussians, where $x _ { 1 } sim mathcal { N } ( pmb { mu } _ { 1 } , sigma _ { 1 } ^ { 2 } )$ and $x _ { 2 } sim mathcal { N } ( mu _ { 2 } , sigma _ { 2 } ^ { 2 } )$ , one can show (Exercise 2.4) that if $y = x _ { 1 } + x _ { 2 }$ then \nHence the convolution of two Gaussians is a Gaussian. \n2.8.6 Central limit theorem \nNow consider $N _ { mathcal { D } }$ random variables with pdf’s (not necessarily Gaussian) $p _ { n } ( x )$ , each with mean $mu$ and variance $sigma ^ { 2 }$ . We assume each variable is independent and identically distributed or iid for short, which means $X _ { n } sim p ( X )$ are independent samples from the same distribution. Let $begin{array} { r } { S _ { N _ { mathcal { D } } } = sum _ { n = 1 } ^ { N _ { mathcal { D } } } X _ { n } } end{array}$ be the sum of the rv’s. One can show that, as $N$ increases, the distribution of this sum approaches \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "The convolution theorem"
    },
    {
        "content": "over ${ 1 , 2 , ldots , 6 }$ . Let $y = x _ { 1 } + x _ { 2 }$ be the sum of the dice. We have \nContinuing in this way, we find $p ( y = 4 ) = 3 / 3 6$ , $p ( y = 5 ) = 4 / 3 6$ , $p ( y = 6 ) = 5 / 3 6$ , $p ( y = 7 ) = 6 / 3 6$ , $p ( y = 8 ) = 5 / 3 6$ , $p ( y = 9 ) = 4 / 3 6$ , $p ( y = 1 0 ) = 3 / 3 6$ , $p ( y = 1 1 ) = 2 / 3 6$ and $p ( y = 1 2 ) = 1 / 3 6$ . See Figure 2.22 for a plot. We see that the distribution looks like a Gaussian; we explain the reasons for this in Section 2.8.6. \nWe can also compute the pdf of the sum of two continuous rv’s. For example, in the case of Gaussians, where $x _ { 1 } sim mathcal { N } ( pmb { mu } _ { 1 } , sigma _ { 1 } ^ { 2 } )$ and $x _ { 2 } sim mathcal { N } ( mu _ { 2 } , sigma _ { 2 } ^ { 2 } )$ , one can show (Exercise 2.4) that if $y = x _ { 1 } + x _ { 2 }$ then \nHence the convolution of two Gaussians is a Gaussian. \n2.8.6 Central limit theorem \nNow consider $N _ { mathcal { D } }$ random variables with pdf’s (not necessarily Gaussian) $p _ { n } ( x )$ , each with mean $mu$ and variance $sigma ^ { 2 }$ . We assume each variable is independent and identically distributed or iid for short, which means $X _ { n } sim p ( X )$ are independent samples from the same distribution. Let $begin{array} { r } { S _ { N _ { mathcal { D } } } = sum _ { n = 1 } ^ { N _ { mathcal { D } } } X _ { n } } end{array}$ be the sum of the rv’s. One can show that, as $N$ increases, the distribution of this sum approaches \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHence the distribution of the quantity \nconverges to the standard normal, where $overline { { X } } = S _ { N } / N$ is the sample mean. This is called the central limit theorem. See e.g., [Jay03, p222] or [Ric95, p169] for a proof. \nIn Figure 2.23 we give an example in which we compute the sample mean of rv’s drawn from a beta distribution. We see that the sampling distribution of this mean rapidly converges to a Gaussian distribution. \n2.8.7 Monte Carlo approximation \nSuppose $_ { x }$ is a random variable, and $boldsymbol { y } = f ( boldsymbol { x } )$ is some function of $_ { x }$ . It is often difficult to compute the induced distribution $p ( pmb { y } )$ analytically. One simple but powerful alternative is to draw a large number of samples from the $_ { x }$ ’s distribution, and then to use these samples (instead of the distribution) to approximate $p ( pmb { y } )$ . \nFor example, suppose $x sim mathrm { U n i f } ( - 1 , 1 )$ and $y = f ( x ) = x ^ { 2 }$ . We can approximate $p ( y )$ by drawing many samples from $p ( x )$ (using a uniform random number generator), squaring them, and computing the resulting empirical distribution, which is given by \nThis is just an equally weighted “sum of spikes”, each centered on one of the samples (see Section 2.7.6). By using enough samples, we can approximate $p ( y )$ rather well. See Figure 2.24 for an illustration. This approach is called a Monte Carlo approximation to the distribution. (The term “Monte Carlo” comes from the name of a famous gambling casino in Monaco.) Monte Carlo techniques were \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 first developed in the area of statistical physics — in particular, during development of the atomic bomb — but are now widely used in statistics and machine learning as well. More details can be found in the sequel to this book, [Mur23], as well as specialized books on the topic, such as [Liu01; RC04; KTB11; BZ20].",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "Central limit theorem"
    },
    {
        "content": "Hence the distribution of the quantity \nconverges to the standard normal, where $overline { { X } } = S _ { N } / N$ is the sample mean. This is called the central limit theorem. See e.g., [Jay03, p222] or [Ric95, p169] for a proof. \nIn Figure 2.23 we give an example in which we compute the sample mean of rv’s drawn from a beta distribution. We see that the sampling distribution of this mean rapidly converges to a Gaussian distribution. \n2.8.7 Monte Carlo approximation \nSuppose $_ { x }$ is a random variable, and $boldsymbol { y } = f ( boldsymbol { x } )$ is some function of $_ { x }$ . It is often difficult to compute the induced distribution $p ( pmb { y } )$ analytically. One simple but powerful alternative is to draw a large number of samples from the $_ { x }$ ’s distribution, and then to use these samples (instead of the distribution) to approximate $p ( pmb { y } )$ . \nFor example, suppose $x sim mathrm { U n i f } ( - 1 , 1 )$ and $y = f ( x ) = x ^ { 2 }$ . We can approximate $p ( y )$ by drawing many samples from $p ( x )$ (using a uniform random number generator), squaring them, and computing the resulting empirical distribution, which is given by \nThis is just an equally weighted “sum of spikes”, each centered on one of the samples (see Section 2.7.6). By using enough samples, we can approximate $p ( y )$ rather well. See Figure 2.24 for an illustration. This approach is called a Monte Carlo approximation to the distribution. (The term “Monte Carlo” comes from the name of a famous gambling casino in Monaco.) Monte Carlo techniques were \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 first developed in the area of statistical physics — in particular, during development of the atomic bomb — but are now widely used in statistics and machine learning as well. More details can be found in the sequel to this book, [Mur23], as well as specialized books on the topic, such as [Liu01; RC04; KTB11; BZ20]. \n\n2.9 Exercises \nExercise 2.1 [Conditional independence $^ * ]$ (Source: Koller.) \na. Let $H in { 1 , ldots , K }$ be a discrete random variable, and let $e _ { 1 }$ and $e _ { 2 }$ be the observed values of two other random variables $E _ { 1 }$ and $E _ { 2 }$ . Suppose we wish to calculate the vector \nWhich of the following sets of numbers are sufficient for the calculation? \ni. $P ( e _ { 1 } , e _ { 2 } )$ , $P ( H )$ , $P ( e _ { 1 } | H )$ , $P ( e _ { 2 } | H )$ ii. $P ( e _ { 1 } , e _ { 2 } )$ , $P ( H )$ , $P ( e _ { 1 } , e _ { 2 } | H )$ iii. $( e _ { 1 } | H ) , P ( e _ { 2 } | H ) , P ( H )$ \nb. Now suppose we now assume $E _ { 1 } perp E _ { 2 } | H$ (i.e., $E _ { 1 }$ and $E _ { 2 } ^ { prime }$ are conditionally independent given $H$ ). Which of the above 3 sets are sufficient now? \nShow your calculations as well as giving the final result. Hint: use Bayes rule. \nExercise 2.2 [Pairwise independence does not imply mutual independence] We say that two random variables are pairwise independent if \nand hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Transformations of random variables *",
        "subsubsection": "Monte Carlo approximation"
    },
    {
        "content": "2.9 Exercises \nExercise 2.1 [Conditional independence $^ * ]$ (Source: Koller.) \na. Let $H in { 1 , ldots , K }$ be a discrete random variable, and let $e _ { 1 }$ and $e _ { 2 }$ be the observed values of two other random variables $E _ { 1 }$ and $E _ { 2 }$ . Suppose we wish to calculate the vector \nWhich of the following sets of numbers are sufficient for the calculation? \ni. $P ( e _ { 1 } , e _ { 2 } )$ , $P ( H )$ , $P ( e _ { 1 } | H )$ , $P ( e _ { 2 } | H )$ ii. $P ( e _ { 1 } , e _ { 2 } )$ , $P ( H )$ , $P ( e _ { 1 } , e _ { 2 } | H )$ iii. $( e _ { 1 } | H ) , P ( e _ { 2 } | H ) , P ( H )$ \nb. Now suppose we now assume $E _ { 1 } perp E _ { 2 } | H$ (i.e., $E _ { 1 }$ and $E _ { 2 } ^ { prime }$ are conditionally independent given $H$ ). Which of the above 3 sets are sufficient now? \nShow your calculations as well as giving the final result. Hint: use Bayes rule. \nExercise 2.2 [Pairwise independence does not imply mutual independence] We say that two random variables are pairwise independent if \nand hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe say that $n$ random variables are mutually independent if \nand hence \nShow that pairwise independence between all pairs of variables does not necessarily imply mutual independence.   \nIt suffices to give a counter example. \nExercise 2.3 [Conditional independence iff joint factorizes *] In the text we said $X perp Y | Z$ iff \nfor all $x , y , z$ such that $p ( z ) > 0$ . Now prove the following alternative definition: $X perp Y | Z$ iff there exist functions $g$ and $h$ such that \nfor all $x , y , z$ such that $p ( z ) > 0$ . \nExercise 2.4 [Convolution of two Gaussians is a Gaussian] Show that the convolution of two Gaussians is a Gaussian, i.e., \nwhere $y = x _ { 1 } + x _ { 2 }$ , $x _ { 1 } sim mathcal { N } ( mu _ { 1 } , sigma _ { 1 } ^ { 2 } )$ and $x _ { 2 } sim mathcal { N } ( mu _ { 2 } , sigma _ { 2 } ^ { 2 } )$ . \nExercise 2.5 [Expected value of the minimum of two rv’s $^ *$ ] \nSuppose $X , Y$ are two points sampled independently and uniformly at random from the interval $[ 0 , 1 ]$ . What is the expected location of the leftmost point? \nExercise 2.6 [Variance of a sum] Show that the variance of a sum is \nwhere $operatorname { C o v } left[ X , Y right]$ is the covariance between $X$ and $Y$ . \nExercise 2.7 [Deriving the inverse gamma density *] Let $X sim operatorname { G a } ( a , b )$ , and $Y = 1 / X$ . Derive the distribution of $Y$ . \nExercise 2.8 [Mean, mode, variance for the beta distribution] Suppose $theta sim operatorname { B e t a } ( a , b )$ . Show that the mean, mode and variance are given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nExercise 2.9 [Bayes rule for medical diagnosis *] \nAfter your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive for a serious disease, and that the test is 99% accurate (i.e., the probability of testing positive given that you have the disease is 0.99, as is the probability of testing negative given that you don’t have the disease). The good news is that this is a rare disease, striking only one in 10,000 people. What are the chances that you actually have the disease? (Show your calculations as well as giving the final result.) \nExercise 2.10 [Legal reasoning] \n(Source: Peter Lee.) Suppose a crime has been committed. Blood is found at the scene for which there is no innocent explanation. It is of a type which is present in $1 %$ of the population. \na. The prosecutor claims: “There is a $1 %$ chance that the defendant would have the crime blood type if he were innocent. Thus there is a 99% chance that he is guilty”. This is known as the prosecutor’s fallacy. What is wrong with this argument?   \nb. The defender claims: “The crime occurred in a city of 800,000 people. The blood type would be found in approximately 8000 people. The evidence has provided a probability of just 1 in 8000 that the defendant is guilty, and thus has no relevance.” This is known as the defender’s fallacy. What is wrong with this argument? \nExercise 2.11 [Probabilities are sensitive to the form of the question that was used to generate the answer (Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin flip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability $_ { 1 / 2 }$ . The other possibilities—two boys or two girls—have probabilities 1/4 and $1 / 4$ . \n\na. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is a girl?   \nb. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability that the other child is a girl? \nExercise 2.12 [Normalization constant for a 1D Gaussian] The normalization constant for a zero-mean Gaussian is given by \nwhere $a = - infty$ and $b = infty$ . To compute this, consider its square \nLet us change variables from cartesian $( x , y )$ to polar $( r , theta )$ using $x  =  r cos theta$ and $y  =  r sin theta$ . Since $d x d y = r d r d theta$ , and $c o s ^ { 2 } theta + sin ^ { 2 } theta = 1$ , we have \nEvaluate this integral and hence show $Z = sqrt { sigma ^ { 2 } 2 pi }$ . Hint 1: separate the integral into a product of two terms, the first of which (involving $d theta$ ) is constant, so is easy. Hint 2: if $u = e ^ { - r ^ { 2 } / 2 sigma ^ { 2 } }$ then $begin{array} { r } { d u / d r = - frac { 1 } { sigma ^ { 2 } } r e ^ { - r ^ { 2 } / 2 sigma ^ { 2 } } } end{array}$ , so the second integral is also easy (since $begin{array} { r } { int u ^ { prime } ( r ) d r = u ( r ) , } end{array}$ ). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n3 Probability: Multivariate Models \n3.1 Joint distributions for multiple random variables \nIn this section, we discuss various ways to measure the dependence of one or more variables on each other. \n3.1.1 Covariance \nThe covariance between two rv’s $X$ and $Y$ measures the degree to which $X$ and $Y$ are (linearly) related. Covariance is defined as \nIf $_ { x }$ is a $D$ -dimensional random vector, its covariance matrix is defined to be the following symmetric, positive semi definite matrix: \nfrom which we get the important result \nAnother useful result is that the covariance of a linear transformation is given by \nas shown in Exercise 3.4. \nThe cross-covariance between two random vectors is defined as",
        "chapter": "I Foundations",
        "section": "Probability: Univariate Models",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "3 Probability: Multivariate Models \n3.1 Joint distributions for multiple random variables \nIn this section, we discuss various ways to measure the dependence of one or more variables on each other. \n3.1.1 Covariance \nThe covariance between two rv’s $X$ and $Y$ measures the degree to which $X$ and $Y$ are (linearly) related. Covariance is defined as \nIf $_ { x }$ is a $D$ -dimensional random vector, its covariance matrix is defined to be the following symmetric, positive semi definite matrix: \nfrom which we get the important result \nAnother useful result is that the covariance of a linear transformation is given by \nas shown in Exercise 3.4. \nThe cross-covariance between two random vectors is defined as \n3.1.2 Correlation \nCovariances can be between negative and positive infinity. Sometimes it is more convenient to work with a normalized measure, with a finite lower and upper bound. The (Pearson) correlation coefficient between $X$ and $Y$ is defined as \nOne can show (Exercise 3.2) that $- 1 le rho le 1$ . \nOne can also show that corr $[ X , Y ] = 1$ if and only if $Y = a X + b$ (and $a > 0$ ) for some parameters $a$ and $b$ , i.e., if there is a linear relationship between $X$ and $Y$ (see Exercise 3.3). Intuitively one might expect the correlation coefficient to be related to the slope of the regression line, i.e., the coefficient $a$ in the expression $Y = a X + b$ . However, as we show in Equation (11.27), the regression coefficient is in fact given by $a = operatorname { C o v } left[ X , Y right] / mathbb { V } left[ X right]$ . In Figure 3.1, we show that the correlation coefficient can be 0 for strong, but nonlinear, relationships. (Compare to Figure 6.6.) Thus a better way to think of the correlation coefficient is as $a$ degree of linearity. (See correlation2d.ipynb for a demo to illustrate this idea.) \nIn the case of a vector $_ { x }$ of related random variables, the correlation matrix is given by \nThis can be written more compactly as \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Joint distributions for multiple random variables",
        "subsubsection": "Covariance"
    },
    {
        "content": "3.1.2 Correlation \nCovariances can be between negative and positive infinity. Sometimes it is more convenient to work with a normalized measure, with a finite lower and upper bound. The (Pearson) correlation coefficient between $X$ and $Y$ is defined as \nOne can show (Exercise 3.2) that $- 1 le rho le 1$ . \nOne can also show that corr $[ X , Y ] = 1$ if and only if $Y = a X + b$ (and $a > 0$ ) for some parameters $a$ and $b$ , i.e., if there is a linear relationship between $X$ and $Y$ (see Exercise 3.3). Intuitively one might expect the correlation coefficient to be related to the slope of the regression line, i.e., the coefficient $a$ in the expression $Y = a X + b$ . However, as we show in Equation (11.27), the regression coefficient is in fact given by $a = operatorname { C o v } left[ X , Y right] / mathbb { V } left[ X right]$ . In Figure 3.1, we show that the correlation coefficient can be 0 for strong, but nonlinear, relationships. (Compare to Figure 6.6.) Thus a better way to think of the correlation coefficient is as $a$ degree of linearity. (See correlation2d.ipynb for a demo to illustrate this idea.) \nIn the case of a vector $_ { x }$ of related random variables, the correlation matrix is given by \nThis can be written more compactly as \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere ${ bf K } _ { x x }$ is the auto-covariance matrix \nand $mathbf { R } _ { x x } = mathbb { E } left[ pmb { x } pmb { x } ^ { top } right]$ is the autocorrelation matrix. \n3.1.3 Uncorrelated does not imply independent \nIf $X$ and $Y$ are independent, meaning $p ( X , Y )  : =  : p ( X ) p ( Y )$ , then $mathrm { C o v } [ X , Y ] = 0$ , and hence corr $[ X , Y ] = 0$ . So independent implies uncorrelated. However, the converse is not true: uncorrelated does not imply independent. For example, let $X sim U ( - 1 , 1 )$ and $Y = X ^ { 2 }$ . Clearly $Y$ is dependent on $X$ (in fact, $Y$ is uniquely determined by $X$ ), yet one can show (Exercise 3.1) that corr $[ X , Y ] = 0$ . Some striking examples of this fact are shown in Figure 3.1. This shows several data sets where there is clear dependence between $X$ and $Y$ , and yet the correlation coefficient is 0. A more general measure of dependence between random variables is mutual information, discussed in Section 6.3. This is zero only if the variables truly are independent. \n3.1.4 Correlation does not imply causation \nIt is well known that “correlation does not imply causation”. For example, consider Figure 3.2. In red, we plot $x _ { 1 : T }$ , where $x _ { t }$ is the amount of ice cream sold in month $t$ . In yellow, we plot $y _ { 1 : T }$ , where $y _ { t }$ is the violent crime rate in month $t$ . (Quantities have been rescaled to make the plots overlap.) We see a strong correlation between these signals. Indeed, it is sometimes claimed that “eating ice cream causes murder” [Pet13]. Of course, this is just a spurious correlation, due to a hidden common cause, namely the weather. Hot weather increases ice cream sales, for obvious \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license reasons. Hot weather also increases violent crime; the reason for this is hotly (ahem) debated; some claim it is due to an increase in anger [And01], but other claim it is merely due to more people being outside [Ash18], where most murders occur.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Joint distributions for multiple random variables",
        "subsubsection": "Correlation"
    },
    {
        "content": "where ${ bf K } _ { x x }$ is the auto-covariance matrix \nand $mathbf { R } _ { x x } = mathbb { E } left[ pmb { x } pmb { x } ^ { top } right]$ is the autocorrelation matrix. \n3.1.3 Uncorrelated does not imply independent \nIf $X$ and $Y$ are independent, meaning $p ( X , Y )  : =  : p ( X ) p ( Y )$ , then $mathrm { C o v } [ X , Y ] = 0$ , and hence corr $[ X , Y ] = 0$ . So independent implies uncorrelated. However, the converse is not true: uncorrelated does not imply independent. For example, let $X sim U ( - 1 , 1 )$ and $Y = X ^ { 2 }$ . Clearly $Y$ is dependent on $X$ (in fact, $Y$ is uniquely determined by $X$ ), yet one can show (Exercise 3.1) that corr $[ X , Y ] = 0$ . Some striking examples of this fact are shown in Figure 3.1. This shows several data sets where there is clear dependence between $X$ and $Y$ , and yet the correlation coefficient is 0. A more general measure of dependence between random variables is mutual information, discussed in Section 6.3. This is zero only if the variables truly are independent. \n3.1.4 Correlation does not imply causation \nIt is well known that “correlation does not imply causation”. For example, consider Figure 3.2. In red, we plot $x _ { 1 : T }$ , where $x _ { t }$ is the amount of ice cream sold in month $t$ . In yellow, we plot $y _ { 1 : T }$ , where $y _ { t }$ is the violent crime rate in month $t$ . (Quantities have been rescaled to make the plots overlap.) We see a strong correlation between these signals. Indeed, it is sometimes claimed that “eating ice cream causes murder” [Pet13]. Of course, this is just a spurious correlation, due to a hidden common cause, namely the weather. Hot weather increases ice cream sales, for obvious \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license reasons. Hot weather also increases violent crime; the reason for this is hotly (ahem) debated; some claim it is due to an increase in anger [And01], but other claim it is merely due to more people being outside [Ash18], where most murders occur.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Joint distributions for multiple random variables",
        "subsubsection": "Uncorrelated does not imply independent"
    },
    {
        "content": "where ${ bf K } _ { x x }$ is the auto-covariance matrix \nand $mathbf { R } _ { x x } = mathbb { E } left[ pmb { x } pmb { x } ^ { top } right]$ is the autocorrelation matrix. \n3.1.3 Uncorrelated does not imply independent \nIf $X$ and $Y$ are independent, meaning $p ( X , Y )  : =  : p ( X ) p ( Y )$ , then $mathrm { C o v } [ X , Y ] = 0$ , and hence corr $[ X , Y ] = 0$ . So independent implies uncorrelated. However, the converse is not true: uncorrelated does not imply independent. For example, let $X sim U ( - 1 , 1 )$ and $Y = X ^ { 2 }$ . Clearly $Y$ is dependent on $X$ (in fact, $Y$ is uniquely determined by $X$ ), yet one can show (Exercise 3.1) that corr $[ X , Y ] = 0$ . Some striking examples of this fact are shown in Figure 3.1. This shows several data sets where there is clear dependence between $X$ and $Y$ , and yet the correlation coefficient is 0. A more general measure of dependence between random variables is mutual information, discussed in Section 6.3. This is zero only if the variables truly are independent. \n3.1.4 Correlation does not imply causation \nIt is well known that “correlation does not imply causation”. For example, consider Figure 3.2. In red, we plot $x _ { 1 : T }$ , where $x _ { t }$ is the amount of ice cream sold in month $t$ . In yellow, we plot $y _ { 1 : T }$ , where $y _ { t }$ is the violent crime rate in month $t$ . (Quantities have been rescaled to make the plots overlap.) We see a strong correlation between these signals. Indeed, it is sometimes claimed that “eating ice cream causes murder” [Pet13]. Of course, this is just a spurious correlation, due to a hidden common cause, namely the weather. Hot weather increases ice cream sales, for obvious \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license reasons. Hot weather also increases violent crime; the reason for this is hotly (ahem) debated; some claim it is due to an increase in anger [And01], but other claim it is merely due to more people being outside [Ash18], where most murders occur. \n\nAnother famous example concerns the positive correlation between birth rates and the presence of storks (a kind of bird). This has given rise to the urban legend that storks deliver babies [Mat00]. Of course, the true reason for the correlation is more likely due to hidden factors, such as increased living standards and hence more food. Many more amusing examples of such spurious correlations can be found in [Vig15]. \nThese examples serve as a “warning sign”, that we should not treat the ability for $x$ to predict $y$ as an indicator that x causes y. \n3.1.5 Simpson’s paradox \nSimpson’s paradox says that a statistical trend or relationship that appears in several different groups of data can disappear or reverse sign when these groups are combined. This results in counterintuitive behavior if we misinterpret claims of statistical dependence in a causal way. \nA visualization of the paradox is given in Figure 3.3. Overall, we see that $y$ decreases with $x$ , but within each subpopulation, $y$ increases with $x$ . \nFor a recent real-world example of Simpson’s paradox in the context of COVID-19, consider Figure 3.4(a). This shows that the case fatality rate (CFR) of COVID-19 in Italy is less than in China in each age group, but is higher overall. The reason for this is that there are more older people in Italy, as shown in Figure 3.4(b). In other words, Figure 3.4(a) shows $p ( F = 1 | A , C )$ , where $A$ is age, $C$ is country, and $F = 1$ is the event that someone dies from COVID-19, and Figure 3.4(b) shows $p ( A | C )$ , which is the probability someone is in age bucket $A$ for country $C$ . Combining these, we find $p ( F = 1 | C = mathrm { I t a l y } ) > p ( F = 1 | C = mathrm { C h i n a } )$ . See [KGS20] for more details. \n3.2 The multivariate Gaussian (normal) distribution \nThe most widely used joint probability distribution for continuous random variables is the multivariate Gaussian or multivariate normal (MVN). This is mostly because it is mathematically convenient, but also because the Gaussian assumption is fairly reasonable in many cases (see the discussion in Section 2.6.4). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Joint distributions for multiple random variables",
        "subsubsection": "Correlation does not imply causation"
    },
    {
        "content": "Another famous example concerns the positive correlation between birth rates and the presence of storks (a kind of bird). This has given rise to the urban legend that storks deliver babies [Mat00]. Of course, the true reason for the correlation is more likely due to hidden factors, such as increased living standards and hence more food. Many more amusing examples of such spurious correlations can be found in [Vig15]. \nThese examples serve as a “warning sign”, that we should not treat the ability for $x$ to predict $y$ as an indicator that x causes y. \n3.1.5 Simpson’s paradox \nSimpson’s paradox says that a statistical trend or relationship that appears in several different groups of data can disappear or reverse sign when these groups are combined. This results in counterintuitive behavior if we misinterpret claims of statistical dependence in a causal way. \nA visualization of the paradox is given in Figure 3.3. Overall, we see that $y$ decreases with $x$ , but within each subpopulation, $y$ increases with $x$ . \nFor a recent real-world example of Simpson’s paradox in the context of COVID-19, consider Figure 3.4(a). This shows that the case fatality rate (CFR) of COVID-19 in Italy is less than in China in each age group, but is higher overall. The reason for this is that there are more older people in Italy, as shown in Figure 3.4(b). In other words, Figure 3.4(a) shows $p ( F = 1 | A , C )$ , where $A$ is age, $C$ is country, and $F = 1$ is the event that someone dies from COVID-19, and Figure 3.4(b) shows $p ( A | C )$ , which is the probability someone is in age bucket $A$ for country $C$ . Combining these, we find $p ( F = 1 | C = mathrm { I t a l y } ) > p ( F = 1 | C = mathrm { C h i n a } )$ . See [KGS20] for more details. \n3.2 The multivariate Gaussian (normal) distribution \nThe most widely used joint probability distribution for continuous random variables is the multivariate Gaussian or multivariate normal (MVN). This is mostly because it is mathematically convenient, but also because the Gaussian assumption is fairly reasonable in many cases (see the discussion in Section 2.6.4). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Joint distributions for multiple random variables",
        "subsubsection": "Simpson's paradox"
    },
    {
        "content": "3.2.1 Definition \nThe MVN density is defined by the following: \nwhere $pmb { mu } = mathbb { E } left[ pmb { y } right] in mathbb { R } ^ { D }$ is the mean vector, and $pmb { Sigma } = mathrm { C o v } left[ pmb { y } right]$ is the $D times D$ covariance matrix, defined as follows: \nwhere \n$operatorname { C o v } left[ Y _ { i } , Y _ { j } right] triangleq mathbb { E } left[ ( Y _ { i } - mathbb { E } left[ Y _ { i } right] ) ( Y _ { j } - mathbb { E } left[ Y _ { j } right] ) right] = mathbb { E } left[ Y _ { i } Y _ { j } right] - mathbb { E } left[ Y _ { i } right] mathbb { E } left[ Y _ { j } right]$ and $mathbb { V } left[ Y _ { i } right] = operatorname { C o v } left[ Y _ { i } , Y _ { i } right]$ . From Equation (3.12), we get the important result \nThe normalization constant in Equation (3.11) ${ cal Z } = ( 2 pi ) ^ { D / 2 } | Sigma | ^ { 1 / 2 }$ just ensures that the pdf integrates to 1 (see Exercise 3.6). \nIn 2d, the MVN is known as the bivariate Gaussian distribution. Its pdf can be represented as $pmb { y } sim mathcal { N } ( pmb { mu } , pmb { Sigma } )$ , where $ b { y } in mathbb { R } ^ { 2 }$ , $textstyle mu in mathbb { R } ^ { 2 }$ and \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $rho$ is the correlation coefficient, defined by \nOne can show (Exercise 3.2) that $- 1 le mathrm { c o r r } left[ Y _ { 1 } , Y _ { 2 } right] le 1$ . Expanding out the pdf in the 2d case gives the following rather intimidating-looking result: \nFigure 3.5 and Figure 3.6 plot some MVN densities in 2d for three different kinds of covariance matrices. A full covariance matrix has $D ( D + 1 ) / 2$ parameters, where we divide by 2 since $pmb { Sigma }$ is \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 symmetric. (The reason for the elliptical shape is explained in Section 7.4.4, where we discuss the geometry of quadratic forms.) A diagonal covariance matrix has $D$ parameters, and has 0s in the off-diagonal terms. A spherical covariance matrix, also called isotropic covariance matrix, has the form $begin{array} { r } { pmb { Sigma } = sigma ^ { 2 } mathbf { I } _ { D } } end{array}$ , so it only has one free parameter, namely $sigma ^ { 2 }$ . \n\n3.2.2 Mahalanobis distance \nIn this section, we attempt to gain some insights into the geometric shape of the Gaussian pdf in multiple dimensions. To do this, we will consider the shape of the level sets of constant (log) probability. \nThe log probability at a specific point $pmb { y }$ is given by \nThe dependence on $pmb { y }$ can be expressed in terms of the Mahalanobis distance $Delta$ between $pmb { y }$ and $pmb { mu }$ , whose square is defined as follows: \nThus contours of constant (log) probability are equivalent to contours of constant Mahalanobis distance. \nTo gain insight into the contours of constant Mahalanobis distance, we exploit the fact that $pmb { Sigma }$ , and hence $pmb { Lambda } = pmb { Sigma } ^ { - 1 }$ , are both positive definite matrices (by assumption). Consider the following eigendecomposition (Section 7.4) of $pmb { Sigma }$ : \nWe can similarly write \nLet us define $z _ { d } triangleq pmb { u } _ { d } ^ { intercal } ( pmb { y } - pmb { mu } )$ , so ${ pmb z } = { bf U } ( { pmb y } - { pmb mu } )$ . Then we can rewrite the Mahalanobis distance as follows: \nAs we discuss in Section 7.4.4, this means we can interpret the Mahalanobis distance as Euclidean distance in a new coordinate frame $mathscr { z }$ in which we rotate $pmb { y }$ by $mathbf { U }$ and scale by $pmb { Lambda }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The multivariate Gaussian (normal) distribution",
        "subsubsection": "Definition"
    },
    {
        "content": "3.2.2 Mahalanobis distance \nIn this section, we attempt to gain some insights into the geometric shape of the Gaussian pdf in multiple dimensions. To do this, we will consider the shape of the level sets of constant (log) probability. \nThe log probability at a specific point $pmb { y }$ is given by \nThe dependence on $pmb { y }$ can be expressed in terms of the Mahalanobis distance $Delta$ between $pmb { y }$ and $pmb { mu }$ , whose square is defined as follows: \nThus contours of constant (log) probability are equivalent to contours of constant Mahalanobis distance. \nTo gain insight into the contours of constant Mahalanobis distance, we exploit the fact that $pmb { Sigma }$ , and hence $pmb { Lambda } = pmb { Sigma } ^ { - 1 }$ , are both positive definite matrices (by assumption). Consider the following eigendecomposition (Section 7.4) of $pmb { Sigma }$ : \nWe can similarly write \nLet us define $z _ { d } triangleq pmb { u } _ { d } ^ { intercal } ( pmb { y } - pmb { mu } )$ , so ${ pmb z } = { bf U } ( { pmb y } - { pmb mu } )$ . Then we can rewrite the Mahalanobis distance as follows: \nAs we discuss in Section 7.4.4, this means we can interpret the Mahalanobis distance as Euclidean distance in a new coordinate frame $mathscr { z }$ in which we rotate $pmb { y }$ by $mathbf { U }$ and scale by $pmb { Lambda }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFor example, in 2d, let us consider the set of points $( z _ { 1 } , z _ { 2 } )$ that satisfy this equation: \nSince these points have the same Mahalanobis distance, they correspond to points of equal probability. Hence we see that the contours of equal probability density of a 2d Gaussian lie along ellipses. This is illustrated in Figure 7.6. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elongated it is. \n3.2.3 Marginals and conditionals of an MVN * \nSuppose $pmb { y } = ( pmb { y } _ { 1 } , pmb { y } _ { 2 } )$ is jointly Gaussian with parameters \nwhere $pmb { Lambda }$ is the precision matrix. Then the marginals are given by \nand the posterior conditional is given by \nThese equations are of such crucial importance in this book that we have put a box around them, so you can easily find them later. For the derivation of these results (which relies on computing the Schur complement ${ boldsymbol Sigma } / { boldsymbol Sigma } _ { 2 2 } = { boldsymbol Sigma } _ { 1 1 } - { boldsymbol Sigma } _ { 1 2 } { boldsymbol Sigma } _ { 2 2 } ^ { - 1 } { boldsymbol Sigma } _ { 2 1 } )$ , see Section 7.3.5. \nWe see that both the marginal and conditional distributions are themselves Gaussian. For the marginals, we just extract the rows and columns corresponding to $mathbf { boldsymbol { mathsf { y } } } _ { 1 }$ or $mathbf { nabla } mathbf { pmb { y } } _ { 2 }$ . For the conditional, we have to do a bit more work. However, it is not that complicated: the conditional mean is just a linear function of $mathbf { mathcal { { y } } } _ { 2 }$ , and the conditional covariance is just a constant matrix that is independent of $mathbf { mathcal { { y } } } _ { 2 }$ . We give three different (but equivalent) expressions for the posterior mean, and two different (but equivalent) expressions for the posterior covariance; each one is useful in different circumstances. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The multivariate Gaussian (normal) distribution",
        "subsubsection": "Mahalanobis distance"
    },
    {
        "content": "For example, in 2d, let us consider the set of points $( z _ { 1 } , z _ { 2 } )$ that satisfy this equation: \nSince these points have the same Mahalanobis distance, they correspond to points of equal probability. Hence we see that the contours of equal probability density of a 2d Gaussian lie along ellipses. This is illustrated in Figure 7.6. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elongated it is. \n3.2.3 Marginals and conditionals of an MVN * \nSuppose $pmb { y } = ( pmb { y } _ { 1 } , pmb { y } _ { 2 } )$ is jointly Gaussian with parameters \nwhere $pmb { Lambda }$ is the precision matrix. Then the marginals are given by \nand the posterior conditional is given by \nThese equations are of such crucial importance in this book that we have put a box around them, so you can easily find them later. For the derivation of these results (which relies on computing the Schur complement ${ boldsymbol Sigma } / { boldsymbol Sigma } _ { 2 2 } = { boldsymbol Sigma } _ { 1 1 } - { boldsymbol Sigma } _ { 1 2 } { boldsymbol Sigma } _ { 2 2 } ^ { - 1 } { boldsymbol Sigma } _ { 2 1 } )$ , see Section 7.3.5. \nWe see that both the marginal and conditional distributions are themselves Gaussian. For the marginals, we just extract the rows and columns corresponding to $mathbf { boldsymbol { mathsf { y } } } _ { 1 }$ or $mathbf { nabla } mathbf { pmb { y } } _ { 2 }$ . For the conditional, we have to do a bit more work. However, it is not that complicated: the conditional mean is just a linear function of $mathbf { mathcal { { y } } } _ { 2 }$ , and the conditional covariance is just a constant matrix that is independent of $mathbf { mathcal { { y } } } _ { 2 }$ . We give three different (but equivalent) expressions for the posterior mean, and two different (but equivalent) expressions for the posterior covariance; each one is useful in different circumstances. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n3.2.4 Example: conditioning a 2d Gaussian \nLet us consider a 2d example. The covariance matrix is \nThe marginal $p ( y _ { 1 } )$ is a 1D Gaussian, obtained by projecting the joint distribution onto the $y _ { 1 }$ line: \nSuppose we observe $Y _ { 2 } = y _ { 2 }$ ; the conditional $p ( y _ { 1 } | y _ { 2 } )$ is obtained by “slicing” the joint distribution through the $Y _ { 2 } = y _ { 2 }$ line: \nIf $sigma _ { 1 } = sigma _ { 2 } = sigma$ , we get \nFor example, suppose $rho = 0 . 8$ , $sigma _ { 1 } = sigma _ { 2 } = 1$ , $mu _ { 1 } = mu _ { 2 } = 0$ , and $y _ { 2 } = 1$ . We see that $mathbb { E } left[ y _ { 1 } | y _ { 2 } = 1 right] =$ 0.8, which makes sense, since $rho = 0 . 8$ means that we believe that if $y _ { 2 }$ increases by 1 (beyond its mean), then $y _ { 1 }$ increases by 0.8. We also see $mathbb { V } left[ y _ { 1 } | y _ { 2 } = 1 right] = 1 - 0 . 8 ^ { 2 } = 0 . 3 6$ . This also makes sense: our uncertainty about $y _ { 1 }$ has gone down, since we have learned something about $y _ { 1 }$ (indirectly) by observing $y _ { 2 }$ . If $rho = 0$ , we get $p ( y _ { 1 } | y _ { 2 } ) = mathcal { N } left( y _ { 1 } | mu _ { 1 } ,  sigma _ { 1 } ^ { 2 } right)$ , since $y _ { 2 }$ conveys no information about $y _ { 1 }$ if they are uncorrelated (and hence independent). \n3.2.5 Example: Imputing missing values * \nAs an example application of the above results, suppose we observe some parts (dimensions) of $textbf {  { y } }$ , with the remaining parts being missing or unobserved. We can exploit the correlation amongst the dimensions (encoded by the covariance matrix) to infer the missing entries; this is called missing value imputation. \nFigure 3.7 shows a simple example. We sampled $N$ vectors from a $D = 1 0$ -dimensional Gaussian, and then deliberately “hid” $5 0 %$ of the data in each sample (row). We then inferred the missing entries given the observed entries and the true model parameters.1 More precisely, for each row $n$ of the data matrix, we compute $p ( pmb { y } _ { n , h } | pmb { y } _ { n , v } , pmb theta )$ , where $mathbf { nabla } _ { mathbf { v } }$ are the indices of the visible entries in that row, $^ { h }$ are the remaining indices of the hidden entries, and $pmb theta = ( pmb mu , pmb Sigma )$ . From this, we compute the marginal distribution of each missing variable $i in boldsymbol { h }$ , $p ( y _ { n , i } | pmb { y } _ { n , pmb { v } } , pmb { theta } )$ . From the marginal, we compute the posterior mean, $bar { y } _ { n , i } = mathbb { E } left[ y _ { n , i } vert y _ { n , v } , pmb { theta } right]$ . \nThe posterior mean represents our “best guess” about the true value of that entry, in the sense that it minimizes our expected squared error, as explained in Chapter 5. We can use $mathbb { V } left[ y _ { n , i } | y _ { n , v } , pmb theta right]$ as a measure of confidence in this guess, although this is not shown. Alternatively, we could draw multiple posterior samples from $p ( pmb { y } _ { n , h } | pmb { y } _ { n , v } , pmb theta )$ ; this is called multiple imputation, and provides a more robust estimate to downstream algorithms that consume the “filled in” data.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The multivariate Gaussian (normal) distribution",
        "subsubsection": "Marginals and conditionals of an MVN *"
    },
    {
        "content": "3.2.4 Example: conditioning a 2d Gaussian \nLet us consider a 2d example. The covariance matrix is \nThe marginal $p ( y _ { 1 } )$ is a 1D Gaussian, obtained by projecting the joint distribution onto the $y _ { 1 }$ line: \nSuppose we observe $Y _ { 2 } = y _ { 2 }$ ; the conditional $p ( y _ { 1 } | y _ { 2 } )$ is obtained by “slicing” the joint distribution through the $Y _ { 2 } = y _ { 2 }$ line: \nIf $sigma _ { 1 } = sigma _ { 2 } = sigma$ , we get \nFor example, suppose $rho = 0 . 8$ , $sigma _ { 1 } = sigma _ { 2 } = 1$ , $mu _ { 1 } = mu _ { 2 } = 0$ , and $y _ { 2 } = 1$ . We see that $mathbb { E } left[ y _ { 1 } | y _ { 2 } = 1 right] =$ 0.8, which makes sense, since $rho = 0 . 8$ means that we believe that if $y _ { 2 }$ increases by 1 (beyond its mean), then $y _ { 1 }$ increases by 0.8. We also see $mathbb { V } left[ y _ { 1 } | y _ { 2 } = 1 right] = 1 - 0 . 8 ^ { 2 } = 0 . 3 6$ . This also makes sense: our uncertainty about $y _ { 1 }$ has gone down, since we have learned something about $y _ { 1 }$ (indirectly) by observing $y _ { 2 }$ . If $rho = 0$ , we get $p ( y _ { 1 } | y _ { 2 } ) = mathcal { N } left( y _ { 1 } | mu _ { 1 } ,  sigma _ { 1 } ^ { 2 } right)$ , since $y _ { 2 }$ conveys no information about $y _ { 1 }$ if they are uncorrelated (and hence independent). \n3.2.5 Example: Imputing missing values * \nAs an example application of the above results, suppose we observe some parts (dimensions) of $textbf {  { y } }$ , with the remaining parts being missing or unobserved. We can exploit the correlation amongst the dimensions (encoded by the covariance matrix) to infer the missing entries; this is called missing value imputation. \nFigure 3.7 shows a simple example. We sampled $N$ vectors from a $D = 1 0$ -dimensional Gaussian, and then deliberately “hid” $5 0 %$ of the data in each sample (row). We then inferred the missing entries given the observed entries and the true model parameters.1 More precisely, for each row $n$ of the data matrix, we compute $p ( pmb { y } _ { n , h } | pmb { y } _ { n , v } , pmb theta )$ , where $mathbf { nabla } _ { mathbf { v } }$ are the indices of the visible entries in that row, $^ { h }$ are the remaining indices of the hidden entries, and $pmb theta = ( pmb mu , pmb Sigma )$ . From this, we compute the marginal distribution of each missing variable $i in boldsymbol { h }$ , $p ( y _ { n , i } | pmb { y } _ { n , pmb { v } } , pmb { theta } )$ . From the marginal, we compute the posterior mean, $bar { y } _ { n , i } = mathbb { E } left[ y _ { n , i } vert y _ { n , v } , pmb { theta } right]$ . \nThe posterior mean represents our “best guess” about the true value of that entry, in the sense that it minimizes our expected squared error, as explained in Chapter 5. We can use $mathbb { V } left[ y _ { n , i } | y _ { n , v } , pmb theta right]$ as a measure of confidence in this guess, although this is not shown. Alternatively, we could draw multiple posterior samples from $p ( pmb { y } _ { n , h } | pmb { y } _ { n , v } , pmb theta )$ ; this is called multiple imputation, and provides a more robust estimate to downstream algorithms that consume the “filled in” data.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The multivariate Gaussian (normal) distribution",
        "subsubsection": "Example: conditioning a 2d Gaussian"
    },
    {
        "content": "3.2.4 Example: conditioning a 2d Gaussian \nLet us consider a 2d example. The covariance matrix is \nThe marginal $p ( y _ { 1 } )$ is a 1D Gaussian, obtained by projecting the joint distribution onto the $y _ { 1 }$ line: \nSuppose we observe $Y _ { 2 } = y _ { 2 }$ ; the conditional $p ( y _ { 1 } | y _ { 2 } )$ is obtained by “slicing” the joint distribution through the $Y _ { 2 } = y _ { 2 }$ line: \nIf $sigma _ { 1 } = sigma _ { 2 } = sigma$ , we get \nFor example, suppose $rho = 0 . 8$ , $sigma _ { 1 } = sigma _ { 2 } = 1$ , $mu _ { 1 } = mu _ { 2 } = 0$ , and $y _ { 2 } = 1$ . We see that $mathbb { E } left[ y _ { 1 } | y _ { 2 } = 1 right] =$ 0.8, which makes sense, since $rho = 0 . 8$ means that we believe that if $y _ { 2 }$ increases by 1 (beyond its mean), then $y _ { 1 }$ increases by 0.8. We also see $mathbb { V } left[ y _ { 1 } | y _ { 2 } = 1 right] = 1 - 0 . 8 ^ { 2 } = 0 . 3 6$ . This also makes sense: our uncertainty about $y _ { 1 }$ has gone down, since we have learned something about $y _ { 1 }$ (indirectly) by observing $y _ { 2 }$ . If $rho = 0$ , we get $p ( y _ { 1 } | y _ { 2 } ) = mathcal { N } left( y _ { 1 } | mu _ { 1 } ,  sigma _ { 1 } ^ { 2 } right)$ , since $y _ { 2 }$ conveys no information about $y _ { 1 }$ if they are uncorrelated (and hence independent). \n3.2.5 Example: Imputing missing values * \nAs an example application of the above results, suppose we observe some parts (dimensions) of $textbf {  { y } }$ , with the remaining parts being missing or unobserved. We can exploit the correlation amongst the dimensions (encoded by the covariance matrix) to infer the missing entries; this is called missing value imputation. \nFigure 3.7 shows a simple example. We sampled $N$ vectors from a $D = 1 0$ -dimensional Gaussian, and then deliberately “hid” $5 0 %$ of the data in each sample (row). We then inferred the missing entries given the observed entries and the true model parameters.1 More precisely, for each row $n$ of the data matrix, we compute $p ( pmb { y } _ { n , h } | pmb { y } _ { n , v } , pmb theta )$ , where $mathbf { nabla } _ { mathbf { v } }$ are the indices of the visible entries in that row, $^ { h }$ are the remaining indices of the hidden entries, and $pmb theta = ( pmb mu , pmb Sigma )$ . From this, we compute the marginal distribution of each missing variable $i in boldsymbol { h }$ , $p ( y _ { n , i } | pmb { y } _ { n , pmb { v } } , pmb { theta } )$ . From the marginal, we compute the posterior mean, $bar { y } _ { n , i } = mathbb { E } left[ y _ { n , i } vert y _ { n , v } , pmb { theta } right]$ . \nThe posterior mean represents our “best guess” about the true value of that entry, in the sense that it minimizes our expected squared error, as explained in Chapter 5. We can use $mathbb { V } left[ y _ { n , i } | y _ { n , v } , pmb theta right]$ as a measure of confidence in this guess, although this is not shown. Alternatively, we could draw multiple posterior samples from $p ( pmb { y } _ { n , h } | pmb { y } _ { n , v } , pmb theta )$ ; this is called multiple imputation, and provides a more robust estimate to downstream algorithms that consume the “filled in” data. \n3.3 Linear Gaussian systems * \nIn Section 3.2.3, we conditioned on noise-free observations to infer the posterior over the hidden parts of a Gaussian random vector. In this section, we extend this approach to handle noisy observations. Let $z in mathbb { R } ^ { L }$ be an unknown vector of values, and $pmb { y } in mathbb { R } ^ { D }$ be some noisy measurement of $_ { z }$ . We assume these variables are related by the following joint distribution: \nwhere $mathbf { W }$ is a matrix of size $D times L$ . This is an example of a linear Gaussian system. \nThe corresponding joint distribution, $p ( z , y ) = p ( z ) p ( y | z )$ , is a $L + D$ dimensional Gaussian, with mean and covariance given by \nBy applying the Gaussian conditioning formula in Equation (3.28) to the joint $p ( { pmb y } , z )$ we can compute the posterior $p ( boldsymbol { z } | boldsymbol { y } )$ , as we explain below. This can be interpreted as inverting the $z  y$ arrow in the generative model from latents to observations. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The multivariate Gaussian (normal) distribution",
        "subsubsection": "Example: Imputing missing values *"
    },
    {
        "content": "3.3.1 Bayes rule for Gaussians \nThe posterior over the latent is given by \nThis is known as Bayes rule for Gaussians. Furthermore, the normalization constant of the posterior is given by \nWe see that the Gaussian prior $p ( z )$ , combined with the Gaussian likelihood $p ( pmb { y } | pmb { z } )$ , results in a Gaussian posterior $p ( boldsymbol { z } | boldsymbol { y } )$ . Thus Gaussians are closed under Bayesian conditioning. To describe this more generally, we say that the Gaussian prior is a conjugate prior for the Gaussian likelihood, since the posterior distribution has the same type as the prior. We discuss the notion of conjugate priors in more detail in Section 4.6.1. \nIn the sections below, we give various applications of this result. But first, we give the derivation. \n3.3.2 Derivation * \nWe now derive Equation 3.37. The basic idea is to derive the joint distribution, $p ( z , y ) = p ( z ) p ( y | z )$ , and then to use the results from Section 3.2.3 for computing $p ( boldsymbol { z } | boldsymbol { y } )$ . \nIn more detail, we proceed as follows. The log of the joint distribution is as follows (dropping irrelevant constants): \nThis is clearly a joint Gaussian distribution, since it is the exponential of a quadratic form. \nExpanding out the quadratic terms involving $mathscr { z }$ and $textbf {  { y } }$ , and ignoring linear and constant terms, we have \nwhere the precision matrix of the joint is defined as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Linear Gaussian systems *",
        "subsubsection": "Bayes rule for Gaussians"
    },
    {
        "content": "3.3.1 Bayes rule for Gaussians \nThe posterior over the latent is given by \nThis is known as Bayes rule for Gaussians. Furthermore, the normalization constant of the posterior is given by \nWe see that the Gaussian prior $p ( z )$ , combined with the Gaussian likelihood $p ( pmb { y } | pmb { z } )$ , results in a Gaussian posterior $p ( boldsymbol { z } | boldsymbol { y } )$ . Thus Gaussians are closed under Bayesian conditioning. To describe this more generally, we say that the Gaussian prior is a conjugate prior for the Gaussian likelihood, since the posterior distribution has the same type as the prior. We discuss the notion of conjugate priors in more detail in Section 4.6.1. \nIn the sections below, we give various applications of this result. But first, we give the derivation. \n3.3.2 Derivation * \nWe now derive Equation 3.37. The basic idea is to derive the joint distribution, $p ( z , y ) = p ( z ) p ( y | z )$ , and then to use the results from Section 3.2.3 for computing $p ( boldsymbol { z } | boldsymbol { y } )$ . \nIn more detail, we proceed as follows. The log of the joint distribution is as follows (dropping irrelevant constants): \nThis is clearly a joint Gaussian distribution, since it is the exponential of a quadratic form. \nExpanding out the quadratic terms involving $mathscr { z }$ and $textbf {  { y } }$ , and ignoring linear and constant terms, we have \nwhere the precision matrix of the joint is defined as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFrom Equation 3.28, and using the fact that $pmb { mu } _ { y } = mathbf { W } pmb { mu } _ { z } + pmb { b }$ , we have \n3.3.3 Example: Inferring an unknown scalar \nSuppose we make $N$ noisy measurements of some underlying quantity ; let us assume the $y _ { i }$ $z$ measurement noise has fixed precision $lambda _ { y } = 1 / sigma ^ { 2 }$ , so the likelihood is \nNow let us use a Gaussian prior for the value of the unknown source: \nWe want to compute $p ( z | y _ { 1 } , dots , y _ { N } , sigma ^ { 2 } )$ . We can convert this to a form that lets us apply Bayes rule for Gaussians by defining $pmb { y } = ( y _ { 1 } , dots y _ { N } )$ , ${ bf W } = { bf 1 } _ { N }$ (an $N times 1$ column vector of $^ { 1 }$ ’s), and $pmb { Sigma } _ { y } ^ { - 1 } = mathrm { d i a g } ( lambda _ { y } mathbf { I } )$ . Then we get \nThese equations are quite intuitive: the posterior precision $lambda _ { N }$ is the prior precision $lambda _ { 0 }$ plus $N$ units of measurement precision $lambda _ { y }$ . Also, the posterior mean $mu _ { N }$ is a convex combination of the MLE $overline { y }$ and the prior mean $mu _ { 0 }$ . This makes it clear that the posterior mean is a compromise between the MLE and the prior. If the prior is weak relative to the signal strength ( $lambda _ { 0 }$ is small relative to $lambda _ { y }$ ), we put more weight on the MLE. If the prior is strong relative to the signal strength ( $lambda _ { 0 }$ is large relative to $lambda _ { y }$ ), we put more weight on the prior. This is illustrated in Figure 3.8. \nNote that the posterior mean is written in terms of $N lambda _ { y } overline { { y } }$ , so having $N$ measurements each of precision $lambda _ { y }$ is like having one measurement with value $overline { y }$ and precision $N lambda _ { y }$ . \nWe can rewrite the results in terms of the posterior variance, rather than posterior precision, as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Linear Gaussian systems *",
        "subsubsection": "Derivation *"
    },
    {
        "content": "From Equation 3.28, and using the fact that $pmb { mu } _ { y } = mathbf { W } pmb { mu } _ { z } + pmb { b }$ , we have \n3.3.3 Example: Inferring an unknown scalar \nSuppose we make $N$ noisy measurements of some underlying quantity ; let us assume the $y _ { i }$ $z$ measurement noise has fixed precision $lambda _ { y } = 1 / sigma ^ { 2 }$ , so the likelihood is \nNow let us use a Gaussian prior for the value of the unknown source: \nWe want to compute $p ( z | y _ { 1 } , dots , y _ { N } , sigma ^ { 2 } )$ . We can convert this to a form that lets us apply Bayes rule for Gaussians by defining $pmb { y } = ( y _ { 1 } , dots y _ { N } )$ , ${ bf W } = { bf 1 } _ { N }$ (an $N times 1$ column vector of $^ { 1 }$ ’s), and $pmb { Sigma } _ { y } ^ { - 1 } = mathrm { d i a g } ( lambda _ { y } mathbf { I } )$ . Then we get \nThese equations are quite intuitive: the posterior precision $lambda _ { N }$ is the prior precision $lambda _ { 0 }$ plus $N$ units of measurement precision $lambda _ { y }$ . Also, the posterior mean $mu _ { N }$ is a convex combination of the MLE $overline { y }$ and the prior mean $mu _ { 0 }$ . This makes it clear that the posterior mean is a compromise between the MLE and the prior. If the prior is weak relative to the signal strength ( $lambda _ { 0 }$ is small relative to $lambda _ { y }$ ), we put more weight on the MLE. If the prior is strong relative to the signal strength ( $lambda _ { 0 }$ is large relative to $lambda _ { y }$ ), we put more weight on the prior. This is illustrated in Figure 3.8. \nNote that the posterior mean is written in terms of $N lambda _ { y } overline { { y } }$ , so having $N$ measurements each of precision $lambda _ { y }$ is like having one measurement with value $overline { y }$ and precision $N lambda _ { y }$ . \nWe can rewrite the results in terms of the posterior variance, rather than posterior precision, as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $tau _ { 0 } ^ { 2 } = 1 / lambda _ { 0 }$ is the prior variance and $tau _ { N } ^ { 2 } = 1 / lambda _ { N }$ is the posterior variance. \nWe can also compute the posterior sequentially, by updating after each observation. If $N = 1$ , we can rewrite the posterior after seeing a single observation as follows (where we define $Sigma _ { y } = sigma ^ { 2 }$ , $Sigma _ { 0 } = tau _ { 0 } ^ { 2 }$ and $Sigma _ { 1 } = tau _ { 1 } ^ { 2 }$ to be the variances of the likelihood, prior and posterior): \nWe can rewrite the posterior mean in 3 different ways: \nThe first equation is a convex combination of the prior and the data. The second equation is the prior mean adjusted towards the data. The third equation is the data adjusted towards the prior mean; this is called shrinkage. These are all equivalent ways of expressing the tradeoff between likelihood and prior. If $Sigma _ { 0 }$ is small relative to $Sigma _ { y }$ , corresponding to a strong prior, the amount of shrinkage is large (see Figure 3.8(a)), whereas if $Sigma _ { 0 }$ is large relative to $Sigma _ { y }$ , corresponding to a weak prior, the amount of shrinkage is small (see Figure 3.8(b)). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nAnother way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio, which is defined as follows: \nwhere $z sim mathcal { N } ( mu _ { 0 } , Sigma _ { 0 } )$ is the true signal, $y = z + epsilon$ is the observed signal, and $epsilon sim mathcal { N } ( 0 , Sigma _ { y } )$ is the noise term. \n3.3.4 Example: inferring an unknown vector \nSuppose we have an unknown quantity of interest, $z in mathbb { R } ^ { D }$ , which we endow with a Gaussian prior, $p ( z ) = mathcal { N } ( pmb { mu } _ { z } , pmb { Sigma } _ { z } )$ . If we “know nothing” about $mathscr { z }$ a priori, we can set $Sigma _ { z } = mathrm { infty } mathbf { I }$ , which means we are completely uncertain about what the value of $_ { z }$ should be. (In practice, we can use a large but finite value for the covariance.) By symmetry, it seems reasonable to set $pmb { mu } _ { z } = mathbf { 0 }$ . \nNow suppose we make $N$ noisy but independent measurements of $boldsymbol { z }$ , $pmb { y } _ { n } sim mathcal { N } ( z , pmb { Sigma } _ { y } )$ , each of size $D$ . We can represent the likelihood as follows: \nNote that we can replace the $N$ observations with their average, $overline { { { y } } }$ , provided we scale down the covariance by $1 / N$ to compensate. Setting $mathbf { W } = mathbf { I }$ , $mathbf { { boldsymbol { b } } = 0 }$ , we can then use Bayes rule for Gaussian to compute the posterior over $boldsymbol { z }$ : \nwhere $hat { pmb { mu } }$ and $hat { Sigma }$ are the parameters of the posterior. \nFigure 3.9 gives a 2d example. We can think of $boldsymbol { z }$ as representing the true, but unknown, location of an object in 2d space, such as a missile or airplane, and the ${ bf { y } } _ { n }$ as being noisy observations, such as radar “blips”. As we receive more blips, we are better able to localize the source. (In the sequel to this book, [Mur23], we discuss the Kalman filter algorithm, which extends this idea to a temporal sequence of observations.) \nThe posterior uncertainty about each component of $boldsymbol { z }$ location vector depends on how reliable the sensor is in each of these dimensions. In the above example, the measurement noise in dimension 1 is higher than in dimension 2, so we have more posterior uncertainty about $z _ { 1 }$ (horizontal axis) than about $z _ { 2 }$ (vertical axis). \n3.3.5 Example: sensor fusion \nIn this section, we extend Section 3.3.4, to the case where we have multiple measurements, coming from different sensors, each with different reliabilities. That is, the model has the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $M$ is the number of sensors (measurement devices), and $N _ { m }$ is the number of observations from sensor $m$ , and $pmb { y } = pmb { y } _ { 1 : N , 1 : M } in mathbb { R } ^ { K }$ . Our goal is to combine the evidence together, to compute $p ( boldsymbol { z } | boldsymbol { y } )$ . This is known as sensor fusion.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Linear Gaussian systems *",
        "subsubsection": "Example: Inferring an unknown scalar"
    },
    {
        "content": "Another way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio, which is defined as follows: \nwhere $z sim mathcal { N } ( mu _ { 0 } , Sigma _ { 0 } )$ is the true signal, $y = z + epsilon$ is the observed signal, and $epsilon sim mathcal { N } ( 0 , Sigma _ { y } )$ is the noise term. \n3.3.4 Example: inferring an unknown vector \nSuppose we have an unknown quantity of interest, $z in mathbb { R } ^ { D }$ , which we endow with a Gaussian prior, $p ( z ) = mathcal { N } ( pmb { mu } _ { z } , pmb { Sigma } _ { z } )$ . If we “know nothing” about $mathscr { z }$ a priori, we can set $Sigma _ { z } = mathrm { infty } mathbf { I }$ , which means we are completely uncertain about what the value of $_ { z }$ should be. (In practice, we can use a large but finite value for the covariance.) By symmetry, it seems reasonable to set $pmb { mu } _ { z } = mathbf { 0 }$ . \nNow suppose we make $N$ noisy but independent measurements of $boldsymbol { z }$ , $pmb { y } _ { n } sim mathcal { N } ( z , pmb { Sigma } _ { y } )$ , each of size $D$ . We can represent the likelihood as follows: \nNote that we can replace the $N$ observations with their average, $overline { { { y } } }$ , provided we scale down the covariance by $1 / N$ to compensate. Setting $mathbf { W } = mathbf { I }$ , $mathbf { { boldsymbol { b } } = 0 }$ , we can then use Bayes rule for Gaussian to compute the posterior over $boldsymbol { z }$ : \nwhere $hat { pmb { mu } }$ and $hat { Sigma }$ are the parameters of the posterior. \nFigure 3.9 gives a 2d example. We can think of $boldsymbol { z }$ as representing the true, but unknown, location of an object in 2d space, such as a missile or airplane, and the ${ bf { y } } _ { n }$ as being noisy observations, such as radar “blips”. As we receive more blips, we are better able to localize the source. (In the sequel to this book, [Mur23], we discuss the Kalman filter algorithm, which extends this idea to a temporal sequence of observations.) \nThe posterior uncertainty about each component of $boldsymbol { z }$ location vector depends on how reliable the sensor is in each of these dimensions. In the above example, the measurement noise in dimension 1 is higher than in dimension 2, so we have more posterior uncertainty about $z _ { 1 }$ (horizontal axis) than about $z _ { 2 }$ (vertical axis). \n3.3.5 Example: sensor fusion \nIn this section, we extend Section 3.3.4, to the case where we have multiple measurements, coming from different sensors, each with different reliabilities. That is, the model has the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $M$ is the number of sensors (measurement devices), and $N _ { m }$ is the number of observations from sensor $m$ , and $pmb { y } = pmb { y } _ { 1 : N , 1 : M } in mathbb { R } ^ { K }$ . Our goal is to combine the evidence together, to compute $p ( boldsymbol { z } | boldsymbol { y } )$ . This is known as sensor fusion.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Linear Gaussian systems *",
        "subsubsection": "Example: inferring an unknown vector"
    },
    {
        "content": "Another way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio, which is defined as follows: \nwhere $z sim mathcal { N } ( mu _ { 0 } , Sigma _ { 0 } )$ is the true signal, $y = z + epsilon$ is the observed signal, and $epsilon sim mathcal { N } ( 0 , Sigma _ { y } )$ is the noise term. \n3.3.4 Example: inferring an unknown vector \nSuppose we have an unknown quantity of interest, $z in mathbb { R } ^ { D }$ , which we endow with a Gaussian prior, $p ( z ) = mathcal { N } ( pmb { mu } _ { z } , pmb { Sigma } _ { z } )$ . If we “know nothing” about $mathscr { z }$ a priori, we can set $Sigma _ { z } = mathrm { infty } mathbf { I }$ , which means we are completely uncertain about what the value of $_ { z }$ should be. (In practice, we can use a large but finite value for the covariance.) By symmetry, it seems reasonable to set $pmb { mu } _ { z } = mathbf { 0 }$ . \nNow suppose we make $N$ noisy but independent measurements of $boldsymbol { z }$ , $pmb { y } _ { n } sim mathcal { N } ( z , pmb { Sigma } _ { y } )$ , each of size $D$ . We can represent the likelihood as follows: \nNote that we can replace the $N$ observations with their average, $overline { { { y } } }$ , provided we scale down the covariance by $1 / N$ to compensate. Setting $mathbf { W } = mathbf { I }$ , $mathbf { { boldsymbol { b } } = 0 }$ , we can then use Bayes rule for Gaussian to compute the posterior over $boldsymbol { z }$ : \nwhere $hat { pmb { mu } }$ and $hat { Sigma }$ are the parameters of the posterior. \nFigure 3.9 gives a 2d example. We can think of $boldsymbol { z }$ as representing the true, but unknown, location of an object in 2d space, such as a missile or airplane, and the ${ bf { y } } _ { n }$ as being noisy observations, such as radar “blips”. As we receive more blips, we are better able to localize the source. (In the sequel to this book, [Mur23], we discuss the Kalman filter algorithm, which extends this idea to a temporal sequence of observations.) \nThe posterior uncertainty about each component of $boldsymbol { z }$ location vector depends on how reliable the sensor is in each of these dimensions. In the above example, the measurement noise in dimension 1 is higher than in dimension 2, so we have more posterior uncertainty about $z _ { 1 }$ (horizontal axis) than about $z _ { 2 }$ (vertical axis). \n3.3.5 Example: sensor fusion \nIn this section, we extend Section 3.3.4, to the case where we have multiple measurements, coming from different sensors, each with different reliabilities. That is, the model has the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $M$ is the number of sensors (measurement devices), and $N _ { m }$ is the number of observations from sensor $m$ , and $pmb { y } = pmb { y } _ { 1 : N , 1 : M } in mathbb { R } ^ { K }$ . Our goal is to combine the evidence together, to compute $p ( boldsymbol { z } | boldsymbol { y } )$ . This is known as sensor fusion. \n\nWe now give a simple example, where there are just two sensors, so $pmb { y } _ { 1 } sim mathcal { N } ( z , pmb { Sigma } _ { 1 } )$ and $y _ { 2 } sim$ $mathcal { N } ( z , pmb { Sigma } _ { 2 } )$ . Pictorially, we can represent this example as $y _ { 1 } left. z right. y _ { 2 }$ . We can combine $mathbf { boldsymbol { mathsf { y } } } _ { 1 }$ and $mathbf { boldsymbol { mathsf { y } } } _ { 2 }$ into a single vector $pmb { y }$ , so the model can be represented as $z  [ { pmb y } _ { 1 } , { pmb y } _ { 2 } ]$ , where $p ( pmb { y } | boldsymbol { z } ) = mathcal { N } ( pmb { y } | mathbf { W } boldsymbol { z } , pmb { Sigma } _ { y } )$ , where $mathbf { W } = [ mathbf { I } ; mathbf { I } ]$ and $Sigma _ { y } = [ Sigma _ { 1 } , mathbf { 0 } ; mathbf { 0 } , Sigma _ { 2 } ]$ are block-structured matrices. We can then apply Bayes’ rule for Gaussians to compute $p ( boldsymbol { z } | boldsymbol { y } )$ . \nFigure 3.10(a) gives a 2d example, where we set $Sigma _ { 1 } = Sigma _ { 2 } = 0 . 0 1 mathbf { I } _ { 2 }$ , so both sensors are equally reliable. In this case, the posterior mean is halfway between the two observations, $mathbf { boldsymbol { mathsf { y } } } _ { 1 }$ and $mathbf { boldsymbol { mathsf { y } } } _ { 2 }$ . In Figure 3.10(b), we set $pmb { Sigma } _ { 1 } = 0 . 0 5 mathbf { I } _ { 2 }$ and $pmb { Sigma } _ { 2 } = 0 . 0 1 mathbf { I } _ { 2 }$ , so sensor 2 is more reliable than sensor 1. In this case, the posterior mean is closer to $mathbf { boldsymbol { mathsf { y } } } _ { 2 }$ . In Figure 3.10(c), we set \nso sensor $1$ is more reliable in the second component (vertical direction), and sensor 2 is more reliable in the first component (horizontal direction). In this case, the posterior mean uses ${ bf { y } } _ { 1 }$ ’s vertical component and $mathbf { boldsymbol { mathsf { y } } } _ { 2 }$ ’s horizontal component. \n3.4 The exponential family * \nIn this section, we define the exponential family, which includes many common probability distributions. The exponential family plays a crucial role in statistics and machine learning. In this book, we mainly use it in the context of generalized linear models, which we discuss in Chapter 12. We will see more applications of the exponential family in the sequel to this book, [Mur23]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Linear Gaussian systems *",
        "subsubsection": "Example: sensor fusion"
    },
    {
        "content": "3.4.1 Definition \nConsider a family of probability distributions parameterized by $pmb { eta } in mathbb { R } ^ { K }$ with fixed support over $mathcal { V } ^ { D } subseteq mathbb { R } ^ { D }$ . We say that the distribution $p ( pmb { y } | pmb { eta } )$ is in the exponential family if its density can be written in the following way: \nwhere $h ( boldsymbol { y } )$ is a scaling constant (also known as the base measure, often 1), $mathcal { T } ( pmb { y } ) in mathbb { R } ^ { K }$ are the sufficient statistics, $eta$ are the natural parameters or canonical parameters, $Z ( eta )$ is a normalization constant known as the partition function, and $A ( pmb { eta } ) = log Z ( pmb { eta } )$ is the log partition function. One can show that $A$ is a convex function over the concave set $Omega triangleq { pmb { eta } in mathbb { R } ^ { K } : A ( pmb { eta } ) < infty }$ . \nIt is convenient if the natural parameters are independent of each other. Formally, we say that an exponential family is minimal if there is no $pmb { eta } in mathbb { R } ^ { K } setminus { 0 }$ such that $eta ^ { mathsf { T } } mathcal { T } ( pmb { y } ) = 0$ . This last condition can be violated in the case of multinomial distributions, because of the sum to one constraint on the parameters; however, it is easy to reparameterize the distribution using $K - 1$ independent parameters, as we show below. \nEquation (3.71) can be generalized by defining $eta = f ( phi )$ , where $phi$ is some other, possibly smaller, set of parameters. In this case, the distribution has the form \nIf the mapping from $phi$ to $eta$ is nonlinear, we call this a curved exponential family. If $pmb { eta } = f ( phi ) = phi$ , the model is said to be in canonical form. If, in addition, $boldsymbol { mathcal { T } } ( boldsymbol { mathsf { pmb { y } } } ) = boldsymbol { mathsf { pmb { y } } }$ , we say this is a natural exponential family or NEF. In this case, it can be written as \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n3.4.2 Example \nAs a simple example, let us consider the Bernoulli distribution. We can write this in exponential family form as follows: \nwhere $mathcal { T } ( y ) = left[ mathbb { I } left( y = 1 right) , mathbb { I } left( y = 0 right) right]$ , $pmb { eta } = [ mathrm { l o g } ( mu ) , mathrm { l o g } ( 1 - mu ) ]$ , and $mu$ is the mean parameter. However, this is an over-complete representation since there is a linear dependence between the features. We can see this as follows: \nIf the representation is overcomplete, $eta$ is not uniquely identifiable. It is common to use a minimal representation, which means there is a unique $eta$ associated with the distribution. In this case, we can just define \nWe can put this into exponential family form by defining \nWe can recover the mean parameter $mu$ from the canonical parameter $eta$ using \nwhich we recognize as the logistic (sigmoid) function. \nSee the sequel to this book, [Mur23], for more examples. \n3.4.3 Log partition function is cumulant generating function \nThe first and second cumulants of a distribution are its mean $mathbb { E } left[ Y right]$ and variance $mathbb { V } left[ Y right]$ , whereas the first and second moments are $mathbb { E } left[ Y right]$ and $mathbb { E } leftlfloor Y ^ { 2 } rightrfloor$ . We can also compute higher order cumulants (and moments). An important property of the exponential family is that derivatives of the log partition function can be used to generate all the cumulants of the sufficient statistics. In particular, the first and second cumulants are given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The exponential family *",
        "subsubsection": "Definition"
    },
    {
        "content": "3.4.2 Example \nAs a simple example, let us consider the Bernoulli distribution. We can write this in exponential family form as follows: \nwhere $mathcal { T } ( y ) = left[ mathbb { I } left( y = 1 right) , mathbb { I } left( y = 0 right) right]$ , $pmb { eta } = [ mathrm { l o g } ( mu ) , mathrm { l o g } ( 1 - mu ) ]$ , and $mu$ is the mean parameter. However, this is an over-complete representation since there is a linear dependence between the features. We can see this as follows: \nIf the representation is overcomplete, $eta$ is not uniquely identifiable. It is common to use a minimal representation, which means there is a unique $eta$ associated with the distribution. In this case, we can just define \nWe can put this into exponential family form by defining \nWe can recover the mean parameter $mu$ from the canonical parameter $eta$ using \nwhich we recognize as the logistic (sigmoid) function. \nSee the sequel to this book, [Mur23], for more examples. \n3.4.3 Log partition function is cumulant generating function \nThe first and second cumulants of a distribution are its mean $mathbb { E } left[ Y right]$ and variance $mathbb { V } left[ Y right]$ , whereas the first and second moments are $mathbb { E } left[ Y right]$ and $mathbb { E } leftlfloor Y ^ { 2 } rightrfloor$ . We can also compute higher order cumulants (and moments). An important property of the exponential family is that derivatives of the log partition function can be used to generate all the cumulants of the sufficient statistics. In particular, the first and second cumulants are given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The exponential family *",
        "subsubsection": "Example"
    },
    {
        "content": "3.4.2 Example \nAs a simple example, let us consider the Bernoulli distribution. We can write this in exponential family form as follows: \nwhere $mathcal { T } ( y ) = left[ mathbb { I } left( y = 1 right) , mathbb { I } left( y = 0 right) right]$ , $pmb { eta } = [ mathrm { l o g } ( mu ) , mathrm { l o g } ( 1 - mu ) ]$ , and $mu$ is the mean parameter. However, this is an over-complete representation since there is a linear dependence between the features. We can see this as follows: \nIf the representation is overcomplete, $eta$ is not uniquely identifiable. It is common to use a minimal representation, which means there is a unique $eta$ associated with the distribution. In this case, we can just define \nWe can put this into exponential family form by defining \nWe can recover the mean parameter $mu$ from the canonical parameter $eta$ using \nwhich we recognize as the logistic (sigmoid) function. \nSee the sequel to this book, [Mur23], for more examples. \n3.4.3 Log partition function is cumulant generating function \nThe first and second cumulants of a distribution are its mean $mathbb { E } left[ Y right]$ and variance $mathbb { V } left[ Y right]$ , whereas the first and second moments are $mathbb { E } left[ Y right]$ and $mathbb { E } leftlfloor Y ^ { 2 } rightrfloor$ . We can also compute higher order cumulants (and moments). An important property of the exponential family is that derivatives of the log partition function can be used to generate all the cumulants of the sufficient statistics. In particular, the first and second cumulants are given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFrom the above result, we see that the Hessian is positive definite, and hence $A ( pmb { eta } )$ is convex in $eta$ . Since the log likelihood has the form $log p ( { pmb y } | { pmb eta } ) = { pmb eta } ^ { 1 } T ( { pmb y } ) - A ( { pmb eta } ) + mathrm { c o n s t }$ , we see that this is concave, and hence the MLE has a unique global maximum. \n3.4.4 Maximum entropy derivation of the exponential family \nSuppose we want to find a distribution $p ( { pmb x } )$ to describe some data, where all we know are the expected values ( $F _ { k }$ ) of certain features or functions $f _ { k } ( { pmb x } )$ : \nFor example, $f _ { 1 }$ might compute $x$ , $f _ { 2 }$ might compute $x ^ { 2 }$ , making $F _ { 1 }$ the empirical mean and $F _ { 2 }$ the empirical second moment. Our prior belief in the distribution is $q ( x )$ . \nTo formalize what we mean by “least number of assumptions”, we will search for the distribution that is as close as possible to our prior $q ( { pmb x } )$ , in the sense of KL divergence (Section 6.2), while satisfying our constraints: \nIf we use a uniform prior, $q ( pmb { x } ) propto 1$ , minimizing the KL divergence is equivalent to maximizing the entropy (Section 6.1): \nThe result is called a maximum entropy model. \nTo minimize the KL subject to the constraints in Equation (3.86), and the constraint that $p ( { pmb x } ) geq 0$ and $begin{array} { r } { sum _ { pmb { x } } p ( pmb { x } ) = 1 } end{array}$ , we will use Lagrange multipliers (see Section 8.5.1). The Lagrangian is given by \nWe can use the calculus of variations to take derivatives wrt the function $p$ , but we will adopt a simpler approach and treat $mathbf { nabla } _ { mathbf { p } }$ as a fixed length vector (since we are assuming that $_ { x }$ is discrete). Then we have \nSetting ∂p ∂J = 0 for each c yields \nwhere we have defined $Z triangleq e ^ { 1 + lambda _ { 0 } }$ . Using the sum-to-one constraint, we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The exponential family *",
        "subsubsection": "Log partition function is cumulant generating function"
    },
    {
        "content": "From the above result, we see that the Hessian is positive definite, and hence $A ( pmb { eta } )$ is convex in $eta$ . Since the log likelihood has the form $log p ( { pmb y } | { pmb eta } ) = { pmb eta } ^ { 1 } T ( { pmb y } ) - A ( { pmb eta } ) + mathrm { c o n s t }$ , we see that this is concave, and hence the MLE has a unique global maximum. \n3.4.4 Maximum entropy derivation of the exponential family \nSuppose we want to find a distribution $p ( { pmb x } )$ to describe some data, where all we know are the expected values ( $F _ { k }$ ) of certain features or functions $f _ { k } ( { pmb x } )$ : \nFor example, $f _ { 1 }$ might compute $x$ , $f _ { 2 }$ might compute $x ^ { 2 }$ , making $F _ { 1 }$ the empirical mean and $F _ { 2 }$ the empirical second moment. Our prior belief in the distribution is $q ( x )$ . \nTo formalize what we mean by “least number of assumptions”, we will search for the distribution that is as close as possible to our prior $q ( { pmb x } )$ , in the sense of KL divergence (Section 6.2), while satisfying our constraints: \nIf we use a uniform prior, $q ( pmb { x } ) propto 1$ , minimizing the KL divergence is equivalent to maximizing the entropy (Section 6.1): \nThe result is called a maximum entropy model. \nTo minimize the KL subject to the constraints in Equation (3.86), and the constraint that $p ( { pmb x } ) geq 0$ and $begin{array} { r } { sum _ { pmb { x } } p ( pmb { x } ) = 1 } end{array}$ , we will use Lagrange multipliers (see Section 8.5.1). The Lagrangian is given by \nWe can use the calculus of variations to take derivatives wrt the function $p$ , but we will adopt a simpler approach and treat $mathbf { nabla } _ { mathbf { p } }$ as a fixed length vector (since we are assuming that $_ { x }$ is discrete). Then we have \nSetting ∂p ∂J = 0 for each c yields \nwhere we have defined $Z triangleq e ^ { 1 + lambda _ { 0 } }$ . Using the sum-to-one constraint, we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHence the normalization constant is given by \nThis has exactly the form of the exponential family, where ${ pmb f } ( { pmb x } )$ is the vector of sufficient statistics, $- lambda$ are the natural parameters, and $q ( { pmb x } )$ is our base measure. \nFor example, if the features are $f _ { 1 } ( x ) = x$ and $f _ { 2 } ( x ) = x ^ { 2 }$ , and we want to match the first and second moments, we get the Gaussian disribution. \n3.5 Mixture models \nOne way to create more complex probability models is to take a convex combination of simple distributions. This is called a mixture model. This has the form \nwhere $p _ { k }$ is the $k$ ’th mixture component, and $pi _ { k }$ are the mixture weights which satisfy $0 leq pi _ { k } leq 1$ and $textstyle sum _ { k = 1 } ^ { K } pi _ { k } = 1$ . \nWe can re-express this model as a hierarchical model, in which we introduce the discrete latent variable $z in { 1 , ldots , K }$ , which specifies which distribution to use for generating the output $textbf {  { y } }$ . The prior on this latent variable is $p ( z = k | pmb theta ) = pi _ { k }$ , and the conditional is $p ( pmb { y } | boldsymbol { z } = k , pmb { theta } ) = p _ { k } ( pmb { y } ) = p ( pmb { y } | pmb { theta } _ { k } )$ . That is, we define the following joint model: \nwhere $pmb theta = ( pi _ { 1 } , ldots , pi _ { K } , pmb theta _ { 1 } , ldots , pmb theta _ { K } )$ are all the model parameters. The “generative story” for the data is that we first sample a specific component $z$ , and then we generate the observations $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ using the parameters chosen according to the value of $z$ . By marginalizing out $z$ , we recover Equation (3.94): \nWe can create different kinds of mixture model by varying the base distribution $p _ { k }$ , as we illustrate below. \n3.5.1 Gaussian mixture models \nA Gaussian mixture model or GMM, also called a mixture of Gaussians (MoG), is defined as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "The exponential family *",
        "subsubsection": "Maximum entropy derivation of the exponential family"
    },
    {
        "content": "Hence the normalization constant is given by \nThis has exactly the form of the exponential family, where ${ pmb f } ( { pmb x } )$ is the vector of sufficient statistics, $- lambda$ are the natural parameters, and $q ( { pmb x } )$ is our base measure. \nFor example, if the features are $f _ { 1 } ( x ) = x$ and $f _ { 2 } ( x ) = x ^ { 2 }$ , and we want to match the first and second moments, we get the Gaussian disribution. \n3.5 Mixture models \nOne way to create more complex probability models is to take a convex combination of simple distributions. This is called a mixture model. This has the form \nwhere $p _ { k }$ is the $k$ ’th mixture component, and $pi _ { k }$ are the mixture weights which satisfy $0 leq pi _ { k } leq 1$ and $textstyle sum _ { k = 1 } ^ { K } pi _ { k } = 1$ . \nWe can re-express this model as a hierarchical model, in which we introduce the discrete latent variable $z in { 1 , ldots , K }$ , which specifies which distribution to use for generating the output $textbf {  { y } }$ . The prior on this latent variable is $p ( z = k | pmb theta ) = pi _ { k }$ , and the conditional is $p ( pmb { y } | boldsymbol { z } = k , pmb { theta } ) = p _ { k } ( pmb { y } ) = p ( pmb { y } | pmb { theta } _ { k } )$ . That is, we define the following joint model: \nwhere $pmb theta = ( pi _ { 1 } , ldots , pi _ { K } , pmb theta _ { 1 } , ldots , pmb theta _ { K } )$ are all the model parameters. The “generative story” for the data is that we first sample a specific component $z$ , and then we generate the observations $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ using the parameters chosen according to the value of $z$ . By marginalizing out $z$ , we recover Equation (3.94): \nWe can create different kinds of mixture model by varying the base distribution $p _ { k }$ , as we illustrate below. \n3.5.1 Gaussian mixture models \nA Gaussian mixture model or GMM, also called a mixture of Gaussians (MoG), is defined as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIn Figure 3.11 we show the density defined by a mixture of 3 Gaussians in 2d. Each mixture component is represented by a different set of elliptical contours. If we let the number of mixture components grow sufficiently large, a GMM can approximate any smooth distribution over $mathbb { R } ^ { D }$ . \nGMMs are often used for unsupervised clustering of real-valued data samples $pmb { y } _ { n } in mathbb { R } ^ { D }$ . This works in two stages. First we fit the model e.g., by computing the MLE $hat { pmb { theta } } = mathrm { a r g m a x } log p ( mathcal { D } | pmb { theta } )$ , where $mathcal { D } = { pmb { y } _ { n } : n = 1 : N }$ . (We discuss how to compute this MLE in Section 8.7.3.) Then we associate each data point ${ bf { nabla } } mathbf { pmb { y } } _ { n }$ with a discrete latent or hidden variable $z _ { n } in { 1 , ldots , K }$ which specifies the identity of the mixture component or cluster which was used to generate ${ bf { y } } _ { n }$ . These latent identities are unknown, but we can compute a posterior over them using Bayes rule: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThe quantity $r _ { n k }$ is called the responsibility of cluster $k$ for data point $n$ . Given the responsibilities, we can compute the most probable cluster assignment as follows: \nThis is known as hard clustering. (If we use the responsibilities to fractionally assign each data point to different clusters, it is called soft clustering.) See Figure 3.12 for an example. \nIf we have a uniform prior over $z _ { n }$ , and we use spherical Gaussians with $boldsymbol { Sigma } _ { k } = mathbf { I }$ , the hard clustering problem reduces to \nIn other words, we assign each data point to its closest centroid, as measured by Euclidean distance.   \nThis is the basis of the $mathbf { K }$ -means clustering algorithm, which we discuss in Section 21.3. \n3.5.2 Bernoulli mixture models \nIf the data is binary valued, we can use a Bernoulli mixture model or BMM (also called a mixture of Bernoullis), where each mixture component has the following form: \nHere $mu _ { d k }$ is the probability that bit $d$ turns on in cluster $k$ . \nAs an example, we fit a BMM using $K = 2 0$ components to the MNIST dataset (Section 3.5.2). (We use the EM algorithm to do this fitting, which is similar to EM for GMMs discussed in Section 8.7.3; however we can also use SGD to fit the model, which is more efficient for large datasets.2 ) The resulting parameters for each mixture component (i.e., $pmb { mu } _ { k }$ and $pi _ { k }$ ) are shown in Figure 3.13. We see that the model has “discovered” a representation of each type of digit. (Some digits are represented multiple times, since the model does not know the “true” number of classes. See Section 21.3.7 for more information on how to choose the number $K$ of mixture components.)",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Mixture models",
        "subsubsection": "Gaussian mixture models"
    },
    {
        "content": "The quantity $r _ { n k }$ is called the responsibility of cluster $k$ for data point $n$ . Given the responsibilities, we can compute the most probable cluster assignment as follows: \nThis is known as hard clustering. (If we use the responsibilities to fractionally assign each data point to different clusters, it is called soft clustering.) See Figure 3.12 for an example. \nIf we have a uniform prior over $z _ { n }$ , and we use spherical Gaussians with $boldsymbol { Sigma } _ { k } = mathbf { I }$ , the hard clustering problem reduces to \nIn other words, we assign each data point to its closest centroid, as measured by Euclidean distance.   \nThis is the basis of the $mathbf { K }$ -means clustering algorithm, which we discuss in Section 21.3. \n3.5.2 Bernoulli mixture models \nIf the data is binary valued, we can use a Bernoulli mixture model or BMM (also called a mixture of Bernoullis), where each mixture component has the following form: \nHere $mu _ { d k }$ is the probability that bit $d$ turns on in cluster $k$ . \nAs an example, we fit a BMM using $K = 2 0$ components to the MNIST dataset (Section 3.5.2). (We use the EM algorithm to do this fitting, which is similar to EM for GMMs discussed in Section 8.7.3; however we can also use SGD to fit the model, which is more efficient for large datasets.2 ) The resulting parameters for each mixture component (i.e., $pmb { mu } _ { k }$ and $pi _ { k }$ ) are shown in Figure 3.13. We see that the model has “discovered” a representation of each type of digit. (Some digits are represented multiple times, since the model does not know the “true” number of classes. See Section 21.3.7 for more information on how to choose the number $K$ of mixture components.) \n\n3.6 Probabilistic graphical models * \nI basically know of two principles for treating complicated systems in simple ways: the first is the principle of modularity and the second is the principle of abstraction. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. — Michael Jordan, 1997 (quoted in [Fre98]). \nWe have now introduced a few simple probabilistic building blocks. In Section 3.3, we showed one way to combine some Gaussian building blocks to build a high dimensional distribution $p ( pmb { y } )$ from simpler parts, namely the marginal $p ( pmb { y } _ { 1 } )$ and the conditional $p ( pmb { y } _ { 2 } | pmb { y } _ { 1 } )$ . This idea can be extended to define joint distributions over sets of many random variables. The key assumption we will make is that some variables are conditionally independent of others. We will represent our CI assumptions using graphs, as we briefly explain below. (See the sequel to this book, [Mur23], for more information.) \n3.6.1 Representation \nA probabilistic graphical model or PGM is a joint probability distribution that uses a graph structure to encode conditional independence assumptions. When the graph is a directed acyclic \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 graph or DAG, the model is sometimes called a Bayesian network, although there is nothing inherently Bayesian about such models.",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Mixture models",
        "subsubsection": "Bernoulli mixture models"
    },
    {
        "content": "3.6 Probabilistic graphical models * \nI basically know of two principles for treating complicated systems in simple ways: the first is the principle of modularity and the second is the principle of abstraction. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. — Michael Jordan, 1997 (quoted in [Fre98]). \nWe have now introduced a few simple probabilistic building blocks. In Section 3.3, we showed one way to combine some Gaussian building blocks to build a high dimensional distribution $p ( pmb { y } )$ from simpler parts, namely the marginal $p ( pmb { y } _ { 1 } )$ and the conditional $p ( pmb { y } _ { 2 } | pmb { y } _ { 1 } )$ . This idea can be extended to define joint distributions over sets of many random variables. The key assumption we will make is that some variables are conditionally independent of others. We will represent our CI assumptions using graphs, as we briefly explain below. (See the sequel to this book, [Mur23], for more information.) \n3.6.1 Representation \nA probabilistic graphical model or PGM is a joint probability distribution that uses a graph structure to encode conditional independence assumptions. When the graph is a directed acyclic \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 graph or DAG, the model is sometimes called a Bayesian network, although there is nothing inherently Bayesian about such models. \n\nThe basic idea in PGMs is that each node in the graph represents a random variable, and each edge represents a direct dependency. More precisely, each lack of edge represents a conditional independency. In the DAG case, we can number the nodes in topological order (parents before children), and then we connect them such that each node is conditionally independent of all its predecessors given its parents: \nwhere $mathrm { p a } ( i )$ are the parents of node $i$ , and $mathrm { p r e d } ( i )$ are the predecessors of node $i$ in the ordering. (This is called the ordered Markov property.) Consequently, we can represent the joint distribution as follows: \nwhere $N _ { G }$ is the number of nodes in the graph. \n3.6.1.1 Example: water sprinkler network \nSuppose we want to model the dependencies between 4 random variables: $C$ (whether it is cloudy season or not), $R$ (whether it is raining or not), $S$ (whether the water sprinkler is on or not), and $W$ (whether the grass is wet or not). We know that the cloudy season makes rain more likely, so we add a $C  R$ arc. We know that the cloudy season makes turning on a water sprinkler less likely, so we add a $C  S$ arc. Finally, we know that either rain or sprinklers can cause the grass to get wet, so we add $S  W$ and $R to W$ edges. \nFormally, this defines the following joint distribution: \nwhere we strike through terms that are not needed due to the conditional independence properties of the model. \nEach term $p ( Y _ { i } | mathbf { Y _ { p a } } ( i ) )$ is a called the conditional probability distribution or CPD for node $i$ . This can be any kind of distribution we like. In Figure 3.14, we assume each CPD is a conditional categorical distribution, which can be represented as a conditional probability table or CPT. We can represent the $i$ ’th CPT as follows: \nThis satisfies the properties $0 leq theta _ { i j k } leq 1$ and $begin{array} { r } { sum _ { k = 1 } ^ { K _ { i } } theta _ { i j k } = 1 } end{array}$ for each row $j$ . Here $i$ indexes nodes, $i in [ N _ { G } ]$ ; $k$ indexes node states, $k in [ K _ { i } ]$ , where $K _ { i }$ is the number of states for node $i$ ; and $j$ indexes joint parent states, $j in [ J _ { i } ]$ , where $begin{array} { r } { J _ { i } = prod _ { p in mathrm { p a } ( i ) } K _ { p } } end{array}$ . For example, the wet grass node has 2 binary parents, so there are 4 parent states. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n3.6.1.2 Example: Markov chain \nSuppose we want to create a joint probability distribution over variable-length sequences, $p ( boldsymbol { y } _ { 1 : T } )$ . If each variable $y _ { t }$ represents a word from a vocabulary with $K$ possible values, so $y _ { t } in { 1 , ldots , K }$ , the resulting model represents a distribution over possible sentences of length $T$ ; this is often called a language model. \nBy the chain rule of probability, we can represent any joint distribution over $T$ variables as follows: \nUnfortunately, the number of parameters needed to represent each conditional distribution $p ( y _ { t } | y _ { 1 : t - 1 } )$ grows exponentially with $t$ . However, suppose we make the conditional independence assumption that the future, $mathbf { mathscr { y } } _ { t + 1 : T }$ , is independent of the past, $mathbf { delta } _ { mathbf { delta } } mathbf { cdot } mathbf { delta } _ { t - 1 }$ , given the present, $y _ { t }$ . This is called the first order Markov condition, and is repesented by the PGM in Figure 3.15(a). With this assumption, we can write the joint distribution as follows: \nThis is called a Markov chain, Markov model or autoregressive model of order 1. \nThe function $p ( y _ { t } | y _ { t - 1 } )$ is called the transition function, transition kernel or Markov kernel. This is just a conditional distribution over the states at time $t$ given the state at time $t - 1$ , and hence it satisfies the conditions $p ( y _ { t } | y _ { t - 1 } ) ge 0$ and $begin{array} { r } { sum _ { k = 1 } ^ { K } p ( y _ { t } = k | y _ { t - 1 } = j ) = 1 } end{array}$ . We can represent this CPT as a stochastic matrix, $A _ { j k } = p ( y _ { t } = k | y _ { t - 1 } = j $ ), where each row sums to 1. This is known as the state transition matrix. We assume this matrix is the same for all time steps, so the model is said to be homogeneous, stationary, or time-invariant. This is an example of parameter tying, since the same parameter is shared by multiple variables. This assumption allows us to model an arbitrary number of variables using a fixed number of parameters. \nThe first-order Markov assumption is rather strong. Fortunately, we can easily generalize first-order models to depend on the last $M$ observations, thus creating a model of order (memory length) $M$ : \nThis is called an $M ^ { prime } { bf t h }$ order Markov model. For example, if $M = 2$ , $y _ { t }$ depends on $y _ { t - 1 }$ and $_ { y _ { t - 2 } }$ , as shown in Figure 3.15(b). This is called a trigram model, since it models the distribution \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 over word triples. If we use $M = 1$ , we get a bigram model, which models the distribution over word pairs. \n\nFor large vocabulary sizes, the number of parameters needed to estimate the conditional distributions for $M$ -gram models for large $M$ can become prohibitive. In this case, we need to make additional assumptions beyond conditional independence. For example, we can assume that $p big ( y _ { t } | y _ { t - M : t - 1 } big )$ can be represented as a low-rank matrix, or in terms of some kind of neural network. This is called a neural language model. See Chapter 15 for details. \n3.6.2 Inference \nA PGM defines a joint probability distribution. We can therefore use the rules of marginalization and conditioning to compute $p ( mathbf { Y } _ { i } | mathbf { Y } _ { j } = pmb { y } _ { j } )$ for any sets of variables $i$ and $j$ . Efficient algorithms to perform this computation are discussed in the sequel to this book, [Mur23]. \nFor example, consider the water sprinkler example in Figure 3.14. Our prior belief that it has rained is given by $p ( R = 1 ) = 0 . 5$ . If we see that the grass is wet, then our posterior belief that it has rained changes to $p ( R = 1 | W = 1 ) = 0 . 7 0 7 9$ . Now suppose we also notice the water sprinkler was turned on: our belief that it rained goes down to $p ( R = 1 | W = 1 , S = 1 ) = 0 . 3 2 0 4$ . This negative mutual interaction between multiple causes of some observations is called the explaining away effect, also known as Berkson’s paradox. (See sprinkler_pgm.ipynb for some code that reproduces these calculations.) \n3.6.3 Learning \nIf the parameters of the CPDs are unknown, we can view them as additional random variables, add them as nodes to the graph, and then treat them as hidden variables to be inferred. Figure 3.16(a) shows a simple example, in which we have $N$ iid random variables, ${ bf { y } } _ { n }$ , all drawn from the same distribution with common parameter $pmb theta$ . (The shaded nodes represent observed values, whereas the unshaded (hollow) nodes represent latent variables or parameters.) \nMore precisely, the model encodes the following “generative story” about the data: \nwhere $p ( pmb theta )$ is some (unspecified) prior over the parameters, and $p ( pmb { y } | pmb { theta } )$ is some specified likelihood function. The corresponding joint distribution has the form \nwhere $mathcal { D } = ( pmb { y } _ { 1 } , dots , pmb { y } _ { N } )$ . By virtue of the iid assumption, the likelihood can be rewritten as follows: \nNotice that the order of the data vectors is not important for defining this model, i.e., we can permute the numbering of the leaf nodes in the PGM. When this property holds, we say that the data is exchangeable. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Probabilistic graphical models *",
        "subsubsection": "Representation"
    },
    {
        "content": "For large vocabulary sizes, the number of parameters needed to estimate the conditional distributions for $M$ -gram models for large $M$ can become prohibitive. In this case, we need to make additional assumptions beyond conditional independence. For example, we can assume that $p big ( y _ { t } | y _ { t - M : t - 1 } big )$ can be represented as a low-rank matrix, or in terms of some kind of neural network. This is called a neural language model. See Chapter 15 for details. \n3.6.2 Inference \nA PGM defines a joint probability distribution. We can therefore use the rules of marginalization and conditioning to compute $p ( mathbf { Y } _ { i } | mathbf { Y } _ { j } = pmb { y } _ { j } )$ for any sets of variables $i$ and $j$ . Efficient algorithms to perform this computation are discussed in the sequel to this book, [Mur23]. \nFor example, consider the water sprinkler example in Figure 3.14. Our prior belief that it has rained is given by $p ( R = 1 ) = 0 . 5$ . If we see that the grass is wet, then our posterior belief that it has rained changes to $p ( R = 1 | W = 1 ) = 0 . 7 0 7 9$ . Now suppose we also notice the water sprinkler was turned on: our belief that it rained goes down to $p ( R = 1 | W = 1 , S = 1 ) = 0 . 3 2 0 4$ . This negative mutual interaction between multiple causes of some observations is called the explaining away effect, also known as Berkson’s paradox. (See sprinkler_pgm.ipynb for some code that reproduces these calculations.) \n3.6.3 Learning \nIf the parameters of the CPDs are unknown, we can view them as additional random variables, add them as nodes to the graph, and then treat them as hidden variables to be inferred. Figure 3.16(a) shows a simple example, in which we have $N$ iid random variables, ${ bf { y } } _ { n }$ , all drawn from the same distribution with common parameter $pmb theta$ . (The shaded nodes represent observed values, whereas the unshaded (hollow) nodes represent latent variables or parameters.) \nMore precisely, the model encodes the following “generative story” about the data: \nwhere $p ( pmb theta )$ is some (unspecified) prior over the parameters, and $p ( pmb { y } | pmb { theta } )$ is some specified likelihood function. The corresponding joint distribution has the form \nwhere $mathcal { D } = ( pmb { y } _ { 1 } , dots , pmb { y } _ { N } )$ . By virtue of the iid assumption, the likelihood can be rewritten as follows: \nNotice that the order of the data vectors is not important for defining this model, i.e., we can permute the numbering of the leaf nodes in the PGM. When this property holds, we say that the data is exchangeable. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Probabilistic graphical models *",
        "subsubsection": "Inference"
    },
    {
        "content": "For large vocabulary sizes, the number of parameters needed to estimate the conditional distributions for $M$ -gram models for large $M$ can become prohibitive. In this case, we need to make additional assumptions beyond conditional independence. For example, we can assume that $p big ( y _ { t } | y _ { t - M : t - 1 } big )$ can be represented as a low-rank matrix, or in terms of some kind of neural network. This is called a neural language model. See Chapter 15 for details. \n3.6.2 Inference \nA PGM defines a joint probability distribution. We can therefore use the rules of marginalization and conditioning to compute $p ( mathbf { Y } _ { i } | mathbf { Y } _ { j } = pmb { y } _ { j } )$ for any sets of variables $i$ and $j$ . Efficient algorithms to perform this computation are discussed in the sequel to this book, [Mur23]. \nFor example, consider the water sprinkler example in Figure 3.14. Our prior belief that it has rained is given by $p ( R = 1 ) = 0 . 5$ . If we see that the grass is wet, then our posterior belief that it has rained changes to $p ( R = 1 | W = 1 ) = 0 . 7 0 7 9$ . Now suppose we also notice the water sprinkler was turned on: our belief that it rained goes down to $p ( R = 1 | W = 1 , S = 1 ) = 0 . 3 2 0 4$ . This negative mutual interaction between multiple causes of some observations is called the explaining away effect, also known as Berkson’s paradox. (See sprinkler_pgm.ipynb for some code that reproduces these calculations.) \n3.6.3 Learning \nIf the parameters of the CPDs are unknown, we can view them as additional random variables, add them as nodes to the graph, and then treat them as hidden variables to be inferred. Figure 3.16(a) shows a simple example, in which we have $N$ iid random variables, ${ bf { y } } _ { n }$ , all drawn from the same distribution with common parameter $pmb theta$ . (The shaded nodes represent observed values, whereas the unshaded (hollow) nodes represent latent variables or parameters.) \nMore precisely, the model encodes the following “generative story” about the data: \nwhere $p ( pmb theta )$ is some (unspecified) prior over the parameters, and $p ( pmb { y } | pmb { theta } )$ is some specified likelihood function. The corresponding joint distribution has the form \nwhere $mathcal { D } = ( pmb { y } _ { 1 } , dots , pmb { y } _ { N } )$ . By virtue of the iid assumption, the likelihood can be rewritten as follows: \nNotice that the order of the data vectors is not important for defining this model, i.e., we can permute the numbering of the leaf nodes in the PGM. When this property holds, we say that the data is exchangeable. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n3.6.3.1 Plate notation \nIn Figure 3.16(a), we see that the $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ nodes are repeated $N$ times. To avoid visual clutter, it is common to use a form of syntactic sugar called plates. This is a notational convention in which we draw a little box around the repeated variables, with the understanding that nodes within the box will get repeated when the model is unrolled. We often write the number of copies or repetitions in the bottom right corner of the box. This is illustrated in Figure 3.16(b). This notation is widely used to represent certain kinds of Bayesian model. \nFigure 3.17 shows a more interesting example, in which we represent a GMM (Section 3.5.1) as a graphical model. We see that this encodes the joint distribution \nWe see that the latent variables $z _ { n }$ as well as the unknown paramters, $pmb theta = ( pmb pi , pmb mu _ { 1 : K } , pmb Sigma _ { 1 : K } )$ , are all shown as unshaded nodes. \n3.7 Exercises \nExercise 3.1 [Uncorrelated does not imply independent *] \nLet $X sim U ( - 1 , 1 )$ and $Y = X ^ { 2 }$ . Clearly $Y$ is dependent on $X$ (in fact, $Y$ is uniquely determined by $X$ ).   \nHowever, show that $rho ( X , Y ) = 0$ . Hint: if $X sim U ( a , b )$ then $E [ X ] = ( a + b ) / 2$ and $mathbb { V } left[ X right] = ( b - a ) ^ { 2 } / 1 2$ . \nExercise 3.2 [Correlation coefficient is between -1 and $^ +$ 1] Prove that $- 1 le rho ( X , Y ) le 1$ \nExercise 3.3 [Correlation coefficient for linearly related variables is $pm 1$ *]   \nShow that, if $Y = a X + b$ for some parameters $a > 0$ and $b$ , then $rho ( X , Y ) = 1$ . Similarly show that if $a < 0$ , then $rho ( X , Y ) = - 1$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Probabilistic graphical models *",
        "subsubsection": "Learning"
    },
    {
        "content": "3.6.3.1 Plate notation \nIn Figure 3.16(a), we see that the $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ nodes are repeated $N$ times. To avoid visual clutter, it is common to use a form of syntactic sugar called plates. This is a notational convention in which we draw a little box around the repeated variables, with the understanding that nodes within the box will get repeated when the model is unrolled. We often write the number of copies or repetitions in the bottom right corner of the box. This is illustrated in Figure 3.16(b). This notation is widely used to represent certain kinds of Bayesian model. \nFigure 3.17 shows a more interesting example, in which we represent a GMM (Section 3.5.1) as a graphical model. We see that this encodes the joint distribution \nWe see that the latent variables $z _ { n }$ as well as the unknown paramters, $pmb theta = ( pmb pi , pmb mu _ { 1 : K } , pmb Sigma _ { 1 : K } )$ , are all shown as unshaded nodes. \n3.7 Exercises \nExercise 3.1 [Uncorrelated does not imply independent *] \nLet $X sim U ( - 1 , 1 )$ and $Y = X ^ { 2 }$ . Clearly $Y$ is dependent on $X$ (in fact, $Y$ is uniquely determined by $X$ ).   \nHowever, show that $rho ( X , Y ) = 0$ . Hint: if $X sim U ( a , b )$ then $E [ X ] = ( a + b ) / 2$ and $mathbb { V } left[ X right] = ( b - a ) ^ { 2 } / 1 2$ . \nExercise 3.2 [Correlation coefficient is between -1 and $^ +$ 1] Prove that $- 1 le rho ( X , Y ) le 1$ \nExercise 3.3 [Correlation coefficient for linearly related variables is $pm 1$ *]   \nShow that, if $Y = a X + b$ for some parameters $a > 0$ and $b$ , then $rho ( X , Y ) = 1$ . Similarly show that if $a < 0$ , then $rho ( X , Y ) = - 1$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nExercise 3.4 [Linear combinations of random variables] Let $_ { pmb { x } }$ be a random vector with mean $_ { m }$ and covariance matrix $Sigma$ . Let $mathbf { A }$ and $mathbf { B }$ be matrices. \na. Derive the covariance matrix of ${ bf A } x$ .   \nb. Show that $operatorname { t r } ( mathbf { A } mathbf { B } ) = operatorname { t r } ( mathbf { B } mathbf { A } )$ .   \nc. Derive an expression for $mathbb { E } left[ pmb { x } ^ { T } mathbf { A } pmb { x } right]$ . Exercise 3.5 [Gaussian vs jointly Gaussian ]   \nLet $X sim mathcal { N } ( 0 , 1 )$ and $Y = W X$ , where $p ( W = - 1 ) = p ( W = 1 ) = 0 . 5$ . It is clear that $X$ and $Y$ are not independent, since $Y$ is a function of $X$ .   \na. Show $Y sim { mathcal { N } } ( 0 , 1 )$ .   \nb. Show $operatorname { C o v } left[ X , Y right] = 0$ . Thus $X$ and $Y$ are uncorrelated but dependent, even though they are Gaussian. Hint: use the definition of covariance \n\nand the rule of iterated expectation \nExercise 3.6 [Normalization constant for a multidimensional Gaussian] Prove that the normalization constant for a $d$ -dimensional Gaussian is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHint: diagonalize $pmb { Sigma }$ and use the fact that $| Sigma | = prod _ { i atop r } lambda _ { i }$ to write the joint pdf as a product of $d$ one-dimensional Gaussians in a transformed coordinate system. (QYou will need the change of variables formula.) Finally, use the normalization constant for univariate Gaussians. \nExercise 3.7 [Sensor fusion with known variances in 1d] \nSuppose we have two sensors with known (and different) variances $v _ { 1 }$ and $_ { v _ { 2 } }$ , but unknown (and the same) mean $mu$ . Suppose we observe $n _ { 1 }$ observations $y _ { i } ^ { ( 1 ) } sim mathcal { N } ( mu , v _ { 1 } )$ from the first sensor and $n _ { 2 }$ observations $y _ { i } ^ { ( 2 ) } sim mathcal { N } ( mu , v _ { 2 } )$ from the second sensor. (For example, suppose $mu$ is the true temperature outside, and sensor $^ { 1 }$ is a precise (low variance) digital thermosensing device, and sensor 2 is an imprecise (high variance) mercury thermometer.) Let $mathcal { D }$ represent all the data from both sensors. What is the posterior $p ( mu | mathcal { D } )$ , assuming a non-informative prior for $mu$ (which we can simulate using a Gaussian with a precision of $0$ )? Give an explicit expression for the posterior mean and variance. \nExercise 3.8 [Show that the Student distribution can be written as a Gaussian scale mixture] \nShow that a Student distribution can be written as a Gaussian scale mixture, where we use a Gamma mixing distribution on the precision $alpha$ , i.e. \nThis can be viewed as an infinite mixture of Gaussians, with different precisions. \n4 Statistics \n4.1 Introduction \nIn Chapter 2–Chapter 3, we assumed all the parameters $pmb theta$ of our probability models were known. In this chapter, we discuss how to learn these parameters from data. \nThe process of estimating $pmb theta$ from $mathcal { D }$ is called model fitting, or training, and is at the heart of machine learning. There are many methods for producing such estimates, but most boil down to an optimization problem of the form \nwhere ${ mathcal { L } } ( theta )$ is some kind of loss function or objective function. We discuss several different loss functions in this chapter. In some cases, we also discuss how to solve the optimization problem in closed form. In general, however, we will need to use some kind of generic optimization algorithm, which we discuss in Chapter 8. \nIn addition to computing a point estimate, $hat { pmb { theta } }$ , we discuss how to model our uncertainty or confidence in this estimate. In statistics, the process of quantifying uncertainty about an unknown quantity estimated from a finite sample of data is called inference. We will discuss both Bayesian and frequentist approaches to inference.1 \n4.2 Maximum likelihood estimation (MLE) \nThe most common approach to parameter estimation is to pick the parameters that assign the highest probability to the training data; this is called maximum likelihood estimation or MLE. We give more details below, and then give a series of worked examples. \n4.2.1 Definition \nWe define the MLE as follows:",
        "chapter": "I Foundations",
        "section": "Probability: Multivariate Models",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "4 Statistics \n4.1 Introduction \nIn Chapter 2–Chapter 3, we assumed all the parameters $pmb theta$ of our probability models were known. In this chapter, we discuss how to learn these parameters from data. \nThe process of estimating $pmb theta$ from $mathcal { D }$ is called model fitting, or training, and is at the heart of machine learning. There are many methods for producing such estimates, but most boil down to an optimization problem of the form \nwhere ${ mathcal { L } } ( theta )$ is some kind of loss function or objective function. We discuss several different loss functions in this chapter. In some cases, we also discuss how to solve the optimization problem in closed form. In general, however, we will need to use some kind of generic optimization algorithm, which we discuss in Chapter 8. \nIn addition to computing a point estimate, $hat { pmb { theta } }$ , we discuss how to model our uncertainty or confidence in this estimate. In statistics, the process of quantifying uncertainty about an unknown quantity estimated from a finite sample of data is called inference. We will discuss both Bayesian and frequentist approaches to inference.1 \n4.2 Maximum likelihood estimation (MLE) \nThe most common approach to parameter estimation is to pick the parameters that assign the highest probability to the training data; this is called maximum likelihood estimation or MLE. We give more details below, and then give a series of worked examples. \n4.2.1 Definition \nWe define the MLE as follows:",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "4 Statistics \n4.1 Introduction \nIn Chapter 2–Chapter 3, we assumed all the parameters $pmb theta$ of our probability models were known. In this chapter, we discuss how to learn these parameters from data. \nThe process of estimating $pmb theta$ from $mathcal { D }$ is called model fitting, or training, and is at the heart of machine learning. There are many methods for producing such estimates, but most boil down to an optimization problem of the form \nwhere ${ mathcal { L } } ( theta )$ is some kind of loss function or objective function. We discuss several different loss functions in this chapter. In some cases, we also discuss how to solve the optimization problem in closed form. In general, however, we will need to use some kind of generic optimization algorithm, which we discuss in Chapter 8. \nIn addition to computing a point estimate, $hat { pmb { theta } }$ , we discuss how to model our uncertainty or confidence in this estimate. In statistics, the process of quantifying uncertainty about an unknown quantity estimated from a finite sample of data is called inference. We will discuss both Bayesian and frequentist approaches to inference.1 \n4.2 Maximum likelihood estimation (MLE) \nThe most common approach to parameter estimation is to pick the parameters that assign the highest probability to the training data; this is called maximum likelihood estimation or MLE. We give more details below, and then give a series of worked examples. \n4.2.1 Definition \nWe define the MLE as follows: \nWe usually assume the training examples are independently sampled from the same distribution, so the (conditional) likelihood becomes \nThis is known as the iid assumption, which stands for “independent and identically distributed”. We usually work with the log likelihood, which is given by \nThis decomposes into a sum of terms, one per example. Thus the MLE is given by \nSince most optimization algorithms (such as those discussed in Chapter 8) are designed to minimize cost functions, we can redefine the objective function to be the (conditional) negative log likelihood or NLL: \nMinimizing this will give the MLE. If the model is unconditional (unsupervised), the MLE becomes \nsince we have outputs ${ bf { y } } _ { n }$ but no inputs ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . \nAlternatively we may want to maximize the joint likelihood of inputs and outputs. The MLE in this case becomes \n4.2.2 Justification for MLE \nThere are several ways to justify the method of MLE. One way is to view it as simple point approximation to the Bayesian posterior $p ( pmb { theta } | mathcal { D } )$ using a uniform prior, as explained in Section 4.6.7.1.",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Definition"
    },
    {
        "content": "We usually assume the training examples are independently sampled from the same distribution, so the (conditional) likelihood becomes \nThis is known as the iid assumption, which stands for “independent and identically distributed”. We usually work with the log likelihood, which is given by \nThis decomposes into a sum of terms, one per example. Thus the MLE is given by \nSince most optimization algorithms (such as those discussed in Chapter 8) are designed to minimize cost functions, we can redefine the objective function to be the (conditional) negative log likelihood or NLL: \nMinimizing this will give the MLE. If the model is unconditional (unsupervised), the MLE becomes \nsince we have outputs ${ bf { y } } _ { n }$ but no inputs ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . \nAlternatively we may want to maximize the joint likelihood of inputs and outputs. The MLE in this case becomes \n4.2.2 Justification for MLE \nThere are several ways to justify the method of MLE. One way is to view it as simple point approximation to the Bayesian posterior $p ( pmb { theta } | mathcal { D } )$ using a uniform prior, as explained in Section 4.6.7.1. \nIn particular, suppose we approximate the posterior by a delta function, $p ( pmb theta | mathcal { D } ) = delta ( pmb theta - hat { pmb theta } _ { mathrm { m a p } } )$ , where $hat { pmb { theta } } _ { mathrm { m a p } }$ is the posterior mode, given by \nIf we use a uniform prior, $p ( pmb theta ) propto 1$ , the MAP estimate becomes equal to the MLE, $hat { pmb { theta } } _ { mathrm { m a p } } = hat { pmb { theta } } _ { mathrm { m l e } }$ . Another way to justify the use of the MLE is that the resulting predictive distribution $p ( boldsymbol { y } | hat { boldsymbol { theta } } _ { mathrm { m l e } } )$ is as close as possible (in a sense to be defined below) to the empirical distribution of the data. In the unconditional case, the empirical distribution is defined by \nWe see that the empirical distribution is a series of delta functions or “spikes” at the observed training points. We want to create a model whose distribution $q ( y ) = p ( y | pmb { theta } )$ is similar to $p _ { mathcal { D } } ( pmb { y } )$ . \nA standard way to measure the (dis)similarity between probability distributions $p$ and $q$ is the Kullback Leibler divergence, or KL divergence. We give the details in Section 6.2, but in brief this is defined as \nwhere $mathbb { H } left( p right)$ is the entropy of $p$ (see Section 6.1), and $mathbb { H } _ { c e } left( p , q right)$ is the cross-entropy of $p$ and $q$ (see Section 6.1.2). One can show that $D _ { mathbb { K L } } left( p parallel q right) ge 0$ , with equality iff $p = q$ . \nIf we define $q ( pmb { y } ) = p ( pmb { y } | pmb { theta } )$ , and set $p ( pmb { y } ) = p _ { mathcal { D } } ( pmb { y } )$ , then the KL divergence becomes \nThe first term is a constant which we can ignore, leaving just the NLL. Thus minimizing the KL is equivalent to minimizing the NLL which is equivalent to computing the MLE, as in Equation (4.7). We can generalize the above results to the supervised (conditional) setting by using the following empirical distribution: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe expected KL then becomes \nMinimizing this is equivalent to minimizing the conditional NLL in Equation (4.6). \n4.2.3 Example: MLE for the Bernoulli distribution \nSuppose $Y$ is a random variable representing a coin toss, where the event $Y = 1$ corresponds to heads and $Y = 0$ corresponds to tails. Let $theta = p ( Y = 1 )$ be the probability of heads. The probability distribution for this rv is the Bernoulli, which we introduced in Section 2.4. \nThe NLL for the Bernoulli distribution is given by \nwhere we have defined $begin{array} { r } { N _ { 1 } = sum _ { n = 1 } ^ { N _ { D } } mathbb { I } left( y _ { n } = 1 right) } end{array}$ and $begin{array} { r } { N _ { 0 } = sum _ { n = 1 } ^ { N _ { mathcal { D } } } mathbb { I } left( y _ { n } = 0 right) } end{array}$ , representing the number of heads and tails. (The NLL for the binomial is the same as for the Bernoulli, modulo an irrelevant $binom { N } { c }$ term, which is a constant independent of $theta$ .) These two numbers are called the sufficient statistics of the data, since they summarize everything we need to know about $mathcal { D }$ . The total count, $N = N _ { 0 } + N _ { 1 }$ , is called the sample size. \nThe MLE can be found by solving $begin{array} { r } { frac { d } { d theta } mathrm { N L L } ( theta ) = 0 } end{array}$ . The derivative of the NLL is \nand hence the MLE is given by \nWe see that this is just the empirical fraction of heads, which is an intuitive result. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Justification for MLE"
    },
    {
        "content": "The expected KL then becomes \nMinimizing this is equivalent to minimizing the conditional NLL in Equation (4.6). \n4.2.3 Example: MLE for the Bernoulli distribution \nSuppose $Y$ is a random variable representing a coin toss, where the event $Y = 1$ corresponds to heads and $Y = 0$ corresponds to tails. Let $theta = p ( Y = 1 )$ be the probability of heads. The probability distribution for this rv is the Bernoulli, which we introduced in Section 2.4. \nThe NLL for the Bernoulli distribution is given by \nwhere we have defined $begin{array} { r } { N _ { 1 } = sum _ { n = 1 } ^ { N _ { D } } mathbb { I } left( y _ { n } = 1 right) } end{array}$ and $begin{array} { r } { N _ { 0 } = sum _ { n = 1 } ^ { N _ { mathcal { D } } } mathbb { I } left( y _ { n } = 0 right) } end{array}$ , representing the number of heads and tails. (The NLL for the binomial is the same as for the Bernoulli, modulo an irrelevant $binom { N } { c }$ term, which is a constant independent of $theta$ .) These two numbers are called the sufficient statistics of the data, since they summarize everything we need to know about $mathcal { D }$ . The total count, $N = N _ { 0 } + N _ { 1 }$ , is called the sample size. \nThe MLE can be found by solving $begin{array} { r } { frac { d } { d theta } mathrm { N L L } ( theta ) = 0 } end{array}$ . The derivative of the NLL is \nand hence the MLE is given by \nWe see that this is just the empirical fraction of heads, which is an intuitive result. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n4.2.4 Example: MLE for the categorical distribution \nSuppose we roll a $K$ -sided dice $N$ times. Let $Y _ { n } in { 1 , ldots , K }$ be the $boldsymbol { n }$ ’th outcome, where $Y _ { n } sim mathrm { C a t } ( pmb theta )$ . We want to estimate the probabilities $pmb theta$ from the dataset $mathcal { D } = { y _ { n } : n = 1 : N }$ . The NLL is given by \nwhere $N _ { k }$ is the number of times the event $Y = k$ is observed. (The NLL for the multinomial is the same, up to irrelevant scale factors.) \nTo compute the MLE, we have to minimize the NLL subject to the constraint that $textstyle sum _ { k = 1 } ^ { K } theta _ { k } = 1$ To do this, we will use the method of Lagrange multipliers (see Section 8.5.1).3 \nThe Lagrangian is as follows: \nTaking derivatives with respect to $lambda$ yields the original constraint: \nTaking derivatives with respect to $theta _ { k }$ yields \nWe can solve for $lambda$ using the sum-to-one constraint: \nThus the MLE is given by \nwhich is just the empirical fraction of times event $k$ occurs. \n4.2.5 Example: MLE for the univariate Gaussian \nSuppose $Y sim { mathcal { N } } ( mu , sigma ^ { 2 } )$ and let $mathcal { D } = { y _ { n } : n = 1 : N }$ be an iid sample of size $N$ . We can estimate the parameters $pmb theta = ( mu , sigma ^ { 2 } )$ using MLE as follows. First, we derive the NLL, which is given by",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Example: MLE for the Bernoulli distribution"
    },
    {
        "content": "4.2.4 Example: MLE for the categorical distribution \nSuppose we roll a $K$ -sided dice $N$ times. Let $Y _ { n } in { 1 , ldots , K }$ be the $boldsymbol { n }$ ’th outcome, where $Y _ { n } sim mathrm { C a t } ( pmb theta )$ . We want to estimate the probabilities $pmb theta$ from the dataset $mathcal { D } = { y _ { n } : n = 1 : N }$ . The NLL is given by \nwhere $N _ { k }$ is the number of times the event $Y = k$ is observed. (The NLL for the multinomial is the same, up to irrelevant scale factors.) \nTo compute the MLE, we have to minimize the NLL subject to the constraint that $textstyle sum _ { k = 1 } ^ { K } theta _ { k } = 1$ To do this, we will use the method of Lagrange multipliers (see Section 8.5.1).3 \nThe Lagrangian is as follows: \nTaking derivatives with respect to $lambda$ yields the original constraint: \nTaking derivatives with respect to $theta _ { k }$ yields \nWe can solve for $lambda$ using the sum-to-one constraint: \nThus the MLE is given by \nwhich is just the empirical fraction of times event $k$ occurs. \n4.2.5 Example: MLE for the univariate Gaussian \nSuppose $Y sim { mathcal { N } } ( mu , sigma ^ { 2 } )$ and let $mathcal { D } = { y _ { n } : n = 1 : N }$ be an iid sample of size $N$ . We can estimate the parameters $pmb theta = ( mu , sigma ^ { 2 } )$ using MLE as follows. First, we derive the NLL, which is given by",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Example: MLE for the categorical distribution"
    },
    {
        "content": "4.2.4 Example: MLE for the categorical distribution \nSuppose we roll a $K$ -sided dice $N$ times. Let $Y _ { n } in { 1 , ldots , K }$ be the $boldsymbol { n }$ ’th outcome, where $Y _ { n } sim mathrm { C a t } ( pmb theta )$ . We want to estimate the probabilities $pmb theta$ from the dataset $mathcal { D } = { y _ { n } : n = 1 : N }$ . The NLL is given by \nwhere $N _ { k }$ is the number of times the event $Y = k$ is observed. (The NLL for the multinomial is the same, up to irrelevant scale factors.) \nTo compute the MLE, we have to minimize the NLL subject to the constraint that $textstyle sum _ { k = 1 } ^ { K } theta _ { k } = 1$ To do this, we will use the method of Lagrange multipliers (see Section 8.5.1).3 \nThe Lagrangian is as follows: \nTaking derivatives with respect to $lambda$ yields the original constraint: \nTaking derivatives with respect to $theta _ { k }$ yields \nWe can solve for $lambda$ using the sum-to-one constraint: \nThus the MLE is given by \nwhich is just the empirical fraction of times event $k$ occurs. \n4.2.5 Example: MLE for the univariate Gaussian \nSuppose $Y sim { mathcal { N } } ( mu , sigma ^ { 2 } )$ and let $mathcal { D } = { y _ { n } : n = 1 : N }$ be an iid sample of size $N$ . We can estimate the parameters $pmb theta = ( mu , sigma ^ { 2 } )$ using MLE as follows. First, we derive the NLL, which is given by \nThe minimum of this function must satisfy the following conditions, which we explain in Section 8.1.1.1: \nSo all we have to do is to find this stationary point. Some simple calculus (Exercise 4.1) shows that the solution is given by the following: \nThe quantities $y$ and $s ^ { 2 }$ are called the sufficient statistics of the data, since they are sufficient to compute the MLE, without loss of information relative to using the raw data itself. \nNote that you might be used to seeing the estimate for the variance written as \nwhere we divide by $N - 1$ . This is not the MLE, but is a different kind of estimate, which happens to be unbiased (unlike the MLE); see Section 4.7.6.1 for details.4 \n4.2.6 Example: MLE for the multivariate Gaussian \nIn this section, we derive the maximum likelihood estimate for the parameters of a multivariate Gaussian. \nFirst, let us write the log-likelihood, dropping irrelevant constants: \nwhere $pmb { Lambda } = pmb { Sigma } ^ { - 1 }$ is the precision matrix (inverse covariance matrix).",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Example: MLE for the univariate Gaussian"
    },
    {
        "content": "The minimum of this function must satisfy the following conditions, which we explain in Section 8.1.1.1: \nSo all we have to do is to find this stationary point. Some simple calculus (Exercise 4.1) shows that the solution is given by the following: \nThe quantities $y$ and $s ^ { 2 }$ are called the sufficient statistics of the data, since they are sufficient to compute the MLE, without loss of information relative to using the raw data itself. \nNote that you might be used to seeing the estimate for the variance written as \nwhere we divide by $N - 1$ . This is not the MLE, but is a different kind of estimate, which happens to be unbiased (unlike the MLE); see Section 4.7.6.1 for details.4 \n4.2.6 Example: MLE for the multivariate Gaussian \nIn this section, we derive the maximum likelihood estimate for the parameters of a multivariate Gaussian. \nFirst, let us write the log-likelihood, dropping irrelevant constants: \nwhere $pmb { Lambda } = pmb { Sigma } ^ { - 1 }$ is the precision matrix (inverse covariance matrix). \n4.2.6.1 MLE for the mean \nUsing the substitution $z _ { n } = y _ { n } - mu$ , the derivative of a quadratic form (Equation (7.264)) and the chain rule of calculus, we have \nsince ∂znT = I. Hence \nSo the MLE of $pmb { mu }$ is just the empirical mean. \n4.2.6.2 MLE for the covariance matrix \nWe can use the trace trick (Equation (7.36)) to rewrite the log-likelihood in terms of the precision matrix $pmb { Lambda } = pmb { Sigma } ^ { - 1 }$ as follows: \nwhere $mathbf { S } _ { overline { { y } } }$ is the scatter matrix centered on $overline { { { y } } }$ . \nWe can rewrite the scatter matrix in a more compact form as follows: \nwhere \nis the centering matrix, which converts $mathbf { Y }$ to $dot { mathbf { Y } }$ by subtracting the mean $begin{array} { r } { overline { { pmb { y } } } = frac { 1 } { N } mathbf { Y } ^ { 1 } mathbf { 1 } _ { N } } end{array}$ off every row. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nUsing results from Section 7.8, we can compute derivatives of the loss with respect to $pmb { Lambda }$ to get \nThus the MLE for the covariance matrix is the empirical covariance matrix. See Figure 4.1a for an example. \nSometimes it is more convenient to work with the correlation matrix defined in Equation (3.8). This can be computed using \nwhere $mathrm { d i a g } ( Sigma ) ^ { - frac { 1 } { 2 } }$ is a diagonal matrix containing the entries $1 / sigma _ { i }$ . See Figure 4.1b for an example. Note, however, that the MLE may overfit or be numerically unstable, especially when the number of samples $N$ is small compared to the number of dimensions $D$ . The main problem is that $pmb { Sigma }$ has $O ( D ^ { 2 } )$ parameters, so we may need a lot of data to reliably estimate it. In particular, as we see from Equation (4.51), the MLE for a full covariance matrix is singular if $N _ { mathcal { D } } < D$ . And even when $N _ { mathcal { D } } > D$ , the MLE can be ill-conditioned, meaning it is close to singular. We discuss solutions to this problem in Section 4.5.2. \n4.2.7 Example: MLE for linear regression \nWe briefly mentioned linear regression in Section 2.6.3. Recall that it corresponds to the following model: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Example: MLE for the multivariate Gaussian"
    },
    {
        "content": "Using results from Section 7.8, we can compute derivatives of the loss with respect to $pmb { Lambda }$ to get \nThus the MLE for the covariance matrix is the empirical covariance matrix. See Figure 4.1a for an example. \nSometimes it is more convenient to work with the correlation matrix defined in Equation (3.8). This can be computed using \nwhere $mathrm { d i a g } ( Sigma ) ^ { - frac { 1 } { 2 } }$ is a diagonal matrix containing the entries $1 / sigma _ { i }$ . See Figure 4.1b for an example. Note, however, that the MLE may overfit or be numerically unstable, especially when the number of samples $N$ is small compared to the number of dimensions $D$ . The main problem is that $pmb { Sigma }$ has $O ( D ^ { 2 } )$ parameters, so we may need a lot of data to reliably estimate it. In particular, as we see from Equation (4.51), the MLE for a full covariance matrix is singular if $N _ { mathcal { D } } < D$ . And even when $N _ { mathcal { D } } > D$ , the MLE can be ill-conditioned, meaning it is close to singular. We discuss solutions to this problem in Section 4.5.2. \n4.2.7 Example: MLE for linear regression \nWe briefly mentioned linear regression in Section 2.6.3. Recall that it corresponds to the following model: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $pmb theta = ( pmb w , pmb sigma ^ { 2 } )$ . Let us assume for now that $sigma ^ { 2 }$ is fixed, and focus on estimating the weights $mathbf { boldsymbol { w } }$ . The negative log likelihood or NLL is given by \nDropping the irrelevant additive constants gives the following simplified objective, known as the residual sum of squares or RSS: \nwhere $r _ { n }$ the $n$ ’th residual error. Scaling by the number of examples $N$ gives the mean squared error or MSE: \nFinally, taking the square root gives the root mean squared error or RMSE: \nWe can compute the MLE by minimizing the NLL, RSS, MSE or RMSE. All will give the same results, since these objective functions are all the same, up to irrelevant constants Let us focus on the RSS objective. It can be written in matrix notation as follows: \nIn Section 11.2.2.1, we prove that the optimum, which occurs where $nabla _ { mathbf { boldsymbol { w } } } mathrm { R S S } ( mathbf { boldsymbol { w } } ) = mathbf { 0 }$ , satisfies th following equation: \nThis is called the ordinary least squares or OLS estimate, and is equivalent to the MLE. \n4.3 Empirical risk minimization (ERM) \nWe can generalize MLE by replacing the (conditional) log loss term in Equation (4.6), $ell ( { pmb y } _ { n } , { pmb theta } ; { pmb x } _ { n } ) =$ $- log p ( pmb { y } _ { n } | pmb { x } _ { n } , pmb { theta } )$ , with any other loss function, to get \nThis is known as empirical risk minimization or ERM, since it is the expected loss where the expectation is taken wrt the empirical distribution. See Section 5.4 for more details. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Maximum likelihood estimation (MLE)",
        "subsubsection": "Example: MLE for linear regression"
    },
    {
        "content": "4.3.1 Example: minimizing the misclassification rate \nIf we are solving a classification problem, we might want to use 0-1 loss: \nwhere $f ( { pmb x } ; { pmb theta } )$ is some kind of predictor. The empirical risk becomes \nThis is just the empirical misclassification rate on the training set. \nNote that for binary problems, we can rewrite the misclassifcation rate in the following notation. Let $tilde { y } in { - 1 , + 1 }$ be the true label, and $hat { y } in { - 1 , + 1 } = f ( pmb { x } ; pmb { theta } )$ be our prediction. We define the 0-1 loss as follows: \nThe corresponding empirical risk becomes \nwhere the dependence on ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and $pmb theta$ is implicit. \n4.3.2 Surrogate loss \nUnfortunately, the 0-1 loss used in Section 4.3.1 is a non-smooth step function, as shown in Figure 4.2, making it difficult to optimize. (In fact, it is NP-hard [BDEL03].) In this section we consider the use of a surrogate loss function [BJM06]. The surrogate is usually chosen to be a maximally tight convex upper bound, which is then easy to minimize. \nFor example, consider a probabilistic binary classifier, which produces the following distribution over labels: \nwhere $eta = f ( pmb { x } ; pmb { theta } )$ is the log odds. Hence the log loss is given by \nFigure 4.2 shows that this is a smooth upper bound to the 0-1 loss, where we plot the loss vs the quantity $tilde { y } eta$ , known as the margin, since it defines a “margin of safety” away from the threshold value of 0. Thus we see that minimizing the negative log likelihood is equivalent to minimizing a (fairly tight) upper bound on the empirical 0-1 loss. \nAnother convex upper bound to 0-1 loss is the hinge loss, which is defined as follows: \nThis is plotted in Figure 4.2; we see that it has the shape of a partially open door hinge. This is convex upper bound to the 0-1 loss, although it is only piecewise differentiable, not everywhere differentiable. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Empirical risk minimization (ERM)",
        "subsubsection": "Example: minimizing the misclassification rate"
    },
    {
        "content": "4.3.1 Example: minimizing the misclassification rate \nIf we are solving a classification problem, we might want to use 0-1 loss: \nwhere $f ( { pmb x } ; { pmb theta } )$ is some kind of predictor. The empirical risk becomes \nThis is just the empirical misclassification rate on the training set. \nNote that for binary problems, we can rewrite the misclassifcation rate in the following notation. Let $tilde { y } in { - 1 , + 1 }$ be the true label, and $hat { y } in { - 1 , + 1 } = f ( pmb { x } ; pmb { theta } )$ be our prediction. We define the 0-1 loss as follows: \nThe corresponding empirical risk becomes \nwhere the dependence on ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and $pmb theta$ is implicit. \n4.3.2 Surrogate loss \nUnfortunately, the 0-1 loss used in Section 4.3.1 is a non-smooth step function, as shown in Figure 4.2, making it difficult to optimize. (In fact, it is NP-hard [BDEL03].) In this section we consider the use of a surrogate loss function [BJM06]. The surrogate is usually chosen to be a maximally tight convex upper bound, which is then easy to minimize. \nFor example, consider a probabilistic binary classifier, which produces the following distribution over labels: \nwhere $eta = f ( pmb { x } ; pmb { theta } )$ is the log odds. Hence the log loss is given by \nFigure 4.2 shows that this is a smooth upper bound to the 0-1 loss, where we plot the loss vs the quantity $tilde { y } eta$ , known as the margin, since it defines a “margin of safety” away from the threshold value of 0. Thus we see that minimizing the negative log likelihood is equivalent to minimizing a (fairly tight) upper bound on the empirical 0-1 loss. \nAnother convex upper bound to 0-1 loss is the hinge loss, which is defined as follows: \nThis is plotted in Figure 4.2; we see that it has the shape of a partially open door hinge. This is convex upper bound to the 0-1 loss, although it is only piecewise differentiable, not everywhere differentiable. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n4.4 Other estimation methods * \n4.4.1 The method of moments \nComputing the MLE requires solving the equation $nabla _ { pmb { theta } } mathrm { N L L } ( pmb { theta } ) = mathbf { 0 }$ . Sometimes this is computationally difficult. In such cases, we may be able to use a simpler approach known as the method of moments (MOM). In this approach, we equate the theoretical moments of the distribution to the empirical moments, and solve the resulting set of $K$ simultaneous equations, where $K$ is the number of parameters. The theoretical moments are given by $mu _ { k } = operatorname { mathbb { E } } leftlfloor Y ^ { k } rightrfloor$ , for $k = 1 : K$ , and the empirical moments are given by \nso we just need to solve $mu _ { k } = hat { mu } _ { k }$ for each $k$ . We give some examples below. \nThe method of moments is simple, but it is theoretically inferior to the MLE approach, since it may not use all the data as efficiently. (For details on these theoretical results, see e.g., [CB02].) Furthermore, it can sometimes produce inconsistent results (see Section 4.4.1.2). However, when it produces valid estimates, it can be used to initialize iterative algorithms that are used to optimize the NLL (see e.g., [AHK12]), thus combining the computational efficiency of MOM with the statistical accuracy of MLE. \n4.4.1.1 Example: MOM for the univariate Gaussian \nFor example, consider the case of a univariate Gaussian distribution. From Section 4.2.5, we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Empirical risk minimization (ERM)",
        "subsubsection": "Surrogate loss"
    },
    {
        "content": "4.4 Other estimation methods * \n4.4.1 The method of moments \nComputing the MLE requires solving the equation $nabla _ { pmb { theta } } mathrm { N L L } ( pmb { theta } ) = mathbf { 0 }$ . Sometimes this is computationally difficult. In such cases, we may be able to use a simpler approach known as the method of moments (MOM). In this approach, we equate the theoretical moments of the distribution to the empirical moments, and solve the resulting set of $K$ simultaneous equations, where $K$ is the number of parameters. The theoretical moments are given by $mu _ { k } = operatorname { mathbb { E } } leftlfloor Y ^ { k } rightrfloor$ , for $k = 1 : K$ , and the empirical moments are given by \nso we just need to solve $mu _ { k } = hat { mu } _ { k }$ for each $k$ . We give some examples below. \nThe method of moments is simple, but it is theoretically inferior to the MLE approach, since it may not use all the data as efficiently. (For details on these theoretical results, see e.g., [CB02].) Furthermore, it can sometimes produce inconsistent results (see Section 4.4.1.2). However, when it produces valid estimates, it can be used to initialize iterative algorithms that are used to optimize the NLL (see e.g., [AHK12]), thus combining the computational efficiency of MOM with the statistical accuracy of MLE. \n4.4.1.1 Example: MOM for the univariate Gaussian \nFor example, consider the case of a univariate Gaussian distribution. From Section 4.2.5, we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $overline { y }$ is the empirical mean and $s ^ { 2 }$ is the empirical average sum of squares. so $hat { mu } = overline { { y } }$ and $hat { sigma } ^ { 2 } = s ^ { 2 } - overline { { y } } ^ { 2 }$ . In this case, the MOM estimate is the same as the MLE, but this is not always the case. \n4.4.1.2 Example: MOM for the uniform distribution \nIn this section, we give an example of the MOM applied to the uniform distribution. Our presentation follows the wikipedia page.5 Let $Y sim mathrm { U n i f } ( theta _ { 1 } , theta _ { 2 } )$ be a uniform random variable, so \nThe first two moments are \nInverting these equations gives \nUnfortunately this estimator can sometimes give invalid results. For example, suppose $mathcal { D } =$ ${ 0 , 0 , 0 , 0 , 1 }$ . The empirical moments are $hat { mu } _ { 1 } = textstyle frac { 1 } { 5 }$ and $hat { mu } _ { 2 } = textstyle frac { 1 } { 5 }$ , so the estimated parameters are $begin{array} { r } { hat { theta } _ { 1 } = frac { 1 } { 5 } - frac { 2 sqrt { 3 } } { 5 } = - 0 . 4 9 3 } end{array}$ and $begin{array} { r } { hat { theta } _ { 2 } = frac { 1 } { 5 } + frac { 2 sqrt { 3 } } { 5 } = 0 . 8 9 3 } end{array}$ . However, these cannot possibly be the correct parameters, since if $theta _ { 2 } = 0 . 8 9 3$ , we cannot generate a sample as large as 1. \nBy contrast, consider the MLE. Let $y _ { ( 1 ) } leq y _ { ( 2 ) } leq cdot cdot cdot leq y _ { ( N ) }$ be the order statistics of the data (i.e., the values sorted in increasing order). Let $theta = theta _ { 2 } - theta _ { 1 }$ . Then the likelihood is given by \nWithin the permitted bounds for $theta$ , the derivative of the log likelihood is given by \nHence the likelihood is a decreasing function of $theta$ , so we should pick \nIn the above example, we get $widehat { theta } _ { 1 } = 0$ and $hat { theta } _ { 2 } = 1$ , as one would expect. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n4.4.2 Online (recursive) estimation \nIf the entire dataset $mathcal { D }$ is available before training starts, we say that we are doing batch learning. However, in some cases, the data set arrives sequentially, so $mathcal { D } = { pmb { y } _ { 1 } , pmb { y } _ { 2 } , dots }$ in an unbounded stream. In this case, we want to perform online learning. \nLet $hat { pmb { theta } } _ { t - 1 }$ be our estimate (e.g., MLE) given $mathcal { D } _ { 1 : t - 1 }$ . To ensure our learning algorithm takes constant time per update, we need to find a learning rule of the form \nThis is called a recursive update. Below we give some examples of such online learning methods. \n4.4.2.1 Example: recursive MLE for the mean of a Gaussian \nLet us reconsider the example from Section 4.2.5 where we computed the MLE for a univariate Gaussian. We know that the batch estimate for the mean is given by \nThis is just a running sum of the data, so we can easily convert this into a recursive estimate as follows: \nThis is known as a moving average. \nWe see from Equation (4.81) that the new estimate is the old estimate plus a correction term. The size of the correction diminishes over time (i.e., as we get more samples). However, if the distribution is changing, we want to give more weight to more recent data examples. We discuss how to do this in Section 4.4.2.2. \n4.4.2.2 Exponentially-weighted moving average \nEquation (4.81) shows how to compute the moving average of a signal. In this section, we show how to adjust this to give more weight to more recent examples. In particular, we will compute the following exponentially weighted moving average or EWMA, also called an exponential moving average or EMA: \nwhere $0 < beta < 1$ . The contribution of a data point $k$ steps in the past is weighted by $beta ^ { k } ( 1 - beta )$ . Thus the contribution from old data is exponentially decreasing. In particular, we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Other estimation methods *",
        "subsubsection": "The method of moments"
    },
    {
        "content": "4.4.2 Online (recursive) estimation \nIf the entire dataset $mathcal { D }$ is available before training starts, we say that we are doing batch learning. However, in some cases, the data set arrives sequentially, so $mathcal { D } = { pmb { y } _ { 1 } , pmb { y } _ { 2 } , dots }$ in an unbounded stream. In this case, we want to perform online learning. \nLet $hat { pmb { theta } } _ { t - 1 }$ be our estimate (e.g., MLE) given $mathcal { D } _ { 1 : t - 1 }$ . To ensure our learning algorithm takes constant time per update, we need to find a learning rule of the form \nThis is called a recursive update. Below we give some examples of such online learning methods. \n4.4.2.1 Example: recursive MLE for the mean of a Gaussian \nLet us reconsider the example from Section 4.2.5 where we computed the MLE for a univariate Gaussian. We know that the batch estimate for the mean is given by \nThis is just a running sum of the data, so we can easily convert this into a recursive estimate as follows: \nThis is known as a moving average. \nWe see from Equation (4.81) that the new estimate is the old estimate plus a correction term. The size of the correction diminishes over time (i.e., as we get more samples). However, if the distribution is changing, we want to give more weight to more recent data examples. We discuss how to do this in Section 4.4.2.2. \n4.4.2.2 Exponentially-weighted moving average \nEquation (4.81) shows how to compute the moving average of a signal. In this section, we show how to adjust this to give more weight to more recent examples. In particular, we will compute the following exponentially weighted moving average or EWMA, also called an exponential moving average or EMA: \nwhere $0 < beta < 1$ . The contribution of a data point $k$ steps in the past is weighted by $beta ^ { k } ( 1 - beta )$ . Thus the contribution from old data is exponentially decreasing. In particular, we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe sum of a geometric series is given by \nHence \nSince $0 < beta < 1$ , we have $beta ^ { t + 1 }  0$ as $t to infty$ , so smaller $beta$ forgets the past more quickly, and adapts to the more recent data more rapidly. This is illustrated in Figure 4.3. \nSince the initial estimate starts from $hat { pmb { mu } } _ { 0 } = { bf 0 }$ , there is an initial bias. This can be corrected by scaling as follows [KB15]: \n(Note that the update in Equation (4.82) is still applied to the uncorrected EMA, $hat { pmb { mu } } _ { t - 1 }$ , before being corrected for the current time step.) The benefit of this is illustrated in Figure 4.3. \n4.5 Regularization \nA fundamental problem with MLE, and ERM, is that it will try to pick parameters that minimize loss on the training set, but this may not result in a model that has low loss on future data. This is called overfitting. \nAs a simple example, suppose we want to predict the probability of heads when tossing a coin. We toss it $N = 3$ times and observe 3 heads. The MLE is $hat { theta } _ { mathrm { m l e } } = N _ { 1 } / ( N _ { 0 } + N _ { 1 } ) = 3 / ( 3 + 0 ) = 1$ (see Section 4.2.3). However, if we use $mathrm { B e r } ( y | hat { theta } _ { mathrm { m l e } } )$ to make predictions, we will predict that all future coin tosses will also be heads, which seems rather unlikely. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Other estimation methods *",
        "subsubsection": "Online (recursive) estimation"
    },
    {
        "content": "The core of the problem is that the model has enough parameters to perfectly fit the observed training data, so it can perfectly match the empirical distribution. However, in most cases the empirical distribution is not the same as the true distribution, so putting all the probability mass on the observed set of $N$ examples will not leave over any probability for novel data in the future. That is, the model may not generalize. \nThe main solution to overfitting is to use regularization, which means to add a penalty term to the NLL (or empirical risk). Thus we optimize an objective of the form \nwhere $lambda geq 0$ is the regularization parameter, and $C ( pmb theta )$ is some form of complexity penalty. A common complexity penalty is to use $C ( pmb theta ) = - log p ( pmb theta )$ , where $p ( pmb theta )$ is the prior for $pmb theta$ . If $ell$ is the log loss, the regularized objective becomes \nBy setting $lambda = 1$ and rescaling $p ( pmb theta )$ appropriately, we can equivalently minimize the following: \nMinimizing this is equivalent to maximizing the log posterior: \nThis is known as MAP estimation, which stands for maximum a posterior estimation. \n4.5.1 Example: MAP estimation for the Bernoulli distribution \nConsider again the coin tossing example. If we observe just one head, the MLE is $theta _ { mathrm { m l e } } = 1$ , which predicts that all future coin tosses will also show up heads. To avoid such overfitting, we can add a penalty to $theta$ to discourage “extreme” values, such as $theta = 0$ or $theta = 1$ . We can do this by using a beta distribution as our prior, $p ( theta ) = operatorname { B e t a } ( theta | a , b )$ , where $a , b > 1$ encourages values of $theta$ near to $a / ( a + b )$ (see Section 2.7.4 for details). The log likelihood plus log prior becomes \nUsing the method from Section 4.2.3 we find that the MAP estimate is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf we set $a = b = 2$ (which weakly favors a value of $theta$ near 0.5), the estimate becomes \nThis is called add-one smoothing, and is a simple but widely used technique to avoid the zero count problem. (See also Section 4.6.2.9.) \nThe zero-count problem, and overfitting more generally, is analogous to a problem in philosophy called the black swan paradox. This is based on the ancient Western conception that all swans were white. In that context, a black swan was a metaphor for something that could not exist. (Black swans were discovered in Australia by European explorers in the 17th Century.) The term “black swan paradox” was first coined by the famous philosopher of science Karl Popper; the term has also been used as the title of a recent popular book [Tal07]. This paradox was used to illustrate the problem of induction, which is the problem of how to draw general conclusions about the future from specific observations from the past. The solution to the paradox is to admit that induction is in general impossible, and that the best we can do is to make plausible guesses about what the future might hold, by combining the empirical data with prior knowledge. \n4.5.2 Example: MAP estimation for the multivariate Gaussian * \nIn Section 4.2.6, we showed that the MLE for the mean of an MVN is the empirical mean, $hat { pmb { mu } } _ { mathrm { m l e } } = overline { { pmb { y } } }$ .   \nWe also showed that the MLE for the covariance is the empirical covariance, $begin{array} { r } { hat { bf Z } = frac { 1 } { N } { bf S } _ { overline { { pmb { y } } } } } end{array}$ . \nIn high dimensions the estimate for $pmb { Sigma }$ can easily become singular. One solution to this is to perform MAP estimation, as we explain below. \n4.5.2.1 Shrinkage estimate \nA convenient prior to use for $pmb { Sigma }$ is the inverse Wishart prior. This is a distribution over positive definite matrices, where the parameters are defined in terms of a prior scatter matrix, $breve { mathbf { S } }$ , and a prior sample size or strength $breve { N }$ . One can show that the resulting MAP estimate is given by \nwhere $begin{array} { r } { lambda = frac { widecheck N } { widecheck N + N } } end{array}$ controls the amount of regularization. \nA common choice (see e.g., [FR07, p6]) for the prior scatter matrix is to use $breve { mathbf { S } }$ = $breve { N }$ $mathrm { d i a g } ( hat { Sigma } _ { mathrm { m l e } } )$ . With this choice, we find that the MAP estimate for $pmb { Sigma }$ is given by \nThus we see that the diagonal entries are equal to their ML estimates, and the off-diagonal elements are “shrunk” somewhat towards 0. This technique is therefore called shrinkage estimation. \nThe other parameter we need to set is $lambda$ , which controls the amount of regularization (shrinkage towards the MLE). It is common to set $lambda$ by cross validation (Section 4.5.5). Alternatively, we can use the closed-form formula provided in [LW04a; LW04b; SS05], which is the optimal \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 frequentist estimate if we use squared loss. This is implemented in the sklearn function https://scikitlearn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html.",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Example: MAP estimation for the Bernoulli distribution"
    },
    {
        "content": "If we set $a = b = 2$ (which weakly favors a value of $theta$ near 0.5), the estimate becomes \nThis is called add-one smoothing, and is a simple but widely used technique to avoid the zero count problem. (See also Section 4.6.2.9.) \nThe zero-count problem, and overfitting more generally, is analogous to a problem in philosophy called the black swan paradox. This is based on the ancient Western conception that all swans were white. In that context, a black swan was a metaphor for something that could not exist. (Black swans were discovered in Australia by European explorers in the 17th Century.) The term “black swan paradox” was first coined by the famous philosopher of science Karl Popper; the term has also been used as the title of a recent popular book [Tal07]. This paradox was used to illustrate the problem of induction, which is the problem of how to draw general conclusions about the future from specific observations from the past. The solution to the paradox is to admit that induction is in general impossible, and that the best we can do is to make plausible guesses about what the future might hold, by combining the empirical data with prior knowledge. \n4.5.2 Example: MAP estimation for the multivariate Gaussian * \nIn Section 4.2.6, we showed that the MLE for the mean of an MVN is the empirical mean, $hat { pmb { mu } } _ { mathrm { m l e } } = overline { { pmb { y } } }$ .   \nWe also showed that the MLE for the covariance is the empirical covariance, $begin{array} { r } { hat { bf Z } = frac { 1 } { N } { bf S } _ { overline { { pmb { y } } } } } end{array}$ . \nIn high dimensions the estimate for $pmb { Sigma }$ can easily become singular. One solution to this is to perform MAP estimation, as we explain below. \n4.5.2.1 Shrinkage estimate \nA convenient prior to use for $pmb { Sigma }$ is the inverse Wishart prior. This is a distribution over positive definite matrices, where the parameters are defined in terms of a prior scatter matrix, $breve { mathbf { S } }$ , and a prior sample size or strength $breve { N }$ . One can show that the resulting MAP estimate is given by \nwhere $begin{array} { r } { lambda = frac { widecheck N } { widecheck N + N } } end{array}$ controls the amount of regularization. \nA common choice (see e.g., [FR07, p6]) for the prior scatter matrix is to use $breve { mathbf { S } }$ = $breve { N }$ $mathrm { d i a g } ( hat { Sigma } _ { mathrm { m l e } } )$ . With this choice, we find that the MAP estimate for $pmb { Sigma }$ is given by \nThus we see that the diagonal entries are equal to their ML estimates, and the off-diagonal elements are “shrunk” somewhat towards 0. This technique is therefore called shrinkage estimation. \nThe other parameter we need to set is $lambda$ , which controls the amount of regularization (shrinkage towards the MLE). It is common to set $lambda$ by cross validation (Section 4.5.5). Alternatively, we can use the closed-form formula provided in [LW04a; LW04b; SS05], which is the optimal \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 frequentist estimate if we use squared loss. This is implemented in the sklearn function https://scikitlearn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html. \n\nThe benefits of this approach are illustrated in Figure 4.4. We consider fitting a 50-dimensional Gaussian to $N = 1 0 0$ , $N = 5 0$ and $N = 2 5$ data points. We see that the MAP estimate is always well-conditioned, unlike the MLE (see Section 7.1.4.4 for a discussion of condition numbers). In particular, we see that the eigenvalue spectrum of the MAP estimate is much closer to that of the true matrix than the MLE’s spectrum. The eigenvectors, however, are unaffected. \n4.5.3 Example: weight decay \nIn Figure 1.7, we saw how using polynomial regression with too high of a degree can result in overfitting. One solution is to reduce the degree of the polynomial. However, a more general solution is to penalize the magnitude of the weights (regression coefficients). We can do this by using a zero-mean Gaussian prior, $p ( { boldsymbol { w } } )$ . The resulting MAP estimate is given by \nwhere $begin{array} { r } { lvert | boldsymbol { w } rvert | _ { 2 } ^ { 2 } = sum _ { d = 1 } ^ { D } w _ { d } ^ { 2 } } end{array}$ . (We write $pmb { w }$ rather than $pmb theta$ , since it only really make sense to penalize the magnitude of weight vectors, rather than other parameters, such as bias terms or noise variances.) \nEquation (4.99) is called $ell _ { 2 }$ regularization or weight decay. The larger the value of $lambda$ , the more the parameters are penalized for being “large” (deviating from the zero-mean prior), and thus the less flexible the model. \nIn the case of linear regression, this kind of penalization scheme is called ridge regression. For example, consider the polynomial regression example from Section 1.2.2.2, where the predictor has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Example: MAP estimation for the multivariate Gaussian *"
    },
    {
        "content": "The benefits of this approach are illustrated in Figure 4.4. We consider fitting a 50-dimensional Gaussian to $N = 1 0 0$ , $N = 5 0$ and $N = 2 5$ data points. We see that the MAP estimate is always well-conditioned, unlike the MLE (see Section 7.1.4.4 for a discussion of condition numbers). In particular, we see that the eigenvalue spectrum of the MAP estimate is much closer to that of the true matrix than the MLE’s spectrum. The eigenvectors, however, are unaffected. \n4.5.3 Example: weight decay \nIn Figure 1.7, we saw how using polynomial regression with too high of a degree can result in overfitting. One solution is to reduce the degree of the polynomial. However, a more general solution is to penalize the magnitude of the weights (regression coefficients). We can do this by using a zero-mean Gaussian prior, $p ( { boldsymbol { w } } )$ . The resulting MAP estimate is given by \nwhere $begin{array} { r } { lvert | boldsymbol { w } rvert | _ { 2 } ^ { 2 } = sum _ { d = 1 } ^ { D } w _ { d } ^ { 2 } } end{array}$ . (We write $pmb { w }$ rather than $pmb theta$ , since it only really make sense to penalize the magnitude of weight vectors, rather than other parameters, such as bias terms or noise variances.) \nEquation (4.99) is called $ell _ { 2 }$ regularization or weight decay. The larger the value of $lambda$ , the more the parameters are penalized for being “large” (deviating from the zero-mean prior), and thus the less flexible the model. \nIn the case of linear regression, this kind of penalization scheme is called ridge regression. For example, consider the polynomial regression example from Section 1.2.2.2, where the predictor has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nSuppose we use a high degree polynomial, say $D = 1 4$ , even though we have a small dataset with just $N = 2 1$ examples. MLE for the parameters will enable the model to fit the data very well, by carefully adjusting the weights, but the resulting function is very “wiggly”, thus resulting in overfitting. Figure 4.5 illustrates how increasing $lambda$ can reduce overfitting. For more details on ridge regression, see Section 11.3. \n4.5.4 Picking the regularizer using a validation set \nA key question when using regularization is how to choose the strength of the regularizer $lambda$ : a small value means we will focus on minimizing empirical risk, which may result in overfitting, whereas a large value means we will focus on staying close to the prior, which may result in underfitting. \nIn this section, we describe a simple but very widely used method for choosing $lambda$ . The basic idea is to partition the data into two disjoint sets, the training set $mathscr { D } _ { mathrm { t r a i n } }$ and a validation set $mathcal { D } _ { mathrm { v a l i d } }$ (also called a development set). (Often we use about 80% of the data for the training set, and \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Example: weight decay"
    },
    {
        "content": "Suppose we use a high degree polynomial, say $D = 1 4$ , even though we have a small dataset with just $N = 2 1$ examples. MLE for the parameters will enable the model to fit the data very well, by carefully adjusting the weights, but the resulting function is very “wiggly”, thus resulting in overfitting. Figure 4.5 illustrates how increasing $lambda$ can reduce overfitting. For more details on ridge regression, see Section 11.3. \n4.5.4 Picking the regularizer using a validation set \nA key question when using regularization is how to choose the strength of the regularizer $lambda$ : a small value means we will focus on minimizing empirical risk, which may result in overfitting, whereas a large value means we will focus on staying close to the prior, which may result in underfitting. \nIn this section, we describe a simple but very widely used method for choosing $lambda$ . The basic idea is to partition the data into two disjoint sets, the training set $mathscr { D } _ { mathrm { t r a i n } }$ and a validation set $mathcal { D } _ { mathrm { v a l i d } }$ (also called a development set). (Often we use about 80% of the data for the training set, and \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n$2 0 %$ for the validation set.) We fit the model on $mathscr { D } _ { mathrm { t r a i n } }$ (for each setting of $lambda$ ) and then evaluate its performance on $mathcal { D } _ { mathrm { v a l i d } }$ . We then pick the value of $lambda$ that results in the best validation performance. (This optimization method is a 1d example of grid search, discussed in Section 8.8.) \nTo explain the method in more detail, we need some notation. Let us define the regularized empirical risk on a dataset as follows: \nFor each $lambda$ , we compute the parameter estimate \nWe then compute the validation risk: \nThis is an estimate of the population risk, which is the expected loss under the true distribution $p ^ { * } ( { pmb x } , { pmb y } )$ . Finally we pick \n(This requires fitting the model once for each value of $lambda$ in $boldsymbol { S }$ , although in some cases, this can be done more efficiently.) \nAfter picking $lambda ^ { * }$ , we can refit the model to the entire dataset, $mathcal { D } = mathcal { D } _ { mathrm { t r a i n } } cup mathcal { D } _ { mathrm { v a l i d } }$ , to get \n4.5.5 Cross-validation \nThe above technique in Section 4.5.4 can work very well. However, if the size of the training set is small, leaving aside 20% for a validation set can result in an unreliable estimate of the model parameters. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Picking the regularizer using a validation set"
    },
    {
        "content": "$2 0 %$ for the validation set.) We fit the model on $mathscr { D } _ { mathrm { t r a i n } }$ (for each setting of $lambda$ ) and then evaluate its performance on $mathcal { D } _ { mathrm { v a l i d } }$ . We then pick the value of $lambda$ that results in the best validation performance. (This optimization method is a 1d example of grid search, discussed in Section 8.8.) \nTo explain the method in more detail, we need some notation. Let us define the regularized empirical risk on a dataset as follows: \nFor each $lambda$ , we compute the parameter estimate \nWe then compute the validation risk: \nThis is an estimate of the population risk, which is the expected loss under the true distribution $p ^ { * } ( { pmb x } , { pmb y } )$ . Finally we pick \n(This requires fitting the model once for each value of $lambda$ in $boldsymbol { S }$ , although in some cases, this can be done more efficiently.) \nAfter picking $lambda ^ { * }$ , we can refit the model to the entire dataset, $mathcal { D } = mathcal { D } _ { mathrm { t r a i n } } cup mathcal { D } _ { mathrm { v a l i d } }$ , to get \n4.5.5 Cross-validation \nThe above technique in Section 4.5.4 can work very well. However, if the size of the training set is small, leaving aside 20% for a validation set can result in an unreliable estimate of the model parameters. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nA simple but popular solution to this is to use cross validation (CV). The idea is as follows: we split the training data into $K$ folds; then, for each fold $k in { 1 , ldots , K }$ , we train on all the folds but the $k$ ’th, and test on the $k$ ’th, in a round-robin fashion, as sketched in Figure 4.6. Formally, we have \nwhere $mathcal { D } _ { k }$ is the data in the $k$ ’th fold, and $mathcal { D } _ { - k }$ is all the other data. This is called the cross-validated risk. Figure 4.6 illustrates this procedure for $K = 5$ . If we set $K = N$ , we get a method known as leave-one-out cross-validation, since we always train on $N - 1$ items and test on the remaining one. \nWe can use the CV estimate as an objective inside of an optimization routine to pick the optimal hyperparameter, $ddot { lambda } = mathrm { a r g m i n } _ { lambda } R _ { lambda } ^ { mathrm { c v } }$ . Finally we combine all the available data (training and validation), and re-estimate the model parameters using $begin{array} { r } { hat { pmb theta } = mathrm { a r g m i n } _ { pmb theta } R _ { hat { lambda } } ( pmb theta , mathcal { D } ) } end{array}$ . See Section 5.4.3 for more details. \n4.5.5.1 The one standard error rule \nCV gives an estimate of $hat { R } _ { lambda }$ , but does not give any measure of uncertainty. A standard frequentist measure of uncertainty of an estimate is the standard error of the mean, which is the mean of the sampling distribution of the estimate (see Section 4.7.1). We can compute this as follows. First let $L _ { n } = ell ( { pmb y } _ { n } , f ( { pmb x } _ { n } ; hat { pmb theta } _ { lambda } ( { pmb D } _ { - n } ) )$ be the loss on the $n$ ’th example, where we use the parameters that were estimated using whichever training fold excludes $n$ . (Note that $L _ { n }$ depends on $lambda$ , but we drop this from the notation.) Next let $begin{array} { r } { hat { mu } = frac { 1 } { N } sum _ { n = 1 } ^ { N } L _ { n } } end{array}$ be the empirical mean and $begin{array} { r } { hat { sigma } ^ { 2 } = frac { 1 } { N } sum _ { n = 1 } ^ { N } ( L _ { n } - hat { mu } ) ^ { 2 } } end{array}$ be the empirical variance. Given this, we define our estimate to be $hat { mu }$ , and the standard error of this estimate to be $begin{array} { r } { mathrm { s e } ( hat { mu } ) = frac { ddot { sigma } } { sqrt { N _ { mathscr D } } } } end{array}$ . Note that $sigma$ measures the intrinsic variability of $L _ { n }$ across samples, whereas $mathrm { s e } ( hat { mu } )$ measures our uncertainty about the mean $hat { mu }$ . \nSuppose we apply CV to a set of models and compute the mean and se of their estimated risks. A common heuristic for picking a model from these noisy estimates is to pick the value which corresponds to the simplest model whose risk is no more than one standard error above the risk of the best model; this is called the one-standard error rule [HTF01, p216]. \n4.5.5.2 Example: ridge regression \nAs an example, consider picking the strength of the $ell _ { 2 }$ regularizer for the ridge regression problem in Section 4.5.3. In Figure 4.7a, we plot the error vs $log ( lambda )$ on the train set (blue) and test set (red curve). We see that the test error has a U-shaped curve, where it decreases as we increase the regularizer, and then increases as we start to underfit. In Figure 4.7b, we plot the 5-fold CV estimate of the test MSE vs $log ( lambda )$ . We see that the minimum CV error is close the optimal value for the test set (although it does underestimate the spike in the test error for large lambda, due to the small sample size.) \n4.5.6 Early stopping \nA very simple form of regularization, which is often very effective in practice (especially for complex models), is known as early stopping. This leverages the fact that optimization algorithms are \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 iterative, and so they take many steps to move away from the initial parameter estimates. If we detect signs of overfitting (by monitoring performance on the validation set), we can stop the optimization process, to prevent the model memorizing too much information about the training set. See Figure 4.8 for an illustration.",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Cross-validation"
    },
    {
        "content": "A simple but popular solution to this is to use cross validation (CV). The idea is as follows: we split the training data into $K$ folds; then, for each fold $k in { 1 , ldots , K }$ , we train on all the folds but the $k$ ’th, and test on the $k$ ’th, in a round-robin fashion, as sketched in Figure 4.6. Formally, we have \nwhere $mathcal { D } _ { k }$ is the data in the $k$ ’th fold, and $mathcal { D } _ { - k }$ is all the other data. This is called the cross-validated risk. Figure 4.6 illustrates this procedure for $K = 5$ . If we set $K = N$ , we get a method known as leave-one-out cross-validation, since we always train on $N - 1$ items and test on the remaining one. \nWe can use the CV estimate as an objective inside of an optimization routine to pick the optimal hyperparameter, $ddot { lambda } = mathrm { a r g m i n } _ { lambda } R _ { lambda } ^ { mathrm { c v } }$ . Finally we combine all the available data (training and validation), and re-estimate the model parameters using $begin{array} { r } { hat { pmb theta } = mathrm { a r g m i n } _ { pmb theta } R _ { hat { lambda } } ( pmb theta , mathcal { D } ) } end{array}$ . See Section 5.4.3 for more details. \n4.5.5.1 The one standard error rule \nCV gives an estimate of $hat { R } _ { lambda }$ , but does not give any measure of uncertainty. A standard frequentist measure of uncertainty of an estimate is the standard error of the mean, which is the mean of the sampling distribution of the estimate (see Section 4.7.1). We can compute this as follows. First let $L _ { n } = ell ( { pmb y } _ { n } , f ( { pmb x } _ { n } ; hat { pmb theta } _ { lambda } ( { pmb D } _ { - n } ) )$ be the loss on the $n$ ’th example, where we use the parameters that were estimated using whichever training fold excludes $n$ . (Note that $L _ { n }$ depends on $lambda$ , but we drop this from the notation.) Next let $begin{array} { r } { hat { mu } = frac { 1 } { N } sum _ { n = 1 } ^ { N } L _ { n } } end{array}$ be the empirical mean and $begin{array} { r } { hat { sigma } ^ { 2 } = frac { 1 } { N } sum _ { n = 1 } ^ { N } ( L _ { n } - hat { mu } ) ^ { 2 } } end{array}$ be the empirical variance. Given this, we define our estimate to be $hat { mu }$ , and the standard error of this estimate to be $begin{array} { r } { mathrm { s e } ( hat { mu } ) = frac { ddot { sigma } } { sqrt { N _ { mathscr D } } } } end{array}$ . Note that $sigma$ measures the intrinsic variability of $L _ { n }$ across samples, whereas $mathrm { s e } ( hat { mu } )$ measures our uncertainty about the mean $hat { mu }$ . \nSuppose we apply CV to a set of models and compute the mean and se of their estimated risks. A common heuristic for picking a model from these noisy estimates is to pick the value which corresponds to the simplest model whose risk is no more than one standard error above the risk of the best model; this is called the one-standard error rule [HTF01, p216]. \n4.5.5.2 Example: ridge regression \nAs an example, consider picking the strength of the $ell _ { 2 }$ regularizer for the ridge regression problem in Section 4.5.3. In Figure 4.7a, we plot the error vs $log ( lambda )$ on the train set (blue) and test set (red curve). We see that the test error has a U-shaped curve, where it decreases as we increase the regularizer, and then increases as we start to underfit. In Figure 4.7b, we plot the 5-fold CV estimate of the test MSE vs $log ( lambda )$ . We see that the minimum CV error is close the optimal value for the test set (although it does underestimate the spike in the test error for large lambda, due to the small sample size.) \n4.5.6 Early stopping \nA very simple form of regularization, which is often very effective in practice (especially for complex models), is known as early stopping. This leverages the fact that optimization algorithms are \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 iterative, and so they take many steps to move away from the initial parameter estimates. If we detect signs of overfitting (by monitoring performance on the validation set), we can stop the optimization process, to prevent the model memorizing too much information about the training set. See Figure 4.8 for an illustration. \n\n4.5.7 Using more data \nAs the amount of data increases, the chance of overfitting (for a model of fixed complexity) decreases (assuming the data contains suitably informative examples, and is not too redundant). This is illustrated in Figure 4.9. We show the MSE on the training and test sets for four different models (polynomials of increasing degree) as a function of the training set size $N$ . (A plot of error vs training set size is known as a learning curve.) The horizontal black line represents the Bayes error, which is the error of the optimal predictor (the true model) due to inherent noise. (In this example, the true model is a degree 2 polynomial, and the noise has a variance of $sigma ^ { 2 } = 4$ ; this is called the noise floor, since we cannot go below it.) \nWe notice several interesting things. First, the test error for degree 1 remains high, even as $N$ increases, since the model is too simple to capture the truth; this is called underfitting. The test error for the other models decreases to the optimal level (the noise floor), but it decreases more rapidly for the simpler models, since they have fewer parameters to estimate. The gap between the test error and training error is larger for more complex models, but decreases as $N$ grows. \nAnother interesting thing we can note is that the training error (blue line) initially increases with $N$ , at least for the models that are sufficiently flexible. The reason for this is as follows: as the data set gets larger, we observe more distinct input-output pattern combinations, so the task of fitting the data becomes harder. However, eventually the training set will come to resemble the test set, and \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Early stopping"
    },
    {
        "content": "4.5.7 Using more data \nAs the amount of data increases, the chance of overfitting (for a model of fixed complexity) decreases (assuming the data contains suitably informative examples, and is not too redundant). This is illustrated in Figure 4.9. We show the MSE on the training and test sets for four different models (polynomials of increasing degree) as a function of the training set size $N$ . (A plot of error vs training set size is known as a learning curve.) The horizontal black line represents the Bayes error, which is the error of the optimal predictor (the true model) due to inherent noise. (In this example, the true model is a degree 2 polynomial, and the noise has a variance of $sigma ^ { 2 } = 4$ ; this is called the noise floor, since we cannot go below it.) \nWe notice several interesting things. First, the test error for degree 1 remains high, even as $N$ increases, since the model is too simple to capture the truth; this is called underfitting. The test error for the other models decreases to the optimal level (the noise floor), but it decreases more rapidly for the simpler models, since they have fewer parameters to estimate. The gap between the test error and training error is larger for more complex models, but decreases as $N$ grows. \nAnother interesting thing we can note is that the training error (blue line) initially increases with $N$ , at least for the models that are sufficiently flexible. The reason for this is as follows: as the data set gets larger, we observe more distinct input-output pattern combinations, so the task of fitting the data becomes harder. However, eventually the training set will come to resemble the test set, and \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the error rates will converge, and will reflect the optimal performance of that model. \n\n4.6 Bayesian statistics * \nSo far, we have discussed several ways to estimate parameters from data. However, these approaches ignore any uncertainty in the estimates, which can be important for some applications, such as active learning, or avoiding overfitting, or just knowing how much to trust the estimate of some scientifically meaningful quantity. In statistics, modeling uncertainty about parameters using a probability distribution (as opposed to just computing a point estimate) is known as inference. \nIn this section, we use the posterior distribution to represent our uncertainty. This is the approach adopted in the field of Bayesian statistics. We give a brief introduction here, but more details can be found in the sequel to this book, [Mur23], as well as other good books, such as [Lam18; Kru15; McE20; Gel+14]. \nTo compute the posterior, we start with a prior distribution $p ( pmb theta )$ , which reflects what we know before seeing the data. We then define a likelihood function $p ( { mathcal { D } } | mathbf { theta } )$ , which reflects the data we expect to see for each setting of the parameters. We then use Bayes rule to condition the prior on the observed data to compute the posterior $p ( pmb { theta } | mathcal { D } )$ as follows: \nThe denominator $p ( mathcal D )$ is called the marginal likelihood, since it is computed by marginalizing over (or integrating out) the unknown $pmb theta$ . This can be interpreted as the average probability of the data, where the average is wrt the prior. Note, however, that $p ( mathcal D )$ is a constant, independent of $pmb theta$ , so we will often ignore it when we just want to infer the relative probabilities of $pmb theta$ values. \nEquation (4.107) is analogous to the use of Bayes rule for COVID-19 testing in Section 2.3.1. The difference is that the unknowns correspond to parameters of a statistical model, rather than the unknown disease state of a patient. In addition, we usually condition on a set of observations $mathcal { D }$ , as opposed to a single observation (such as a single test outcome). In particular, for a supervised or conditional model, the observed data has the form $mathcal { D } = { ( boldsymbol { x } _ { n } , boldsymbol { y } _ { n } ) : n = 1 : N }$ . For an unsupervised or unconditional model, the observed data has the form $mathcal { D } = { ( pmb { y } _ { n } ) : n = 1 : N }$ . \nOnce we have computed the posterior over the parameters, we can compute the posterior predictive distribution over outputs given inputs by marginalizing out the unknown parameters. In the supervised/ conditional case, this becomes \nThis can be viewed as a form of Bayes model averaging (BMA), since we are making predictions using an infinite set of models (parameter values), each one weighted by how likely it is. The use of BMA reduces the chance of overfitting (Section 1.2.3), since we are not just using the single best model. \n4.6.1 Conjugate priors \nIn this section, we consider a set of (prior, likelihood) pairs for which we can compute the posterior in closed form. In particular, we will use priors that are “conjugate” to the likelihood. We say that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license a prior $p ( pmb { theta } ) in mathcal { F }$ is a conjugate prior for a likelihood function $p ( mathcal { D } | pmb { theta } )$ if the posterior is in the same parameterized family as the prior, i.e., $p ( pmb { theta } | mathcal { D } ) in mathcal { F }$ . In other words, $mathcal { F }$ is closed under Bayesian updating. If the family $mathcal { F }$ corresponds to the exponential family (defined in Section 3.4), then the computations can be performed in closed form.",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Regularization",
        "subsubsection": "Using more data"
    },
    {
        "content": "4.6 Bayesian statistics * \nSo far, we have discussed several ways to estimate parameters from data. However, these approaches ignore any uncertainty in the estimates, which can be important for some applications, such as active learning, or avoiding overfitting, or just knowing how much to trust the estimate of some scientifically meaningful quantity. In statistics, modeling uncertainty about parameters using a probability distribution (as opposed to just computing a point estimate) is known as inference. \nIn this section, we use the posterior distribution to represent our uncertainty. This is the approach adopted in the field of Bayesian statistics. We give a brief introduction here, but more details can be found in the sequel to this book, [Mur23], as well as other good books, such as [Lam18; Kru15; McE20; Gel+14]. \nTo compute the posterior, we start with a prior distribution $p ( pmb theta )$ , which reflects what we know before seeing the data. We then define a likelihood function $p ( { mathcal { D } } | mathbf { theta } )$ , which reflects the data we expect to see for each setting of the parameters. We then use Bayes rule to condition the prior on the observed data to compute the posterior $p ( pmb { theta } | mathcal { D } )$ as follows: \nThe denominator $p ( mathcal D )$ is called the marginal likelihood, since it is computed by marginalizing over (or integrating out) the unknown $pmb theta$ . This can be interpreted as the average probability of the data, where the average is wrt the prior. Note, however, that $p ( mathcal D )$ is a constant, independent of $pmb theta$ , so we will often ignore it when we just want to infer the relative probabilities of $pmb theta$ values. \nEquation (4.107) is analogous to the use of Bayes rule for COVID-19 testing in Section 2.3.1. The difference is that the unknowns correspond to parameters of a statistical model, rather than the unknown disease state of a patient. In addition, we usually condition on a set of observations $mathcal { D }$ , as opposed to a single observation (such as a single test outcome). In particular, for a supervised or conditional model, the observed data has the form $mathcal { D } = { ( boldsymbol { x } _ { n } , boldsymbol { y } _ { n } ) : n = 1 : N }$ . For an unsupervised or unconditional model, the observed data has the form $mathcal { D } = { ( pmb { y } _ { n } ) : n = 1 : N }$ . \nOnce we have computed the posterior over the parameters, we can compute the posterior predictive distribution over outputs given inputs by marginalizing out the unknown parameters. In the supervised/ conditional case, this becomes \nThis can be viewed as a form of Bayes model averaging (BMA), since we are making predictions using an infinite set of models (parameter values), each one weighted by how likely it is. The use of BMA reduces the chance of overfitting (Section 1.2.3), since we are not just using the single best model. \n4.6.1 Conjugate priors \nIn this section, we consider a set of (prior, likelihood) pairs for which we can compute the posterior in closed form. In particular, we will use priors that are “conjugate” to the likelihood. We say that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license a prior $p ( pmb { theta } ) in mathcal { F }$ is a conjugate prior for a likelihood function $p ( mathcal { D } | pmb { theta } )$ if the posterior is in the same parameterized family as the prior, i.e., $p ( pmb { theta } | mathcal { D } ) in mathcal { F }$ . In other words, $mathcal { F }$ is closed under Bayesian updating. If the family $mathcal { F }$ corresponds to the exponential family (defined in Section 3.4), then the computations can be performed in closed form. \n\nIn the sections below, we give some common examples of this framework, which we will use later in the book. For simplicity, we focus on unconditional models (i.e., there are only outcomes or targets $y$ , and no inputs or features $_ { x }$ ); we relax this assumption in Section 4.6.7. \n4.6.2 The beta-binomial model \nSuppose we toss a coin $N$ times, and want to infer the probability of heads. Let $y _ { n } = 1$ denote the event that the $n$ ’th trial was heads, $y _ { n } = 0$ represent the event that the $n$ ’th trial was tails, and let $mathcal { D } = { y _ { n } : n = 1 : N }$ be all the data. We assume $y _ { n } sim operatorname { B e r } ( theta )$ , where $theta in [ 0 , 1 ]$ is the rate parameter (probability of heads). In this section, we discuss how to compute $p ( theta | mathcal { D } )$ . \n4.6.2.1 Bernoulli likelihood \nWe assume the data are iid or independent and identically distributed. Thus the likelihood has the form \nwhere we have defined $begin{array} { r } { N _ { 1 } = sum _ { n = 1 } ^ { N _ { D } } mathbb { I } left( y _ { n } = 1 right) } end{array}$ and $begin{array} { r } { N _ { 0 } = sum _ { n = 1 } ^ { N _ { D } } mathbb { I } left( y _ { n } = 0 right) } end{array}$ , representing the number of heads and tails. These counts are called the sufficient statistics of the data, since this is all we need to know about $mathcal { D }$ to infer $theta$ . The total count, $N = N _ { 0 } + N _ { 1 }$ , is called the sample size. \n4.6.2.2 Binomial likelihood \nNote that we can also consider a Binomial likelihood model, in which we perform $N$ trials and observe the number of heads, $y$ , rather than observing a sequence of coin tosses. Now the likelihood has the following form: \nThe scaling factor $binom { N } { y }$ is independent of $theta$ , so we can ignore it. Thus this likelihood is proportional to the Bernoulli likelihood in Equation (4.109), so our inferences about $theta$ will be the same for both models. \n4.6.2.3 Prior \nTo simplify the computations, we will assume that the prior $p ( pmb theta ) in mathcal { F }$ is a conjugate prior for the likelihood function $p ( pmb { y } | pmb { theta } )$ . This means that the posterior is in the same parameterized family as the prior, i.e., $p ( pmb theta | mathcal { D } ) in mathcal { F }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "Conjugate priors"
    },
    {
        "content": "In the sections below, we give some common examples of this framework, which we will use later in the book. For simplicity, we focus on unconditional models (i.e., there are only outcomes or targets $y$ , and no inputs or features $_ { x }$ ); we relax this assumption in Section 4.6.7. \n4.6.2 The beta-binomial model \nSuppose we toss a coin $N$ times, and want to infer the probability of heads. Let $y _ { n } = 1$ denote the event that the $n$ ’th trial was heads, $y _ { n } = 0$ represent the event that the $n$ ’th trial was tails, and let $mathcal { D } = { y _ { n } : n = 1 : N }$ be all the data. We assume $y _ { n } sim operatorname { B e r } ( theta )$ , where $theta in [ 0 , 1 ]$ is the rate parameter (probability of heads). In this section, we discuss how to compute $p ( theta | mathcal { D } )$ . \n4.6.2.1 Bernoulli likelihood \nWe assume the data are iid or independent and identically distributed. Thus the likelihood has the form \nwhere we have defined $begin{array} { r } { N _ { 1 } = sum _ { n = 1 } ^ { N _ { D } } mathbb { I } left( y _ { n } = 1 right) } end{array}$ and $begin{array} { r } { N _ { 0 } = sum _ { n = 1 } ^ { N _ { D } } mathbb { I } left( y _ { n } = 0 right) } end{array}$ , representing the number of heads and tails. These counts are called the sufficient statistics of the data, since this is all we need to know about $mathcal { D }$ to infer $theta$ . The total count, $N = N _ { 0 } + N _ { 1 }$ , is called the sample size. \n4.6.2.2 Binomial likelihood \nNote that we can also consider a Binomial likelihood model, in which we perform $N$ trials and observe the number of heads, $y$ , rather than observing a sequence of coin tosses. Now the likelihood has the following form: \nThe scaling factor $binom { N } { y }$ is independent of $theta$ , so we can ignore it. Thus this likelihood is proportional to the Bernoulli likelihood in Equation (4.109), so our inferences about $theta$ will be the same for both models. \n4.6.2.3 Prior \nTo simplify the computations, we will assume that the prior $p ( pmb theta ) in mathcal { F }$ is a conjugate prior for the likelihood function $p ( pmb { y } | pmb { theta } )$ . This means that the posterior is in the same parameterized family as the prior, i.e., $p ( pmb theta | mathcal { D } ) in mathcal { F }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo ensure this property when using the Bernoulli (or Binomial) likelihood, we should use a prior of the following form: \nWe recognize this as the pdf of a beta distribution (see Section 2.7.4). \n4.6.2.4 Posterior \nIf we multiply the Bernoulli likelihood in Equation (4.109) with the beta prior in Equation (2.13 we get a beta posterior: \nwhere $widehat { alpha } overset { Delta } { = } widecheck { alpha } + N _ { 1 }$ and ${ widehat { beta } } { triangleq } { breve { beta } } + N _ { 0 }$ are the parameters of the posterior. Since the posterior has the same functional form as the prior, we say that the beta distribution is a conjugate prior for the Bernoulli likelihood. \nThe parameters of the prior are called hyper-parameters. It is clear that (in this example) the hyper-parameters play a role analogous to the sufficient statistics; they are therefore often called pseudo counts. We see that we can compute the posterior by simply adding the observed counts (from the likelihood) to the pseudo counts (from the prior). \nThe strength of the prior is controlled by $breve { N } = breve { alpha } + breve { beta }$ ; this is called the equivalent sample size, since it plays a role analogous to the observed sample size, $N = N _ { 0 } + N _ { 1 }$ . \n4.6.2.5 Example \nFor example, suppose we set $breve { alpha }$ = ${ vec { beta } } = 2$ . This is like saying we believe we have already seen two heads and two tails before we see the actual data; this is a very weak preference for the value of $theta = 0 . 5$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe effect of using this prior is illustrated in Figure 4.10a. We see the posterior (blue line) is a “compromise” between the prior (red line) and the likelihood (black line). \nIf we set $scriptstyle { breve { alpha } } = { breve { beta } } = 1$ , the corresponding prior becomes the uniform distribution: \nThe effect of using this prior is illustrated in Figure 4.10b. We see that the posterior has exactly the same shape as the likelihood, since the prior was “uninformative”. \n4.6.2.6 Posterior mode (MAP estimate) \nThe most probable value of the parameter is given by the MAP estimate \nOne can show that this is given by \nIf we use a $mathrm { B e t a } ( theta | 2 , 2 )$ prior, this amounts to add-one smoothing: \nIf we use a uniform prior, $p ( theta ) propto 1$ , the MAP estimate becomes the MLE, since $log p ( theta ) = 0$ : \nWhen we use a Beta prior, the uniform distribution is $scriptstyle { breve { alpha } } = { breve { beta } } = 1$ . In this case, the MAP estimate reduces to the MLE: \nIf $N _ { 1 } = 0$ , we will estimate that $p ( Y = 1 ) = 0 . 0$ , which says that we do not predict any future observations to be 1. This is a very extreme estimate, that is likely due to insufficient data. We can solve this problem using a MAP estimate with a stronger prior, or using a fully Bayesian approach, in which we marginalize out $theta$ instead of estimating it, as explained in Section 4.6.2.9. \n4.6.2.7 Posterior mean \nThe posterior mode can be a poor summary of the posterior, since it corresponds to a single point.   \nThe posterior mean is a more robust estimate, since it integrates over the whole space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIf $p ( theta | mathcal { D } ) = mathrm { B e t a } ( theta | widehat { alpha } , widehat { beta } )$ , then the posterior mean is given by \nwhere $scriptstyle { widehat { N } } = { widehat { beta } } + { widehat { alpha } }$ is the strength (equivalent sample size) of the posterior. \nWe will now show that the posterior mean is a convex combination of the prior mean, $scriptstyle { m = breve { alpha } } / breve { N }$ (where $breve { N } ^ { triangleq } breve { alpha } + breve { beta }$ is the prior strength), and the MLE: $begin{array} { r } { hat { theta } _ { mathrm { m l e } } = frac { N _ { mathcal { D } 1 } } { N _ { mathcal { D } } } } end{array}$ \nwhere $begin{array} { r } { lambda = frac { widetilde { N } } { widetilde { N } } } end{array}$ is the ratio of the prior to posterior equivalent sample size. So the weaker the prior, the smaller is $lambda$ , and hence the closer the posterior mean is to the MLE. \n4.6.2.8 Posterior variance \nTo capture some notion of uncertainty in our estimate, a common approach is to compute the standard error of our estimate, which is just the posterior standard deviation: \nIn the case of the Bernoulli model, we showed that the posterior is a beta distribution. The variance of the beta posterior is given by \nwhere $widehat { alpha } { = } breve { alpha } { + } N _ { 1 }$ and $hat { beta }$ = $bigcup { } + N _ { 0 }$ . If $N _ { mathscr D } gg breve { alpha } + breve { beta }$ , this simplifies to \nwhere $hat { theta }$ is the MLE. Hence the standard error is given by \nWe see that the uncertainty goes down at a rate of $1 / sqrt { N }$ . We also see that the uncertainty (variance) is maximized when $hat { theta } = 0 . 5$ , and is minimized when $hat { theta }$ is close to $0$ or $^ { 1 }$ . This makes sense, since it is easier to be sure that a coin is biased than to be sure that it is fair. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n4.6.2.9 Posterior predictive \nSuppose we want to predict future observations. A very common approach is to first compute an estimate of the parameters based on training data, $ { hat { theta } } (  { mathcal { D } } )$ , and then to plug that parameter back into the model and use $p ( y | hat { pmb theta } )$ to predict the future; this is called a plug-in approximation. However, this can result in overfitting. As an extreme example, suppose we have seen $N _ { mathcal { D } } = 3$ heads in a row. The MLE is $hat { theta } = 3 / 3 = 1$ . However, if we use this estimate, we would predict that tails are impossible. \nOne solution to this is to compute a MAP estimate, and plug that in, as we discussed in Section 4.5.1 Here we discuss a fully Bayesian solution, in which we marginalize out $theta$ . \nBernoulli model \nFor the Bernoulli model, the resulting posterior predictive distribution has the form \nIn Section 4.5.1, we had to use the Beta(2,2) prior to recover add-one smoothing, which is a rather unnatural prior. In the Bayesian approach, we can get the same effect using a uniform prior, $p ( theta ) = mathrm { B e t a } ( theta | 1 , 1 )$ , since the predictive distribution becomes \nThis is known as Laplace’s rule of succession. See Figure 4.11 for an illustration of this in the sequential setting. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nBinomial model \nNow suppose we were interested in predicting the number of heads in $M > 1$ future coin tossing trials, i.e., we are using the binomial model instead of the Bernoulli model. The posterior over $theta$ is the same as before, but the posterior predictive distribution is different: \nWe recognize the integral as the normalization constant for a $operatorname { B e t a } ( { widehat { alpha } } + y , M - y + { widehat { beta } } )$ distribution. Hence \nThus we find that the posterior predictive is given by the following, known as the (compound) beta-binomial distribution: \nIn Figure 4.12(a), we plot the posterior predictive density for $M = 1 0$ after seeing $N _ { mathscr { D } 1 } = 4$ heads and $N _ { mathscr { D } 0 } = 1$ tails, when using a uniform Beta(1,1) prior. In Figure 4.12(b), we plot the plug-in approximation, given by \nwhere $hat { theta }$ is the MAP estimate. Looking at Figure 4.12, we see that the Bayesian prediction has longer tails, spreading its probability mass more widely, and is therefore less prone to overfitting and black-swan type paradoxes. (Note that we use a uniform prior in both cases, so the difference is not arising due to the use of a prior; rather, it is due to the fact that the Bayesian approach integrates out the unknown parameters when making its predictions.) \n4.6.2.10 Marginal likelihood \nThe marginal likelihood or evidence for a model $mathcal { M }$ is defined as \nWhen performing inference for the parameters of a specific model, we can ignore this term, since it is constant wrt $pmb theta$ . However, this quantity plays a vital role when choosing between different models, as we discuss in Section 5.2.2. It is also useful for estimating the hyperparameters from data (an approach known as empirical Bayes), as we discuss in Section 4.6.5.3. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIn general, computing the marginal likelihood can be hard. However, in the case of the betaBernoulli model, the marginal likelihood is proportional to the ratio of the posterior normalizer to the prior normalizer. To see this, recall that the posterior for the beta-binomial models is given by $p ( theta | mathcal { D } ) = mathrm { B e t a } ( theta | a ^ { prime } , b ^ { prime } )$ , where $a ^ { prime } = a + N _ { D 1 }$ and $b ^ { prime } = b + N _ { mathcal { D } 0 }$ . We know the normalization constant of the posterior is $B ( a ^ { prime } , b ^ { prime } )$ . Hence \nSo \nThe marginal likelihood for the beta-Bernoulli model is the same as above, except it is missing the $binom { N _ { D } } { N _ { D 1 } }$ term. \n4.6.2.11 Mixtures of conjugate priors \nThe beta distribution is a conjugate prior for the binomial likelihood, which enables us to easily compute the posterior in closed form, as we have seen. However, this prior is rather restrictive. For example, suppose we want to predict the outcome of a coin toss at a casino, and we believe that the coin may be fair, but may equally likely be biased towards heads. This prior cannot be represented \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 by a beta distribution. Fortunately, it can be represented as a mixture of beta distributions. For example, we might use \n\nIf $theta$ comes from the first distribution, the coin is fair, but if it comes from the second, it is biased towards heads. \nWe can represent a mixture by introducing a latent indicator variable $h$ , where $h = k$ means that $theta$ comes from mixture component $k$ . The prior has the form \nwhere each $p ( theta | h = k )$ is conjugate, and $p ( h = k )$ are called the (prior) mixing weights. One can show (Exercise 4.6) that the posterior can also be written as a mixture of conjugate distributions as follows: \nwhere $p ( h = k | mathcal { D } )$ are the posterior mixing weights given by \nHere the quantity $p ( mathcal { D } | h = k )$ is the marginal likelihood for mixture component $k$ (see Section 4.6.2.10). Returning to our example above, if we have the prior in Equation (4.144), and we observe $N _ { D 1 } = 2 0$ heads and $N _ {  Ḋ mathcal Ḋ D Ḍ Ḍ _ { 0 } } = 1 0$ tails, then, using Equation (4.143), the posterior becomes \nSee Figure 4.13 for an illustration. \nWe can compute the posterior probability that the coin is biased towards heads as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf we just used a single Beta(20,20) prior, we would get a slightly smaller value of $operatorname* { P r } ( theta > 0 . 5 | mathcal { D } ) =$ 0.8858. So if we were “suspicious” initially that the casino might be using a biased coin, our fears would be confirmed more quickly than if we had to be convinced starting with an open mind. \n4.6.3 The Dirichlet-multinomial model \nIn this section, we generalize the results from Section 4.6.2 from binary variables (e.g., coins) to $K$ -ary variables (e.g., dice). \n4.6.3.1 Likelihood \nLet $Y sim { mathrm { C a t } } ( pmb { theta } )$ be a discrete random variable drawn from a categorical distribution. The likelihood has the form \nwhere $begin{array} { r } { N _ { c } = sum _ { n } mathbb { I } left( y _ { n } = c right) } end{array}$ . \n4.6.3.2 Prior \nThe conjugate prior for a categorical distribution is the Dirichlet distribution, which is a multivariate generalization of the beta distribution. This has support over the probability simplex, defined by \nThe pdf of the Dirichlet is defined as follows: \nwhere $B ( breve { alpha } )$ is the multivariate beta function, \nFigure 4.14 shows some plots of the Dirichlet when $K = 3$ . We see that $begin{array} { r } { breve { alpha } _ { 0 } = sum _ { k } breve { alpha } _ { k } } end{array}$ controls the strength of the distribution (how peaked it is), and the $breve { alpha } _ { k }$ control where the peak occurs. For example, $operatorname { D i r } ( 1 , 1 , 1 )$ is a uniform distribution, $operatorname { D i r } ( 2 , 2 , 2 )$ is a broad distribution centered at $( 1 / 3 , 1 / 3 , 1 / 3 )$ , and $operatorname { D i r } ( 2 0 , 2 0 , 2 0 )$ is a narrow distribution centered at $( 1 / 3 , 1 / 3 , 1 / 3 )$ . $mathrm { D i r } ( 3 , 3 , 2 0 )$ is an asymmetric distribution that puts more density in one of the corners. If $breve { alpha } _ { k } < 1$ for all $k$ , we get “spikes” at the corners of the simplex. Samples from the distribution when $breve { alpha } _ { k } < 1$ will be sparse, as shown in Figure 4.15. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "The beta-binomial model"
    },
    {
        "content": "If we just used a single Beta(20,20) prior, we would get a slightly smaller value of $operatorname* { P r } ( theta > 0 . 5 | mathcal { D } ) =$ 0.8858. So if we were “suspicious” initially that the casino might be using a biased coin, our fears would be confirmed more quickly than if we had to be convinced starting with an open mind. \n4.6.3 The Dirichlet-multinomial model \nIn this section, we generalize the results from Section 4.6.2 from binary variables (e.g., coins) to $K$ -ary variables (e.g., dice). \n4.6.3.1 Likelihood \nLet $Y sim { mathrm { C a t } } ( pmb { theta } )$ be a discrete random variable drawn from a categorical distribution. The likelihood has the form \nwhere $begin{array} { r } { N _ { c } = sum _ { n } mathbb { I } left( y _ { n } = c right) } end{array}$ . \n4.6.3.2 Prior \nThe conjugate prior for a categorical distribution is the Dirichlet distribution, which is a multivariate generalization of the beta distribution. This has support over the probability simplex, defined by \nThe pdf of the Dirichlet is defined as follows: \nwhere $B ( breve { alpha } )$ is the multivariate beta function, \nFigure 4.14 shows some plots of the Dirichlet when $K = 3$ . We see that $begin{array} { r } { breve { alpha } _ { 0 } = sum _ { k } breve { alpha } _ { k } } end{array}$ controls the strength of the distribution (how peaked it is), and the $breve { alpha } _ { k }$ control where the peak occurs. For example, $operatorname { D i r } ( 1 , 1 , 1 )$ is a uniform distribution, $operatorname { D i r } ( 2 , 2 , 2 )$ is a broad distribution centered at $( 1 / 3 , 1 / 3 , 1 / 3 )$ , and $operatorname { D i r } ( 2 0 , 2 0 , 2 0 )$ is a narrow distribution centered at $( 1 / 3 , 1 / 3 , 1 / 3 )$ . $mathrm { D i r } ( 3 , 3 , 2 0 )$ is an asymmetric distribution that puts more density in one of the corners. If $breve { alpha } _ { k } < 1$ for all $k$ , we get “spikes” at the corners of the simplex. Samples from the distribution when $breve { alpha } _ { k } < 1$ will be sparse, as shown in Figure 4.15. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n4.6.3.3 Posterior \nWe can combine the multinomial likelihood and Dirichlet prior to compute the posterior, as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $widehat { alpha } _ { k } { = } breve { alpha } _ { k } + N _ { k }$ are the parameters of the posterior. So we see that the posterior can be computed by adding the empirical counts to the prior counts. \nThe posterior mean is given by \nThe posterior mode, which corresponds to the MAP estimate, is given by \nIf we use $breve { alpha } _ { k } = 1$ , corresponding to a uniform prior, the MAP becomes the MLE: \n(See Section 4.2.4 for a more direct derivation of this result.) \n4.6.3.4 Posterior predictive \nThe posterior predictive distribution is given by \nIn other words, the posterior predictive distribution is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $overline { { pmb { theta } } } triangleq mathbb { E } left[ pmb { theta } | mathcal { D } right]$ are the posterior mean parameters. If instead we plug-in the MAP estimate, we will suffer from the zero-count problem. The only way to get the same effect as add-one smoothing is to use a MAP estimate with $breve { alpha } _ { c } = 2$ . \nEquation (4.162) gives the probability of a single future event, conditioned on past observations ${ pmb y } = ( y _ { 1 } , dots , y _ { N } )$ . In some cases, we want to know the probability of observing a batch of future data, say $tilde { pmb { y } } = ( tilde { y } _ { 1 } , dots , tilde { y } _ { M } )$ . We can compute this as follows: \nThe denominator is the marginal likelihood of the training data, and the numerator is the marginal likelihood of the training and future test data. We discuss how to compute such marginal likelihoods in Section 4.6.3.5. \n4.6.3.5 Marginal likelihood \nBy the same reasoning as in Section 4.6.2.10, one can show that the marginal likelihood for the Dirichlet-categorical model is given by \nwhere \nHence we can rewrite the above result in the following form, which is what is usually presented in the literature: \n4.6.4 The Gaussian-Gaussian model \nIn this section, we derive the posterior for the parameters of a Gaussian distribution. For simplicity, we assume the variance is known. (The general case is discussed in the sequel to this book, [Mur23], as well as other standard references on Bayesian statistics.) \n4.6.4.1 Univariate case \nIf $sigma ^ { 2 }$ is a known constant, the likelihood for $mu$ has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "The Dirichlet-multinomial model"
    },
    {
        "content": "where $overline { { pmb { theta } } } triangleq mathbb { E } left[ pmb { theta } | mathcal { D } right]$ are the posterior mean parameters. If instead we plug-in the MAP estimate, we will suffer from the zero-count problem. The only way to get the same effect as add-one smoothing is to use a MAP estimate with $breve { alpha } _ { c } = 2$ . \nEquation (4.162) gives the probability of a single future event, conditioned on past observations ${ pmb y } = ( y _ { 1 } , dots , y _ { N } )$ . In some cases, we want to know the probability of observing a batch of future data, say $tilde { pmb { y } } = ( tilde { y } _ { 1 } , dots , tilde { y } _ { M } )$ . We can compute this as follows: \nThe denominator is the marginal likelihood of the training data, and the numerator is the marginal likelihood of the training and future test data. We discuss how to compute such marginal likelihoods in Section 4.6.3.5. \n4.6.3.5 Marginal likelihood \nBy the same reasoning as in Section 4.6.2.10, one can show that the marginal likelihood for the Dirichlet-categorical model is given by \nwhere \nHence we can rewrite the above result in the following form, which is what is usually presented in the literature: \n4.6.4 The Gaussian-Gaussian model \nIn this section, we derive the posterior for the parameters of a Gaussian distribution. For simplicity, we assume the variance is known. (The general case is discussed in the sequel to this book, [Mur23], as well as other standard references on Bayesian statistics.) \n4.6.4.1 Univariate case \nIf $sigma ^ { 2 }$ is a known constant, the likelihood for $mu$ has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nOne can show that the conjugate prior is another Gaussian, $mathcal { N } ( mu | breve { m } , breve { tau } ^ { 2 } )$ . Applying Bayes’ rule for Gaussians, as in Section 4.6.4.1, we find that the corresponding posterior is given by \nwhere $textstyle { overline { { y } } } triangleq { frac { 1 } { N } } sum _ { n = 1 } ^ { N } y _ { n }$ is the empirical mean. \nThis result is easier to understand if we work in terms of the precision parameters, which are just inverse variances. Specifically, let $kappa = 1 / sigma ^ { 2 }$ be the observation precision, and $breve { lambda } = 1 / breve { tau } ^ { 2 }$ be the precision of the prior. We can then rewrite the posterior as follows: \nThese equations are quite intuitive: the posterior precision $hat { lambda }$ is the prior precision $breve { lambda }$ plus $N$ units of measurement precision $kappa$ . Also, the posterior mean $acute { m }$ is a convex combination of the empirical mean $y$ and the prior mean $breve { m }$ . This makes it clear that the posterior mean is a compromise between the empirical mean and the prior. If the prior is weak relative to the signal strength ( $breve { lambda }$ is small relative to $kappa$ ), we put more weight on the empirical mean. If the prior is strong relative to the signal strength ( $breve { lambda }$ is large relative to $kappa$ ), we put more weight on the prior. This is illustrated in Figure 4.16. Note also that the posterior mean is written in terms of $N kappa overline { { y } }$ , so having $N$ measurements each of precision $kappa$ is like having one measurement with value $overline { y }$ and precision $N kappa$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nPosterior after seeing $N = 1$ examples \nTo gain further insight into these equations, consider the posterior after seeing a single data point $y$ (so $N = 1$ ). Then the posterior mean can be written in the following equivalent ways: \nThe first equation is a convex combination of the prior mean and the data. The second equation is the prior mean adjusted towards the data $y$ . The third equation is the data adjusted towards the prior mean; this is called a shrinkage estimate. This is easier to see if we define the weight $w = left. breve { lambda } right/ hat { lambda }$ , which is the ratio of the prior to posterior precision. Then we have \nNote that, for a Gaussian, the posterior mean and posterior mode are the same. Thus we can use the above equations to perform MAP estimation. See Exercise 4.2 for a simple example. \nPosterior variance \nIn addition to the posterior mean or mode of $mu$ , we might be interested in the posterior variance, which gives us a measure of confidence in our estimate. The square root of this is called the standard error of the mean: \nSuppose we use an uninformative prior for $mu$ by setting $overset { smile } { lambda } = 0$ (see Section 4.6.5.1). In this case, the posterior mean is equal to the MLE, $mathrm { { acute { m } } = mathrm { { overline { { y } } } } }$ . Suppose, in addition, that we approximate $sigma ^ { 2 }$ by the sample variance \nHence $widehat { lambda } { = } N widehat { kappa } = N / s ^ { 2 }$ , so the SEM becomes \nThus we see that the uncertainty in $mu$ is reduced at a rate of $1 / sqrt { N }$ . \nIn addition, we can use the fact that $9 5 %$ of a Gaussian distribution is contained within 2 standard deviations of the mean to approximate the $9 5 %$ credible interval for $mu$ using \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n4.6.4.2 Multivariate case \nFor $D$ -dimensional data, the likelihood has the form \nwhere $begin{array} { r } { overline { { pmb { y } } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } { pmb { y } _ { n } } } end{array}$ . T us we replace the set of observations with their mean, and scale down the $N$ \nFor simplicity, we will use a conjugate prior, which in this case is a Gaussian, namely \nWe can derive a Gaussian posterior for $pmb { mu }$ based on the results in Section 3.3.1 We get \nFigure 4.17 gives a 2d example of these results. \n4.6.5 Beyond conjugate priors \nWe have seen various examples of conjugate priors, all of which have come from the exponential family (see Section 3.4). These priors have the advantage of being easy to interpret (in terms of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 sufficient statistics from a virtual prior dataset), and easy to compute with. However, for most models, there is no prior in the exponential family that is conjugate to the likelihood. Furthermore, even where there is a conjugate prior, the assumption of conjugacy may be too limiting. Therefore in the sections below, we briefly discuss various other kinds of priors.",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "The Gaussian-Gaussian model"
    },
    {
        "content": "4.6.4.2 Multivariate case \nFor $D$ -dimensional data, the likelihood has the form \nwhere $begin{array} { r } { overline { { pmb { y } } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } { pmb { y } _ { n } } } end{array}$ . T us we replace the set of observations with their mean, and scale down the $N$ \nFor simplicity, we will use a conjugate prior, which in this case is a Gaussian, namely \nWe can derive a Gaussian posterior for $pmb { mu }$ based on the results in Section 3.3.1 We get \nFigure 4.17 gives a 2d example of these results. \n4.6.5 Beyond conjugate priors \nWe have seen various examples of conjugate priors, all of which have come from the exponential family (see Section 3.4). These priors have the advantage of being easy to interpret (in terms of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 sufficient statistics from a virtual prior dataset), and easy to compute with. However, for most models, there is no prior in the exponential family that is conjugate to the likelihood. Furthermore, even where there is a conjugate prior, the assumption of conjugacy may be too limiting. Therefore in the sections below, we briefly discuss various other kinds of priors. \n\n4.6.5.1 Noninformative priors \nWhen we have little or no domain specific knowledge, it is desirable to use an uninformative, noninformative or objective priors, to “let the data speak for itself”. For example, if we want to infer a real valued quantity, such as a location parameter $mu in mathbb { R }$ , we can use a flat prior $p ( mu ) propto 1$ . This can be viewed as an “infinitely wide” Gaussian. \nUnfortunately, there is no unique way to define uninformative priors, and they all encode some kind of knowledge. It is therefore better to use the term diffuse prior, minimally informative prior or default prior. See the sequel to this book, [Mur23], for more details. \n4.6.5.2 Hierarchical priors \nBayesian models require specifying a prior $p ( pmb theta )$ for the parameters. The parameters of the prior are called hyperparameters, and will be denoted by $phi$ . If these are unknown, we can put a prior on them; this defines a hierarchical Bayesian model, or multi-level model, which can visualize like this: $phi  theta  mathcal { D }$ . We assume the prior on the hyper-parameters is fixed (e.g., we may use some kind of minimally informative prior), so the joint distribution has the form \nThe hope is that we can learn the hyperparameters by treating the parameters themselves as datapoints. This is useful when we have multiple related parameters that need to be estimated (e.g., from different subpopulations, or muliple tasks); this provides a learning signal to the top level of the model. See the sequel to this book, [Mur23], for details. \n4.6.5.3 Empirical priors \nIn Section 4.6.5.2, we discussed hierarchical Bayes as a way to infer parameters from data. Unfortunately, posterior inference in such models can be computationally challenging. In this section, we discuss a computationally convenient approximation, in which we first compute a point estimate of the hyperparameters, $hat { phi }$ , and then compute the conditional posterior, $p ( pmb { theta } | hat { phi } , mathcal { D } )$ , rather than the joint posterior, $p ( theta , phi | mathcal { D } )$ . \nTo estimate the hyper-parameters, we can maximize the marginal likelihood: \nThis technique is known as type II maximum likelihood, since we are optimizing the hyperparameters, rather than the parameters. Once we have estimated $hat { phi }$ , we compute the posterior $p ( pmb { theta } | hat { phi } , mathcal { D } )$ in the usual way. \nSince we are estimating the prior parameters from data, this approach is empirical Bayes (EB) [CL96]. This violates the principle that the prior should be chosen independently of the data. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHowever, we can view it as a computationally cheap approximation to inference in the full hierarchical Bayesian model, just as we viewed MAP estimation as an approximation to inference in the one level model $theta  mathcal { D }$ . In fact, we can construct a hierarchy in which the more integrals one performs, the “more Bayesian” one becomes, as shown below. \nNote that ML-II is less likely to overfit than “regular” maximum likelihood, because there are typically fewer hyper-parameters $phi$ than there are parameters $pmb theta$ . See the sequel to this book, [Mur23], for details. \n4.6.6 Credible intervals \nA posterior distribution is (usually) a high dimensional object that is hard to visualize and work with. A common way to summarize such a distribution is to compute a point estimate, such as the posterior mean or mode, and then to compute a credible interval, which quantifies the uncertainty associated with that estimate. (A credible interval is not the same as a confidence interval, which is a concept from frequentist statistics which we discuss in Section 4.7.4.) \nMore precisely, we define a $1 0 0 ( 1 - alpha ) %$ credible interval to be a (contiguous) region $C = ( ell , u )$ (standing for lower and upper) which contains $1 - alpha$ of the posterior probability mass, i.e., \nThere may be many intervals that satisfy Equation (4.192), so we usually choose one such that there is $( 1 - alpha ) / 2$ mass in each tail; this is called a central interval. If the posterior has a known functional form, we can compute the posterior central interval using $ell = F ^ { - 1 } ( alpha / 2 )$ and $u = F ^ { - 1 } ( 1 - alpha / 2 )$ , where $F$ is the cdf of the posterior, and $F ^ { - 1 }$ is the inverse cdf. For example, if the posterior is Gaussian, $p ( theta | mathcal { D } ) = mathcal { N } ( 0 , 1 )$ , and $alpha = 0 . 0 5$ , then we have $ell = Phi ^ { - 1 } ( alpha / 2 ) = - 1 . 9 6$ , and $u = Phi ^ { - 1 } ( 1 - alpha / 2 ) = 1 . 9 6$ , where $Phi$ denotes the cdf of the Gaussian. This is illustrated in Figure 2.2b. This justifies the common practice of quoting a credible interval in the form of $mu pm 2 sigma$ , where $mu$ represents the posterior mean, $sigma$ represents the posterior standard deviation, and 2 is a good approximation to 1.96. \nIn general, it is often hard to compute the inverse cdf of the posterior. In this case, a simple alternative is to draw samples from the posterior, and then to use a Monte Carlo approximation to the posterior quantiles: we simply sort the $S$ samples, and find the one that occurs at location $alpha / S$ along the sorted list. As $S to infty$ , this converges to the true quantile. See beta_credible_int_demo.ipynb for a demo of this. \nA problem with central intervals is that there might be points outside the central interval which have higher probability than points that are inside, as illustrated in Figure 4.18(a). This motivates an alternative quantity known as the highest posterior density or HPD region, which is the set of points which have a probability above some threshold. More precisely we find the threshold $p ^ { * }$ on \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "Beyond conjugate priors"
    },
    {
        "content": "However, we can view it as a computationally cheap approximation to inference in the full hierarchical Bayesian model, just as we viewed MAP estimation as an approximation to inference in the one level model $theta  mathcal { D }$ . In fact, we can construct a hierarchy in which the more integrals one performs, the “more Bayesian” one becomes, as shown below. \nNote that ML-II is less likely to overfit than “regular” maximum likelihood, because there are typically fewer hyper-parameters $phi$ than there are parameters $pmb theta$ . See the sequel to this book, [Mur23], for details. \n4.6.6 Credible intervals \nA posterior distribution is (usually) a high dimensional object that is hard to visualize and work with. A common way to summarize such a distribution is to compute a point estimate, such as the posterior mean or mode, and then to compute a credible interval, which quantifies the uncertainty associated with that estimate. (A credible interval is not the same as a confidence interval, which is a concept from frequentist statistics which we discuss in Section 4.7.4.) \nMore precisely, we define a $1 0 0 ( 1 - alpha ) %$ credible interval to be a (contiguous) region $C = ( ell , u )$ (standing for lower and upper) which contains $1 - alpha$ of the posterior probability mass, i.e., \nThere may be many intervals that satisfy Equation (4.192), so we usually choose one such that there is $( 1 - alpha ) / 2$ mass in each tail; this is called a central interval. If the posterior has a known functional form, we can compute the posterior central interval using $ell = F ^ { - 1 } ( alpha / 2 )$ and $u = F ^ { - 1 } ( 1 - alpha / 2 )$ , where $F$ is the cdf of the posterior, and $F ^ { - 1 }$ is the inverse cdf. For example, if the posterior is Gaussian, $p ( theta | mathcal { D } ) = mathcal { N } ( 0 , 1 )$ , and $alpha = 0 . 0 5$ , then we have $ell = Phi ^ { - 1 } ( alpha / 2 ) = - 1 . 9 6$ , and $u = Phi ^ { - 1 } ( 1 - alpha / 2 ) = 1 . 9 6$ , where $Phi$ denotes the cdf of the Gaussian. This is illustrated in Figure 2.2b. This justifies the common practice of quoting a credible interval in the form of $mu pm 2 sigma$ , where $mu$ represents the posterior mean, $sigma$ represents the posterior standard deviation, and 2 is a good approximation to 1.96. \nIn general, it is often hard to compute the inverse cdf of the posterior. In this case, a simple alternative is to draw samples from the posterior, and then to use a Monte Carlo approximation to the posterior quantiles: we simply sort the $S$ samples, and find the one that occurs at location $alpha / S$ along the sorted list. As $S to infty$ , this converges to the true quantile. See beta_credible_int_demo.ipynb for a demo of this. \nA problem with central intervals is that there might be points outside the central interval which have higher probability than points that are inside, as illustrated in Figure 4.18(a). This motivates an alternative quantity known as the highest posterior density or HPD region, which is the set of points which have a probability above some threshold. More precisely we find the threshold $p ^ { * }$ on \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nthe pdf such that \nand then define the HPD as \nIn 1d, the HPD region is sometimes called a highest density interval or HDI. For example, Figure 4.18(b) shows the 95% HDI of a $mathrm { B e t a } ( 3 , 9 )$ distribution, which is (0.04, 0.48). We see that this is narrower than the central interval, even though it still contains $9 5 %$ of the mass; furthermore, every point inside of it has higher density than every point outside of it. \nFor a unimodal distribution, the HDI will be the narrowest interval around the mode containing $9 5 %$ of the mass. To see this, imagine “water filling” in reverse, where we lower the level until $9 5 %$ of the mass is revealed, and only 5% is submerged. This gives a simple algorithm for computing HDIs in the 1d case: simply search over points such that the interval contains $9 5 %$ of the mass and has minimal width. This can be done by 1d numerical optimization if we know the inverse CDF of the distribution, or by search over the sorted data points if we have a bag of samples (see betaHPD.ipynb for some code). \nIf the posterior is multimodal, the HDI may not even be a connected region: see Figure 4.19(b) for an example. However, summarizing multimodal posteriors is always difficult. \n4.6.7 Bayesian machine learning \nSo far, we have focused on unconditional models of the form $p ( pmb { y } | pmb { theta } )$ . In supervised machine learning, we use conditional models of the form $p ( pmb { y } | pmb { x } , pmb theta )$ . The posterior over the parameters is now $p ( pmb { theta } | mathcal { D } )$ , where $mathcal { D } = { ( pmb { x } _ { n } , pmb { y } _ { n } ) : n = 1 : N }$ . Computing this posterior can be done using the principles we have already discussed. This approach is called Bayesian machine learning, since we are “being Bayesian” about the model parameters. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "Credible intervals"
    },
    {
        "content": "the pdf such that \nand then define the HPD as \nIn 1d, the HPD region is sometimes called a highest density interval or HDI. For example, Figure 4.18(b) shows the 95% HDI of a $mathrm { B e t a } ( 3 , 9 )$ distribution, which is (0.04, 0.48). We see that this is narrower than the central interval, even though it still contains $9 5 %$ of the mass; furthermore, every point inside of it has higher density than every point outside of it. \nFor a unimodal distribution, the HDI will be the narrowest interval around the mode containing $9 5 %$ of the mass. To see this, imagine “water filling” in reverse, where we lower the level until $9 5 %$ of the mass is revealed, and only 5% is submerged. This gives a simple algorithm for computing HDIs in the 1d case: simply search over points such that the interval contains $9 5 %$ of the mass and has minimal width. This can be done by 1d numerical optimization if we know the inverse CDF of the distribution, or by search over the sorted data points if we have a bag of samples (see betaHPD.ipynb for some code). \nIf the posterior is multimodal, the HDI may not even be a connected region: see Figure 4.19(b) for an example. However, summarizing multimodal posteriors is always difficult. \n4.6.7 Bayesian machine learning \nSo far, we have focused on unconditional models of the form $p ( pmb { y } | pmb { theta } )$ . In supervised machine learning, we use conditional models of the form $p ( pmb { y } | pmb { x } , pmb theta )$ . The posterior over the parameters is now $p ( pmb { theta } | mathcal { D } )$ , where $mathcal { D } = { ( pmb { x } _ { n } , pmb { y } _ { n } ) : n = 1 : N }$ . Computing this posterior can be done using the principles we have already discussed. This approach is called Bayesian machine learning, since we are “being Bayesian” about the model parameters. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n4.6.7.1 Plugin approximation \nOnce we have computed the posterior over the parameters, we can compute the posterior predictive distribution over outputs given inputs by marginalizing out the unknown parameters: \nOf course, computing this integral is often intractable. A very simple approximation is to assume there is just a single best model, $hat { pmb { theta } }$ , such as the MLE. This is equivalent to approximating the posterior as an infinitely narrow, but infinitely tall, “spike” at the chosen value. We can write this as follows: \nwhere $delta$ is the Dirac delta function (see Section 2.6.5). If we use this approximation, then the predictive distribution can be obtained by simply “plugging in” the point estimate into the likelihood: \nThis follows from the sifting property of delta functions (Equation (2.129)). \nThe approach in Equation (4.197) is called a plug-in approximation. This approach is equivalent to the standard approach used in most of machine learning, in which we first fit the model (i.e. compute a point estimate $hat { pmb { theta } }$ ) and then use it to make predicitons. However, the standard (plug-in) approach can suffer from overfitting and overconfidence, as we discussed in Section 1.2.3. The fully Bayesian approach avoids this by marginalizing out the parameters, but can be expensive. Fortunately, even simple approximations, in which we average over a few plausible parameter values, can improve performance. We give some examples of this below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n4.6.7.2 Example: scalar input, binary output \nSuppose we want to perform binary classification, so $y in { 0 , 1 }$ . We will use a model of the form \nwhere \nis the sigmoid or logistic function which maps $mathbb { R } to [ 0 , 1 ]$ , and $operatorname { B e r } ( y | mu )$ is the Bernoulli distribution with mean $mu$ (see Section 2.4 for details). In other words, \nThis model is called logistic regression. (We discuss this in more detail in Chapter 10.) \nLet us apply this model to the task of determining if an iris flower is of type Setosa or Versicolor, $y _ { n } in { 0 , 1 }$ , given information about the sepal length, $x _ { n }$ . (See Section 1.2.1.1 for a description of the iris dataset.) \nWe first fit a 1d logistic regression model of the following form \nto the dataset $mathcal { D } = { ( x _ { n } , y _ { n } ) }$ using maximum likelihood estimation. (See Section 10.2.3 for details on how to compute the MLE for this model.) Figure 4.20a shows the plugin approximation to the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nposterior predictive, $p ( y = 1 | x , hat { pmb { theta } } )$ , where $hat { pmb { theta } }$ is the MLE of the parameters. We see that we become more confident that the flower is of type Versicolor as the sepal length gets larger, as represented by the sigmoidal (S-shaped) logistic function. \nThe decision boundary is defined to be the input value $x ^ { * }$ where $p ( y = 1 | x ^ { * } ; hat { pmb theta } ) = 0 . 5$ . We can solve for this value as follows: \nFrom Figure 4.20a, we see that $x ^ { * } approx 5 . 5$ cm. \nHowever, the above approach does not model the uncertainty in our estimate of the parameters, and therefore ignores the induced uncertainty in the output probabilities, and the location of the decision boundary. To capture this additional uncertainty, we can use a Bayesian approach to approximate the posterior $p ( pmb { theta } | mathcal { D } )$ . (See Section 10.5 for details.) Given this, we can approximate the posterior predictive distribution using a Monte Carlo approximation: \nwhere $theta ^ { s } sim p ( theta | mathcal { D } )$ is a posterior sample. Figure 4.20b plots the mean and $9 5 %$ credible interval of this function. We see that there is now a range of predicted probabilities for each input. We can also compute a distribution over the location of the decision boundary by using the Monte Carlo approximation \nwhere $( b ^ { s } , w ^ { s } ) = pmb { theta } ^ { s }$ . The $9 5 %$ credible interval for this distribution is shown by the “fat” vertical line in Figure 4.20b. \nAlthough carefully modeling our uncertainty may not matter for this application, it can be important in risk-sensitive applications, such as health care and finance, as we discuss in Chapter 5. \n4.6.7.3 Example: binary input, scalar output \nNow suppose we want to predict the delivery time for a package, $y in mathbb { R }$ , if shipped by company A vs B. We can encode the company id using a binary feature $x in { 0 , 1 }$ , where $x = 0$ means company A and $x = 1$ means company B. We will use the following discriminative model for this problem: \nwhere $mathcal { N } ( y | mu , sigma ^ { 2 } )$ is the Gaussian distribution \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 and $pmb theta = ( mu _ { 0 } , mu _ { 1 } , sigma _ { 0 } , sigma _ { 1 } )$ are the parameters of the model. We can fit this model using maximum likelihood estimation as we discuss in Section 4.2.5; alternatively, we can adopt a Bayesian approach, as we discuss in Section 4.6.4. \n\nThe advantage of the Bayesian approach is that by capturing uncertainty in the parameters $pmb theta$ , we also capture uncertainty in our forecasts $p ( boldsymbol { y } | boldsymbol { x } , mathcal { D } )$ , whereas using a plug-in approximation $p ( boldsymbol { y } | boldsymbol { x } , hat { pmb { theta } } )$ would underestimate this uncertainty. For example, suppose we have only used each company once, so our training set has the form $mathcal { D } = { ( x _ { 1 } = 0 , y _ { 1 } = 1 5 ,$ ), ( $x _ { 2 } = 1 , y _ { 2 } = 2 0 ) _ { . }$ . As we show in Section 4.2.5, the MLE for the means will be the empirical means, $hat { mu } _ { 0 } = 1 5$ and $hat { mu } _ { 1 } = 2 0$ , but the MLE for the standard deviations will be zero, $hat { sigma } _ { 0 } = hat { sigma } _ { 1 } = 0$ , since we only have a single sample from each “class”. The resulting plug-in prediction will therefore not capture any uncertainty. \nTo see why modeling the uncertainty is important, consider Figure 4.21. We see that the expected time of arrival (ETA) for company A is less than for company B; however, the variance of A’s distribution is larger, which makes it a risky choice if you want to be confident the package will arrive by the specified deadline. (For more details on how to choose optimal actions in the presence of uncertainty, see Chapter 5.) \nOf course, the above example is extreme, because we assumed we only had one example from each delivery company. However, this kind of problem occurs whenever we have few examples of a given kind of input, as can happen whenever the data has a long tail of novel patterns, such as a new combination of words or categorical features. \n4.6.7.4 Scaling up \nThe above examples were both extremely simple, involving 1d input and 1d output, and just 2–4 parameters. Most practical problems involve high dimensional inputs, and sometimes high dimensional outputs, and therefore use models with lots of parameters. Unfortunately, computing the posterior, $p ( pmb { theta } | mathcal { D } )$ , and the posterior predictive, $p ( pmb { y } | pmb { x } , mathcal { D } )$ , can be computationally challenging in such cases. We discuss this issue in Section 4.6.8. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n4.6.8 Computational issues \nGiven a likelihood $p ( mathcal { D } | mathbf { theta } )$ and a prior $p ( pmb theta )$ , we can compute the posterior $p ( pmb { theta } | mathcal { D } )$ using Bayes’ rule. However, actually performing this computation is usually intractable, except for simple special cases, such as conjugate models (Section 4.6.1), or models where all the latent variables come from a small finite set of possible values. We therefore need to approximate the posterior. There are a large variety of methods for performing approximate posterior inference, which trade off accuracy, simplicity, and speed. We briefly discuss some of these algorithms below, but go into more detail in the sequel to this book, [Mur23]. (See also [MFR20] for a review of various approximate inference methods, starting with Bayes’ original method in 1763.) \nAs a running example, we will use the problem of approximating the posterior of a beta-Bernoulli model. Specifically, the goal is to approximate \nwhere $mathcal { D }$ consists of 10 heads and 1 tail (so the total number of observations is $N = 1 1$ ), and we use a uniform prior. Although we can compute this posterior exactly (see Figure 4.22), using the method discussed in Section 4.6.2, this serves as a useful pedagogical example since we can compare the approximation to the exact answer. Also, since the target distribution is just 1d, it is easy to visualize the results. (Note, however, that the problem is not completely trivial, since the posterior is highly skewed, due to the use of an imbalanced sample of 10 heads and 1 tail.) \n4.6.8.1 Grid approximation \nThe simplest approach to approximate posterior inference is to partition the space of possible values for the unknowns into a finite set of possibilities, call them $pmb { theta } _ { 1 } , ldots , pmb { theta } _ { K }$ , and then to approximate the posterior by brute-force enumeration, as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "Bayesian machine learning"
    },
    {
        "content": "4.6.8 Computational issues \nGiven a likelihood $p ( mathcal { D } | mathbf { theta } )$ and a prior $p ( pmb theta )$ , we can compute the posterior $p ( pmb { theta } | mathcal { D } )$ using Bayes’ rule. However, actually performing this computation is usually intractable, except for simple special cases, such as conjugate models (Section 4.6.1), or models where all the latent variables come from a small finite set of possible values. We therefore need to approximate the posterior. There are a large variety of methods for performing approximate posterior inference, which trade off accuracy, simplicity, and speed. We briefly discuss some of these algorithms below, but go into more detail in the sequel to this book, [Mur23]. (See also [MFR20] for a review of various approximate inference methods, starting with Bayes’ original method in 1763.) \nAs a running example, we will use the problem of approximating the posterior of a beta-Bernoulli model. Specifically, the goal is to approximate \nwhere $mathcal { D }$ consists of 10 heads and 1 tail (so the total number of observations is $N = 1 1$ ), and we use a uniform prior. Although we can compute this posterior exactly (see Figure 4.22), using the method discussed in Section 4.6.2, this serves as a useful pedagogical example since we can compare the approximation to the exact answer. Also, since the target distribution is just 1d, it is easy to visualize the results. (Note, however, that the problem is not completely trivial, since the posterior is highly skewed, due to the use of an imbalanced sample of 10 heads and 1 tail.) \n4.6.8.1 Grid approximation \nThe simplest approach to approximate posterior inference is to partition the space of possible values for the unknowns into a finite set of possibilities, call them $pmb { theta } _ { 1 } , ldots , pmb { theta } _ { K }$ , and then to approximate the posterior by brute-force enumeration, as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis is called a grid approximation. In Figure 4.22a, we illustrate this method applied to our 1d problem. We see that it is easily able to capture the skewed posterior. Unfortunately, this approach does not scale to problems in more than 2 or 3 dimensions, because the number of grid points grows exponentially with the number of dimensions. \n4.6.8.2 Quadratic (Laplace) approximation \nIn this section, we discuss a simple way to approximate the posterior using a multivariate Gaussian;   \nthis is known as a Laplace approximation or a quadratic approximation (see e.g., [TK86;   \nRMC09]). \nTo derive this, suppose we write the posterior as follows: \nwhere $mathcal { E } ( pmb { theta } ) = - log p ( pmb { theta } , mathcal { D } )$ is called an energy function, and $Z = p ( { mathcal { D } } )$ is the normalization constant. Performing a Taylor series expansion around the mode $hat { pmb { theta } }$ (i.e., the lowest energy state) we get \nwhere $pmb { g }$ is the gradient at the mode, and $mathbf { H }$ is the Hessian. Since $hat { pmb { theta } }$ is the mode, the gradient term is zero. Hence \nThe last line follows from normalization constant of the multivariate Gaussian. \nThe Laplace approximation is easy to apply, since we can leverage existing optimization algorithms to compute the MAP estimate, and then we just have to compute the Hessian at the mode. (In high dimensional spaces, we can use a diagonal approximation.) \nIn Figure 4.22b, we illustrate this method applied to our 1d problem. Unfortunately we see that it is not a particularly good approximation. This is because the posterior is skewed, whereas a Gaussian is symmetric. In addition, the parameter of interest lies in the constrained interval $theta in [ 0 , 1 ]$ , whereas the Gaussian assumes an unconstrained space, $pmb theta in mathbb R$ . Fortunately, we can solve this latter problem by using a change of variable. For example, in this case we can apply the Laplace approximation to $alpha = log mathrm { i t } ( theta )$ . This is a common trick to simplify the job of inference. \n4.6.8.3 Variational approximation \nIn Section 4.6.8.2, we discussed the Laplace approximation, which uses an optimization procedure to find the MAP estimate, and then approximates the curvature of the posterior at that point based on the Hessian. In this section, we discuss variational inference (VI), which is another optimization-based approach to posterior inference, but which has much more modeling flexibility (and thus can give a much more accurate approximation). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nVI attempts to approximate an intractable probability distribution, such as $p ( pmb { theta } | mathcal { D } )$ , with one that is tractable, $q ( pmb theta )$ , so as to minimize some discrepancy $D$ between the distributions: \nwhere $mathcal { Q }$ is some tractable family of distributions (e.g., multivariate Gaussian). If we define $D$ to be the KL divergence (see Section 6.2), then we can derive a lower bound to the log marginal likelihood; this quantity is known as the evidence lower bound or ELBO. By maximizing the ELBO, we can improve the quality of the posterior approximation. See the sequel to this book, [Mur23], for details. \n4.6.8.4 Markov Chain Monte Carlo (MCMC) approximation \nAlthough VI is a fast, optimization-based method, it can give a biased approximation to the posterior, since it is restricted to a specific function form $q in mathcal { Q }$ . A more flexible approach is to use a nonparametric approximation in terms of a set of samples, $begin{array} { r } { q ( pmb { theta } ) approx frac { 1 } { S } sum _ { s = 1 } ^ { S } delta ( pmb { theta } - pmb { theta } ^ { s } ) } end{array}$ . This is called a Monte Carlo approximation to the posterior. The key issue is how to create the posterior samples $theta ^ { s } sim p ( theta | mathcal { D } )$ efficiently, without having to evaluate the normalization constant $begin{array} { r } { p ( mathcal { D } ) = int p ( theta , mathcal { D } ) d theta } end{array}$ . A common approach to this problem is known as Markov chain Monte Carlo or MCMC. If we augment this algorithm with gradient-based information, derived from $nabla log p ( pmb theta , mathcal { D } )$ , we can significantly speed up the method; this is called Hamiltonian Monte Carlo or HMC. See the sequel to this book, [Mur23], for details. \n4.7 Frequentist statistics * \nThe approach to statistical inference that we described in Section 4.6 is called Bayesian statistics. It treats parameters of models just like any other unknown random variable, and applies the rules of probability theory to infer them from data. Attempts have been made to devise approaches to statistical inference that avoid treating parameters like random variables, and which thus avoid the use of priors and Bayes rule. This alternative approach is known as frequentist statistics, classical statistics or orthodox statistics. \nThe basic idea (formalized in Section 4.7.1) is to to represent uncertainty by calculating how a quantity estimated from data (such as a parameter or a predicted label) would change if the data were changed. It is this notion of variation across repeated trials that forms the basis for modeling uncertainty used by the frequentist approach. By contrast, the Bayesian approach views probability in terms of information rather than repeated trials. This allows the Bayesian to compute the probability of one-off events, as we discussed in Section 2.1.1. Perhaps more importantly, the Bayesian approach avoids certain paradoxes that plague the frequentist approach (see Section 4.7.5 and Section 5.5.4). These pathologies led the famous statistician George Box to say: \nI believe that it would be very difficult to persuade an intelligent person that current [frequentist] statistical practice was sensible, but that there would be much less difficulty with an approach via likelihood and Bayes’ theorem. — George Box, 1962 (quoted in [Jay76]). \nNevertheless, it is useful to be familiar with frequentist statistics, since it is widely used, and has some key concepts that are useful even for Bayesians [Rub84]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Bayesian statistics *",
        "subsubsection": "Computational issues"
    },
    {
        "content": "4.7.1 Sampling distributions \nIn frequentist statistics, uncertainty is not represented by the posterior distribution of a random variable, but instead by the sampling distribution of an estimator. (We define these two terms below.) \nAs explained in the section on decision theory in Section 5.1, an estimator is a decision procedure that specifies what action to take given some observed data. In the context of parameter estimation, where the action space is to return a parameter vector, we will denote this by $hat { pmb { theta } } = pi ( mathcal { D } )$ . For example, $hat { pmb { theta } }$ could be the maximum likelihood estimate, the MAP estimate, or the method of moments estimate. \nThe sampling distribution of an estimator is the distribution of results we would see if we applied the estimator multiple times to different datasets sampled from some distribution; in the context of parameter estimation, it is the distribution of $hat { pmb { theta } }$ , viewed as a random variable that depends on the random sample $mathcal { D }$ . In more detail, imagine sampling $S$ different data sets, each of size $N$ , from some true model $p ( { pmb x } | pmb theta ^ { * } )$ to generate \nWe denote this by $mathcal { D } ^ { ( s ) } sim theta ^ { * }$ for brevity. Now apply the estimator to each $mathcal { D } ^ { ( s ) }$ to get a set of estimates, ${ hat { pmb { theta } } ( mathcal { D } ^ { ( s ) } ) }$ . As we let $S to infty$ , the distribution induced by this set is the sampling distribution of the estimator. More precisely, we have \nIn some cases, we can compute this analytically, as we discuss in Section 4.7.2, although typically we need to approximate it by Monte Carlo, as we discuss in Section 4.7.3. \n4.7.2 Gaussian approximation of the sampling distribution of the MLE \nThe most common estimator is the MLE. When the sample size becomes large, the sampling distribution of the MLE for certain models becomes Gaussian. This is known as the asymptotic normality of the sampling distribution. More formally, we have the following result: \nTheorem 4.7.1. If the parameters are identifiable, then \nwhere $mathbf { F } ( pmb theta ^ { * } )$ is the Fisher information matrix, defined in Equation (4.220). \nThe Fisher information matrix measures the amount of curvature of the log-likelihood surface at its peak, as we show below. \nMore formally, the Fisher information matrix (FIM) is defined to be the covariance of the gradient of the log likelihood (also called the score function): \nHence the $( i , j )$ ’th entry has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Frequentist statistics *",
        "subsubsection": "Sampling distributions"
    },
    {
        "content": "4.7.1 Sampling distributions \nIn frequentist statistics, uncertainty is not represented by the posterior distribution of a random variable, but instead by the sampling distribution of an estimator. (We define these two terms below.) \nAs explained in the section on decision theory in Section 5.1, an estimator is a decision procedure that specifies what action to take given some observed data. In the context of parameter estimation, where the action space is to return a parameter vector, we will denote this by $hat { pmb { theta } } = pi ( mathcal { D } )$ . For example, $hat { pmb { theta } }$ could be the maximum likelihood estimate, the MAP estimate, or the method of moments estimate. \nThe sampling distribution of an estimator is the distribution of results we would see if we applied the estimator multiple times to different datasets sampled from some distribution; in the context of parameter estimation, it is the distribution of $hat { pmb { theta } }$ , viewed as a random variable that depends on the random sample $mathcal { D }$ . In more detail, imagine sampling $S$ different data sets, each of size $N$ , from some true model $p ( { pmb x } | pmb theta ^ { * } )$ to generate \nWe denote this by $mathcal { D } ^ { ( s ) } sim theta ^ { * }$ for brevity. Now apply the estimator to each $mathcal { D } ^ { ( s ) }$ to get a set of estimates, ${ hat { pmb { theta } } ( mathcal { D } ^ { ( s ) } ) }$ . As we let $S to infty$ , the distribution induced by this set is the sampling distribution of the estimator. More precisely, we have \nIn some cases, we can compute this analytically, as we discuss in Section 4.7.2, although typically we need to approximate it by Monte Carlo, as we discuss in Section 4.7.3. \n4.7.2 Gaussian approximation of the sampling distribution of the MLE \nThe most common estimator is the MLE. When the sample size becomes large, the sampling distribution of the MLE for certain models becomes Gaussian. This is known as the asymptotic normality of the sampling distribution. More formally, we have the following result: \nTheorem 4.7.1. If the parameters are identifiable, then \nwhere $mathbf { F } ( pmb theta ^ { * } )$ is the Fisher information matrix, defined in Equation (4.220). \nThe Fisher information matrix measures the amount of curvature of the log-likelihood surface at its peak, as we show below. \nMore formally, the Fisher information matrix (FIM) is defined to be the covariance of the gradient of the log likelihood (also called the score function): \nHence the $( i , j )$ ’th entry has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nOne can show the following result. \nTheorem 4.7.2. If $log p ( { pmb x } | { pmb theta } )$ is twice differentiable, and under certain regularity conditions, the FIM is equal to the expected Hessian of the NLL, i.e., \nThus we can interpret the FIM as the Hessian of the NLL. \nThis helps us understand the result in Equation (4.219): a log-likelihood function with high curvature (large Hessian) will result in a low variance estimate, since the parameters are “well determined” by the data, and hence robust to repeated sampling. \n4.7.3 Bootstrap approximation of the sampling distribution of any estimator \nIn cases where the estimator is a complex function of the data (e.g., not just an MLE), or when the sample size is small, we can approximate its sampling distribution using a Monte Carlo technique known as the bootstrap. \nThe idea is simple. If we knew the true parameters $theta ^ { * }$ , we could generate many (say $S$ ) fake datasets, each of size $N _ { mathcal { D } }$ , from the true distribution, using ${ tilde { mathcal { D } } } ^ { ( s ) } = { mathbf { x } _ { n } sim p ( mathbf { boldsymbol { x } } _ { n } | mathbf { boldsymbol { theta } } ^ { * } ) : n = 1 : N }$ . We could then compute our estimate from each sample, $hat { pmb { theta } } ^ { s } = pi ( tilde { mathcal { D } } ^ { ( s ) } )$ and use the empirical distribution of the resulting $hat { pmb { theta } } ^ { s }$ as our estimate of the sampling distribution, as in Equation (4.218). Since $theta ^ { * }$ is unknown, the idea of the parametric bootstrap is to generate each sampled dataset using $ { hat { theta } } = pi (  { D } )$ instead of $pmb { theta } ^ { * }$ , i.e., we use $tilde { mathcal { D } } ^ { ( s ) } = { pmb { x } _ { n } sim p ( pmb { x } _ { n } | hat { pmb { theta } } ) : n = 1 : N }$ in Equation (4.218). This is a plug-in approximation to the sampling distribution. \nThe above approach requires that we have a parametric generative model for the data, $p ( { boldsymbol { mathbf { mathit { x } } } } | mathbf { boldsymbol { theta } } )$ . An alternative, called the non-parametric bootstrap, is to sample $N$ data points from the original dataset with replacement. This creates a new distribution $mathcal { D } ^ { ( s ) }$ which has the same size as the original. However, the number of unique data points in a bootstrap sample is just $0 . 6 3 2 times N$ , on average. (To see this, note that the probability an item is picked at least once is $( 1 - ( 1 - 1 / N ) ^ { N } )$ , which approaches $1 - e ^ { - 1 } approx 0 . 6 3 2$ for large $N$ .) \nFigure 4.23(a-b) shows an example where we compute the sampling distribution of the MLE for a Bernoulli using the parametric bootstrap. (Results using the non-parametric bootstrap are essentially the same.) When $N = 1 0$ , we see that the sampling distribution is asymmetric, and therefore quite far from Gaussian, but when $N _ { mathit { D } } = 1 0 0$ , the distribution looks more Gaussian, as theory suggests (see Section 4.7.2). \n4.7.3.1 Bootstrap is a “poor man’s” posterior \nA natural question is: what is the connection between the parameter estimates $hat { pmb { theta } } ^ { s } = pi ( mathcal { D } ^ { ( s ) } )$ computed by the bootstrap and parameter values sampled from the posterior, $theta ^ { s } sim p ( cdot | mathcal { D } )$ ? Conceptually they are quite different. But in the common case that the estimator is MLE and the prior is not very strong, they can be quite similar. For example, Figure 4.23(c-d) shows an example where we compute the posterior using a uniform Beta(1,1) prior, and then sample from it. We see that the posterior and the sampling distribution are quite similar. So one can think of the bootstrap distribution as a “poor man’s” posterior [HTF01, p235]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Frequentist statistics *",
        "subsubsection": "Gaussian approximation of the sampling distribution of the MLE"
    },
    {
        "content": "One can show the following result. \nTheorem 4.7.2. If $log p ( { pmb x } | { pmb theta } )$ is twice differentiable, and under certain regularity conditions, the FIM is equal to the expected Hessian of the NLL, i.e., \nThus we can interpret the FIM as the Hessian of the NLL. \nThis helps us understand the result in Equation (4.219): a log-likelihood function with high curvature (large Hessian) will result in a low variance estimate, since the parameters are “well determined” by the data, and hence robust to repeated sampling. \n4.7.3 Bootstrap approximation of the sampling distribution of any estimator \nIn cases where the estimator is a complex function of the data (e.g., not just an MLE), or when the sample size is small, we can approximate its sampling distribution using a Monte Carlo technique known as the bootstrap. \nThe idea is simple. If we knew the true parameters $theta ^ { * }$ , we could generate many (say $S$ ) fake datasets, each of size $N _ { mathcal { D } }$ , from the true distribution, using ${ tilde { mathcal { D } } } ^ { ( s ) } = { mathbf { x } _ { n } sim p ( mathbf { boldsymbol { x } } _ { n } | mathbf { boldsymbol { theta } } ^ { * } ) : n = 1 : N }$ . We could then compute our estimate from each sample, $hat { pmb { theta } } ^ { s } = pi ( tilde { mathcal { D } } ^ { ( s ) } )$ and use the empirical distribution of the resulting $hat { pmb { theta } } ^ { s }$ as our estimate of the sampling distribution, as in Equation (4.218). Since $theta ^ { * }$ is unknown, the idea of the parametric bootstrap is to generate each sampled dataset using $ { hat { theta } } = pi (  { D } )$ instead of $pmb { theta } ^ { * }$ , i.e., we use $tilde { mathcal { D } } ^ { ( s ) } = { pmb { x } _ { n } sim p ( pmb { x } _ { n } | hat { pmb { theta } } ) : n = 1 : N }$ in Equation (4.218). This is a plug-in approximation to the sampling distribution. \nThe above approach requires that we have a parametric generative model for the data, $p ( { boldsymbol { mathbf { mathit { x } } } } | mathbf { boldsymbol { theta } } )$ . An alternative, called the non-parametric bootstrap, is to sample $N$ data points from the original dataset with replacement. This creates a new distribution $mathcal { D } ^ { ( s ) }$ which has the same size as the original. However, the number of unique data points in a bootstrap sample is just $0 . 6 3 2 times N$ , on average. (To see this, note that the probability an item is picked at least once is $( 1 - ( 1 - 1 / N ) ^ { N } )$ , which approaches $1 - e ^ { - 1 } approx 0 . 6 3 2$ for large $N$ .) \nFigure 4.23(a-b) shows an example where we compute the sampling distribution of the MLE for a Bernoulli using the parametric bootstrap. (Results using the non-parametric bootstrap are essentially the same.) When $N = 1 0$ , we see that the sampling distribution is asymmetric, and therefore quite far from Gaussian, but when $N _ { mathit { D } } = 1 0 0$ , the distribution looks more Gaussian, as theory suggests (see Section 4.7.2). \n4.7.3.1 Bootstrap is a “poor man’s” posterior \nA natural question is: what is the connection between the parameter estimates $hat { pmb { theta } } ^ { s } = pi ( mathcal { D } ^ { ( s ) } )$ computed by the bootstrap and parameter values sampled from the posterior, $theta ^ { s } sim p ( cdot | mathcal { D } )$ ? Conceptually they are quite different. But in the common case that the estimator is MLE and the prior is not very strong, they can be quite similar. For example, Figure 4.23(c-d) shows an example where we compute the posterior using a uniform Beta(1,1) prior, and then sample from it. We see that the posterior and the sampling distribution are quite similar. So one can think of the bootstrap distribution as a “poor man’s” posterior [HTF01, p235]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHowever, perhaps surprisingly, bootstrap can be slower than posterior sampling. The reason is that the bootstrap has to generate $S$ sampled datasets, and then fit a model to each one. By contrast, in posterior sampling, we only have to “fit” a model once given a single dataset. (Some methods for speeding up the bootstrap when applied to massive data sets are discussed in [Kle+11].) \n4.7.4 Confidence intervals \nIn frequentist statistics, we use the variability induced by the sampling distribution as a way to estimate uncertainty of a parameter estimate. More precisely, we define a $1 0 0 ( 1 - alpha ) %$ confidence interval for a parameter estimate $theta$ as any interval $I ( tilde { mathcal { D } } ) = ( ell ( tilde { mathcal { D } } ) , u ( tilde { mathcal { D } } ) )$ derived from a hypothetical dataset $ddot { mathcal { D } }$ such that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Frequentist statistics *",
        "subsubsection": "Bootstrap approximation of the sampling distribution of any estimator"
    },
    {
        "content": "However, perhaps surprisingly, bootstrap can be slower than posterior sampling. The reason is that the bootstrap has to generate $S$ sampled datasets, and then fit a model to each one. By contrast, in posterior sampling, we only have to “fit” a model once given a single dataset. (Some methods for speeding up the bootstrap when applied to massive data sets are discussed in [Kle+11].) \n4.7.4 Confidence intervals \nIn frequentist statistics, we use the variability induced by the sampling distribution as a way to estimate uncertainty of a parameter estimate. More precisely, we define a $1 0 0 ( 1 - alpha ) %$ confidence interval for a parameter estimate $theta$ as any interval $I ( tilde { mathcal { D } } ) = ( ell ( tilde { mathcal { D } } ) , u ( tilde { mathcal { D } } ) )$ derived from a hypothetical dataset $ddot { mathcal { D } }$ such that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIt is common to set $alpha = 0 . 0 5$ , which yields a 95% CI. This means that, if we repeatedly sampled data, and compute $I ( tilde { mathcal { D } } )$ for each such dataset, then about $9 5 %$ of such intervals will contain the true parameter $theta$ . \nNote, however, that Equation (4.223) does not mean that for any particular dataset that $theta in I ( mathcal { D } )$ with $9 5 %$ probability; this is what a Bayesian credible interval computes (Section 4.6.6), but is not what a frequentist confidence interval computes. For more details on this important distinction, see Section 4.7.5. \nLet us put aside such “philosophical” concerns, and discuss how to compute a confidence interval. Suppose that $hat { theta }$ is an estimate of the parameter $theta$ . Let $theta ^ { * }$ be its true but unknown value. Also, suppose that the sampling distribution of $Delta = widehat { theta } - theta ^ { * }$ is known. Let $underline { { delta } }$ and $overline { { delta } }$ denote its $alpha / 2$ and $1 - alpha / 2$ quantiles. Hence \nRearranging we get \nAnd hence \nis a $1 0 0 ( 1 - alpha ) %$ confidence interval. \nIn some cases, we can analytically compute the distribution of $Delta = hat { theta } - theta ^ { * }$ . This can be used to derive exact confidence intervals. However, it is more common to assume a Gaussian approximation to the sampling distribution, as in Section 4.7.2. In this case, we have $sqrt { N F ( hat { theta } ) } ( hat { theta } - theta ^ { * } ) sim mathcal { N } ( 0 , 1 )$ . Hence we can compute an approximate CI using \nwhere $z _ { alpha / 2 }$ is the $alpha / 2$ quantile of the Gaussian cdf, and $hat { mathrm { s e } } = { 1 } / { sqrt { N F ( hat { theta } ) } }$ is the estimated standard error. If we set $alpha = 0 . 0 5$ , we have $z _ { alpha / 2 } = 1 . 9 6$ , which justifies the common approximation $hat { theta } pm 2 mathrm { s } mathrm { hat { e } }$ . \nIf the Gaussian approximation is not a good one, we can use a bootstrap approximation (see Section 4.7.3). In particular, we sample $S$ datasets from $ { hat { theta } } (  { mathcal { D } } )$ , and apply the estimator to each one to get $hat { theta } ( mathcal { D } ^ { ( s ) } )$ ; we then use the empirical distribution of $hat { theta } ( mathcal { D } ) - hat { theta } ( mathcal { D } ^ { ( s ) } )$ as an approximation to the sampling distribution of $Delta$ . \n4.7.5 Caution: Confidence intervals are not credible \nA $9 5 %$ frequentist confidence interval for a parameter $theta$ is defined as any interval $I ( tilde { mathcal { D } } )$ such that $operatorname* { P r } ( theta in I ( tilde { mathcal { D } } ) | tilde { mathcal { D } } sim theta ) = 0 . 9 5$ , as we explain in Section 4.7.4. This does not mean that the parameter is $9 5 %$ likely to live inside this interval given the observed data. That quantity — which is usually what we want to compute — is instead given by the Bayesian credible interval $p ( theta in I | mathcal { D } )$ , as we explain in Section 4.6.6. These concepts are quite different: In the frequentist approach, $theta$ is treated as an unknown fixed constant, and the data is treated as random. In the Bayesian approach, we treat the data as fixed (since it is known) and the parameter as random (since it is unknown). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Frequentist statistics *",
        "subsubsection": "Confidence intervals"
    },
    {
        "content": "It is common to set $alpha = 0 . 0 5$ , which yields a 95% CI. This means that, if we repeatedly sampled data, and compute $I ( tilde { mathcal { D } } )$ for each such dataset, then about $9 5 %$ of such intervals will contain the true parameter $theta$ . \nNote, however, that Equation (4.223) does not mean that for any particular dataset that $theta in I ( mathcal { D } )$ with $9 5 %$ probability; this is what a Bayesian credible interval computes (Section 4.6.6), but is not what a frequentist confidence interval computes. For more details on this important distinction, see Section 4.7.5. \nLet us put aside such “philosophical” concerns, and discuss how to compute a confidence interval. Suppose that $hat { theta }$ is an estimate of the parameter $theta$ . Let $theta ^ { * }$ be its true but unknown value. Also, suppose that the sampling distribution of $Delta = widehat { theta } - theta ^ { * }$ is known. Let $underline { { delta } }$ and $overline { { delta } }$ denote its $alpha / 2$ and $1 - alpha / 2$ quantiles. Hence \nRearranging we get \nAnd hence \nis a $1 0 0 ( 1 - alpha ) %$ confidence interval. \nIn some cases, we can analytically compute the distribution of $Delta = hat { theta } - theta ^ { * }$ . This can be used to derive exact confidence intervals. However, it is more common to assume a Gaussian approximation to the sampling distribution, as in Section 4.7.2. In this case, we have $sqrt { N F ( hat { theta } ) } ( hat { theta } - theta ^ { * } ) sim mathcal { N } ( 0 , 1 )$ . Hence we can compute an approximate CI using \nwhere $z _ { alpha / 2 }$ is the $alpha / 2$ quantile of the Gaussian cdf, and $hat { mathrm { s e } } = { 1 } / { sqrt { N F ( hat { theta } ) } }$ is the estimated standard error. If we set $alpha = 0 . 0 5$ , we have $z _ { alpha / 2 } = 1 . 9 6$ , which justifies the common approximation $hat { theta } pm 2 mathrm { s } mathrm { hat { e } }$ . \nIf the Gaussian approximation is not a good one, we can use a bootstrap approximation (see Section 4.7.3). In particular, we sample $S$ datasets from $ { hat { theta } } (  { mathcal { D } } )$ , and apply the estimator to each one to get $hat { theta } ( mathcal { D } ^ { ( s ) } )$ ; we then use the empirical distribution of $hat { theta } ( mathcal { D } ) - hat { theta } ( mathcal { D } ^ { ( s ) } )$ as an approximation to the sampling distribution of $Delta$ . \n4.7.5 Caution: Confidence intervals are not credible \nA $9 5 %$ frequentist confidence interval for a parameter $theta$ is defined as any interval $I ( tilde { mathcal { D } } )$ such that $operatorname* { P r } ( theta in I ( tilde { mathcal { D } } ) | tilde { mathcal { D } } sim theta ) = 0 . 9 5$ , as we explain in Section 4.7.4. This does not mean that the parameter is $9 5 %$ likely to live inside this interval given the observed data. That quantity — which is usually what we want to compute — is instead given by the Bayesian credible interval $p ( theta in I | mathcal { D } )$ , as we explain in Section 4.6.6. These concepts are quite different: In the frequentist approach, $theta$ is treated as an unknown fixed constant, and the data is treated as random. In the Bayesian approach, we treat the data as fixed (since it is known) and the parameter as random (since it is unknown). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis counter-intuitive definition of confidence intervals can lead to bizarre results. Consider the following example from [Ber85, p11]. Suppose we draw two integers $mathcal { D } = ( y _ { 1 } , y _ { 2 } )$ from \nIf $theta = 3 9$ , we would expect the following outcomes each with probability 0.25: \nLet $m = operatorname* { m i n } ( y _ { 1 } , y _ { 2 } )$ and define the following interval: \nFor the above samples this yields \nHence Equation (4.230) is clearly a $7 5 %$ CI, since 39 is contained in $3 / 4$ of these intervals. However, if we observe $mathcal { D } = ( 3 9 , 4 0 )$ then $p ( theta = 3 9 | D ) = 1 . 0$ , so we know that $theta$ must be 39, yet we only have $7 5 %$ “confidence” in this fact. We see that the CI will “cover” the true parameter $7 5 %$ of the time, if we compute multiple CIs from different randomly sampled datasets, but if we just have a single observed dataset, and hence a single CI, then the frequentist “coverage” probability can be very misleading. \nAnother, less contrived, example is as follows. Suppose we want to estimate the parameter $theta$ of a Bernoulli distribution. Let $begin{array} { r } { overline { { y } } = frac { 1 } { N _ { D } } sum _ { n = 1 } ^ { N _ { D } } y _ { n } } end{array}$ be the sample mean. The MLE is $hat { theta } = overline { { y } }$ . An approximate $9 5 %$ confidence interval for a Bernoulli parameter is $overline { { y } } pm 1 . 9 6 sqrt { overline { { y } } ( 1 - overline { { y } } ) / N _ { D } }$ (this is called a Wald interval and is based on a Gaussian approximation to the Binomial distribution; compare to Equation (4.128)). Now consider a single trial, where $N _ { mathcal { D } } = 1$ and $y _ { 1 } = 0$ . The MLE is 0, which overfits, as we saw in Section 4.5.1. But our $9 5 %$ confidence interval is also $( 0 , 0 )$ , which seems even worse. It can be argued that the above flaw is because we approximated the true sampling distribution with a Gaussian, or because the sample size was too small, or the parameter “too extreme”. However, the Wald interval can behave badly even for large $N _ { mathcal { D } }$ , and non-extreme parameters [BCD01]. By contrast, a Bayesian credible interval with a non-informative Jeffreys prior behaves in the way we would expect. \nSeveral more interesting examples, along with Python code, can be found at [Van14]. See also [Hoe+14; Mor+16; Lyu+20; Cha+19b], who show that many people, including professional statisticians, misunderstand and misuse frequentist confidence intervals in practice, whereas Bayesian credible intervals do not suffer from these problems. \n4.7.6 The bias-variance tradeoff \nAn estimator is a procedure applied to data which returns an estimand. Let $hat { pmb theta } ( )$ be the estimator, and $ { hat { theta } } (  { mathcal { D } } )$ be the estimand. In frequentist statistics, we treat the data as a random variable, drawn from some true but unknown distribution, $p ^ { * } ( mathcal { D } )$ ; this induces a distribution over the estimand, $p ^ { * } ( hat { pmb { theta } } ( { mathcal { D } } ) )$ , known as the sampling distribution (see Section 4.7.1). In this section, we discuss two key properties of this distribution, its bias and its variance, which we define below. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Frequentist statistics *",
        "subsubsection": "Caution: Confidence intervals are not credible"
    },
    {
        "content": "This counter-intuitive definition of confidence intervals can lead to bizarre results. Consider the following example from [Ber85, p11]. Suppose we draw two integers $mathcal { D } = ( y _ { 1 } , y _ { 2 } )$ from \nIf $theta = 3 9$ , we would expect the following outcomes each with probability 0.25: \nLet $m = operatorname* { m i n } ( y _ { 1 } , y _ { 2 } )$ and define the following interval: \nFor the above samples this yields \nHence Equation (4.230) is clearly a $7 5 %$ CI, since 39 is contained in $3 / 4$ of these intervals. However, if we observe $mathcal { D } = ( 3 9 , 4 0 )$ then $p ( theta = 3 9 | D ) = 1 . 0$ , so we know that $theta$ must be 39, yet we only have $7 5 %$ “confidence” in this fact. We see that the CI will “cover” the true parameter $7 5 %$ of the time, if we compute multiple CIs from different randomly sampled datasets, but if we just have a single observed dataset, and hence a single CI, then the frequentist “coverage” probability can be very misleading. \nAnother, less contrived, example is as follows. Suppose we want to estimate the parameter $theta$ of a Bernoulli distribution. Let $begin{array} { r } { overline { { y } } = frac { 1 } { N _ { D } } sum _ { n = 1 } ^ { N _ { D } } y _ { n } } end{array}$ be the sample mean. The MLE is $hat { theta } = overline { { y } }$ . An approximate $9 5 %$ confidence interval for a Bernoulli parameter is $overline { { y } } pm 1 . 9 6 sqrt { overline { { y } } ( 1 - overline { { y } } ) / N _ { D } }$ (this is called a Wald interval and is based on a Gaussian approximation to the Binomial distribution; compare to Equation (4.128)). Now consider a single trial, where $N _ { mathcal { D } } = 1$ and $y _ { 1 } = 0$ . The MLE is 0, which overfits, as we saw in Section 4.5.1. But our $9 5 %$ confidence interval is also $( 0 , 0 )$ , which seems even worse. It can be argued that the above flaw is because we approximated the true sampling distribution with a Gaussian, or because the sample size was too small, or the parameter “too extreme”. However, the Wald interval can behave badly even for large $N _ { mathcal { D } }$ , and non-extreme parameters [BCD01]. By contrast, a Bayesian credible interval with a non-informative Jeffreys prior behaves in the way we would expect. \nSeveral more interesting examples, along with Python code, can be found at [Van14]. See also [Hoe+14; Mor+16; Lyu+20; Cha+19b], who show that many people, including professional statisticians, misunderstand and misuse frequentist confidence intervals in practice, whereas Bayesian credible intervals do not suffer from these problems. \n4.7.6 The bias-variance tradeoff \nAn estimator is a procedure applied to data which returns an estimand. Let $hat { pmb theta } ( )$ be the estimator, and $ { hat { theta } } (  { mathcal { D } } )$ be the estimand. In frequentist statistics, we treat the data as a random variable, drawn from some true but unknown distribution, $p ^ { * } ( mathcal { D } )$ ; this induces a distribution over the estimand, $p ^ { * } ( hat { pmb { theta } } ( { mathcal { D } } ) )$ , known as the sampling distribution (see Section 4.7.1). In this section, we discuss two key properties of this distribution, its bias and its variance, which we define below. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n4.7.6.1 Bias of an estimator \nThe bias of an estimator is defined as \nwhere $theta ^ { * }$ is the true parameter value, and the expectation is wrt “nature’s distribution” $p ( mathcal { D } | theta ^ { * } )$ . If the bias is zero, the estimator is called unbiased. For example, the MLE for a Gaussian mean is unbiased: \nwhere $textstyle { overline { { x } } }$ is the sample mean. \nHowever, the MLE for a Gaussian variance, $begin{array} { r } { sigma _ { mathrm { m l e } } ^ { 2 } = frac { 1 } { N } sum _ { n = 1 } ^ { N } ( x _ { n } - overline { { x } } ) ^ { 2 } } end{array}$ , is not an unbiased estimator of $sigma ^ { 2 }$ . In fact, one can show (Exercise 4.7) that \nso the ML estimator slightly underestimates the variance. Intuitively, this is because we “use up” one of the data points to estimate the mean, so if we have a sample size of 1, we will estimate the variance to be 0. If, however, $mu$ is known, the ML estimator is unbiased (see Exercise 4.8). \nNow consider the following estimator \nThis is an unbiased estimator, which we can easily prove as follows: \n4.7.6.2 Variance of an estimator \nIt seems intuitively reasonable that we want our estimator to be unbiased. However, being unbiased is not enough. For example, suppose we want to estimate the mean of a Gaussian from $mathcal { D } =$ ${ x _ { 1 } , ldots , x _ { N _ { D } } }$ . The estimator that just looks at the first data point, $hat { theta } ( mathcal { D } ) = x _ { 1 }$ , is an unbiased estimator, but will generally be further from $theta ^ { * }$ than the empirical mean $textstyle { overline { { x } } }$ (which is also unbiased). So the variance of an estimator is also important. \nWe define the variance of an estimator as follows: \nwhere the expectation is taken wrt $p ( mathcal { D } | theta ^ { * } )$ . This measures how much our estimate will change as the data changes. We can extend this to a covariance matrix for vector valued estimators. \nIntuitively we would like the variance of our estimator to be as small as possible. Therefore, a natural question is: how low can the variance go? A famous result, called the Cramer-Rao lower \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 bound, provides a lower bound on the variance of any unbiased estimator. More precisely, let $X _ { 1 } , dots , X _ { N } sim p ( X | theta ^ { * } )$ and $hat { theta } = hat { theta } ( x _ { 1 } , ldots , x _ { N } )$ be an unbiased estimator of $theta ^ { * }$ . Then, under various smoothness assumptions on $p ( X | theta ^ { * } )$ , we have $begin{array} { r } { bar { boldsymbol { gamma } } left[ hat { boldsymbol { theta } } right] geq frac { 1 } { N F left( boldsymbol { theta } ^ { * } right) } } end{array}$ , where $F ( theta ^ { * } )$ is the Fisher information matrix (Section 4.7.2). A proof can be found e.g., in [Ric95, p275]. \n\nIt can be shown that the MLE achieves the Cramer Rao lower bound, and hence has the smallest asymptotic variance of any unbiased estimator. Thus MLE is said to be asymptotically optimal. \n4.7.6.3 The bias-variance tradeoff \nIn this section, we discuss a fundamental tradeoff that needs to be made when picking a method for parameter estimation, assuming our goal is to minimize the mean squared error (MSE) of our estimate. Let $ { hat { theta } } =  { hat { theta } } (  { mathcal { D } } )$ denote the estimate, and $overline { { theta } } = mathbb { E } left[ hat { theta } right]$ denote the expected value of the estimate (as we vary $mathcal { D }$ ). (All expectations and variances are wrt $p ( mathcal { D } | theta ^ { * } )$ , but we drop the explicit conditioning for notational brevity.) Then we have \nIn words, \nMSE = variance + bias2 \nThis is called the bias-variance tradeoff (see e.g., [GBD92]). What it means is that it might be wise to use a biased estimator, so long as it reduces our variance by more than the square of the bias, assuming our goal is to minimize squared error. \n4.7.6.4 Example: MAP estimator for a Gaussian mean \nLet us give an example, based on [Hof09, p79]. Suppose we want to estimate the mean of a Gaussian from $pmb { x } = ( x _ { 1 } , dots , x _ { N _ { mathcal { D } } } )$ . We assume the data is sampled from $x _ { n } sim { mathcal { N } } ( theta ^ { * } = 1 , sigma ^ { 2 } )$ . An obvious estimate is the MLE. This has a bias of 0 and a variance of \nBut we could also use a MAP estimate. In Section 4.6.4.2, we show that the MAP estimate under a Gaussian prior of the form $mathcal { N } ( theta _ { 0 } , sigma ^ { 2 } / kappa _ { 0 } )$ is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $0 leq w leq 1$ controls how much we trust the MLE compared to our prior. The bias and variance are given by \nSo although the MAP estimate is biased (assuming $w < 1$ ), it has lower variance. \nLet us assume that our prior is slightly misspecified, so we use $theta _ { 0 } = 0$ , whereas the truth is $theta ^ { * } = 1$ . In Figure 4.24(a), we see that the sampling distribution of the MAP estimate for $kappa _ { 0 } > 0$ is biased away from the truth, but has lower variance (is narrower) than that of the MLE. \nIn Figure 4.24(b), we plot $mathrm { m s e } ( tilde { x } ) / mathrm { m s e } ( overline { { x } } )$ vs $N$ . We see that the MAP estimate has lower MSE than the MLE for $kappa _ { 0 } in { 1 , 2 }$ . The case $kappa _ { 0 } = 0$ corresponds to the MLE, and the case $kappa _ { 0 } = 3$ corresponds to a strong prior, which hurts performance because the prior mean is wrong. Thus we see that, provided the prior strength is properly “tuned”, a MAP estimate can outperform an ML estimate in terms of minimizing MSE. \n4.7.6.5 Example: MAP estimator for linear regression \nAnother important example of the bias-variance tradeoff arises in ridge regression, which we discuss in Section 11.3. In brief, this corresponds to MAP estimation for linear regression under a Gaussian prior, $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , lambda ^ { - 1 } mathbf { I } )$ The zero-mean prior encourages the weights to be small, which reduces overfitting; the precision term, $lambda$ , controls the strength of this prior. Setting $lambda = 0$ results in the MLE; using $lambda > 0$ results in a biased estimate. To illustrate the effect on the variance, consider a simple example where we fit a 1d ridge regression model using 2 different values of $lambda$ . Figure 4.25 on \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the left plots each individual fitted curve, and on the right plots the average fitted curve. We see that as we increase the strength of the regularizer, the variance decreases, but the bias increases. See also Figure 4.26 where we give a cartoon sketch of the bias variance tradeoff in terms of model complexity. \n\n4.7.6.6 Bias-variance tradeoff for classification \nIf we use 0-1 loss instead of squared error, the frequentist risk is no longer expressible as squared bias plus variance. In fact, one can show (Exercise 7.2 of [HTF09]) that the bias and variance combine multiplicatively. If the estimate is on the correct side of the decision boundary, then the bias is negative, and decreasing the variance will decrease the misclassification rate. But if the estimate is on the wrong side of the decision boundary, then the bias is positive, so it pays to increase the variance [Fri97a]. This little known fact illustrates that the bias-variance tradeoff is not very useful for classification. It is better to focus on expected loss, not directly on bias and variance. We can approximate the expected loss using cross validation, as we discuss in Section 4.5.5. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n4.8 Exercises \nExercise 4.1 [MLE for the univariate Gaussian *] Show that the MLE for a univariate Gaussian is given by \nExercise 4.2 [MAP estimation for 1D Gaussians $^ *$ ] \n(Source: Jaakkola.) \nConsider samples $x _ { 1 } , ldots , x _ { n }$ from a Gaussian random variable with known variance $sigma ^ { 2 }$ and unknown mean $mu$ . We further assume a prior distribution (also Gaussian) over the mean, $mu sim mathcal N ( m , s ^ { 2 } )$ , with fixed mean $m$ and fixed variance $s ^ { 2 }$ . Thus the only unknown is $mu$ . \na. Calculate the MAP estimate $hat { mu } _ { M A P }$ . You can state the result without proof. Alternatively, with a lot more work, you can compute derivatives of the log posterior, set to zero and solve.   \nb. Show that as the number of samples $n$ increase, the MAP estimate converges to the maximum likelihood estimate.   \nc. Suppose $n$ is small and fixed. What does the MAP estimator converge to if we increase the prior variance $s ^ { 2 }$ ?   \nd. Suppose $n$ is small and fixed. What does the MAP estimator converge to if we decrease the prior variance $s ^ { 2 }$ ? \nExercise 4.3 [Gaussian posterior credible interval] \n(Source: DeGroot.) Let $X sim mathcal { N } ( mu , sigma ^ { 2 } = 4 )$ where $mu$ is unknown but has prior $mu sim mathcal N ( mu _ { 0 } , sigma _ { 0 } ^ { 2 } = 9 _ { , }$ ). The posterior after seeing $n$ samples is $mu sim mathcal N ( mu _ { n } , sigma _ { n } ^ { 2 } )$ . (This is called a credible interval, and is the Bayesian analog of a confidence interval.) How big does $n$ have to be to ensure \nwhere $( ell , u )$ is an interval (centered on $mu _ { n }$ ) of width 1 and $D$ is the data? Hint: recall that $9 5 %$ of the probability mass of a Gaussian is within $pm 1 . 9 6 sigma$ of the mean. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Frequentist statistics *",
        "subsubsection": "The bias-variance tradeoff"
    },
    {
        "content": "4.8 Exercises \nExercise 4.1 [MLE for the univariate Gaussian *] Show that the MLE for a univariate Gaussian is given by \nExercise 4.2 [MAP estimation for 1D Gaussians $^ *$ ] \n(Source: Jaakkola.) \nConsider samples $x _ { 1 } , ldots , x _ { n }$ from a Gaussian random variable with known variance $sigma ^ { 2 }$ and unknown mean $mu$ . We further assume a prior distribution (also Gaussian) over the mean, $mu sim mathcal N ( m , s ^ { 2 } )$ , with fixed mean $m$ and fixed variance $s ^ { 2 }$ . Thus the only unknown is $mu$ . \na. Calculate the MAP estimate $hat { mu } _ { M A P }$ . You can state the result without proof. Alternatively, with a lot more work, you can compute derivatives of the log posterior, set to zero and solve.   \nb. Show that as the number of samples $n$ increase, the MAP estimate converges to the maximum likelihood estimate.   \nc. Suppose $n$ is small and fixed. What does the MAP estimator converge to if we increase the prior variance $s ^ { 2 }$ ?   \nd. Suppose $n$ is small and fixed. What does the MAP estimator converge to if we decrease the prior variance $s ^ { 2 }$ ? \nExercise 4.3 [Gaussian posterior credible interval] \n(Source: DeGroot.) Let $X sim mathcal { N } ( mu , sigma ^ { 2 } = 4 )$ where $mu$ is unknown but has prior $mu sim mathcal N ( mu _ { 0 } , sigma _ { 0 } ^ { 2 } = 9 _ { , }$ ). The posterior after seeing $n$ samples is $mu sim mathcal N ( mu _ { n } , sigma _ { n } ^ { 2 } )$ . (This is called a credible interval, and is the Bayesian analog of a confidence interval.) How big does $n$ have to be to ensure \nwhere $( ell , u )$ is an interval (centered on $mu _ { n }$ ) of width 1 and $D$ is the data? Hint: recall that $9 5 %$ of the probability mass of a Gaussian is within $pm 1 . 9 6 sigma$ of the mean. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nExercise 4.4 [BIC for Gaussians $^ * ]$ \n(Source: Jaakkola.) \nThe Bayesian information criterion (BIC) is a penalized log-likelihood function that can be used for model selection. It is defined as \nwhere $d$ is the number of free parameters in the model and $N$ is the number of samples. In this question, we will see how to use this to choose between a full covariance Gaussian and a Gaussian with a diagonal covariance. Obviously a full covariance Gaussian has higher likelihood, but it may not be “worth” the extra parameters if the improvement over a diagonal covariance matrix is too small. So we use the BIC score to choose the model. \nWe can write \nwhere $hat { bf S }$ is the scatter matrix (empirical covariance), the trace of a matrix is the sum of its diagonals, and we have used the trace trick. \na. Derive the BIC score for a Gaussian in $mathcal { D }$ dimensions with full covariance matrix. Simplify your answer as much as possible, exploiting the form of the MLE. Be sure to specify the number of free parameters $d$ .   \nb. Derive the BIC score for a Gaussian in $D$ dimensions with a diagonal covariance matrix. Be sure to specify the number of free parameters $d$ . Hint: for the digaonal case, the ML estimate of $pmb { Sigma }$ is the same as $hat { mathbf { Sigma } } _ { hat { mathbf { Z } } _ { M L } }$ except the off-diagonal terms are zero: \nExercise 4.5 [BIC for a 2d discrete distribution] (Source: Jaakkola.) \nLet $x in { 0 , 1 }$ denote the result of a coin toss ( $x = 0$ for tails, $x = 1$ for heads). The coin is potentially biased, so that heads occurs with probability $theta _ { 1 }$ . Suppose that someone else observes the coin flip and reports to you the outcome, $y$ . But this person is unreliable and only reports the result correctly with probability $theta _ { 2 }$ ; i.e., $p ( boldsymbol { y } | boldsymbol { x } , theta _ { 2 } )$ is given by \nAssume that $theta _ { 2 }$ is independent of $x$ and $theta _ { 1 }$ . \na. Write down the joint probability distribution $p ( x , y | pmb theta )$ as a $2 times 2$ table, in terms of $pmb { theta } = ( theta _ { 1 } , theta _ { 2 } )$ . b. Suppose have the following dataset: $pmb { x } = ( 1 , 1 , 0 , 1 , 1 , 0 , 0 )$ , $pmb { y } = ( 1 , 0 , 0 , 0 , 1 , 0 , 1 )$ . What are the MLEs for $theta _ { 1 }$ and $theta _ { 2 }$ ? Justify your answer. Hint: note that the likelihood function factorizes, \nWhat is $p ( mathcal { D } | widehat { pmb { theta } } , M _ { 2 } )$ where $M _ { 2 }$ denotes this 2-parameter model? (You may leave your answer in fractional form if you wish.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nc. Now consider a model with 4 parameters, $pmb { theta } = ( theta _ { 0 , 0 } , theta _ { 0 , 1 } , theta _ { 1 , 0 } , theta _ { 1 , 1 } )$ , representing $p ( x , y | pmb { theta } ) = theta _ { x , y }$ . (Only 3 of these parameters are free to vary, since they must sum to one.) What is the MLE of $pmb theta$ ? What is $p ( mathcal { D } | hat { pmb { theta } } , M _ { 4 } )$ where $M _ { 4 }$ denotes this 4-parameter model? \nd. Suppose we are not sure which model is correct. We compute the leave-one-out cross validated log likelihood of the 2-parameter model and the 4-parameter model as follows: \nand $widehat { theta } big ( mathcal { D } _ { - i } big ) Big )$ ) denotes the MLE computed on $mathcal { D }$ excluding row $i$ . Which model will CV pick and why? Hint: notice how the table of counts changes when you omit each training case one at a time. \ne. Recall that an alternative to CV is to use the BIC score, defined as \nwhere $operatorname* { d o f } ( M )$ is the number of free parameters in the model, Compute the BIC scores for both models (use log base $e$ ). Which model does BIC prefer? \nExercise 4.6 [A mixture of conjugate priors is conjugate *] Consider a mixture prior \nwhere each $p ( theta | z = k )$ is conjugate to the likelihood. Prove that this is a conjugate prior. \nExercise 4.7 [ML estimator $sigma _ { mathrm { m l e } } ^ { 2 }$ is biased] \nShow that $begin{array} { r } { hat { sigma } _ { M L E } ^ { 2 } = frac { 1 } { N } sum _ { n = 1 } ^ { N } ( x _ { n } - hat { mu } ) ^ { 2 } } end{array}$ is a biased estimator of $sigma ^ { 2 }$ , i.e., show \nHint: note that $X _ { 1 } , ldots , X _ { N }$ are independent, and use the fact that the expectation of a product of independent random variables is the product of the expectations. \nExercise 4.8 [Estimation of $sigma ^ { 2 }$ when $mu$ is known $^ *$ ] \nSuppose we sample $x _ { 1 } , dots , x _ { N } sim { mathcal { N } } ( mu , sigma ^ { 2 } )$ where $mu$ is a known constant. Derive an expression for the MLE for $sigma ^ { 2 }$ in this case. Is it unbiased? \nExercise 4.9 [Variance and MSE of estimators for Gaussian variance $^ *$ ] Prove that the standard error for the MLE for a Gaussian variance is \nHint: use the fact that \nand that $mathbb { V } leftlfloor chi _ { N - 1 } ^ { 2 } rightrfloor = 2 ( N - 1 )$ . Finally, show that $begin{array} { r } { mathrm { M S E } ( sigma _ { mathrm { u n b } } ^ { 2 } ) = frac { 2 N - 1 } { N ^ { 2 } } sigma ^ { 4 } } end{array}$ and $begin{array} { r } { mathrm { M S E } ( sigma _ { mathrm { m l e } } ^ { 2 } ) = frac { 2 } { N - 1 } sigma ^ { 4 } } end{array}$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n5 Decision Theory \n5.1 Bayesian decision theory \nBayesian inference provides the optimal way to update our beliefs about hidden quantities $H$ given observed data $mathbf { X } = { boldsymbol { x } }$ by computing the posterior $p ( H | pmb { x } )$ . However, at the end of the day, we need to turn our beliefs into actions that we can perform in the world. How can we decide which action is best? This is where Bayesian decision theory comes in. In this chapter, we give a brief introduction. For more details, see e.g., [DeG70; KWW22]. \n5.1.1 Basics \nIn decision theory, we assume the decision maker, or agent, has a set of possible actions, $mathcal { A }$ , to choose from. For example, consider the case of a hypothetical doctor treating someone who may have COVID-19. Suppose the actions are to do nothing, or to give the patient an expensive drug with bad side effects, but which can save their life. \nEach of these actions has costs and benefits, which will depend on the underlying state of nature $H in { mathcal { H } }$ . We can encode this information into a loss function $ell ( h , a )$ , that specifies the loss we incur if we take action $a in { mathcal { A } }$ when the state of nature is $h in mathcal { H }$ . \nFor example, suppose the state is defined by the age of the patient (young vs old), and whether they have COVID-19 or not. Note that the age can be observed directly, but the disease state must be inferred from noisy observations, as we discussed in Section 2.3. Thus the state is partially observed. \nLet us assume that the cost of administering a drug is the same, no matter what the state of the patient is. However, the benefits will differ. If the patient is young, we expect them to live a long time, so the cost of not giving the drug if they have COVID-19 is high; but if the patient is old, they have fewer years to live, so the cost of not giving the drug if they have COVID-19 is arguably less (especially in view of the side effects). In medical circles, a common unit of cost is quality-adjusted life years or QALY. Suppose that the expected QALY for a young person is 60, and for an old person is 10. Let us assume the drug costs the equivalent of 8 QALY, due to induced pain and suffering from side effects. Then we get the loss matrix shown in Table 5.1. \nThese numbers reflect relative costs and benefits, and will depend on many factors. The numbers can be derived by asking the decision maker about their preferences about different possible outcomes. It is a theorem of decision theory that any consistent set of preferences can be converted into an ordinal cost scale (see e.g., https://en.wikipedia.org/wiki/Preference_(economics)). \nOnce we have specified the loss function, we can compute the posterior expected loss or risk",
        "chapter": "I Foundations",
        "section": "Statistics",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "5 Decision Theory \n5.1 Bayesian decision theory \nBayesian inference provides the optimal way to update our beliefs about hidden quantities $H$ given observed data $mathbf { X } = { boldsymbol { x } }$ by computing the posterior $p ( H | pmb { x } )$ . However, at the end of the day, we need to turn our beliefs into actions that we can perform in the world. How can we decide which action is best? This is where Bayesian decision theory comes in. In this chapter, we give a brief introduction. For more details, see e.g., [DeG70; KWW22]. \n5.1.1 Basics \nIn decision theory, we assume the decision maker, or agent, has a set of possible actions, $mathcal { A }$ , to choose from. For example, consider the case of a hypothetical doctor treating someone who may have COVID-19. Suppose the actions are to do nothing, or to give the patient an expensive drug with bad side effects, but which can save their life. \nEach of these actions has costs and benefits, which will depend on the underlying state of nature $H in { mathcal { H } }$ . We can encode this information into a loss function $ell ( h , a )$ , that specifies the loss we incur if we take action $a in { mathcal { A } }$ when the state of nature is $h in mathcal { H }$ . \nFor example, suppose the state is defined by the age of the patient (young vs old), and whether they have COVID-19 or not. Note that the age can be observed directly, but the disease state must be inferred from noisy observations, as we discussed in Section 2.3. Thus the state is partially observed. \nLet us assume that the cost of administering a drug is the same, no matter what the state of the patient is. However, the benefits will differ. If the patient is young, we expect them to live a long time, so the cost of not giving the drug if they have COVID-19 is high; but if the patient is old, they have fewer years to live, so the cost of not giving the drug if they have COVID-19 is arguably less (especially in view of the side effects). In medical circles, a common unit of cost is quality-adjusted life years or QALY. Suppose that the expected QALY for a young person is 60, and for an old person is 10. Let us assume the drug costs the equivalent of 8 QALY, due to induced pain and suffering from side effects. Then we get the loss matrix shown in Table 5.1. \nThese numbers reflect relative costs and benefits, and will depend on many factors. The numbers can be derived by asking the decision maker about their preferences about different possible outcomes. It is a theorem of decision theory that any consistent set of preferences can be converted into an ordinal cost scale (see e.g., https://en.wikipedia.org/wiki/Preference_(economics)). \nOnce we have specified the loss function, we can compute the posterior expected loss or risk \nfor each possible action: \nThe optimal policy (also called the Bayes estimator) specifies what action to take for each possible observation so as to minimize the risk: \nAn alternative, but equivalent, way of stating this result is as follows. Let us define a utility function $U ( h , a )$ to be the desirability of each possible action in each possible state. If we set $U ( h , a ) = - ell ( h , a )$ , then the optimal policy is as follows: \nThis is called the maximum expected utility principle. \nLet us return to our COVID-19 example. The observation $_ { x }$ consists of the age (young or old) and the test result (positive or negative). Using the results from Section 2.3.1 on Bayes rule for COVID-19 diagnosis, we can convert the test result into a distribution over disease states (i.e., compute the probability the patient has COVID-19 or not). Given this belief state, and the loss matrix in Table 5.1, we can compute the optimal policy for each possible observation, as shown in Table 5.2. \nWe see from Table 5.2 that the drug should only be given to young people who test positive. If, however, we reduce the cost of the drug from 8 units to 5, then the optimal policy changes: in this case, we should give the drug to everyone who tests positive. The policy can also change depending on the reliability of the test. For example, if we increase the sensitivity from 0.875 to 0.975, then \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the probability that someone has COVID-19 if they test positive increases from 0.80 to 0.81, which changes the optimal policy to be one in which we should administer the drug to everyone who tests positive, even if the drug costs 8 QALY. (See dtheory.ipynb for the code to reproduce this example.) So far, we have implicitly assumed that the agent is risk neutral. This means that their decision is not affected by the degree of certainty in a set of outcomes. For example, such an agent would be indifferent between getting $50 for sure, or a $5 0 %$ chance of $$ 100$ or $$ 0$ . By contrast, a risk averse agent would choose the first. We can generalize the framework of Bayesian decision theory to risk sensitive applications, but we do not pursue the matter here. (See e.g., [Cho+15] for details.) \n\n5.1.2 Classification problems \nIn this section, we use Bayesian decision theory to decide the optimal class label to predict given an observed input $mathbf { boldsymbol { x } } in mathcal { X }$ . \n5.1.2.1 Zero-one loss \nSuppose the states of nature correspond to class labels, so $mathcal { H } = mathcal { Y } = { 1 , . . . , C }$ . Furthermore, suppose the actions also correspond to class labels, so $mathcal { A } = mathcal { y }$ . In this setting, a very commonly used loss function is the zero-one loss $ell _ { 0 1 } ( y ^ { * } , hat { y } )$ , defined as follows: \nWe can write this more concisely as follows: \nIn this case, the posterior expected loss is \nHence the action that minimizes the expected loss is to choose the most probable label: \nThis corresponds to the mode of the posterior distribution, also known as the maximum a posteriori or MAP estimate. \n5.1.2.2 Cost-sensitive classification \nConsider a binary classification problem where the loss function is $ell ( y ^ { * } , hat { y } )$ is as follows: \nLet $p _ { 0 } = p ( y ^ { ast } = 0 | x )$ and $p _ { 1 } = 1 - p _ { 0 }$ . Thus we should choose label $hat { y } = 0$ iff \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Bayesian decision theory",
        "subsubsection": "Basics"
    },
    {
        "content": "5.1.2 Classification problems \nIn this section, we use Bayesian decision theory to decide the optimal class label to predict given an observed input $mathbf { boldsymbol { x } } in mathcal { X }$ . \n5.1.2.1 Zero-one loss \nSuppose the states of nature correspond to class labels, so $mathcal { H } = mathcal { Y } = { 1 , . . . , C }$ . Furthermore, suppose the actions also correspond to class labels, so $mathcal { A } = mathcal { y }$ . In this setting, a very commonly used loss function is the zero-one loss $ell _ { 0 1 } ( y ^ { * } , hat { y } )$ , defined as follows: \nWe can write this more concisely as follows: \nIn this case, the posterior expected loss is \nHence the action that minimizes the expected loss is to choose the most probable label: \nThis corresponds to the mode of the posterior distribution, also known as the maximum a posteriori or MAP estimate. \n5.1.2.2 Cost-sensitive classification \nConsider a binary classification problem where the loss function is $ell ( y ^ { * } , hat { y } )$ is as follows: \nLet $p _ { 0 } = p ( y ^ { ast } = 0 | x )$ and $p _ { 1 } = 1 - p _ { 0 }$ . Thus we should choose label $hat { y } = 0$ iff \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf $ell _ { 0 0 } = ell _ { 1 1 } = 0$ , this simplifies to \nNow suppose $ell _ { 1 0 } = c ell _ { 0 1 }$ , so a false negative costs $c$ times more than a false positive. The decision rule further simplifies to the following: pick $a = 0$ iff $p _ { 1 } < 1 / ( 1 + c )$ . For example, if a false negative costs twice as much as false positive, so $c = 2$ , then we use a decision threshold of $1 / 3$ before declaring a positive. \n5.1.2.3 Classification with the “reject” option \nIn some cases, we may able to say “I don’t know” instead of returning an answer that we don’t really trust; this is called picking the reject option (see e.g., [BW08]). This is particularly important in domains such as medicine and finance where we may be risk averse. \nWe can formalize the reject option as follows. Suppose the states of nature are $mathcal { H } = mathcal { Y } = { 1 , . . . , C }$ , and the actions are $mathcal { A } = mathcal { V } cup { 0 }$ , where action 0 represents the reject action. Now define the following loss function: \nwhere $lambda _ { r }$ is the cost of the reject action, and $lambda _ { e }$ is the cost of a classification error. Exercise 5.1 asks you to show that the optimal action is to pick the reject action if the most probable class has a probability below $begin{array} { r } { lambda ^ { * } = 1 - frac { lambda _ { r } } { lambda _ { e } } }  { . . } end{array}$ ; otherwise you should just pick the most probable class. In other words, the optimal policy is as follows: \nwhere \nSee Figure 5.1 for an illustration. \nOne interesting application of the reject option arises when playing the TV game show Jeopardy. In this game, contestants have to solve various word puzzles and answer a variety of trivia questions, but if they answer incorrectly, they lose money. In 2011, IBM unveiled a computer system called Watson which beat the top human Jeopardy champion. Watson uses a variety of interesting techniques [Fer+10], but the most pertinent one for our present discussion is that it contains a module that estimates how confident it is of its answer. The system only chooses to “buzz in” its answer if sufficiently confident it is correct. \nFor some other methods and applications, see e.g., [Cor+16; GEY19]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTable 5.3: Class confusion matrix for a binary classification problem. TP is the number of true positives, $F P$ is the number of false positives, TN is the number of true negatives, FN is the number of false negatives, $P$ is the true number of positives, $hat { P }$ is the predicted number of positives, $N$ is the true number of negatives, $hat { N }$ is the predicted number of negatives. \n5.1.3 ROC curves \nIn Section 5.1.2.2, we showed that we can pick the optimal label in a binary classification problem by thresholding the probability using a value $tau$ , derived from the relative cost of a false positive and false negative. Instead of picking a single threshold, we can consider using a set of different thresholds, and comparing the resulting performance, as we discuss below. \n5.1.3.1 Class confusion matrices \nFor any fixed threshold $tau$ , we consider the following decision rule: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Bayesian decision theory",
        "subsubsection": "Classification problems"
    },
    {
        "content": "Table 5.3: Class confusion matrix for a binary classification problem. TP is the number of true positives, $F P$ is the number of false positives, TN is the number of true negatives, FN is the number of false negatives, $P$ is the true number of positives, $hat { P }$ is the predicted number of positives, $N$ is the true number of negatives, $hat { N }$ is the predicted number of negatives. \n5.1.3 ROC curves \nIn Section 5.1.2.2, we showed that we can pick the optimal label in a binary classification problem by thresholding the probability using a value $tau$ , derived from the relative cost of a false positive and false negative. Instead of picking a single threshold, we can consider using a set of different thresholds, and comparing the resulting performance, as we discuss below. \n5.1.3.1 Class confusion matrices \nFor any fixed threshold $tau$ , we consider the following decision rule: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can compute the empirical number of false positives (FP) that arise from using this policy on a set of $N$ labeled examples as follows: \nSimilarly, we can compute the empirical number of false negatives (FN), true positives (TP), and true negatives (TN). We can store these results in a $2 times 2$ class confusion matrix $C$ , where $C _ { i j }$ is the number of times an item with true class label $i$ was (mis)classified as having label $j$ . In the case of binary classification problems, the resulting matrix will look like Table 5.3. \nFrom this table, we can compute $p ( hat { y } | y )$ or $p ( y | hat { y } )$ , depending on whether we normalize across the rows or columns. We can derive various summary statistics from these distributions, as summarized in Table 5.4 and Table 5.5. For example, the true positive rate (TPR), also known as the sensitivity, recall or hit rate, is defined as \nand the false positive rate (FPR), also called the false alarm rate, or the type I error rate, is defined as \nWe can now plot the TPR vs FPR as an implicit function of $tau$ . This is called a receiver operating characteristic or ROC curve. See Figure 5.2(a) for an example. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n5.1.3.2 Summarizing ROC curves as a scalar \nThe quality of a ROC curve is often summarized as a single number using the area under the curve or AUC. Higher AUC scores are better; the maximum is obviously 1. Another summary statistic that is used is the equal error rate or EER, also called the cross-over rate, defined as the value which satisfies FPR = FNR. Since FNR $circeq$ 1-TPR, we can compute the EER by drawing a line from the top left to the bottom right and seeing where it intersects the ROC curve (see points A and B in Figure 5.2(a)). Lower EER scores are better; the minimum is obviously 0 (corresponding to the top left corner). \n5.1.3.3 Class imbalance \nIn some problems, there is severe class imbalance. For example, in information retrieval, the set of negatives (irrelevant items) is usually much larger than the set of positives (relevant items). The ROC curve is unaffected by class imbalance, as the TPR and FPR are fractions within the positives and negatives, respectively. However, the usefulness of an ROC curve may be reduced in such cases, since a large change in the absolute number of false positives will not change the false positive rate very much, since FPR is divided by FP+TN (see e.g., [SR15] for discussion). Thus all the “action” happens in the extreme left part of the curve. In such cases, we may choose to use other ways of summarizing the class confusion matrix, such as precision-recall curves, which we discuss in Section 5.1.4. \n5.1.4 Precision-recall curves \nIn some problems, the notion of a “negative” is not well-defined. For example, consider detecting objects in images: if the detector works by classifying patches, then the number of patches examined and hence the number of true negatives — is a parameter of the algorithm, not part of the problem definition. Similarly, information retrieval systems usually get to choose the initial set of candidate \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license items, which are then ranked for relevance; by specifying a cutoff, we can partition this into a positive and negative set, but note that the size of the negative set depends on the total number of items retrieved, which is an algorithm parameter, not part of the problem specification.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Bayesian decision theory",
        "subsubsection": "ROC curves"
    },
    {
        "content": "5.1.3.2 Summarizing ROC curves as a scalar \nThe quality of a ROC curve is often summarized as a single number using the area under the curve or AUC. Higher AUC scores are better; the maximum is obviously 1. Another summary statistic that is used is the equal error rate or EER, also called the cross-over rate, defined as the value which satisfies FPR = FNR. Since FNR $circeq$ 1-TPR, we can compute the EER by drawing a line from the top left to the bottom right and seeing where it intersects the ROC curve (see points A and B in Figure 5.2(a)). Lower EER scores are better; the minimum is obviously 0 (corresponding to the top left corner). \n5.1.3.3 Class imbalance \nIn some problems, there is severe class imbalance. For example, in information retrieval, the set of negatives (irrelevant items) is usually much larger than the set of positives (relevant items). The ROC curve is unaffected by class imbalance, as the TPR and FPR are fractions within the positives and negatives, respectively. However, the usefulness of an ROC curve may be reduced in such cases, since a large change in the absolute number of false positives will not change the false positive rate very much, since FPR is divided by FP+TN (see e.g., [SR15] for discussion). Thus all the “action” happens in the extreme left part of the curve. In such cases, we may choose to use other ways of summarizing the class confusion matrix, such as precision-recall curves, which we discuss in Section 5.1.4. \n5.1.4 Precision-recall curves \nIn some problems, the notion of a “negative” is not well-defined. For example, consider detecting objects in images: if the detector works by classifying patches, then the number of patches examined and hence the number of true negatives — is a parameter of the algorithm, not part of the problem definition. Similarly, information retrieval systems usually get to choose the initial set of candidate \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license items, which are then ranked for relevance; by specifying a cutoff, we can partition this into a positive and negative set, but note that the size of the negative set depends on the total number of items retrieved, which is an algorithm parameter, not part of the problem specification. \n\nIn these kinds of situations, we may choose to use a precision-recall curve to summarize the performance of our system, as we explain below. (See [DG06] for a more detailed discussion of the connection between ROC curves and PR curves.) \n5.1.4.1 Computing precision and recall \nThe key idea is to replace the FPR with a quantity that is computed just from positives, namely the precision: \nThe precision measures what fraction of our detections are actually positive. We can compare this to the recall (which is the same as the TPR), which measures what fraction of the positives we actually detected: \nIf $hat { y } _ { n } in { 0 , 1 }$ is the predicted label, and $y _ { n } in { 0 , 1 }$ is the true label, we can estimate precision and recall using \nWe can now plot the precision vs recall as we vary the threshold $tau$ . See Figure 5.2(b). Hugging the top right is the best one can do. \n5.1.4.2 Summarizing PR curves as a scalar \nThe PR curve can be summarized as a single number in several ways. First, we can quote the precision for a fixed recall level, such as the precision of the first $K = 1 0$ entities recalled. This is called the precision at $mathbf { K }$ score. Alternatively, we can compute the area under the PR curve. However, it is possible that the precision does not drop monotonically with recall. For example, suppose a classifier has $9 0 %$ precision at $1 0 %$ recall, and $9 6 %$ precision at $2 0 %$ recall. In this case, rather than measuring the precision at a recall of $1 0 %$ , we should measure the maximum precision we can achieve with at least a recall of $1 0 %$ (which would be $9 6 %$ ). This is called the interpolated precision. The average of the interpolated precisions is called the average precision; it is equal to the area under the interpolated PR curve, but may not be equal to the area under the raw PR curve.1 The mean average precision or mAP is the mean of the AP over a set of different PR curves. \n5.1.4.3 F-scores \nFor a fixed threshold, corresponding to a single point on the PR curve, we can compute a single precision and recall value, which we will denote by $mathcal { P }$ and $mathcal { R }$ . These are often combined into a single statistic called the $F _ { beta }$ , defined as follows:2 \nor equivalently \nIf we set $beta = 1$ , we get the harmonic mean of precision and recall: \nTo understand why we use the harmonic mean instead of the arithmetic mean, $( mathcal { P } + mathcal { R } ) / 2$ , consider the following scenario. Suppose we recall all entries, so ${ hat { y } } _ { n } = 1$ for all $n$ , and $mathcal { R } = 1$ . In this case, the precision $mathcal { P }$ will be given by the prevalence, $begin{array} { r } { p ( y = 1 ) = frac { sum _ { n } mathbb { I } ( y _ { n } = 1 ) } { N } } end{array}$ . Suppose the prevalence is low, say $p ( y = 1 ) = 1 0 ^ { - 4 }$ . The arithmetic mean of $mathcal { P }$ and $mathcal { R }$ is given by $( mathcal { P } + mathcal { R } ) / 2 = ( 1 0 ^ { - 4 } + 1 ) / 2 approx 5 0 %$ . By contrast, the harmonic mean of this strategy is only $frac { 2 times 1 0 ^ { - 4 } times 1 } { 1 + 1 0 ^ { - 4 } } approx 0 . 0 2 %$ . In general, the harmonic mean is more conservative, and requires both precision and recall to be high. \nUsing $F _ { 1 }$ score weights precision and recall equally. However, if recall is more important, we may use $beta = 2$ , and if precision is more important, we may use $beta = 0 . 5$ . \n5.1.4.4 Class imbalance \nROC curves are insensitive to class imbalance, but PR curves are not, as noted in [Wil20]. To see this, let the fraction of positives in the dataset be $pi = P / ( P + N )$ , and define the ratio $r = P / N = pi / ( 1 - pi )$ . Let $n = P + N$ be the population size. ROC curves are not affected by changes in $r$ , since the TPR is defined as a ratio within the positive examples, and FPR is defined as a ratio within the negative examples. This means it does not matter which class we define as positive, and which we define as negative. \nNow consider PR curves. The precision can be written as \nThus $mathrm { P r e c }  1$ as $pi  1$ and $r  infty$ , and $mathrm { P r e c }  0$ as $pi  0$ and $r  0$ . For example, if we change from a balanced problem where $r = 0 . 5$ to an imbalanced problem where $r = 0 . 1$ (so positives are rarer), the precision at each threshold will drop, and the recall (aka TPR) will stay the same, so the overall PR curve will be lower. Thus if we have multiple binary problems with different prevalences (e.g., object detection of common or rare objects), we should be careful when averaging their precisions [HCD12]. \n\nThe F-score is also affected by class imbalance. To see this, note that we can rewrite the F-score as follows: \n5.1.5 Regression problems \nSo far, we have considered the case where there are a finite number of actions $mathcal { A }$ and states of nature $mathcal { H }$ . In this section, we consider the case where the set of actions and states are both equal to the real line, $mathcal { A } = mathcal { H } = mathbb { R }$ . We will specify various commonly used loss functions for this case (which can be extended to $mathbb { R } ^ { D }$ by computing the loss elementwise.) The resulting decision rules can be used to compute the optimal parameters for an estimator to return, or the optimal action for a robot to take, etc. \n5.1.5.1 L2 loss \nThe most common loss for continuous states and actions is the $ell _ { 2 }$ loss, also called squared error or quadratic loss, which is defined as follows: \nIn this case, the risk is given by \nThe optimal action must satisfy the condition that the derivative of the risk (at that point) is zero (as explained in Chapter 8). Hence the optimal action is to pick the posterior mean: \nThis is often called the minimum mean squared error estimate or MMSE estimate. \n5.1.5.2 L1 loss \nThe $ell _ { 2 }$ loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers. A more robust alternative is the absolute or $ell _ { 1 }$ loss \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Bayesian decision theory",
        "subsubsection": "Precision-recall curves"
    },
    {
        "content": "The F-score is also affected by class imbalance. To see this, note that we can rewrite the F-score as follows: \n5.1.5 Regression problems \nSo far, we have considered the case where there are a finite number of actions $mathcal { A }$ and states of nature $mathcal { H }$ . In this section, we consider the case where the set of actions and states are both equal to the real line, $mathcal { A } = mathcal { H } = mathbb { R }$ . We will specify various commonly used loss functions for this case (which can be extended to $mathbb { R } ^ { D }$ by computing the loss elementwise.) The resulting decision rules can be used to compute the optimal parameters for an estimator to return, or the optimal action for a robot to take, etc. \n5.1.5.1 L2 loss \nThe most common loss for continuous states and actions is the $ell _ { 2 }$ loss, also called squared error or quadratic loss, which is defined as follows: \nIn this case, the risk is given by \nThe optimal action must satisfy the condition that the derivative of the risk (at that point) is zero (as explained in Chapter 8). Hence the optimal action is to pick the posterior mean: \nThis is often called the minimum mean squared error estimate or MMSE estimate. \n5.1.5.2 L1 loss \nThe $ell _ { 2 }$ loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers. A more robust alternative is the absolute or $ell _ { 1 }$ loss \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis is sketched in Figure 5.3. Exercise 5.4 asks you to show that the optimal estimate is the posterior median, i.e., a value $a$ such that $operatorname* { P r } ( h < a | pmb { x } ) = operatorname* { P r } ( h geq a | pmb { x } ) = 0 . 5$ . We can use this for robust regression as discussed in Section 11.6.1. \n5.1.5.3 Huber loss \nAnother robust loss function is the Huber loss [Hub64], defined as follows: \nwhere $r = h - a$ . This is equivalent to $ell _ { 2 }$ for errors that are smaller than $delta$ , and is equivalent to $ell _ { 1 }$ for larger errors. See Figure 5.3 for a plot. We can use this for robust regression as discussed in Section 11.6.3. \n5.1.6 Probabilistic prediction problems \nIn Section 5.1.2, we assumed the set of possible actions was to pick a single class label (or possibly the “reject” or “do not know” action). In Section 5.1.5, we assumed the set of possible actions was to pick a real valued scalar. In this section, we assume the set of possible actions is to pick a probability distribution over some value of interest. That is, we want to perform probabilistic prediction or probabilistic forecasting, rather than predicting a specific value. More precisely, we assume the true “state of nature” is a distribution, $h = p ( Y | x )$ , the action is another distribution, $a = q ( Y | x )$ , and we want to pick $q$ to minimize $mathbb { E } left[ ell ( p , q ) right]$ for a given $x$ . We discuss various possible loss functions below. \n5.1.6.1 KL, cross-entropy and log-loss \nA common form of loss functions for comparing two distributions is the Kullback Leibler divergence, or KL divergence, which is defined as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license (We have assumed the variable $y$ is discrete, for notational simplicity, but this can be generalized to real-valued variables.) In Section 6.2, we show that the KL divergence satisfies the following properties: $D _ { mathbb { K L } } left( p parallel q right) ge 0$ with equality iff $p = q$ . Note that it is an asymmetric function of its arguments.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Bayesian decision theory",
        "subsubsection": "Regression problems"
    },
    {
        "content": "This is sketched in Figure 5.3. Exercise 5.4 asks you to show that the optimal estimate is the posterior median, i.e., a value $a$ such that $operatorname* { P r } ( h < a | pmb { x } ) = operatorname* { P r } ( h geq a | pmb { x } ) = 0 . 5$ . We can use this for robust regression as discussed in Section 11.6.1. \n5.1.5.3 Huber loss \nAnother robust loss function is the Huber loss [Hub64], defined as follows: \nwhere $r = h - a$ . This is equivalent to $ell _ { 2 }$ for errors that are smaller than $delta$ , and is equivalent to $ell _ { 1 }$ for larger errors. See Figure 5.3 for a plot. We can use this for robust regression as discussed in Section 11.6.3. \n5.1.6 Probabilistic prediction problems \nIn Section 5.1.2, we assumed the set of possible actions was to pick a single class label (or possibly the “reject” or “do not know” action). In Section 5.1.5, we assumed the set of possible actions was to pick a real valued scalar. In this section, we assume the set of possible actions is to pick a probability distribution over some value of interest. That is, we want to perform probabilistic prediction or probabilistic forecasting, rather than predicting a specific value. More precisely, we assume the true “state of nature” is a distribution, $h = p ( Y | x )$ , the action is another distribution, $a = q ( Y | x )$ , and we want to pick $q$ to minimize $mathbb { E } left[ ell ( p , q ) right]$ for a given $x$ . We discuss various possible loss functions below. \n5.1.6.1 KL, cross-entropy and log-loss \nA common form of loss functions for comparing two distributions is the Kullback Leibler divergence, or KL divergence, which is defined as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license (We have assumed the variable $y$ is discrete, for notational simplicity, but this can be generalized to real-valued variables.) In Section 6.2, we show that the KL divergence satisfies the following properties: $D _ { mathbb { K L } } left( p parallel q right) ge 0$ with equality iff $p = q$ . Note that it is an asymmetric function of its arguments. \n\nWe can expand the KL as follows: \nThe $mathbb H ( p )$ term is known as the entropy. This is a measure of uncertainty or variance of $p$ ; it is maximal if $p$ is uniform, and is 0 if $p$ is a degenerate or deterministic delta function. Entropy is often used in the field of information theory, which is concerned with optimal ways of compressing and communicating data (see Chapter 6). The optimal coding scheme will allocate fewer bits to more frequent symbols (i.e., values of $Y$ for which $p ( y )$ is large), and more bits to less frequent symbols. A key result states that the number of bits needed to compress a dataset generated by a distribution $p$ is at least $mathbb { H } ( p )$ ; the entropy therefore provides a lower bound on the degree to which we can compress data without losing information. The $mathbb { H } _ { c e } ( p , q )$ term is known as the cross-entropy. This measures the expected number of bits we need to use to compress a dataset coming from distribution $p$ if we design our code using distribution $q$ . Thus the KL is the extra number of bits we need to use to compress the data due to using the incorrect distribution $q$ . If the KL is zero, it means that we can correctly predict the probabilities of all possible future events, and thus we have learned to predict the future as well as an “oracle” that has access to the true distribution $p$ . \nTo find the optimal distribution to use when predicting future data, we can minimize $D _ { mathbb { K L } } left( p parallel q right)$ . Since $mathbb H ( p )$ is a constant wrt $q$ , it can be ignored, and thus we can equivalently minimize the cross-entropy: \nNow consider the special case in which the true state of nature is a degenerate distribution, which puts all its mass on a single outcome, say $c$ , i.e., $h = p ( Y | x ) = mathbb { I } left( Y = c right)$ . This is often called a “one-hot” distribution, since it turns “on” the $c$ ’th element of the vector, and leaves the other elements “off”, as shown in Figure 2.1. In this case, the cross entropy becomes \nThis is known as the log loss of the predictive distribution $q$ when given target label $c$ . \n5.1.6.2 Proper scoring rules \nCross-entropy loss is a very common choice for probabilistic forecasting, but is not the only possible metric. The key property we desire is that the loss function is minimized iff the decision maker picks \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nthe distribution $q$ that matches the true distribution $p$ , i.e., $ell ( p , p ) leq ell ( p , q )$ , with equality iff $p = q$ .   \nSuch a loss function $ell$ is called a proper scoring rule [GR07]. \nWe can show that cross-entropy loss is a proper scoring rule by virtue of the fact that $D _ { mathbb { K L } } left( p parallel p right) leq$ $D _ { mathbb { K L } } left( p parallel q right)$ . However, the $log p ( boldsymbol { y } ) / q ( boldsymbol { y } )$ term can be quite sensitive to errors for low probability events [QC+06]. A common alternative is to use the Brier score [Bri50], which is defined as follows (for a discrete distribution with $C$ values): \nThis is just the squared error of the predictive distribution compared to the true distribution, when viewed as vectors. Since it is based on squared error, the Brier score is less sensitive to extremely rare or extremely common classes. Fortunately, it is also a proper scoring rule. \n5.2 Choosing the “right” model \nIn this section, we consider the setting in which we have several candidate (parametric) models (e.g., neural networks with different numbers of layers), and we want to choose the “right” one. This can be tackled using tools from Bayesian decision theory. \n5.2.1 Bayesian hypothesis testing \nSuppose we have two hypotheses or models, commonly called the null hypothesis, $M _ { 0 }$ , and the alternative hypothesis, $M _ { 1 }$ , and we want to know which one is more likely to be true. This is called hypothesis testing. \nIf we use 0-1 loss, the optimal decision is to pick the alternative hypothesis iff $p ( M _ { 1 } | mathcal { D } ) > p ( M _ { 0 } | mathcal { D } )$ , or equivalently, if $p ( M _ { 1 } | mathcal { D } ) / p ( M _ { 0 } | mathcal { D } ) > 1$ . If we use a uniform prior, $p ( M _ { 0 } ) = p ( M _ { 1 } ) = 0 . 5$ , the decision rule becomes: select $M _ { 1 }$ iff $p ( { mathcal { D } } | M _ { 1 } ) / p ( { mathcal { D } } | M _ { 0 } ) > 1$ . This quantity, which is the ratio of marginal likelihoods of the two models, is known as the Bayes factor: \nThis is like a likelihood ratio, except we integrate out the parameters, which allows us to compare models of different complexity, due to the Bayesian Occam’s razor effect explained in Section 5.2.3. If $B _ { 1 , 0 } > 1$ then we prefer model 1, otherwise we prefer model 0. Of course, it might be that $boldsymbol { B } _ { 1 , 0 }$ is only slightly greater than 1. In that case, we are not very confident that model 1 is better. Jeffreys [Jef61] proposed a scale of evidence for interpreting the magnitude of a Bayes factor, which is shown in Table 5.6. This is a Bayesian alternative to the frequentist concept of a p-value (see Section 5.5.3). We give a worked example of how to compute Bayes factors in Section 5.2.1.1. \n5.2.1.1 Example: Testing if a coin is fair \nAs an example, suppose we observe some coin tosses, and want to decide if the data was generated by a fair coin, $theta = 0 . 5$ , or a potentially biased coin, where $theta$ could be any value in $lfloor 0 , 1 rfloor$ . Let us denote \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Bayesian decision theory",
        "subsubsection": "Probabilistic prediction problems"
    },
    {
        "content": "the distribution $q$ that matches the true distribution $p$ , i.e., $ell ( p , p ) leq ell ( p , q )$ , with equality iff $p = q$ .   \nSuch a loss function $ell$ is called a proper scoring rule [GR07]. \nWe can show that cross-entropy loss is a proper scoring rule by virtue of the fact that $D _ { mathbb { K L } } left( p parallel p right) leq$ $D _ { mathbb { K L } } left( p parallel q right)$ . However, the $log p ( boldsymbol { y } ) / q ( boldsymbol { y } )$ term can be quite sensitive to errors for low probability events [QC+06]. A common alternative is to use the Brier score [Bri50], which is defined as follows (for a discrete distribution with $C$ values): \nThis is just the squared error of the predictive distribution compared to the true distribution, when viewed as vectors. Since it is based on squared error, the Brier score is less sensitive to extremely rare or extremely common classes. Fortunately, it is also a proper scoring rule. \n5.2 Choosing the “right” model \nIn this section, we consider the setting in which we have several candidate (parametric) models (e.g., neural networks with different numbers of layers), and we want to choose the “right” one. This can be tackled using tools from Bayesian decision theory. \n5.2.1 Bayesian hypothesis testing \nSuppose we have two hypotheses or models, commonly called the null hypothesis, $M _ { 0 }$ , and the alternative hypothesis, $M _ { 1 }$ , and we want to know which one is more likely to be true. This is called hypothesis testing. \nIf we use 0-1 loss, the optimal decision is to pick the alternative hypothesis iff $p ( M _ { 1 } | mathcal { D } ) > p ( M _ { 0 } | mathcal { D } )$ , or equivalently, if $p ( M _ { 1 } | mathcal { D } ) / p ( M _ { 0 } | mathcal { D } ) > 1$ . If we use a uniform prior, $p ( M _ { 0 } ) = p ( M _ { 1 } ) = 0 . 5$ , the decision rule becomes: select $M _ { 1 }$ iff $p ( { mathcal { D } } | M _ { 1 } ) / p ( { mathcal { D } } | M _ { 0 } ) > 1$ . This quantity, which is the ratio of marginal likelihoods of the two models, is known as the Bayes factor: \nThis is like a likelihood ratio, except we integrate out the parameters, which allows us to compare models of different complexity, due to the Bayesian Occam’s razor effect explained in Section 5.2.3. If $B _ { 1 , 0 } > 1$ then we prefer model 1, otherwise we prefer model 0. Of course, it might be that $boldsymbol { B } _ { 1 , 0 }$ is only slightly greater than 1. In that case, we are not very confident that model 1 is better. Jeffreys [Jef61] proposed a scale of evidence for interpreting the magnitude of a Bayes factor, which is shown in Table 5.6. This is a Bayesian alternative to the frequentist concept of a p-value (see Section 5.5.3). We give a worked example of how to compute Bayes factors in Section 5.2.1.1. \n5.2.1.1 Example: Testing if a coin is fair \nAs an example, suppose we observe some coin tosses, and want to decide if the data was generated by a fair coin, $theta = 0 . 5$ , or a potentially biased coin, where $theta$ could be any value in $lfloor 0 , 1 rfloor$ . Let us denote \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nthe first model by $M _ { 0 }$ and the second model by $M _ { 1 }$ . The marginal likelihood under $M _ { 0 }$ is simply \nwhere $N$ is the number of coin tosses. From Equation (4.143), the marginal likelihood under $M _ { 1 }$ , using a Beta prior, is \nWe plot $log p ( mathcal { D } | M _ { 1 } )$ vs the number of heads $N _ { 1 }$ in Figure 5.4(a), assuming $N = 5$ and a uniform prior, $alpha _ { 1 } = alpha _ { 0 } = 1$ . (The shape of the curve is not very sensitive to $alpha _ { 1 }$ and $alpha _ { 0 }$ , as long as the prior is symmetric, so $alpha _ { 0 } ~ = alpha _ { 1 }$ .) If we observe 2 or 3 heads, the unbiased coin hypothesis $M _ { 0 }$ is more likely than $M _ { 1 }$ , since $M _ { 0 }$ is a simpler model (it has no free parameters) — it would be a suspicious coincidence if the coin were biased but happened to produce almost exactly 50/50 \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 heads/tails. However, as the counts become more extreme, we favor the biased coin hypothesis. Note that, if we plot the log Bayes factor, $log B _ { 1 , 0 }$ , it will have exactly the same shape, since $log p ( mathcal { D } | M _ { 0 } )$ is a constant. \n\n5.2.2 Bayesian model selection \nNow suppose we have a set $mathcal { M }$ of more than 2 models, and we want to pick the most likely. This is called model selection. We can view this as a decision theory problem, where the action space requires choosing one model, $m in mathcal { M }$ . If we have a 0-1 loss, the optimal action is to pick the most probable model: \nwhere \nis the posterior over models. If the prior over models is uniform, $p ( m ) = 1 / | mathcal { M } |$ , then the MAP model is given by \nThe quantity $p ( mathcal { D } | m )$ is given by \nThis is known as the marginal likelihood, or the evidence for model $m$ . Intuitively, it is the likelihood of the data averaged over all possible parameter values, weighted by the prior $p ( pmb theta | m )$ . If all settings of $pmb theta$ assign high probability to the data, then this is probably a good model. \n5.2.2.1 Example: polynomial regression \nAs an example of Bayesian model selection, we will consider polynomial regression in 1d. Figure 5.5 shows the posterior over three different models, corresponding to polynomials of degrees 1, 2 and 3 fit to $N = 5$ data points. We use a uniform prior over models, and use empirical Bayes to estimate the prior over the regression weights (see Section 11.7.7). We then compute the evidence for each model (see Section 11.7 for details on how to do this). We see that there is not enough data to justify a complex model, so the MAP model is $m = 1$ . Figure 5.6 shows the analogous plot for $N = 3 0$ data points. Now we see that the MAP model is $m = 2$ ; the larger sample size means we can safely pick a more complex model. \n5.2.3 Occam’s razor \nConsider two models, a simple one, $m _ { 1 }$ , and a more complex one, $m _ { 2 }$ . Suppose that both can explain the data by suitably optimizing their parameters, i.e., for which $p ( mathcal { D } | widehat { pmb { theta } } _ { 1 } , m _ { 1 } )$ and $p ( mathcal { D } | hat { pmb { theta } } _ { 2 } , m _ { 2 } )$ are \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license both large. Intuitively we should prefer $m _ { 1 }$ , since it is simpler and just as good as $m _ { 2 }$ . This principle is known as Occam’s razor.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Choosing the ``right'' model",
        "subsubsection": "Bayesian hypothesis testing"
    },
    {
        "content": "5.2.2 Bayesian model selection \nNow suppose we have a set $mathcal { M }$ of more than 2 models, and we want to pick the most likely. This is called model selection. We can view this as a decision theory problem, where the action space requires choosing one model, $m in mathcal { M }$ . If we have a 0-1 loss, the optimal action is to pick the most probable model: \nwhere \nis the posterior over models. If the prior over models is uniform, $p ( m ) = 1 / | mathcal { M } |$ , then the MAP model is given by \nThe quantity $p ( mathcal { D } | m )$ is given by \nThis is known as the marginal likelihood, or the evidence for model $m$ . Intuitively, it is the likelihood of the data averaged over all possible parameter values, weighted by the prior $p ( pmb theta | m )$ . If all settings of $pmb theta$ assign high probability to the data, then this is probably a good model. \n5.2.2.1 Example: polynomial regression \nAs an example of Bayesian model selection, we will consider polynomial regression in 1d. Figure 5.5 shows the posterior over three different models, corresponding to polynomials of degrees 1, 2 and 3 fit to $N = 5$ data points. We use a uniform prior over models, and use empirical Bayes to estimate the prior over the regression weights (see Section 11.7.7). We then compute the evidence for each model (see Section 11.7 for details on how to do this). We see that there is not enough data to justify a complex model, so the MAP model is $m = 1$ . Figure 5.6 shows the analogous plot for $N = 3 0$ data points. Now we see that the MAP model is $m = 2$ ; the larger sample size means we can safely pick a more complex model. \n5.2.3 Occam’s razor \nConsider two models, a simple one, $m _ { 1 }$ , and a more complex one, $m _ { 2 }$ . Suppose that both can explain the data by suitably optimizing their parameters, i.e., for which $p ( mathcal { D } | widehat { pmb { theta } } _ { 1 } , m _ { 1 } )$ and $p ( mathcal { D } | hat { pmb { theta } } _ { 2 } , m _ { 2 } )$ are \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license both large. Intuitively we should prefer $m _ { 1 }$ , since it is simpler and just as good as $m _ { 2 }$ . This principle is known as Occam’s razor.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Choosing the ``right'' model",
        "subsubsection": "Bayesian model selection"
    },
    {
        "content": "5.2.2 Bayesian model selection \nNow suppose we have a set $mathcal { M }$ of more than 2 models, and we want to pick the most likely. This is called model selection. We can view this as a decision theory problem, where the action space requires choosing one model, $m in mathcal { M }$ . If we have a 0-1 loss, the optimal action is to pick the most probable model: \nwhere \nis the posterior over models. If the prior over models is uniform, $p ( m ) = 1 / | mathcal { M } |$ , then the MAP model is given by \nThe quantity $p ( mathcal { D } | m )$ is given by \nThis is known as the marginal likelihood, or the evidence for model $m$ . Intuitively, it is the likelihood of the data averaged over all possible parameter values, weighted by the prior $p ( pmb theta | m )$ . If all settings of $pmb theta$ assign high probability to the data, then this is probably a good model. \n5.2.2.1 Example: polynomial regression \nAs an example of Bayesian model selection, we will consider polynomial regression in 1d. Figure 5.5 shows the posterior over three different models, corresponding to polynomials of degrees 1, 2 and 3 fit to $N = 5$ data points. We use a uniform prior over models, and use empirical Bayes to estimate the prior over the regression weights (see Section 11.7.7). We then compute the evidence for each model (see Section 11.7 for details on how to do this). We see that there is not enough data to justify a complex model, so the MAP model is $m = 1$ . Figure 5.6 shows the analogous plot for $N = 3 0$ data points. Now we see that the MAP model is $m = 2$ ; the larger sample size means we can safely pick a more complex model. \n5.2.3 Occam’s razor \nConsider two models, a simple one, $m _ { 1 }$ , and a more complex one, $m _ { 2 }$ . Suppose that both can explain the data by suitably optimizing their parameters, i.e., for which $p ( mathcal { D } | widehat { pmb { theta } } _ { 1 } , m _ { 1 } )$ and $p ( mathcal { D } | hat { pmb { theta } } _ { 2 } , m _ { 2 } )$ are \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license both large. Intuitively we should prefer $m _ { 1 }$ , since it is simpler and just as good as $m _ { 2 }$ . This principle is known as Occam’s razor. \n\nLet us now see how ranking models based on their marginal likelihood, which involves averaging the likelihood wrt the prior, will give rise to this behavior. The complex model will put less prior probability on the “good” parameters that explain the data, $hat { pmb { theta } } _ { 2 }$ , since the prior must integrate to 1.0 over the entire parameter space. Thus it will take averages in parts of parameter space with low likelihood. By contrast, the simpler model has fewer parameters, so the prior is concentrated over a smaller volume; thus its averages will mostly be in the good part of parameter space, near $hat { pmb { theta } } _ { 1 }$ . Hence we see that the marginal likelihood will prefer the simpler model. This is called the Bayesian Occam’s razor effect [Mac95; MG05]. \nAnother way to understand the Bayesian Occam’s razor effect is to compare the relative predictive abilities of simple and complex models. Since probabilities must sum to one, we have $begin{array} { r } { sum _ { mathcal { D } ^ { prime } } p ( mathcal { D } ^ { prime } | m ) = 1 } end{array}$ , where the sum is over all possible datasets. Complex models, which can predict many things, must spread their predicted probability mass thinly, and hence will not obtain as large a probability for any given data set as simpler models. This is sometimes called the conservation of probability mass principle, and is illustrated in Figure 5.7. On the horizontal axis we plot all possible data sets in order of increasing complexity (measured in some abstract sense). On the vertical axis we plot the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 predictions of 3 possible models: a simple one, $M _ { 1 }$ ; a medium one, $M _ { 2 }$ ; and a complex one, $M _ { 3 }$ . We also indicate the actually observed data $mathcal { D } _ { 0 }$ by a vertical line. Model 1 is too simple and assigns low probability to $mathcal { D } _ { 0 }$ . Model 3 also assigns $mathcal { D } _ { 0 }$ relatively low probability, because it can predict many data sets, and hence it spreads its probability quite widely and thinly. Model 2 is “just right”: it predicts the observed data with a reasonable degree of confidence, but does not predict too many other things. Hence model 2 is the most probable model. \n\n5.2.4 Connection between cross validation and marginal likelihood \nWe have seen how the marginal likelihood helps us choose models of the “right” complexity. In non-Bayesian approaches to model selection, it is standard to use cross validation (Section 4.5.5) for this purpose. \nIt turns out that the marginal likelihood is closely related to the leave-one-out cross-validation (LOO-CV) estimate, as we now show. We start with the marginal likelihood for model $m$ , which we write in sequential form as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Choosing the ``right'' model",
        "subsubsection": "Occam's razor"
    },
    {
        "content": "5.2.4 Connection between cross validation and marginal likelihood \nWe have seen how the marginal likelihood helps us choose models of the “right” complexity. In non-Bayesian approaches to model selection, it is standard to use cross validation (Section 4.5.5) for this purpose. \nIt turns out that the marginal likelihood is closely related to the leave-one-out cross-validation (LOO-CV) estimate, as we now show. We start with the marginal likelihood for model $m$ , which we write in sequential form as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere \nSuppose we use a plugin approximation to the above distribution to get \nThen we get \nThis is similar to a leave-one-out cross-validation estimate of the likelihood, which has the form $begin{array} { r } { frac { 1 } { N } sum _ { n = 1 } ^ { N } log p ( y _ { n } | pmb { x } _ { n } , hat { pmb { theta } } _ { m } ( mathcal { D } _ { 1 : n - 1 , n + 1 : N } ) ) } end{array}$ , except we ignore the $mathcal { D } _ { n + 1 : N }$ part. The intuition behind the connection is this: an overly complex model will overfit the “early” examples and will then predict the remaining ones poorly, and thus will also get a low cross-validation score. See [FH20] for a more detailed discussion of the connection between these performance metrics. \n5.2.5 Information criteria \nThe marginal likelihood, $begin{array} { r } { p ( mathcal { D } | m ) = int p ( mathcal { D } | pmb { theta } , m ) p ( pmb { theta } ) d pmb { theta } } end{array}$ , which is needed for Bayesian model selection discussed in Section 5.2.2, can be difficult to compute, since it requires marginalizing over the entire parameter space. Furthermore, the result can be quite sensitive to the choice of prior. In this section, we discuss some other related metrics for model selection known as information criteria. We only give a brief discussion; see e.g., [GHV14] for further details. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Choosing the ``right'' model",
        "subsubsection": "Connection between cross validation and marginal likelihood"
    },
    {
        "content": "where \nSuppose we use a plugin approximation to the above distribution to get \nThen we get \nThis is similar to a leave-one-out cross-validation estimate of the likelihood, which has the form $begin{array} { r } { frac { 1 } { N } sum _ { n = 1 } ^ { N } log p ( y _ { n } | pmb { x } _ { n } , hat { pmb { theta } } _ { m } ( mathcal { D } _ { 1 : n - 1 , n + 1 : N } ) ) } end{array}$ , except we ignore the $mathcal { D } _ { n + 1 : N }$ part. The intuition behind the connection is this: an overly complex model will overfit the “early” examples and will then predict the remaining ones poorly, and thus will also get a low cross-validation score. See [FH20] for a more detailed discussion of the connection between these performance metrics. \n5.2.5 Information criteria \nThe marginal likelihood, $begin{array} { r } { p ( mathcal { D } | m ) = int p ( mathcal { D } | pmb { theta } , m ) p ( pmb { theta } ) d pmb { theta } } end{array}$ , which is needed for Bayesian model selection discussed in Section 5.2.2, can be difficult to compute, since it requires marginalizing over the entire parameter space. Furthermore, the result can be quite sensitive to the choice of prior. In this section, we discuss some other related metrics for model selection known as information criteria. We only give a brief discussion; see e.g., [GHV14] for further details. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n5.2.5.1 The Bayesian information criterion (BIC) \nThe Bayesian information criterion or BIC [Sch78] can be thought of as a simple approximation to the log marginal likelihood. In particular, if we make a Gaussian approximation to the posterior, as discussed in Section 4.6.8.2, we get (from Equation (4.215)) the following: \nwhere $mathbf { H }$ is the Hessian of the negative log joint, $- log p ( mathcal { D } , pmb { theta } )$ , evaluated at the MAP estimate $hat { pmb { theta } } _ { mathrm { m a p } }$ . We see that Equation (5.56) is the log likelihood plus some penalty terms. If we have a uniform prior, $p ( pmb theta ) propto 1$ , we can drop the prior term, and replace the MAP estimate with the MLE, $hat { pmb { theta } }$ , yielding \nWe now focus on approximating the $log | mathbf { H } |$ term, which is sometimes called the Occam factor, since it is a measure of model complexity (volume of the posterior distribution). We have $mathbf { H } =$ $textstyle sum _ { i = 1 } ^ { N } mathbf { H } _ { i }$ , where $mathbf { H } _ { i } = nabla nabla log p ( mathcal { D } _ { i } | pmb { theta } )$ . Let us approximate each $mathbf { H } _ { i }$ by a fixed matrix $hat { bf H }$ . Then we have \nwhere $D = dim ( theta )$ and we have assumed $mathbf { H }$ is full rank. We can drop the $log | hat { mathbf { H } } |$ term, since it is independent of $N$ , and thus will get overwhelmed by the likelihood. Putting all the pieces together, we get the BIC score that we want to maximize: \nWe can also define the BIC loss, that we want to minimize, by multiplying by -2: \n(The use of 2 as a scale factor is chosen to simplify the expression when using a model with a Gaussian likelihood.) \n5.2.5.2 Akaike information criterion \nThe Akaike information criterion [Aka74] is closely related to the BIC. It has the form \nThis penalizes complex models less heavily than BIC, since the regularization term is independent of $N$ . This estimator can be derived from a frequentist perspective. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n5.2.5.3 Minimum description length (MDL) \nWe can think about the problem of scoring different models in terms of information theory (Chapter 6). The goal is for the sender to communicate the data to the receiver. First the sender needs to specify which model $m$ to use; this takes $C ( m ) = - log p ( m )$ bits (see Section 6.1). Then the receiver can fit the model, by computing $hat { pmb { theta } } _ { m }$ , and can thus approximately reconstruct the data. To perfectly reconstruct the data, the sender needs to send the residual errors that cannot be explained by the model; this takes $begin{array} { r } { - L ( m ) = - log p ( mathcal { D } | hat { theta } , m ) = - sum _ { n } log p ( y _ { n } | hat { theta } , m ) } end{array}$ bits. The total cost is \nWe see that has the same basic form as BIC/AIC. Choosing the model which minimizes $J ( m )$ is known as the minimum description length or MDL principle. See e.g., [HY01] for details. \n5.2.6 Posterior inference over effect sizes and Bayesian significance testing \nThe approach to hypothesis testing discussed in Section 5.2.1 relies on computing the Bayes factors for the null vs the alternative model, $p ( mathcal { D } | H _ { 0 } ) / p ( mathcal { D } | H _ { 1 } )$ . Unfortunately, computing the necessary marginal likelihoods can be computationally difficult, and the results can be sensitive to the choice of prior. Furthermore, we are often more interested in estimating an effect size, which is the difference in magnitude between two parameters, rather than in deciding if an effect size is 0 (null hypothesis) or not (alternative hypothesis) — the latter is called a point null hypothesis, and is often regarded as an irrelevant “straw man” (see e.g., [Mak+19] and references therein). \nFor example, suppose we have two classifiers, $m _ { 1 }$ and $m _ { 2 }$ , and we want to know which one is better. That is, we want to perform a comparison of classifiers. Let $mu _ { 1 }$ and $mu _ { 2 }$ be their average accuracies, and let $delta = mu _ { 1 } - mu _ { 2 }$ be the difference in their accuracies. The probability that model 1 is more accurate, on average, than model 2 is given by $p ( delta > 0 | mathcal { D } )$ . However, even if this probability is large, the improvement may be not be practically significant. So it is better to compute a probability such as $p ( delta > epsilon | mathcal { D } )$ or $p ( | delta | > epsilon | mathcal { D } )$ , where $epsilon$ represents the minimal magnitude of effect size that is meaningful for the problem at hand. This is called a one-sided test or two-sided test. \nMore generally, let $R = [ - epsilon , epsilon ]$ represent a region of practical equivalence or ROPE [Kru15; KL17]. We can define 3 events of interest: the null hypothesis $H _ { 0 } : delta in R$ , which says both methods are practically the same (which is a more realistic assumption than $H _ { 0 } : delta = 0$ ); $H _ { A } : delta > epsilon$ , which says $m _ { 1 }$ is better than $m _ { 2 }$ ; and $H _ { B } : delta < - epsilon$ , which says $m _ { 2 }$ is better than $m _ { 1 }$ . To choose amongst these 3 hypotheses, we just have to compute $p ( delta | mathcal { D } )$ , which avoids the need to compute Bayes factors. In the sections below, we discuss how to compute this quantity using two different kinds of model. \n5.2.6.1 Bayesian t-test for difference in means \nSuppose we have two classifiers, $m _ { 1 }$ and $m _ { 2 }$ , which are evaluated on the same set of $N$ test examples. Let $e _ { i } ^ { m }$ be the error of method $m$ on test example $i$ . (Or this could be the conditional log likelihood, $e _ { i } ^ { m } = log p ^ { m } ( y _ { i } | pmb { x } _ { i } )$ .) Since the classifiers are applied to the same data, we can use a paired test for comparing them, which is more sensitive than looking at average performance, since the factors that make one example easy or hard to classify (e.g., due to label noise) will be shared by both methods. Thus we will work with the differences, $d _ { i } = e _ { i } ^ { 1 } - e _ { i } ^ { 2 }$ . We assume $d _ { i } sim mathcal { N } ( delta , sigma ^ { 2 } )$ . We are interested in $p ( delta | pmb { d } )$ , where $pmb { d } = ( d _ { 1 } , ldots , d _ { N } )$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Choosing the ``right'' model",
        "subsubsection": "Information criteria"
    },
    {
        "content": "5.2.5.3 Minimum description length (MDL) \nWe can think about the problem of scoring different models in terms of information theory (Chapter 6). The goal is for the sender to communicate the data to the receiver. First the sender needs to specify which model $m$ to use; this takes $C ( m ) = - log p ( m )$ bits (see Section 6.1). Then the receiver can fit the model, by computing $hat { pmb { theta } } _ { m }$ , and can thus approximately reconstruct the data. To perfectly reconstruct the data, the sender needs to send the residual errors that cannot be explained by the model; this takes $begin{array} { r } { - L ( m ) = - log p ( mathcal { D } | hat { theta } , m ) = - sum _ { n } log p ( y _ { n } | hat { theta } , m ) } end{array}$ bits. The total cost is \nWe see that has the same basic form as BIC/AIC. Choosing the model which minimizes $J ( m )$ is known as the minimum description length or MDL principle. See e.g., [HY01] for details. \n5.2.6 Posterior inference over effect sizes and Bayesian significance testing \nThe approach to hypothesis testing discussed in Section 5.2.1 relies on computing the Bayes factors for the null vs the alternative model, $p ( mathcal { D } | H _ { 0 } ) / p ( mathcal { D } | H _ { 1 } )$ . Unfortunately, computing the necessary marginal likelihoods can be computationally difficult, and the results can be sensitive to the choice of prior. Furthermore, we are often more interested in estimating an effect size, which is the difference in magnitude between two parameters, rather than in deciding if an effect size is 0 (null hypothesis) or not (alternative hypothesis) — the latter is called a point null hypothesis, and is often regarded as an irrelevant “straw man” (see e.g., [Mak+19] and references therein). \nFor example, suppose we have two classifiers, $m _ { 1 }$ and $m _ { 2 }$ , and we want to know which one is better. That is, we want to perform a comparison of classifiers. Let $mu _ { 1 }$ and $mu _ { 2 }$ be their average accuracies, and let $delta = mu _ { 1 } - mu _ { 2 }$ be the difference in their accuracies. The probability that model 1 is more accurate, on average, than model 2 is given by $p ( delta > 0 | mathcal { D } )$ . However, even if this probability is large, the improvement may be not be practically significant. So it is better to compute a probability such as $p ( delta > epsilon | mathcal { D } )$ or $p ( | delta | > epsilon | mathcal { D } )$ , where $epsilon$ represents the minimal magnitude of effect size that is meaningful for the problem at hand. This is called a one-sided test or two-sided test. \nMore generally, let $R = [ - epsilon , epsilon ]$ represent a region of practical equivalence or ROPE [Kru15; KL17]. We can define 3 events of interest: the null hypothesis $H _ { 0 } : delta in R$ , which says both methods are practically the same (which is a more realistic assumption than $H _ { 0 } : delta = 0$ ); $H _ { A } : delta > epsilon$ , which says $m _ { 1 }$ is better than $m _ { 2 }$ ; and $H _ { B } : delta < - epsilon$ , which says $m _ { 2 }$ is better than $m _ { 1 }$ . To choose amongst these 3 hypotheses, we just have to compute $p ( delta | mathcal { D } )$ , which avoids the need to compute Bayes factors. In the sections below, we discuss how to compute this quantity using two different kinds of model. \n5.2.6.1 Bayesian t-test for difference in means \nSuppose we have two classifiers, $m _ { 1 }$ and $m _ { 2 }$ , which are evaluated on the same set of $N$ test examples. Let $e _ { i } ^ { m }$ be the error of method $m$ on test example $i$ . (Or this could be the conditional log likelihood, $e _ { i } ^ { m } = log p ^ { m } ( y _ { i } | pmb { x } _ { i } )$ .) Since the classifiers are applied to the same data, we can use a paired test for comparing them, which is more sensitive than looking at average performance, since the factors that make one example easy or hard to classify (e.g., due to label noise) will be shared by both methods. Thus we will work with the differences, $d _ { i } = e _ { i } ^ { 1 } - e _ { i } ^ { 2 }$ . We assume $d _ { i } sim mathcal { N } ( delta , sigma ^ { 2 } )$ . We are interested in $p ( delta | pmb { d } )$ , where $pmb { d } = ( d _ { 1 } , ldots , d _ { N } )$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIf we use an uninformative prior for the unknown parameters $( delta , sigma )$ , one can show that the posterior marginal for the mean is given by a Student distribution: \nwhere $textstyle mu = { frac { 1 } { N } } sum _ { i = 1 } ^ { N } d _ { i }$ is the sample mean, and $begin{array} { r } { s ^ { 2 } = frac { 1 } { N - 1 } sum _ { i = 1 } ^ { N } ( d _ { i } - mu ) ^ { 2 } } end{array}$ is an unbiased estimate of the variance. Hence we can easily compute $p ( | delta | > epsilon | d )$ , with a ROPE of $epsilon = 0 . 0 1$ (say). This is known as a Bayesian t-test [Ben+17]. (See also [Rou+09] for Bayesian t-test based on Bayes factors, and [Die98] for a non-Bayesian approach to comparing classifiers.) \nAn alternative to a formal test is to just plot the posterior $p ( delta | pmb { d } )$ . If this distribution is tightly centered on 0, we can conclude that there is no significant difference between the methods. (In fact, an even simpler approach is to just make a boxplot of the data, ${ d _ { i } }$ , which avoids the need for any formal statistical analysis.) \nNote that this kind of problem arises in many applications, not just evaluating classifiers. For example, suppose we have a set of $N$ people, each of whom is exposed two drugs; let $e _ { i } ^ { m }$ be the outcome (e.g., sickness level) when person $i$ is exposed to drug $m$ , and let $d _ { i } ^ { m } = e _ { i } ^ { 1 } - e _ { i } ^ { 2 }$ be the difference in response. We can then analyse the effect of the drug by computing $p ( delta | pmb { d } )$ as we discussed above. \n5.2.6.2 Bayesian $x ^ { 2 }$ -test for difference in rates \nNow suppose we have two classifiers which are evaluated on different test sets. Let $y _ { m }$ be the number of correct examples from method $m in { 1 , 2 }$ out of $N _ { m }$ trials, so the accuracy rate is $y _ { m } / N _ { m }$ . We assume $y _ { m } sim mathrm { B i n } ( N _ { m } , theta _ { m } )$ , so we are interested in $p ( delta | mathcal { D } )$ , where $delta = theta _ { 1 } - theta _ { 2 }$ , and $mathcal { D } = ( y _ { 1 } , N _ { 1 } , y _ { 2 } , N _ { 2 } )$ is all the data. \nIf we use a uniform prior for $theta _ { 1 }$ and $theta _ { 2 }$ (i.e., $p ( theta _ { j } ) = mathrm { B e t a } ( theta _ { j } | 1 , 1 ) )$ , the posterior is given by \nThe posterior for $delta$ is given by \nWe can then evaluate this for any value of $delta$ that we choose. For example, we can compute \n(We can compute this using 1 dimensional numerical integration or analytically [Coo05].) This is called a Bayesian $chi ^ { 2 }$ -test. \nNote that this kind of problem arises in many applications, not just evaluating classifiers, For example, suppose the two groups are different companies selling the same product on Amazon, and $y _ { m }$ is the number of positive reviews for merchant $m$ . Or suppose the two groups correspond to men and women, and $y _ { m }$ is the number of people in group $m$ who are left handed, and $N _ { m } - y _ { m }$ to be \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license the number who are right handed.3 We can represent the data as a $2 times 2$ contingency table of counts, as shown in Table 5.7. \n\nThe MLEs for the left handedness rate in males and females are $hat { theta } _ { 1 } = 9 / 5 2 = 0 . 1 7 3 1$ and $hat { theta } _ { 2 } = 4 / 4 8 = 0 . 0 4 1 7$ . It seems that there is a difference, but the sample size is low, so we cannot be sure. Hence we will represent our uncertainty by computing $p ( delta | mathcal { D } )$ , where $delta = theta _ { 1 } - theta _ { 2 }$ and $mathcal { D }$ is the table of counts. We find $begin{array} { r } { p ( theta _ { 1 } > theta _ { 2 } | mathcal { D } ) = int _ { 0 } ^ { infty } p ( delta | mathcal { D } ) = 0 . 9 0 1 } end{array}$ , which suggests that left handedness is more common in males, consistent with other studies [PP+20]. \n5.3 Frequentist decision theory \nIn this section, we discuss frequentist decision theory. This is similar to Bayesian decision theory, discussed in Section 5.1, but differs in that there is no prior, and hence no posterior, over the unknown state of nature. Consequently we cannot define the risk as the posterior expected loss. We will consider other definitions in Section 5.3.1. \n5.3.1 Computing the risk of an estimator \nWe define the frequentist risk of an estimator $pi$ given an unknown state of nature $pmb theta$ to be the expected loss when applying that estimator to data $_ { x }$ sampled from the likelihood function $p ( { pmb x } | { pmb theta } )$ : \nWe give an example of this in Section 5.3.1.1. \n5.3.1.1 Example \nLet us give an example, based on [BS94]. Consider the problem of estimating the mean of a Gaussian. We assume the data is sampled from $x _ { n } sim { mathcal { N } } ( theta ^ { * } , sigma ^ { 2 } = 1 )$ ). If we use quadratic loss, $ell _ { 2 } ( theta , { hat { theta } } ) = ( theta - { hat { theta } } ) ^ { 2 }$ , the corresponding risk function is the MSE. \nWe now consider 5 different estimators for computing $theta$ : \n• $pi _ { 1 } ( mathcal { D } ) = overline { { x } }$ , the sample mean • $pi _ { 2 } ( mathcal { D } ) = mathrm { m e d i a n } ( mathcal { D } )$ , the sample median • $pi _ { 3 } ( mathcal { D } ) = theta _ { 0 }$ , a fixed value",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Choosing the ``right'' model",
        "subsubsection": "Posterior inference over effect sizes and Bayesian significance testing"
    },
    {
        "content": "The MLEs for the left handedness rate in males and females are $hat { theta } _ { 1 } = 9 / 5 2 = 0 . 1 7 3 1$ and $hat { theta } _ { 2 } = 4 / 4 8 = 0 . 0 4 1 7$ . It seems that there is a difference, but the sample size is low, so we cannot be sure. Hence we will represent our uncertainty by computing $p ( delta | mathcal { D } )$ , where $delta = theta _ { 1 } - theta _ { 2 }$ and $mathcal { D }$ is the table of counts. We find $begin{array} { r } { p ( theta _ { 1 } > theta _ { 2 } | mathcal { D } ) = int _ { 0 } ^ { infty } p ( delta | mathcal { D } ) = 0 . 9 0 1 } end{array}$ , which suggests that left handedness is more common in males, consistent with other studies [PP+20]. \n5.3 Frequentist decision theory \nIn this section, we discuss frequentist decision theory. This is similar to Bayesian decision theory, discussed in Section 5.1, but differs in that there is no prior, and hence no posterior, over the unknown state of nature. Consequently we cannot define the risk as the posterior expected loss. We will consider other definitions in Section 5.3.1. \n5.3.1 Computing the risk of an estimator \nWe define the frequentist risk of an estimator $pi$ given an unknown state of nature $pmb theta$ to be the expected loss when applying that estimator to data $_ { x }$ sampled from the likelihood function $p ( { pmb x } | { pmb theta } )$ : \nWe give an example of this in Section 5.3.1.1. \n5.3.1.1 Example \nLet us give an example, based on [BS94]. Consider the problem of estimating the mean of a Gaussian. We assume the data is sampled from $x _ { n } sim { mathcal { N } } ( theta ^ { * } , sigma ^ { 2 } = 1 )$ ). If we use quadratic loss, $ell _ { 2 } ( theta , { hat { theta } } ) = ( theta - { hat { theta } } ) ^ { 2 }$ , the corresponding risk function is the MSE. \nWe now consider 5 different estimators for computing $theta$ : \n• $pi _ { 1 } ( mathcal { D } ) = overline { { x } }$ , the sample mean • $pi _ { 2 } ( mathcal { D } ) = mathrm { m e d i a n } ( mathcal { D } )$ , the sample median • $pi _ { 3 } ( mathcal { D } ) = theta _ { 0 }$ , a fixed value \n• $pi _ { kappa } ( mathcal { D } )$ , the posterior mean under a $mathcal { N } ( theta | theta _ { 0 } , sigma ^ { 2 } / kappa )$ prior: \nFor $pi _ { kappa }$ , we use $theta _ { 0 } = 0$ , and consider a weak prior, $kappa = 1$ , and a stronger prior, $kappa = 5$ . \nLet $hat { theta } = hat { theta } ( pmb { x } ) = pi ( pmb { x } )$ be the estimated parameter. The risk of this estimator is given by the MSE. In Section 4.7.6.3, we show that the MSE can be decomposed into squared bias plus variance: \nwhere the bias is defined as $mathrm { b i a s } ( hat { theta } ) = mathbb { E } left[ hat { theta } - theta ^ { * } right]$ . We now use this expression to derive the risk for each estimator. \n$pi _ { 1 }$ is the sample mean. This is unbiased, so its risk is \n$pi _ { 2 }$ is the sample median. This is also unbiased. Furthermore, one can show that its variance is approximately $pi / ( 2 N _ { mathcal { D } } )$ , so the risk is \n$pi _ { 3 }$ returns the constant $theta _ { 0 }$ , so its bias is $( theta ^ { * } - theta _ { 0 } )$ and its variance is zero. Hence the risk is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFinally, $pi _ { 4 }$ is the posterior mean under a Gaussian prior. We can derive its MSE as follows: \nThese functions are plotted in Figure 5.8 for $N _ { mathcal { D } } in { 5 , 2 0 }$ . We see that in general, the best estimator depends on the value of $theta ^ { * }$ , which is unknown. If $theta ^ { * }$ is very close to $theta _ { 0 }$ , then $pi _ { 3 }$ (which just predicts $theta _ { 0 }$ ) is best. If $theta ^ { * }$ is within some reasonable range around $theta _ { 0 }$ , then the posterior mean, which combines the prior guess of $theta _ { 0 }$ with the actual data, is best. If $theta ^ { * }$ is far from $theta _ { 0 }$ , the MLE is best. \n5.3.1.2 Bayes risk \nIn general, the true state of nature $pmb theta$ that generates the data $_ { pmb { x } }$ is unknown, so we cannot compute the risk given in Equation (5.63). One solution to this is to assume a prior $pi _ { 0 }$ for $pmb theta$ , and then average it out. This gives us the Bayes risk, also called the integrated risk: \nA decision rule that minimizes the Bayes risk is known as a Bayes estimator. This is equivalent to the optimal policy recommended by Bayesian decision theory in Equation (5.2) since \nHence we see that picking the optimal action on a case-by-case basis (as in the Bayesian approach) is optimal on average (as in the frequentist approach). In other words, the Bayesian approach provides a good way of achieving frequentist goals. See [BS94, p448] for further discussion of this point. \n5.3.1.3 Maximum risk \nOf course the use of a prior might seem undesirable in the context of frequentist statistics. We can therefore define the maximum risk as follows: \nA decision rule that minimizes the maximum risk is called a minimax estimator, and is denoted $pi _ { M M }$ . For example, in Figure 5.9, we see that $pi _ { 1 }$ has lower worst-case risk than $pi _ { 2 }$ , ranging over all possible values of $pmb theta$ , so it is the minimax estimator. \nMinimax estimators have a certain appeal. However, computing them can be hard. And furthermore, they are very pessimistic. In fact, one can show that all minimax estimators are equivalent to Bayes estimators under a least favorable prior. In most statistical situations (excluding game theoretic ones), assuming nature is an adversary is not a reasonable assumption. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n5.3.2 Consistent estimators \nSuppose we have a dataset $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ where the samples $pmb { x } _ { n } in mathcal { X }$ are generated from a distribution $p ( { pmb x } | pmb theta ^ { * } )$ , where $theta ^ { ast } in Theta$ is the true parameter. Furthermore, suppose the parameters are identifiable, meaning that $p ( mathcal { D } | pmb { theta } ) = p ( mathcal { D } | pmb { theta } ^ { prime } )$ iff $pmb theta = pmb theta ^ { prime }$ for any dataset $mathcal { D }$ . Then we say that an estimator $pi : mathcal { X } ^ { N } to Theta$ is a consistent estimator if $hat { pmb { theta } } ( mathcal { D } )  { pmb { theta } } ^ { * }$ as $N  infty$ (where the arrow denotes convergence in probability). In other words, the procedure $pi$ recovers the true parameter (or a subset of it) in the limit of infinite data. This is equivalent to minimizing the 0-1 loss, $mathcal { L } ( pmb { theta } ^ { * } , hat { pmb { theta } } ) = mathbb { I } left( pmb { theta } ^ { * } neq hat { pmb { theta } } right)$ . An example of a consistent estimator is the maximum likelihood estimator (MLE). \nNote that an estimator can be unbiased but not consistent. For example, consider the estimator $pi ( { x _ { 1 } , ldots , x _ { N } } ) = x _ { N }$ . This is an unbiased estimator of the mean, since $mathbb { E } left[ pi ( mathcal { D } ) right] = mathbb { E } left[ pmb { x } right]$ . But the sampling distribution of $pi ( mathcal { D } )$ does not converge to a fixed value, so it cannot converge to the point $theta ^ { * }$ . \nAlthough consistency is a desirable property, it is of somewhat limited usefulness in practice since most real datasets do not come from our chosen model family (i.e., there is no $pmb { theta } ^ { * }$ such that $p ( cdot | pmb theta ^ { * } )$ generates the observed data $mathcal { D }$ ). In practice, it is more useful to find estimators that minimize some discrepancy measure between the empirical distribution $p _ { mathcal { D } } ( pmb { x } | mathcal { D } )$ and the estimated distribution $p ( { pmb x } | { hat { pmb theta } } )$ . If we use KL divergence as our discrepancy measure, our estimate becomes the MLE. \n5.3.3 Admissible estimators \nWe say that $pi _ { 1 }$ dominates $pi _ { 2 }$ if $R ( pmb theta , pi _ { 1 } ) leq R ( pmb theta , pi _ { 2 } )$ for all $pmb theta$ . The domination is said to be strict if the inequality is strict for some $theta ^ { * }$ . An estimator is said to be admissible if it is not strictly dominated by any other estimator. Interestingly, [Wal47] proved that all admissible decision rules are equivalent to some kind of Bayesian decision rule, under some technical conditions. (See [DR21] for a more general version of this result.) \nFor example, in Figure 5.8, we see that the sample median (dotted red line) always has higher risk than the sample mean (solid blue line). Therefore the sample median is not an admissible estimator for the mean. More surprisingly, one can show that the sample mean is not always an admissible estimator either, even under a Gaussian likelihood model with squared error loss (this is known as Stein’s paradox [Ste56]). \nHowever, the concept of admissibility is of somewhat limited value. For example, let $X sim { mathcal { N } } ( theta , 1 )$ , and consider estimating $theta$ under squared loss. Consider the estimator $pi _ { 1 } ( x ) = theta _ { 0 }$ , where $theta _ { 0 }$ is a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license constant independent of the data. We now show that this is an admissible estimator.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist decision theory",
        "subsubsection": "Computing the risk of an estimator"
    },
    {
        "content": "5.3.2 Consistent estimators \nSuppose we have a dataset $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ where the samples $pmb { x } _ { n } in mathcal { X }$ are generated from a distribution $p ( { pmb x } | pmb theta ^ { * } )$ , where $theta ^ { ast } in Theta$ is the true parameter. Furthermore, suppose the parameters are identifiable, meaning that $p ( mathcal { D } | pmb { theta } ) = p ( mathcal { D } | pmb { theta } ^ { prime } )$ iff $pmb theta = pmb theta ^ { prime }$ for any dataset $mathcal { D }$ . Then we say that an estimator $pi : mathcal { X } ^ { N } to Theta$ is a consistent estimator if $hat { pmb { theta } } ( mathcal { D } )  { pmb { theta } } ^ { * }$ as $N  infty$ (where the arrow denotes convergence in probability). In other words, the procedure $pi$ recovers the true parameter (or a subset of it) in the limit of infinite data. This is equivalent to minimizing the 0-1 loss, $mathcal { L } ( pmb { theta } ^ { * } , hat { pmb { theta } } ) = mathbb { I } left( pmb { theta } ^ { * } neq hat { pmb { theta } } right)$ . An example of a consistent estimator is the maximum likelihood estimator (MLE). \nNote that an estimator can be unbiased but not consistent. For example, consider the estimator $pi ( { x _ { 1 } , ldots , x _ { N } } ) = x _ { N }$ . This is an unbiased estimator of the mean, since $mathbb { E } left[ pi ( mathcal { D } ) right] = mathbb { E } left[ pmb { x } right]$ . But the sampling distribution of $pi ( mathcal { D } )$ does not converge to a fixed value, so it cannot converge to the point $theta ^ { * }$ . \nAlthough consistency is a desirable property, it is of somewhat limited usefulness in practice since most real datasets do not come from our chosen model family (i.e., there is no $pmb { theta } ^ { * }$ such that $p ( cdot | pmb theta ^ { * } )$ generates the observed data $mathcal { D }$ ). In practice, it is more useful to find estimators that minimize some discrepancy measure between the empirical distribution $p _ { mathcal { D } } ( pmb { x } | mathcal { D } )$ and the estimated distribution $p ( { pmb x } | { hat { pmb theta } } )$ . If we use KL divergence as our discrepancy measure, our estimate becomes the MLE. \n5.3.3 Admissible estimators \nWe say that $pi _ { 1 }$ dominates $pi _ { 2 }$ if $R ( pmb theta , pi _ { 1 } ) leq R ( pmb theta , pi _ { 2 } )$ for all $pmb theta$ . The domination is said to be strict if the inequality is strict for some $theta ^ { * }$ . An estimator is said to be admissible if it is not strictly dominated by any other estimator. Interestingly, [Wal47] proved that all admissible decision rules are equivalent to some kind of Bayesian decision rule, under some technical conditions. (See [DR21] for a more general version of this result.) \nFor example, in Figure 5.8, we see that the sample median (dotted red line) always has higher risk than the sample mean (solid blue line). Therefore the sample median is not an admissible estimator for the mean. More surprisingly, one can show that the sample mean is not always an admissible estimator either, even under a Gaussian likelihood model with squared error loss (this is known as Stein’s paradox [Ste56]). \nHowever, the concept of admissibility is of somewhat limited value. For example, let $X sim { mathcal { N } } ( theta , 1 )$ , and consider estimating $theta$ under squared loss. Consider the estimator $pi _ { 1 } ( x ) = theta _ { 0 }$ , where $theta _ { 0 }$ is a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license constant independent of the data. We now show that this is an admissible estimator.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist decision theory",
        "subsubsection": "Consistent estimators"
    },
    {
        "content": "5.3.2 Consistent estimators \nSuppose we have a dataset $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ where the samples $pmb { x } _ { n } in mathcal { X }$ are generated from a distribution $p ( { pmb x } | pmb theta ^ { * } )$ , where $theta ^ { ast } in Theta$ is the true parameter. Furthermore, suppose the parameters are identifiable, meaning that $p ( mathcal { D } | pmb { theta } ) = p ( mathcal { D } | pmb { theta } ^ { prime } )$ iff $pmb theta = pmb theta ^ { prime }$ for any dataset $mathcal { D }$ . Then we say that an estimator $pi : mathcal { X } ^ { N } to Theta$ is a consistent estimator if $hat { pmb { theta } } ( mathcal { D } )  { pmb { theta } } ^ { * }$ as $N  infty$ (where the arrow denotes convergence in probability). In other words, the procedure $pi$ recovers the true parameter (or a subset of it) in the limit of infinite data. This is equivalent to minimizing the 0-1 loss, $mathcal { L } ( pmb { theta } ^ { * } , hat { pmb { theta } } ) = mathbb { I } left( pmb { theta } ^ { * } neq hat { pmb { theta } } right)$ . An example of a consistent estimator is the maximum likelihood estimator (MLE). \nNote that an estimator can be unbiased but not consistent. For example, consider the estimator $pi ( { x _ { 1 } , ldots , x _ { N } } ) = x _ { N }$ . This is an unbiased estimator of the mean, since $mathbb { E } left[ pi ( mathcal { D } ) right] = mathbb { E } left[ pmb { x } right]$ . But the sampling distribution of $pi ( mathcal { D } )$ does not converge to a fixed value, so it cannot converge to the point $theta ^ { * }$ . \nAlthough consistency is a desirable property, it is of somewhat limited usefulness in practice since most real datasets do not come from our chosen model family (i.e., there is no $pmb { theta } ^ { * }$ such that $p ( cdot | pmb theta ^ { * } )$ generates the observed data $mathcal { D }$ ). In practice, it is more useful to find estimators that minimize some discrepancy measure between the empirical distribution $p _ { mathcal { D } } ( pmb { x } | mathcal { D } )$ and the estimated distribution $p ( { pmb x } | { hat { pmb theta } } )$ . If we use KL divergence as our discrepancy measure, our estimate becomes the MLE. \n5.3.3 Admissible estimators \nWe say that $pi _ { 1 }$ dominates $pi _ { 2 }$ if $R ( pmb theta , pi _ { 1 } ) leq R ( pmb theta , pi _ { 2 } )$ for all $pmb theta$ . The domination is said to be strict if the inequality is strict for some $theta ^ { * }$ . An estimator is said to be admissible if it is not strictly dominated by any other estimator. Interestingly, [Wal47] proved that all admissible decision rules are equivalent to some kind of Bayesian decision rule, under some technical conditions. (See [DR21] for a more general version of this result.) \nFor example, in Figure 5.8, we see that the sample median (dotted red line) always has higher risk than the sample mean (solid blue line). Therefore the sample median is not an admissible estimator for the mean. More surprisingly, one can show that the sample mean is not always an admissible estimator either, even under a Gaussian likelihood model with squared error loss (this is known as Stein’s paradox [Ste56]). \nHowever, the concept of admissibility is of somewhat limited value. For example, let $X sim { mathcal { N } } ( theta , 1 )$ , and consider estimating $theta$ under squared loss. Consider the estimator $pi _ { 1 } ( x ) = theta _ { 0 }$ , where $theta _ { 0 }$ is a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license constant independent of the data. We now show that this is an admissible estimator. \n\nTo see this, suppose it were not true. Then there would be some other estimator $pi _ { 2 }$ with smaller risk, so $R ( theta ^ { * } , pi _ { 2 } ) leq R ( theta ^ { * } , pi _ { 1 } )$ , where the inequality must be strict for some $theta ^ { * }$ . Consider the risk at $theta ^ { * } = theta _ { 0 }$ . We have $R ( theta _ { 0 } , pi _ { 1 } ) = 0$ , and \nSince $0 leq R ( theta ^ { * } , pi _ { 2 } ) leq R ( theta ^ { * } , pi _ { 1 } )$ for all $theta ^ { * }$ , and $R ( theta _ { 0 } , pi _ { 1 } ) = 0$ , we have $R ( theta _ { 0 } , pi _ { 2 } ) = 0$ and hence $pi _ { 2 } ( x ) = theta _ { 0 } = pi _ { 1 } ( x )$ . Thus the only way $pi _ { 2 }$ can avoid having higher risk than $pi _ { 1 }$ at $theta _ { 0 }$ is by being equal to $pi _ { 1 }$ . Hence there is no other estimator $pi _ { 2 }$ with strictly lower risk, so $pi _ { 2 }$ is admissible. \nThus we see that the estimator $pi _ { 1 } ( x ) = theta _ { 0 }$ is admissible, even though it ignores the data, so is useless as an estimator. Conversely, it is possible to construct useful estimators that are not admissable (see e.g., [Jay03, Sec 13.7]). \n5.4 Empirical risk minimization \nIn this section, we consider how to apply frequentist decision theory in the context of supervised learning. \n5.4.1 Empirical risk \nIn standard accounts of frequentist decision theory used in statistics textbooks, there is a single unknown “state of nature”, corresponding to the unknown parameters $theta ^ { * }$ of some model, and we define the risk as in Equation (5.63), namely $R ( pi , pmb { theta } ^ { * } ) = mathbb { E } _ { p ( mathcal { D } | pmb { theta } ^ { * } ) } left[ ell ( pmb { theta } ^ { * } , pi ( mathcal { D } ) ) right]$ . \nIn supervised learning, we have a different unknown state of nature (namely the output $y$ ) for each input $_ { x }$ , and our estimator $pi$ is a prediction function ${ hat { y } } = f ( { boldsymbol { mathbf { mathit { x } } } } )$ , and the state of nature is the true distribution $p ^ { * } ( { pmb x } , { pmb y } )$ . Thus the risk of an estimator is as follows: \nThis is called the population risk, since the expectations are taken wrt the true joint distribution $p ^ { * } ( { pmb x } , { pmb y } )$ . Of course, $p ^ { * }$ is unknown, but we can approximate it using the empirical distribution with $N$ samples: \nwhere $p _ { mathcal { D } } ( { pmb x } , { pmb y } ) = p _ { mathrm { t r } } ( { pmb x } , { pmb y } )$ . Plugging this in gives us the empirical risk: \nNote that $R ( f , { mathcal { D } } )$ is a random variable, since it depends on the training set. A natural way to choose the predictor is to use \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist decision theory",
        "subsubsection": "Admissible estimators"
    },
    {
        "content": "To see this, suppose it were not true. Then there would be some other estimator $pi _ { 2 }$ with smaller risk, so $R ( theta ^ { * } , pi _ { 2 } ) leq R ( theta ^ { * } , pi _ { 1 } )$ , where the inequality must be strict for some $theta ^ { * }$ . Consider the risk at $theta ^ { * } = theta _ { 0 }$ . We have $R ( theta _ { 0 } , pi _ { 1 } ) = 0$ , and \nSince $0 leq R ( theta ^ { * } , pi _ { 2 } ) leq R ( theta ^ { * } , pi _ { 1 } )$ for all $theta ^ { * }$ , and $R ( theta _ { 0 } , pi _ { 1 } ) = 0$ , we have $R ( theta _ { 0 } , pi _ { 2 } ) = 0$ and hence $pi _ { 2 } ( x ) = theta _ { 0 } = pi _ { 1 } ( x )$ . Thus the only way $pi _ { 2 }$ can avoid having higher risk than $pi _ { 1 }$ at $theta _ { 0 }$ is by being equal to $pi _ { 1 }$ . Hence there is no other estimator $pi _ { 2 }$ with strictly lower risk, so $pi _ { 2 }$ is admissible. \nThus we see that the estimator $pi _ { 1 } ( x ) = theta _ { 0 }$ is admissible, even though it ignores the data, so is useless as an estimator. Conversely, it is possible to construct useful estimators that are not admissable (see e.g., [Jay03, Sec 13.7]). \n5.4 Empirical risk minimization \nIn this section, we consider how to apply frequentist decision theory in the context of supervised learning. \n5.4.1 Empirical risk \nIn standard accounts of frequentist decision theory used in statistics textbooks, there is a single unknown “state of nature”, corresponding to the unknown parameters $theta ^ { * }$ of some model, and we define the risk as in Equation (5.63), namely $R ( pi , pmb { theta } ^ { * } ) = mathbb { E } _ { p ( mathcal { D } | pmb { theta } ^ { * } ) } left[ ell ( pmb { theta } ^ { * } , pi ( mathcal { D } ) ) right]$ . \nIn supervised learning, we have a different unknown state of nature (namely the output $y$ ) for each input $_ { x }$ , and our estimator $pi$ is a prediction function ${ hat { y } } = f ( { boldsymbol { mathbf { mathit { x } } } } )$ , and the state of nature is the true distribution $p ^ { * } ( { pmb x } , { pmb y } )$ . Thus the risk of an estimator is as follows: \nThis is called the population risk, since the expectations are taken wrt the true joint distribution $p ^ { * } ( { pmb x } , { pmb y } )$ . Of course, $p ^ { * }$ is unknown, but we can approximate it using the empirical distribution with $N$ samples: \nwhere $p _ { mathcal { D } } ( { pmb x } , { pmb y } ) = p _ { mathrm { t r } } ( { pmb x } , { pmb y } )$ . Plugging this in gives us the empirical risk: \nNote that $R ( f , { mathcal { D } } )$ is a random variable, since it depends on the training set. A natural way to choose the predictor is to use \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere we optimize over a specific hypothesis space $mathcal { H }$ of functions. This is called empirical risk minimization (ERM). \n5.4.1.1 Approximation error vs estimation error \nIn this section, we analyze the theoretical performance of functions that are fit using the ERM principle. Let $f ^ { * * } = mathrm { a r g m i n } _ { f } R ( f )$ be the function that achieves the minimal possible population risk, where we optimize over all possible functions. Of course, we cannot consider all possible functions, so let us also define $begin{array} { r } { f ^ { * } = operatorname * { a r g m i n } _ { f in mathcal { H } } R ( f ) } end{array}$ to be the best function in our hypothesis space, $mathcal { H }$ . Unfortunately we cannot compute $f ^ { * }$ , since we cannot compute the population risk, so let us finally define the prediction function that minimizes the empirical risk in our hypothesis space: \nOne can show [BB08] that the risk of our chosen predictor compared to the best possible predictor can be decomposed into two terms, as follows: \nThe first term, $mathcal { E } _ { mathrm { a p p } } ( mathcal { H } )$ , is the approximation error, which measures how closely $mathcal { H }$ can model the true optimal function $f ^ { * * }$ . The second term, $mathcal { E } _ { mathrm { e s t } } ( mathcal { H } , N )$ , is the estimation error or generalization error, which measures the difference in estimated risks due to having a finite training set. We can approximate this by the difference between the training set error and the test set error, using two empirical distributions drawn from $p ^ { * }$ : \nThis difference is often called the generalization gap. \nWe can decrease the approximation error by using a more expressive family of functions $mathcal { H }$ , but this usually increases the generalization error, due to overfitting. We discuss solutions to this tradeoff below. \n5.4.1.2 Regularized risk \nTo avoid the chance of overfitting, it is common to add a complexity penalty to the objective function, giving us the regularized empirical risk: \nwhere $C ( f )$ measures the complexity of the prediction function $f ( { pmb x } ; { pmb theta } )$ , and $lambda geq 0$ , which is known as a hyperparameter, controls the strength of the complexity penalty. (We discuss how to pick $lambda$ in Section 5.4.2.) \nIn practice, we usually work with parametric functions, and apply the regularizer to the parameters themselves. This yields the following form of the objective: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nNote that, if the loss function is log loss, and the regularizer is a negative log prior, the regularized risk is given by \nMinimizing this is equivalent to MAP estimation. \n5.4.2 Structural risk \nA natural way to estimate the hyperparameters is to minimize for the lowest achievable empirical risk: \n(This is an example of bilevel optimization, also called nested optimization.) Unfortunately, this technique will not work, since it will always pick the least amount of regularization, i.e., $hat { lambda } = 0$ . To see this, note that \nwhich is minimized by setting $lambda = 0$ . The problem is that the empirical risk underestimates the population risk, resulting in overfitting when we choose $lambda$ . This is called optimism of the training error. \nIf we knew the regularized population risk $R _ { lambda } ( pmb theta )$ , instead of the regularized empirical risk $R _ { lambda } ( pmb theta , D )$ , we could use it to pick a model of the right complexity (e.g., value of $lambda$ ). This is known as structural risk minimization [Vap98]. There are two main ways to estimate the population risk for a given model (value of $lambda$ ), namely cross-validation (Section 5.4.3), and statistical learning theory (Section 5.4.4), which we discuss below. \n5.4.3 Cross-validation \nIn this section, we discuss a simple way to estimate the population risk for a supervised learning setup. We simply partition the dataset into two, the part used for training the model, and a second part, called the validation set or holdout set, used for assessing the risk. We can fit the model on the training set, and use its performance on the validation set as an approximation to the population risk. \nTo explain the method in more detail, we need some notation. First we make the dependence of the empirical risk on the dataset more explicit as follows: \nLet us also define $hat { pmb { theta } } _ { lambda } ( mathcal { D } ) = mathrm { a r g m i n } _ { pmb { theta } } R _ { lambda } ( mathcal { D } , pmb { theta } )$ . Finally, let $mathscr { D } _ { mathrm { t r a i n } }$ and $mathcal { D } _ { mathrm { v a l i d } }$ be a partition of $mathcal { D }$ . (Often we use about $8 0 %$ of the data for the training set, and $2 0 %$ for the validation set.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Empirical risk minimization",
        "subsubsection": "Empirical risk"
    },
    {
        "content": "Note that, if the loss function is log loss, and the regularizer is a negative log prior, the regularized risk is given by \nMinimizing this is equivalent to MAP estimation. \n5.4.2 Structural risk \nA natural way to estimate the hyperparameters is to minimize for the lowest achievable empirical risk: \n(This is an example of bilevel optimization, also called nested optimization.) Unfortunately, this technique will not work, since it will always pick the least amount of regularization, i.e., $hat { lambda } = 0$ . To see this, note that \nwhich is minimized by setting $lambda = 0$ . The problem is that the empirical risk underestimates the population risk, resulting in overfitting when we choose $lambda$ . This is called optimism of the training error. \nIf we knew the regularized population risk $R _ { lambda } ( pmb theta )$ , instead of the regularized empirical risk $R _ { lambda } ( pmb theta , D )$ , we could use it to pick a model of the right complexity (e.g., value of $lambda$ ). This is known as structural risk minimization [Vap98]. There are two main ways to estimate the population risk for a given model (value of $lambda$ ), namely cross-validation (Section 5.4.3), and statistical learning theory (Section 5.4.4), which we discuss below. \n5.4.3 Cross-validation \nIn this section, we discuss a simple way to estimate the population risk for a supervised learning setup. We simply partition the dataset into two, the part used for training the model, and a second part, called the validation set or holdout set, used for assessing the risk. We can fit the model on the training set, and use its performance on the validation set as an approximation to the population risk. \nTo explain the method in more detail, we need some notation. First we make the dependence of the empirical risk on the dataset more explicit as follows: \nLet us also define $hat { pmb { theta } } _ { lambda } ( mathcal { D } ) = mathrm { a r g m i n } _ { pmb { theta } } R _ { lambda } ( mathcal { D } , pmb { theta } )$ . Finally, let $mathscr { D } _ { mathrm { t r a i n } }$ and $mathcal { D } _ { mathrm { v a l i d } }$ be a partition of $mathcal { D }$ . (Often we use about $8 0 %$ of the data for the training set, and $2 0 %$ for the validation set.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Empirical risk minimization",
        "subsubsection": "Structural risk"
    },
    {
        "content": "Note that, if the loss function is log loss, and the regularizer is a negative log prior, the regularized risk is given by \nMinimizing this is equivalent to MAP estimation. \n5.4.2 Structural risk \nA natural way to estimate the hyperparameters is to minimize for the lowest achievable empirical risk: \n(This is an example of bilevel optimization, also called nested optimization.) Unfortunately, this technique will not work, since it will always pick the least amount of regularization, i.e., $hat { lambda } = 0$ . To see this, note that \nwhich is minimized by setting $lambda = 0$ . The problem is that the empirical risk underestimates the population risk, resulting in overfitting when we choose $lambda$ . This is called optimism of the training error. \nIf we knew the regularized population risk $R _ { lambda } ( pmb theta )$ , instead of the regularized empirical risk $R _ { lambda } ( pmb theta , D )$ , we could use it to pick a model of the right complexity (e.g., value of $lambda$ ). This is known as structural risk minimization [Vap98]. There are two main ways to estimate the population risk for a given model (value of $lambda$ ), namely cross-validation (Section 5.4.3), and statistical learning theory (Section 5.4.4), which we discuss below. \n5.4.3 Cross-validation \nIn this section, we discuss a simple way to estimate the population risk for a supervised learning setup. We simply partition the dataset into two, the part used for training the model, and a second part, called the validation set or holdout set, used for assessing the risk. We can fit the model on the training set, and use its performance on the validation set as an approximation to the population risk. \nTo explain the method in more detail, we need some notation. First we make the dependence of the empirical risk on the dataset more explicit as follows: \nLet us also define $hat { pmb { theta } } _ { lambda } ( mathcal { D } ) = mathrm { a r g m i n } _ { pmb { theta } } R _ { lambda } ( mathcal { D } , pmb { theta } )$ . Finally, let $mathscr { D } _ { mathrm { t r a i n } }$ and $mathcal { D } _ { mathrm { v a l i d } }$ be a partition of $mathcal { D }$ . (Often we use about $8 0 %$ of the data for the training set, and $2 0 %$ for the validation set.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nFor each model $lambda$ , we fit it to the training set to get $begin{array} { r } { hat { pmb { theta } } _ { lambda } ( mathcal { D } _ { mathrm { t r a i n } } ) } end{array}$ . We then use the unregularized empirical risk on the validation set as an estimate of the population risk. This is known as the validation risk: \nNote that we use different data to train and evaluate the model. \nThe above technique can work very well. However, if the number of training cases is small, this technique runs into problems, because the model won’t have enough data to train on, and we won’t have enough data to make a reliable estimate of the future performance. \nA simple but popular solution to this is to use cross validation (CV). The idea is as follows: we split the training data into $K$ folds; then, for each fold $k in { 1 , ldots , K }$ , we train on all the folds but the $k$ ’th, and test on the $k$ ’th, in a round-robin fashion, as sketched in Figure 4.6. Formally, we have \nwhere $mathcal { D } _ { k }$ is the data in the $k$ ’th fold, and $mathcal { D } _ { - k }$ is all the other data. This is called the cross-validated risk. Figure 4.6 illustrates this procedure for $K = 5$ . If we set $K = N$ , we get a method known as leave-one-out cross-validation, since we always train on $N - 1$ items and test on the remaining one. \nWe can use the CV estimate as an objective inside of an optimization routine to pick the optimal hyperparameter, $hat { lambda } = mathrm { a r g m i n } _ { lambda } R _ { lambda } ^ { mathrm { c v } }$ . Finally we combine all the available data (training and validation), and re-estimate the model parameters using $hat { pmb theta } = mathrm { a r g m i n } _ { pmb theta } R _ { hat { lambda } } ( pmb theta , mathcal { D } )$ . \n5.4.4 Statistical learning theory * \nThe principal problem with cross validation is that it is slow, since we have to fit the model multiple times. This motivates the desire to compute analytic approximations or bounds on the population risk. This is studied in the field of statistical learning theory (SLT) (see e.g., [Vap98]). \nMore precisely, the goal of SLT is to upper bound the generalization error with a certain probability. If the bound is satisfied, then we can be confident that a hypothesis that is chosen by minimizing the empirical risk will have low population risk. In the case of binary classifiers, this means the hypothesis will make the correct predictions; in this case we say it is probably approximately correct, and that the hypothesis class is PAC learnable (see e.g., [KV94] for details). \n5.4.4.1 Bounding the generalization error \nIn this section, we establish conditions under which we can prove that a hypothesis class is PAC learnable. Let us initially consider the case where the hypothesis space is finite, with size $dim ( { mathcal { H } } ) = | { mathcal { H } } |$ . In other words, we are selecting a hypothesis from a finite list, rather than optimizing real-valued parameters. In this case, we can prove the following. \nTheorem 5.4.1. For any data distribution $p ^ { * }$ , and any dataset $mathcal { D }$ of size $N _ { mathcal { D } }$ drawn from $p ^ { * }$ , the probability that the generalization error of a binary classifier will be more than $epsilon$ , in the worst case, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Empirical risk minimization",
        "subsubsection": "Cross-validation"
    },
    {
        "content": "For each model $lambda$ , we fit it to the training set to get $begin{array} { r } { hat { pmb { theta } } _ { lambda } ( mathcal { D } _ { mathrm { t r a i n } } ) } end{array}$ . We then use the unregularized empirical risk on the validation set as an estimate of the population risk. This is known as the validation risk: \nNote that we use different data to train and evaluate the model. \nThe above technique can work very well. However, if the number of training cases is small, this technique runs into problems, because the model won’t have enough data to train on, and we won’t have enough data to make a reliable estimate of the future performance. \nA simple but popular solution to this is to use cross validation (CV). The idea is as follows: we split the training data into $K$ folds; then, for each fold $k in { 1 , ldots , K }$ , we train on all the folds but the $k$ ’th, and test on the $k$ ’th, in a round-robin fashion, as sketched in Figure 4.6. Formally, we have \nwhere $mathcal { D } _ { k }$ is the data in the $k$ ’th fold, and $mathcal { D } _ { - k }$ is all the other data. This is called the cross-validated risk. Figure 4.6 illustrates this procedure for $K = 5$ . If we set $K = N$ , we get a method known as leave-one-out cross-validation, since we always train on $N - 1$ items and test on the remaining one. \nWe can use the CV estimate as an objective inside of an optimization routine to pick the optimal hyperparameter, $hat { lambda } = mathrm { a r g m i n } _ { lambda } R _ { lambda } ^ { mathrm { c v } }$ . Finally we combine all the available data (training and validation), and re-estimate the model parameters using $hat { pmb theta } = mathrm { a r g m i n } _ { pmb theta } R _ { hat { lambda } } ( pmb theta , mathcal { D } )$ . \n5.4.4 Statistical learning theory * \nThe principal problem with cross validation is that it is slow, since we have to fit the model multiple times. This motivates the desire to compute analytic approximations or bounds on the population risk. This is studied in the field of statistical learning theory (SLT) (see e.g., [Vap98]). \nMore precisely, the goal of SLT is to upper bound the generalization error with a certain probability. If the bound is satisfied, then we can be confident that a hypothesis that is chosen by minimizing the empirical risk will have low population risk. In the case of binary classifiers, this means the hypothesis will make the correct predictions; in this case we say it is probably approximately correct, and that the hypothesis class is PAC learnable (see e.g., [KV94] for details). \n5.4.4.1 Bounding the generalization error \nIn this section, we establish conditions under which we can prove that a hypothesis class is PAC learnable. Let us initially consider the case where the hypothesis space is finite, with size $dim ( { mathcal { H } } ) = | { mathcal { H } } |$ . In other words, we are selecting a hypothesis from a finite list, rather than optimizing real-valued parameters. In this case, we can prove the following. \nTheorem 5.4.1. For any data distribution $p ^ { * }$ , and any dataset $mathcal { D }$ of size $N _ { mathcal { D } }$ drawn from $p ^ { * }$ , the probability that the generalization error of a binary classifier will be more than $epsilon$ , in the worst case, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nis upper bounded as follows: \nwhere $begin{array} { r } { R ( h , mathcal { D } ) = frac { 1 } { N _ { mathcal { D } } } sum _ { i = 1 } ^ { N } mathbb { I } left( f ( { boldsymbol x } _ { i } ) neq { boldsymbol y } _ { i } ^ { * } right) } end{array}$ is the empirical risk, and $R ( h ) = mathbb { E } left[ mathbb { I } left( f ( pmb { x } ) neq y ^ { * } right) right]$ is the population risk. \nProof. Before we prove this, we introduce two useful results. First, Hoeffding’s inequality, which states that if $E _ { 1 } , dots , E _ { N _ { mathcal { D } } } sim mathrm { B e r } ( theta )$ , then, for any $epsilon > 0$ , \nwhere $begin{array} { r } { overline { { E } } = frac { 1 } { N _ { mathcal { D } } } sum _ { i = 1 } ^ { N _ { mathcal { D } } } E _ { i } } end{array}$ is the empirical error rate, and $theta$ is the true error rate. Second, the union bound, which says that if $A _ { 1 } , ldots , A _ { d }$ are a set of events, then $begin{array} { r } { P ( cup _ { i = 1 } ^ { d } A _ { i } ) leq sum _ { i = 1 } ^ { d } P ( A _ { i } ) } end{array}$ . Using these results, we have \nThis bound tells us that the optimism of the training error increases with $dim ( { mathcal { H } } )$ but decreases with $N _ { mathit { D } } = | mathcal { D } |$ , as is to be expected. \n5.4.4.2 VC dimension \nIf the hypothesis space $mathcal { H }$ is infinite (e.g., we have real-valued parameters), we cannot use $dim ( { mathcal { H } } ) =$ $| mathcal { H } |$ . Instead, we can use a quantity called the VC dimension of the hypothesis class, named after Vapnik and Chervonenkis; this measures the degrees of freedom (effective number of parameters) of the hypothesis class. See e.g., [Vap98] for the details. \nUnfortunately, it is hard to compute the VC dimension for many interesting models, and the upper bounds are usually very loose, making this approach of limited practical value. However, various other, more practical, estimates of generalization error have recently been devised, especially for DNNs, such as [Jia+20]. \n5.5 Frequentist hypothesis testing * \nSuppose we have two hypotheses, known as the null hypothesis $H _ { 0 }$ and an alternative hypothesis $boldsymbol { H } _ { 1 }$ , and we want to choose the one we think is correct on the basis of a dataset $mathcal { D }$ . We could use a Bayesian approach and compute the Bayes factor $p ( H _ { 0 } | mathcal { D } ) / p ( H _ { 1 } | mathcal { D } )$ , as we discussed in Section 5.2.1. However, this requires integrating over all possible parameterizations of the models $H _ { 0 }$ and $H _ { 1 }$ , which can be computationally difficult, and which can be sensitive to the choice of prior. In this section, we consider a frequentist approach to the problem. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Empirical risk minimization",
        "subsubsection": "Statistical learning theory *"
    },
    {
        "content": "5.5.1 Likelihood ratio test \nIf we use 0-1 loss, and assume $p ( H _ { 0 } ) = p ( H _ { 1 } )$ , then the optimal decision rule is to accept $H _ { 0 }$ iff p(D|H0) > 1. This is called the likelihood ratio test. We give some examples of this below. \n5.5.1.1 Example: comparing Gaussian means \nSuppose we are interested in testing whether some data comes from a Gaussian with mean $mu _ { 0 }$ or from a Gaussian with mean $mu _ { 1 }$ . (We assume a known shared variance $sigma ^ { 2 }$ .) This is illustrated in Figure 5.10a, where we plot $p ( x | H _ { 0 } )$ and $p ( x | H _ { 1 } )$ . We can derive the likelihood ratio as follows: \nWe see that this ratio only depends on the observed data via its mean, $textstyle { overline { { x } } }$ . This is an example of a test statistic $operatorname { t e s t } ( mathcal { D } )$ , which is a scalar sufficient statistic for hypothesis testing. From Figure 5.10a, $begin{array} { r } { frac { p ( mathcal { D } | H _ { 0 } ) } { p ( mathcal { D } | H _ { 1 } ) } > 1 } end{array}$ $overline { { x } } < x ^ { * }$ $x ^ { * }$ assuming this point is unique). \n5.5.1.2 Simple vs compound hypotheses \nIn Section 5.5.1.1, the parameters for the null and alternative hypotheses were either fully specified ( $mu _ { 0 }$ and $mu _ { 1 }$ ) or shared $( sigma ^ { 2 } )$ . This is called a simple hypothesis test. In general, a hypothesis might not fully specify all the parameters; this is called a compound hypothesis. In this case, we should integrate out these unknown parameters, as in the Bayesian approach, since a hypothesis with more parameters will always have higher likelihood. As an approximation, we can “maximize them out”, which gives us the maximum likelihood ratio test: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n5.5.2 Null hypothesis significance testing (NHST) \nRather than assuming 0-1 loss, it is conventional to design the decision rule so that it has a type I error rate (the probability of accidentally rejecting the null hypothesis $H _ { 0 }$ ) of $alpha$ . (See Section 5.1.3 for details on error rates of binary decision rules.) The error rate $alpha$ is called the significance of the test. Hence the overall approach is called null hypothesis significance testing or NHST. \nIn our Gaussian mean example, we see from Figure 5.10a that the type I error rate is the vertical shaded blue area: \nHence $x ^ { * } = z _ { alpha } sigma / sqrt { N } + mu _ { 0 }$ , where $z _ { alpha }$ is the upper $alpha$ quantile of the standard Normal. \nThe type II error rate is the probability we accidentally accept the null when the alternative is true: \nThis is shown by the horizontal shaded red area in Figure 5.10a. We define the power of a test as $1 - beta ( mu _ { 1 } )$ ; this is the probability that we reject $H _ { 0 }$ given that $H _ { 1 }$ is true. In other words, it is the ability to correctly recognize that the null hypothesis is wrong. Clearly the least power occurs if $mu _ { 1 } = mu _ { 0 }$ (so the curves overlap); in this case, we have $1 - beta ( mu _ { 1 } ) = alpha ( mu _ { 0 } )$ . As $mu _ { 1 }$ and $mu _ { 0 }$ become further apart, the power approaches 1 (because the shaded red area gets smaller, $beta  0$ ). If we have two tests, $A$ and $B$ , where $operatorname { p o w e r } ( B ) geq operatorname { p o w e r } ( A )$ for the same type I error rate, we say $B$ dominates $A$ . See Figure 5.10b. A test with highest power under $H _ { 1 }$ amongst all tests with significance level $alpha$ is called a most powerful test. It turns out that the likelihood ratio test is a most powerful test, a result known as the Neyman-Pearson lemma. \n5.5.3 p-values \nWhen we reject $H _ { 0 }$ we often say the result is statistically significant at level $alpha$ . However, the result may be statistically significant but not practically significant, depending on how far from the decision boundary the test statistic is. \nRather than arbitrarily declaring a result as significant or not, it is preferable to quote the $mathbf { p }$ -value. This is defined as the probability, under the null hypothesis, of observing a test statistic that is as large or larger than that actually observed: \nIn other words, $mathrm { p v a l } ( mathrm { t e s t _ { o b s } } ) triangleq mathrm { P r } ( mathrm { t e s t _ { n u l l } } geq mathrm { t e s t _ { o b s } } )$ ), where $mathrm {  t e s t { o b s } } = mathrm { t e s t } ( mathcal { D } )$ and $mathrm { t e s t } _ { mathrm { n u l l } } = mathrm { t e s t } ( tilde { mathcal { D } } )$ , where $tilde { mathcal { D } } sim H _ { 0 }$ is hypothetical future data. To see the connection with hypothesis testing, suppose we pick a decision threshold $t ^ { * }$ such that $mathrm { P r } ( mathrm { t e s t } ( tilde { mathcal { D } } ) geq t ^ { * } | H _ { 0 } ) = alpha$ . If we set $t ^ { * } = mathrm { t e s t } ( mathcal { D } )$ , then $alpha = mathrm { p v a l } ( mathrm { t e s t } ( mathcal { D } ) )$ . \nThus if we only accept hypotheses where the p-value is less than $alpha = 0 . 0 5$ , then $9 5 %$ of the time we will correctly reject the null hypothesis. However, this does not mean that the alternative hypothesis \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist hypothesis testing *",
        "subsubsection": "Likelihood ratio test"
    },
    {
        "content": "5.5.2 Null hypothesis significance testing (NHST) \nRather than assuming 0-1 loss, it is conventional to design the decision rule so that it has a type I error rate (the probability of accidentally rejecting the null hypothesis $H _ { 0 }$ ) of $alpha$ . (See Section 5.1.3 for details on error rates of binary decision rules.) The error rate $alpha$ is called the significance of the test. Hence the overall approach is called null hypothesis significance testing or NHST. \nIn our Gaussian mean example, we see from Figure 5.10a that the type I error rate is the vertical shaded blue area: \nHence $x ^ { * } = z _ { alpha } sigma / sqrt { N } + mu _ { 0 }$ , where $z _ { alpha }$ is the upper $alpha$ quantile of the standard Normal. \nThe type II error rate is the probability we accidentally accept the null when the alternative is true: \nThis is shown by the horizontal shaded red area in Figure 5.10a. We define the power of a test as $1 - beta ( mu _ { 1 } )$ ; this is the probability that we reject $H _ { 0 }$ given that $H _ { 1 }$ is true. In other words, it is the ability to correctly recognize that the null hypothesis is wrong. Clearly the least power occurs if $mu _ { 1 } = mu _ { 0 }$ (so the curves overlap); in this case, we have $1 - beta ( mu _ { 1 } ) = alpha ( mu _ { 0 } )$ . As $mu _ { 1 }$ and $mu _ { 0 }$ become further apart, the power approaches 1 (because the shaded red area gets smaller, $beta  0$ ). If we have two tests, $A$ and $B$ , where $operatorname { p o w e r } ( B ) geq operatorname { p o w e r } ( A )$ for the same type I error rate, we say $B$ dominates $A$ . See Figure 5.10b. A test with highest power under $H _ { 1 }$ amongst all tests with significance level $alpha$ is called a most powerful test. It turns out that the likelihood ratio test is a most powerful test, a result known as the Neyman-Pearson lemma. \n5.5.3 p-values \nWhen we reject $H _ { 0 }$ we often say the result is statistically significant at level $alpha$ . However, the result may be statistically significant but not practically significant, depending on how far from the decision boundary the test statistic is. \nRather than arbitrarily declaring a result as significant or not, it is preferable to quote the $mathbf { p }$ -value. This is defined as the probability, under the null hypothesis, of observing a test statistic that is as large or larger than that actually observed: \nIn other words, $mathrm { p v a l } ( mathrm { t e s t _ { o b s } } ) triangleq mathrm { P r } ( mathrm { t e s t _ { n u l l } } geq mathrm { t e s t _ { o b s } } )$ ), where $mathrm {  t e s t { o b s } } = mathrm { t e s t } ( mathcal { D } )$ and $mathrm { t e s t } _ { mathrm { n u l l } } = mathrm { t e s t } ( tilde { mathcal { D } } )$ , where $tilde { mathcal { D } } sim H _ { 0 }$ is hypothetical future data. To see the connection with hypothesis testing, suppose we pick a decision threshold $t ^ { * }$ such that $mathrm { P r } ( mathrm { t e s t } ( tilde { mathcal { D } } ) geq t ^ { * } | H _ { 0 } ) = alpha$ . If we set $t ^ { * } = mathrm { t e s t } ( mathcal { D } )$ , then $alpha = mathrm { p v a l } ( mathrm { t e s t } ( mathcal { D } ) )$ . \nThus if we only accept hypotheses where the p-value is less than $alpha = 0 . 0 5$ , then $9 5 %$ of the time we will correctly reject the null hypothesis. However, this does not mean that the alternative hypothesis \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist hypothesis testing *",
        "subsubsection": "Null hypothesis significance testing (NHST)"
    },
    {
        "content": "5.5.2 Null hypothesis significance testing (NHST) \nRather than assuming 0-1 loss, it is conventional to design the decision rule so that it has a type I error rate (the probability of accidentally rejecting the null hypothesis $H _ { 0 }$ ) of $alpha$ . (See Section 5.1.3 for details on error rates of binary decision rules.) The error rate $alpha$ is called the significance of the test. Hence the overall approach is called null hypothesis significance testing or NHST. \nIn our Gaussian mean example, we see from Figure 5.10a that the type I error rate is the vertical shaded blue area: \nHence $x ^ { * } = z _ { alpha } sigma / sqrt { N } + mu _ { 0 }$ , where $z _ { alpha }$ is the upper $alpha$ quantile of the standard Normal. \nThe type II error rate is the probability we accidentally accept the null when the alternative is true: \nThis is shown by the horizontal shaded red area in Figure 5.10a. We define the power of a test as $1 - beta ( mu _ { 1 } )$ ; this is the probability that we reject $H _ { 0 }$ given that $H _ { 1 }$ is true. In other words, it is the ability to correctly recognize that the null hypothesis is wrong. Clearly the least power occurs if $mu _ { 1 } = mu _ { 0 }$ (so the curves overlap); in this case, we have $1 - beta ( mu _ { 1 } ) = alpha ( mu _ { 0 } )$ . As $mu _ { 1 }$ and $mu _ { 0 }$ become further apart, the power approaches 1 (because the shaded red area gets smaller, $beta  0$ ). If we have two tests, $A$ and $B$ , where $operatorname { p o w e r } ( B ) geq operatorname { p o w e r } ( A )$ for the same type I error rate, we say $B$ dominates $A$ . See Figure 5.10b. A test with highest power under $H _ { 1 }$ amongst all tests with significance level $alpha$ is called a most powerful test. It turns out that the likelihood ratio test is a most powerful test, a result known as the Neyman-Pearson lemma. \n5.5.3 p-values \nWhen we reject $H _ { 0 }$ we often say the result is statistically significant at level $alpha$ . However, the result may be statistically significant but not practically significant, depending on how far from the decision boundary the test statistic is. \nRather than arbitrarily declaring a result as significant or not, it is preferable to quote the $mathbf { p }$ -value. This is defined as the probability, under the null hypothesis, of observing a test statistic that is as large or larger than that actually observed: \nIn other words, $mathrm { p v a l } ( mathrm { t e s t _ { o b s } } ) triangleq mathrm { P r } ( mathrm { t e s t _ { n u l l } } geq mathrm { t e s t _ { o b s } } )$ ), where $mathrm {  t e s t { o b s } } = mathrm { t e s t } ( mathcal { D } )$ and $mathrm { t e s t } _ { mathrm { n u l l } } = mathrm { t e s t } ( tilde { mathcal { D } } )$ , where $tilde { mathcal { D } } sim H _ { 0 }$ is hypothetical future data. To see the connection with hypothesis testing, suppose we pick a decision threshold $t ^ { * }$ such that $mathrm { P r } ( mathrm { t e s t } ( tilde { mathcal { D } } ) geq t ^ { * } | H _ { 0 } ) = alpha$ . If we set $t ^ { * } = mathrm { t e s t } ( mathcal { D } )$ , then $alpha = mathrm { p v a l } ( mathrm { t e s t } ( mathcal { D } ) )$ . \nThus if we only accept hypotheses where the p-value is less than $alpha = 0 . 0 5$ , then $9 5 %$ of the time we will correctly reject the null hypothesis. However, this does not mean that the alternative hypothesis \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n$H _ { 1 }$ is true with probability 0.95. Indeed, even most scientists misinterpret p-values.4 The quantity that most people want to compute is the Bayesian posterior $p ( H _ { 1 } | mathcal { D } ) = 0 . 9 5$ . For more on this important distinction, see Section 5.5.4. \n5.5.4 p-values considered harmful \nA p-value is often interpreted as the likelihood of the data under the null hypothesis, so small values are interpreted to mean that $H _ { 0 }$ is unlikely, and therefore that $boldsymbol { H } _ { 1 }$ is likely. The reasoning is roughly as follows: \nIf $H _ { 0 }$ is true, then this test statistic would probably not occur. This statistic did occur.   \nTherefore $H _ { 0 }$ is probably false. \nHowever, this is invalid reasoning. To see why, consider the following example (from [Coh94]): \nIf a person is an American, then he is probably not a member of Congress. This person is a member of Congress. Therefore he is probably not an American. \nThis is obviously fallacious reasoning. By contrast, the following logical argument is valid reasoning: \nIf a person is a Martian, then he is not a member of Congress. This person is a member of Congress. Therefore he is not a Martian. \nThe difference between these two cases is that the Martian example is using deduction, that is, reasoning forward from logical definitions to their consequences. More precisely, this example uses a rule from logic called modus tollens, in which we start out with a definition of the form $P Rightarrow Q$ ; when we observe $neg Q$ , we can conclude $neg P$ . By contrast, the American example concerns induction, that is, reasoning backwards from observed evidence to probable (but not necessarily true) causes using statistical regularities, not logical definitions. \nTo perform induction, we need to use probabilistic inference (as explained in detail in [Jay03]). In particular, to compute the probability of the null hypothesis, we should use Bayes rule, as follows: \nIf the prior is uniform, so $p ( H _ { 0 } ) = p ( H _ { 1 } ) = 0 . 5$ , this can be rewritten in terms of the likelihood ratio $L R = p ( { mathcal { D } } | H _ { 0 } ) / p ( { mathcal { D } } | H _ { 1 } )$ as follows: \nIn the American Congress example, $mathcal { D }$ is the observation that the person is a member of Congress. The null hypothesis $H _ { 0 }$ is that the person is American, and the alternative hypothesis $boldsymbol { H } _ { 1 }$ is that the person is not American. We assume that $p ( mathcal { D } | H _ { 0 } )$ is low, since most Americans are not members of Congress. However, $p ( mathcal { D } | H _ { 1 } )$ is also low — in fact, in this example, it is 0, since only Americans can be members of Congress. Hence $L R = infty$ , so $p ( H _ { 0 } | mathcal { D } ) = 1 . 0$ , as intuition suggests. Note, however, that NHST ignores $p ( mathcal { D } | H _ { 1 } )$ as well as the prior $p ( H _ { 0 } )$ , so it gives the wrong results — not just in this problem, but in many problems.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist hypothesis testing *",
        "subsubsection": "p-values"
    },
    {
        "content": "$H _ { 1 }$ is true with probability 0.95. Indeed, even most scientists misinterpret p-values.4 The quantity that most people want to compute is the Bayesian posterior $p ( H _ { 1 } | mathcal { D } ) = 0 . 9 5$ . For more on this important distinction, see Section 5.5.4. \n5.5.4 p-values considered harmful \nA p-value is often interpreted as the likelihood of the data under the null hypothesis, so small values are interpreted to mean that $H _ { 0 }$ is unlikely, and therefore that $boldsymbol { H } _ { 1 }$ is likely. The reasoning is roughly as follows: \nIf $H _ { 0 }$ is true, then this test statistic would probably not occur. This statistic did occur.   \nTherefore $H _ { 0 }$ is probably false. \nHowever, this is invalid reasoning. To see why, consider the following example (from [Coh94]): \nIf a person is an American, then he is probably not a member of Congress. This person is a member of Congress. Therefore he is probably not an American. \nThis is obviously fallacious reasoning. By contrast, the following logical argument is valid reasoning: \nIf a person is a Martian, then he is not a member of Congress. This person is a member of Congress. Therefore he is not a Martian. \nThe difference between these two cases is that the Martian example is using deduction, that is, reasoning forward from logical definitions to their consequences. More precisely, this example uses a rule from logic called modus tollens, in which we start out with a definition of the form $P Rightarrow Q$ ; when we observe $neg Q$ , we can conclude $neg P$ . By contrast, the American example concerns induction, that is, reasoning backwards from observed evidence to probable (but not necessarily true) causes using statistical regularities, not logical definitions. \nTo perform induction, we need to use probabilistic inference (as explained in detail in [Jay03]). In particular, to compute the probability of the null hypothesis, we should use Bayes rule, as follows: \nIf the prior is uniform, so $p ( H _ { 0 } ) = p ( H _ { 1 } ) = 0 . 5$ , this can be rewritten in terms of the likelihood ratio $L R = p ( { mathcal { D } } | H _ { 0 } ) / p ( { mathcal { D } } | H _ { 1 } )$ as follows: \nIn the American Congress example, $mathcal { D }$ is the observation that the person is a member of Congress. The null hypothesis $H _ { 0 }$ is that the person is American, and the alternative hypothesis $boldsymbol { H } _ { 1 }$ is that the person is not American. We assume that $p ( mathcal { D } | H _ { 0 } )$ is low, since most Americans are not members of Congress. However, $p ( mathcal { D } | H _ { 1 } )$ is also low — in fact, in this example, it is 0, since only Americans can be members of Congress. Hence $L R = infty$ , so $p ( H _ { 0 } | mathcal { D } ) = 1 . 0$ , as intuition suggests. Note, however, that NHST ignores $p ( mathcal { D } | H _ { 1 } )$ as well as the prior $p ( H _ { 0 } )$ , so it gives the wrong results — not just in this problem, but in many problems. \nIn general there can be huge differences between p-values and $p ( H _ { 0 } | mathcal { D } )$ . In particular, [SBB01] show that even if the p-value is as low as 0.05, the posterior probability of $H _ { 0 }$ can be as high as $3 0 %$ or more, even with a uniform prior. \nConsider this concrete example from [SAM04, p74]. Suppose 200 clinical trials are carried out for some drug, and we get the data in Table 5.8. Suppose we perform a statistical test of whether the drug has a significant effect or not. The test has a type I error rate of $alpha = 9 / 2 0 0 = 0 . 0 4 5$ and a type II error rate of $beta = 2 / 1 0 0 = 0 . 0 2$ . \nWe can compute the probability that the drug is not effective, given that the result is supposedly “significant”, as follows: \nIf we have prior knowledge, based on past experience, that most (say 90%) drugs are ineffective, then we find $p ( H _ { 0 } | ^ { prime } mathrm { s i g n i f i c a n t } ^ { prime } ) = 0 . 3 6$ , which is much more than the 5% probability people usually associate with a p-value of $alpha = 0 . 0 5$ . \nThus we should distrust claims of statistical significance if they violate our prior knowledge. \n5.5.5 Why isn’t everyone a Bayesian? \nIn Section 4.7.5 and Section 5.5.4, we have seen that inference based on frequentist principles can exhibit various forms of counter-intuitive behavior that can sometimes contradict common sense reason, as has been pointed out in multiple articles (see e.g., [Mat98; MS11; Kru13; Gel16; Hoe+14; Lyu+20; Cha+19b; Cla21]). \nThe fundamental reason is that frequentist inference violates the likelihood principle [BW88], which says that inference should be based on the likelihood of the observed data, not on hypothetical future data that you have not observed. Bayes obviously satisfies the likelihood principle, and consequently does not suffer from these pathologies. \nGiven these fundamental flaws of frequentist statistics, and the fact that Bayesian methods do not have such flaws, an obvious question to ask is: “Why isn’t everyone a Bayesian?” The (frequentist) statistician Bradley Efron wrote a paper with exactly this title [Efr86]. His short paper is well worth reading for anyone interested in this topic. Below we quote his opening section: \nThe title is a reasonable question to ask on at least two counts. First of all, everyone used to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 be a Bayesian. Laplace wholeheartedly endorsed Bayes’s formulation of the inference problem, and most 19th-century scientists followed suit. This included Gauss, whose statistical work is usually presented in frequentist terms.",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist hypothesis testing *",
        "subsubsection": "p-values considered harmful"
    },
    {
        "content": "In general there can be huge differences between p-values and $p ( H _ { 0 } | mathcal { D } )$ . In particular, [SBB01] show that even if the p-value is as low as 0.05, the posterior probability of $H _ { 0 }$ can be as high as $3 0 %$ or more, even with a uniform prior. \nConsider this concrete example from [SAM04, p74]. Suppose 200 clinical trials are carried out for some drug, and we get the data in Table 5.8. Suppose we perform a statistical test of whether the drug has a significant effect or not. The test has a type I error rate of $alpha = 9 / 2 0 0 = 0 . 0 4 5$ and a type II error rate of $beta = 2 / 1 0 0 = 0 . 0 2$ . \nWe can compute the probability that the drug is not effective, given that the result is supposedly “significant”, as follows: \nIf we have prior knowledge, based on past experience, that most (say 90%) drugs are ineffective, then we find $p ( H _ { 0 } | ^ { prime } mathrm { s i g n i f i c a n t } ^ { prime } ) = 0 . 3 6$ , which is much more than the 5% probability people usually associate with a p-value of $alpha = 0 . 0 5$ . \nThus we should distrust claims of statistical significance if they violate our prior knowledge. \n5.5.5 Why isn’t everyone a Bayesian? \nIn Section 4.7.5 and Section 5.5.4, we have seen that inference based on frequentist principles can exhibit various forms of counter-intuitive behavior that can sometimes contradict common sense reason, as has been pointed out in multiple articles (see e.g., [Mat98; MS11; Kru13; Gel16; Hoe+14; Lyu+20; Cha+19b; Cla21]). \nThe fundamental reason is that frequentist inference violates the likelihood principle [BW88], which says that inference should be based on the likelihood of the observed data, not on hypothetical future data that you have not observed. Bayes obviously satisfies the likelihood principle, and consequently does not suffer from these pathologies. \nGiven these fundamental flaws of frequentist statistics, and the fact that Bayesian methods do not have such flaws, an obvious question to ask is: “Why isn’t everyone a Bayesian?” The (frequentist) statistician Bradley Efron wrote a paper with exactly this title [Efr86]. His short paper is well worth reading for anyone interested in this topic. Below we quote his opening section: \nThe title is a reasonable question to ask on at least two counts. First of all, everyone used to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 be a Bayesian. Laplace wholeheartedly endorsed Bayes’s formulation of the inference problem, and most 19th-century scientists followed suit. This included Gauss, whose statistical work is usually presented in frequentist terms. \n\nA second and more important point is the cogency of the Bayesian argument. Modern statisticians, following the lead of Savage and de Finetti, have advanced powerful theoretical arguments for preferring Bayesian inference. A byproduct of this work is a disturbing catalogue of inconsistencies in the frequentist point of view. \nNevertheless, everyone is not a Bayesian. The current era (1986) is the first century in which statistics has been widely used for scientific reporting, and in fact, 20th-century statistics is mainly non-Bayesian. However, Lindley (1975) predicts a change for the 21st century. \nTime will tell whether Lindley was right. However, the trends seem to be going in this direction. For example, some journals have banned p-values [TM15; AGM19], and the journal The American Statistician (produced by the American Statistical Association) published a whole special issue warning about the use of p-values and NHST [WSL19]. \nTraditionally, computation has been a barrier to using Bayesian methods, but this is less of an issue these days, due to faster computers and better algorithms (which we will discuss in the sequel to this book, [Mur23]). Another, more fundamental, concern is that the Bayesian approach is only as correct as its modeling assumptions. However, this criticism also applies to frequentist methods, since the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license sampling distribution of an estimator must be derived using assumptions about the data generating mechanism. (In fact [BT73] show that the sampling distributions for the MLE for common models are identical to the posterior distributions under a noninformative prior.) Fortunately, we can check modeling assumptions empirically using cross validation (Section 4.5.5), calibration, and Bayesian model checking. We discuss these topics in the sequel to this book, [Mur23]. \n\nTo summarize, it is worth quoting Donald Rubin, who wrote a paper [Rub84] called “Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician”. In it, he writes \nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice. [They] should attempt to use specifications that lead to approximately calibrated procedures under reasonable deviations from [their assumptions]. [They] should avoid models that are contradicted by observed data in relevant ways — frequency calculations for hypothetical replications can model a model’s adequacy and help to suggest more appropriate models. \n5.6 Exercises \nExercise 5.1 [Reject option in classifiers] \n(Source: [DHS01, Q2.13].) In many classification problems one has the option either of assigning $_ x$ to class $j$ or, if you are too uncertain, of choosing the reject option. If the cost for rejects is less than the cost of falsely classifying the object, it may be the optimal action. Let $alpha _ { i }$ mean you choose action $i$ , for $i = 1 : C + 1$ , where $C$ is the number of classes and $C + 1$ is the reject action. Let $Y = j$ be the true (but unknown) state of nature. Define the loss function as follows \nIn other words, you incur 0 loss if you correctly classify, you incur $lambda _ { r }$ loss (cost) if you choose the reject option, and you incur $lambda _ { s }$ loss (cost) if you make a substitution error (misclassification). \na. Show that the minimum risk is obtained if we decide $Y = j$ if $p ( Y = j | pmb { x } ) geq p ( Y = k | pmb { x } )$ for all $k$ (i.e., $j$ is the most probable class) and if $begin{array} { r } { p ( Y = j | pmb { x } ) ge 1 - frac { lambda _ { r } } { lambda _ { s } } } end{array}$ ; otherwise we decide to reject. b. Describe qualitatively what happens as $lambda _ { r } / lambda _ { s }$ is increased from 0 to 1 (i.e., the relative cost of rejection increases). \nExercise 5.2 [Newsvendor problem *] \nConsider the following classic problem in decision theory / economics. Suppose you are trying to decide how much quantity $Q$ of some product (e.g., newspapers) to buy to maximize your profits. The optimal amount will depend on how much demand $D$ you think there is for your product, as well as its cost to you $C$ and its selling price $P$ . Suppose $mathcal { D }$ is unknown but has pdf $f ( D )$ and cdf $F ( D )$ . We can evaluate the expected profit by considering two cases: if $D > Q$ , then we sell all $Q$ items, and make profit $pi = ( P - C ) Q$ ; but if $D < Q$ , we only sell $mathcal { D }$ items, at profit $( P - C ) D$ , but have wasted $C ( Q - D )$ on the unsold items. So the expected profit if we buy quantity $Q$ is \nSimplify this expression, and then take derivatives wrt $Q$ to show that the optimal quantity $Q ^ { * }$ (which maximizes the expected profit) satisfies \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Frequentist hypothesis testing *",
        "subsubsection": "Why isn't everyone a Bayesian?"
    },
    {
        "content": "To summarize, it is worth quoting Donald Rubin, who wrote a paper [Rub84] called “Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician”. In it, he writes \nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice. [They] should attempt to use specifications that lead to approximately calibrated procedures under reasonable deviations from [their assumptions]. [They] should avoid models that are contradicted by observed data in relevant ways — frequency calculations for hypothetical replications can model a model’s adequacy and help to suggest more appropriate models. \n5.6 Exercises \nExercise 5.1 [Reject option in classifiers] \n(Source: [DHS01, Q2.13].) In many classification problems one has the option either of assigning $_ x$ to class $j$ or, if you are too uncertain, of choosing the reject option. If the cost for rejects is less than the cost of falsely classifying the object, it may be the optimal action. Let $alpha _ { i }$ mean you choose action $i$ , for $i = 1 : C + 1$ , where $C$ is the number of classes and $C + 1$ is the reject action. Let $Y = j$ be the true (but unknown) state of nature. Define the loss function as follows \nIn other words, you incur 0 loss if you correctly classify, you incur $lambda _ { r }$ loss (cost) if you choose the reject option, and you incur $lambda _ { s }$ loss (cost) if you make a substitution error (misclassification). \na. Show that the minimum risk is obtained if we decide $Y = j$ if $p ( Y = j | pmb { x } ) geq p ( Y = k | pmb { x } )$ for all $k$ (i.e., $j$ is the most probable class) and if $begin{array} { r } { p ( Y = j | pmb { x } ) ge 1 - frac { lambda _ { r } } { lambda _ { s } } } end{array}$ ; otherwise we decide to reject. b. Describe qualitatively what happens as $lambda _ { r } / lambda _ { s }$ is increased from 0 to 1 (i.e., the relative cost of rejection increases). \nExercise 5.2 [Newsvendor problem *] \nConsider the following classic problem in decision theory / economics. Suppose you are trying to decide how much quantity $Q$ of some product (e.g., newspapers) to buy to maximize your profits. The optimal amount will depend on how much demand $D$ you think there is for your product, as well as its cost to you $C$ and its selling price $P$ . Suppose $mathcal { D }$ is unknown but has pdf $f ( D )$ and cdf $F ( D )$ . We can evaluate the expected profit by considering two cases: if $D > Q$ , then we sell all $Q$ items, and make profit $pi = ( P - C ) Q$ ; but if $D < Q$ , we only sell $mathcal { D }$ items, at profit $( P - C ) D$ , but have wasted $C ( Q - D )$ on the unsold items. So the expected profit if we buy quantity $Q$ is \nSimplify this expression, and then take derivatives wrt $Q$ to show that the optimal quantity $Q ^ { * }$ (which maximizes the expected profit) satisfies \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nExercise 5.3 [Bayes factors and ROC curves *] \nLet $B = p ( D | H _ { 1 } ) / p ( D | H _ { 0 } )$ be the Bayes factor in favor of model 1. Suppose we plot two ROC curves, one computed by thresholding $B$ , and the other computed by thresholding $p ( H _ { 1 } | D )$ . Will they be the same or different? Explain why. \nExercise 5.4 [Posterior median is optimal estimate under L1 loss] Prove that the posterior median is the optimal estimate under L1 loss. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n6 Information Theory \nIn this chapter, we introduce a few basic concepts from the field of information theory. More details can be found in other books such as [Mac03; CT06], as well as the sequel to this book, [Mur23]. \n6.1 Entropy \nThe entropy of a probability distribution can be interpreted as a measure of uncertainty, or lack of predictability, associated with a random variable drawn from a given distribution, as we explain below. \nWe can also use entropy to define the information content of a data source. For example, suppose we observe a sequence of symbols $X _ { n } sim p$ generated from distribution $p$ . If $p$ has high entropy, it will be hard to predict the value of each osbervation $X _ { n }$ . Hence we say that the dataset ${ mathcal { D } } = ( X _ { 1 } , ldots , X _ { n } )$ has high information content. By contrast, if $p$ is a degenerate distribution with 0 entropy (the minimal value), then every $X _ { n }$ will be the same, so $mathcal { D }$ does not contain much information. (All of this can be formalized in terms of data compression, as we discuss in the sequel to this book.) \n6.1.1 Entropy for discrete random variables \nThe entropy of a discrete random variable $X$ with distribution $p$ over $K$ states is defined by \n(Note that we use the notation $mathbb { H } left( X right)$ to denote the entropy of the rv with distribution $p$ , just as people write V $[ X ]$ to mean the variance of the distribution associated with $X$ ; we could alternatively write $mathbb { H } left( p right)$ .) Usually we use log base 2, in which case the units are called bits (short for binary digits). For example, if $X in { 1 , ldots , 5 }$ with histogram distribution $p = [ 0 . 2 5 , 0 . 2 5 , 0 . 2 , 0 . 1 5 , 0 . 1 5 ]$ , we find $H = 2 . 2 9$ bits. If we use log base $e$ , the units are called nats. \nThe discrete distribution with maximum entropy is the uniform distribution. Hence for a $K$ -ary random variable, the entropy is maximized if $p ( x = k ) = 1 / K$ ; in this case, $mathbb { H } left( X right) = log _ { 2 } K$ . To see this, note that",
        "chapter": "I Foundations",
        "section": "Decision Theory",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "6 Information Theory \nIn this chapter, we introduce a few basic concepts from the field of information theory. More details can be found in other books such as [Mac03; CT06], as well as the sequel to this book, [Mur23]. \n6.1 Entropy \nThe entropy of a probability distribution can be interpreted as a measure of uncertainty, or lack of predictability, associated with a random variable drawn from a given distribution, as we explain below. \nWe can also use entropy to define the information content of a data source. For example, suppose we observe a sequence of symbols $X _ { n } sim p$ generated from distribution $p$ . If $p$ has high entropy, it will be hard to predict the value of each osbervation $X _ { n }$ . Hence we say that the dataset ${ mathcal { D } } = ( X _ { 1 } , ldots , X _ { n } )$ has high information content. By contrast, if $p$ is a degenerate distribution with 0 entropy (the minimal value), then every $X _ { n }$ will be the same, so $mathcal { D }$ does not contain much information. (All of this can be formalized in terms of data compression, as we discuss in the sequel to this book.) \n6.1.1 Entropy for discrete random variables \nThe entropy of a discrete random variable $X$ with distribution $p$ over $K$ states is defined by \n(Note that we use the notation $mathbb { H } left( X right)$ to denote the entropy of the rv with distribution $p$ , just as people write V $[ X ]$ to mean the variance of the distribution associated with $X$ ; we could alternatively write $mathbb { H } left( p right)$ .) Usually we use log base 2, in which case the units are called bits (short for binary digits). For example, if $X in { 1 , ldots , 5 }$ with histogram distribution $p = [ 0 . 2 5 , 0 . 2 5 , 0 . 2 , 0 . 1 5 , 0 . 1 5 ]$ , we find $H = 2 . 2 9$ bits. If we use log base $e$ , the units are called nats. \nThe discrete distribution with maximum entropy is the uniform distribution. Hence for a $K$ -ary random variable, the entropy is maximized if $p ( x = k ) = 1 / K$ ; in this case, $mathbb { H } left( X right) = log _ { 2 } K$ . To see this, note that \n$begin{array} { r }  begin{array} { r l } & { alpha textbf { t } alpha textbf { g } textrm { c c g } textrm { g t } alpha textbf { c g } textrm { g c a } }  & { textrm { t } textrm { t a } textrm { g c t } textrm { g c a } textrm { c c g } textrm { g c a } }  & { textrm { t c a } textrm { g c c } textrm { c c a } textrm { g c a } textrm { g a } textrm { g c a } }  & { alpha textbf { t } alpha textbf { textrm { a } c c g } textrm { c g } textrm { g c a } textrm { c g } textrm { g c a } }  & { textrm { t } textrm { t a } textrm { g c c g } textrm { c t } textrm { a } textrm { a } }  & { textrm { t } textrm { a } textrm { a g c c } textrm { c t } textrm { c g } textrm { t a } textrm { c g } textrm { g t } alpha }  & { textrm { t } textrm { t a } textrm { a g c c } textrm { g t } textrm { t a } textrm { c g } textrm { g c c } textrm { c } }  & { textrm { t } textrm { t a } textrm { g c c g } textrm { g t } textrm { t a c g } textrm { g c c } textrm { c } }  & { alpha textbf { t } textrm { a } textrm { t c c g } textrm { g } textrm { g t } textrm { a c c } textrm { g a } }  & { alpha textbf { textrm { t } alpha textrm { g c } textrm { c g } textrm { g t } textrm { c a } textrm { c g } textrm { g a } textrm { a } }  & { alpha textbf { textrm { c a } textrm { t c c g } textrm { g t } textrm { g a c c g } textrm { g a } textrm { a } } end{array} } } end{array}$ TTAGCCGETACEGEA 12 13 (a) (b) \nConversely, the distribution with minimum entropy (which is zero) is any delta-function that puts all its mass on one state. Such a distribution has no uncertainty. \nFor the special case of binary random variables, $X in { 0 , 1 }$ , we can write $p ( X = 1 ) = theta$ and $p ( X = 0 ) = 1 - theta$ . Hence the entropy becomes \nThis is called the binary entropy function, and is also written $mathbb { H } left( theta right)$ . We plot this in Figure 6.1. We see that the maximum value of 1 bit occurs when the distribution is uniform, $theta = 0 . 5$ . A fair coin requires a single yes/no question to determine its state. \nAs an interesting application of entropy, consider the problem of representing DNA sequence motifs, which is a distribution over short DNA strings. We can estimate this distribution by aligning a set of DNA sequences (e.g., from different species), and then estimating the empirical distribution of each possible nucleotide from the 4 letter alphabet $X sim { A , C , G , T }$ at each location $t$ in the $i$ th sequence as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis $mathbf { N } _ { t }$ is a length four vector counting the number of times each letter appears at each location amongst the set of sequences. This $hat { pmb { theta } } _ { t }$ distribution is known as a motif. We can also compute the most probable letter in each location; this is called the consensus sequence. \nOne way to visually summarize the data is by using a sequence logo, as shown in Figure 6.2(b). We plot the letters A, C, G and T, with the most probable letter on the top; the height of the $t$ ’th bar is defined to be $0 leq 2 - H _ { t } leq 2$ , where $H _ { t }$ is the entropy of $hat { pmb { theta } } _ { t }$ (note that 2 is the maximum possible entropy for a distribution over 4 letters). Thus tall bars correspond to nearly deterministic distributions, which are the locations that are conserved by evolution (e.g., because they are part of a gene coding region). In this example, we see that column 13 is all G’s, and hence has height 2. \nEstimating the entropy of a random variable with many possible states requires estimating its distribution, which can require a lot of data. For example, imagine if $X$ represents the identity of a word in an English document. Since there is a long tail of rare words, and since new words are invented all the time, it can be difficult to reliably estimate $p ( X )$ and hence $mathbb { H } left( X right)$ . For one possible solution to this problem, see [VV13]. \n6.1.2 Cross entropy \nThe cross entropy between distribution $p$ and $q$ is defined by \nOne can show that the cross entropy is the expected number of bits needed to compress some data samples drawn from distribution $p$ using a code based on distribution $q$ . This can be minimized by setting $q = p$ , in which case the expected number of bits of the optimal code is $mathbb { H } ( p , p ) = mathbb { H } ( p )$ — this is known as Shannon’s source coding theorem (see e.g., [CT06]). \n6.1.3 Joint entropy \nThe joint entropy of two random variables $X$ and $Y$ is defined as \nFor example, consider choosing an integer from 1 to 8, $n in { 1 , ldots , 8 }$ . Let $X ( n ) = 1$ if $n$ is even, and $Y ( n ) = 1$ if $n$ is prime: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Entropy",
        "subsubsection": "Entropy for discrete random variables"
    },
    {
        "content": "This $mathbf { N } _ { t }$ is a length four vector counting the number of times each letter appears at each location amongst the set of sequences. This $hat { pmb { theta } } _ { t }$ distribution is known as a motif. We can also compute the most probable letter in each location; this is called the consensus sequence. \nOne way to visually summarize the data is by using a sequence logo, as shown in Figure 6.2(b). We plot the letters A, C, G and T, with the most probable letter on the top; the height of the $t$ ’th bar is defined to be $0 leq 2 - H _ { t } leq 2$ , where $H _ { t }$ is the entropy of $hat { pmb { theta } } _ { t }$ (note that 2 is the maximum possible entropy for a distribution over 4 letters). Thus tall bars correspond to nearly deterministic distributions, which are the locations that are conserved by evolution (e.g., because they are part of a gene coding region). In this example, we see that column 13 is all G’s, and hence has height 2. \nEstimating the entropy of a random variable with many possible states requires estimating its distribution, which can require a lot of data. For example, imagine if $X$ represents the identity of a word in an English document. Since there is a long tail of rare words, and since new words are invented all the time, it can be difficult to reliably estimate $p ( X )$ and hence $mathbb { H } left( X right)$ . For one possible solution to this problem, see [VV13]. \n6.1.2 Cross entropy \nThe cross entropy between distribution $p$ and $q$ is defined by \nOne can show that the cross entropy is the expected number of bits needed to compress some data samples drawn from distribution $p$ using a code based on distribution $q$ . This can be minimized by setting $q = p$ , in which case the expected number of bits of the optimal code is $mathbb { H } ( p , p ) = mathbb { H } ( p )$ — this is known as Shannon’s source coding theorem (see e.g., [CT06]). \n6.1.3 Joint entropy \nThe joint entropy of two random variables $X$ and $Y$ is defined as \nFor example, consider choosing an integer from 1 to 8, $n in { 1 , ldots , 8 }$ . Let $X ( n ) = 1$ if $n$ is even, and $Y ( n ) = 1$ if $n$ is prime: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Entropy",
        "subsubsection": "Cross entropy"
    },
    {
        "content": "This $mathbf { N } _ { t }$ is a length four vector counting the number of times each letter appears at each location amongst the set of sequences. This $hat { pmb { theta } } _ { t }$ distribution is known as a motif. We can also compute the most probable letter in each location; this is called the consensus sequence. \nOne way to visually summarize the data is by using a sequence logo, as shown in Figure 6.2(b). We plot the letters A, C, G and T, with the most probable letter on the top; the height of the $t$ ’th bar is defined to be $0 leq 2 - H _ { t } leq 2$ , where $H _ { t }$ is the entropy of $hat { pmb { theta } } _ { t }$ (note that 2 is the maximum possible entropy for a distribution over 4 letters). Thus tall bars correspond to nearly deterministic distributions, which are the locations that are conserved by evolution (e.g., because they are part of a gene coding region). In this example, we see that column 13 is all G’s, and hence has height 2. \nEstimating the entropy of a random variable with many possible states requires estimating its distribution, which can require a lot of data. For example, imagine if $X$ represents the identity of a word in an English document. Since there is a long tail of rare words, and since new words are invented all the time, it can be difficult to reliably estimate $p ( X )$ and hence $mathbb { H } left( X right)$ . For one possible solution to this problem, see [VV13]. \n6.1.2 Cross entropy \nThe cross entropy between distribution $p$ and $q$ is defined by \nOne can show that the cross entropy is the expected number of bits needed to compress some data samples drawn from distribution $p$ using a code based on distribution $q$ . This can be minimized by setting $q = p$ , in which case the expected number of bits of the optimal code is $mathbb { H } ( p , p ) = mathbb { H } ( p )$ — this is known as Shannon’s source coding theorem (see e.g., [CT06]). \n6.1.3 Joint entropy \nThe joint entropy of two random variables $X$ and $Y$ is defined as \nFor example, consider choosing an integer from 1 to 8, $n in { 1 , ldots , 8 }$ . Let $X ( n ) = 1$ if $n$ is even, and $Y ( n ) = 1$ if $n$ is prime: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe joint distribution is \nso the joint entropy is given by \nClearly the marginal probabilities are uniform: $p ( X = 1 ) = p ( X = 0 ) = p ( Y = 0 ) = p ( Y =$ $1 ) = 0 . 5$ , so $mathbb { H } left( X right) = mathbb { H } left( Y right) = 1$ . Hence $mathbb { H } ( X , Y ) = 1 . 8 1$ bits $< mathbb { H } left( X right) + mathbb { H } left( Y right) = 2$ bits. In fact, this upper bound on the joint entropy holds in general. If $X$ and $Y$ are independent, then $mathbb { H } left( X , Y right) = mathbb { H } left( X right) + mathbb { H } left( Y right)$ , so the bound is tight. This makes intuitive sense: when the parts are correlated in some way, it reduces the “degrees of freedom” of the system, and hence reduces the overall entropy. \nWhat is the lower bound on $mathbb { H } left( X , Y right)$ ? If $Y$ is a deterministic function of $X$ , then $mathbb { H } left( X , Y right) = mathbb { H } left( X right)$ . So \nIntuitively this says combining variables together does not make the entropy go down: you cannot reduce uncertainty merely by adding more unknowns to the problem, you need to observe some data, a topic we discuss in Section 6.1.4. \nWe can extend the definition of joint entropy from two variables to $n$ in the obvious way. \n6.1.4 Conditional entropy \nThe conditional entropy of $Y$ given $X$ is the uncertainty we have in $Y$ after seeing $X$ , averaged over possible values for $X$ : \nIf $Y$ is a deterministic function of $X$ , then knowing $X$ completely determines $Y$ , so $mathbb { H } left( Y | X right) = 0$ . If $X$ and $Y$ are independent, knowing $X$ tells us nothing about $Y$ and $mathbb { H } left( Y | X right) = mathbb { H } left( Y right)$ . Since $mathbb { H } left( X , Y right) leq mathbb { H } left( Y right) + mathbb { H } left( X right)$ , we have \nwith equality iff $X$ and $Y$ are independent. This shows that, on average, conditioning on data never increases one’s uncertainty. The caveat “on average” is necessary because for any particular observation \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Entropy",
        "subsubsection": "Joint entropy"
    },
    {
        "content": "The joint distribution is \nso the joint entropy is given by \nClearly the marginal probabilities are uniform: $p ( X = 1 ) = p ( X = 0 ) = p ( Y = 0 ) = p ( Y =$ $1 ) = 0 . 5$ , so $mathbb { H } left( X right) = mathbb { H } left( Y right) = 1$ . Hence $mathbb { H } ( X , Y ) = 1 . 8 1$ bits $< mathbb { H } left( X right) + mathbb { H } left( Y right) = 2$ bits. In fact, this upper bound on the joint entropy holds in general. If $X$ and $Y$ are independent, then $mathbb { H } left( X , Y right) = mathbb { H } left( X right) + mathbb { H } left( Y right)$ , so the bound is tight. This makes intuitive sense: when the parts are correlated in some way, it reduces the “degrees of freedom” of the system, and hence reduces the overall entropy. \nWhat is the lower bound on $mathbb { H } left( X , Y right)$ ? If $Y$ is a deterministic function of $X$ , then $mathbb { H } left( X , Y right) = mathbb { H } left( X right)$ . So \nIntuitively this says combining variables together does not make the entropy go down: you cannot reduce uncertainty merely by adding more unknowns to the problem, you need to observe some data, a topic we discuss in Section 6.1.4. \nWe can extend the definition of joint entropy from two variables to $n$ in the obvious way. \n6.1.4 Conditional entropy \nThe conditional entropy of $Y$ given $X$ is the uncertainty we have in $Y$ after seeing $X$ , averaged over possible values for $X$ : \nIf $Y$ is a deterministic function of $X$ , then knowing $X$ completely determines $Y$ , so $mathbb { H } left( Y | X right) = 0$ . If $X$ and $Y$ are independent, knowing $X$ tells us nothing about $Y$ and $mathbb { H } left( Y | X right) = mathbb { H } left( Y right)$ . Since $mathbb { H } left( X , Y right) leq mathbb { H } left( Y right) + mathbb { H } left( X right)$ , we have \nwith equality iff $X$ and $Y$ are independent. This shows that, on average, conditioning on data never increases one’s uncertainty. The caveat “on average” is necessary because for any particular observation \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n(value of $X$ ), one may get more “confused” (i.e., $mathbb { H } left( Y | x right) > mathbb { H } left( Y right) .$ ). However, in expectation, looking at the data is a good thing to do. (See also Section 6.3.8.) \nWe can rewrite Equation (6.15) as follows: \nThis can be generalized to get the chain rule for entropy: \n6.1.5 Perplexity \nThe perplexity of a discrete probability distribution $p$ is defined as \nThis is often interpreted as a measure of predictability. For example, suppose $p$ is a uniform distribution over $K$ states. In this case, the perplexity is $K$ . Obviously the lower bound on perplexity is $2 ^ { 0 } = 1$ , which will be achieved if the distribution can perfectly predict outcomes. \nNow suppose we have an empirical distribution based on data $mathcal { D }$ : \nWe can measure how well $p$ predicts $mathcal { D }$ by computing \nPerplexity is often used to evaluate the quality of statistical language models, which is a generative model for sequences of tokens. Suppose the data is a single long document $x$ of length $N$ , and suppose $p$ is a simple unigram model. In this case, the cross entropy term is given by \nand hence the perplexity is given by \nThis is sometimes called the exponentiated cross entropy. We see that this is the geometric mean of the inverse predictive probabilities. \nIn the case of language models, we usually condition on previous words when predicting the next word. For example, in a bigram model, we use a first order Markov model of the form $p ( x _ { i } | x _ { i - 1 } )$ . We define the branching factor of a language model as the number of possible words that can \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license follow any given word. We can thus interpret the perplexity as the weighted average branching factor. For example, suppose the model predicts that each word is equally likely, regardless of context, so $p ( x _ { i } | x _ { i - 1 } ) = 1 / K$ . Then the perplexity is $( ( 1 / K ) ^ { N } ) ^ { - 1 / N } = K$ . If some symbols are more likely than others, and the model correctly reflects this, its perplexity will be lower than $K$ . However, as we show in Section 6.2, we have $mathbb { H } left( p ^ { * } right) leq mathbb { H } _ { c e } left( p ^ { * } , p right)$ , so we can never reduce the perplexity below the entropy of the underlying stochastic process $p ^ { * }$ .",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Entropy",
        "subsubsection": "Conditional entropy"
    },
    {
        "content": "(value of $X$ ), one may get more “confused” (i.e., $mathbb { H } left( Y | x right) > mathbb { H } left( Y right) .$ ). However, in expectation, looking at the data is a good thing to do. (See also Section 6.3.8.) \nWe can rewrite Equation (6.15) as follows: \nThis can be generalized to get the chain rule for entropy: \n6.1.5 Perplexity \nThe perplexity of a discrete probability distribution $p$ is defined as \nThis is often interpreted as a measure of predictability. For example, suppose $p$ is a uniform distribution over $K$ states. In this case, the perplexity is $K$ . Obviously the lower bound on perplexity is $2 ^ { 0 } = 1$ , which will be achieved if the distribution can perfectly predict outcomes. \nNow suppose we have an empirical distribution based on data $mathcal { D }$ : \nWe can measure how well $p$ predicts $mathcal { D }$ by computing \nPerplexity is often used to evaluate the quality of statistical language models, which is a generative model for sequences of tokens. Suppose the data is a single long document $x$ of length $N$ , and suppose $p$ is a simple unigram model. In this case, the cross entropy term is given by \nand hence the perplexity is given by \nThis is sometimes called the exponentiated cross entropy. We see that this is the geometric mean of the inverse predictive probabilities. \nIn the case of language models, we usually condition on previous words when predicting the next word. For example, in a bigram model, we use a first order Markov model of the form $p ( x _ { i } | x _ { i - 1 } )$ . We define the branching factor of a language model as the number of possible words that can \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license follow any given word. We can thus interpret the perplexity as the weighted average branching factor. For example, suppose the model predicts that each word is equally likely, regardless of context, so $p ( x _ { i } | x _ { i - 1 } ) = 1 / K$ . Then the perplexity is $( ( 1 / K ) ^ { N } ) ^ { - 1 / N } = K$ . If some symbols are more likely than others, and the model correctly reflects this, its perplexity will be lower than $K$ . However, as we show in Section 6.2, we have $mathbb { H } left( p ^ { * } right) leq mathbb { H } _ { c e } left( p ^ { * } , p right)$ , so we can never reduce the perplexity below the entropy of the underlying stochastic process $p ^ { * }$ . \n\nSee [JM08, p96] for further discussion of perplexity and its uses in language models. \n6.1.6 Differential entropy for continuous random variables * \nIf $X$ is a continuous random variable with pdf $p ( x )$ , we define the differential entropy as \nassuming this integral exists. For example, suppose $X sim U ( 0 , a )$ . Then \nNote that, unlike the discrete case, differential entropy can be negative. This is because pdf’s can be bigger than 1. For example if $X sim U ( 0 , 1 / 8 )$ , we have $h ( X ) = log _ { 2 } ( 1 / 8 ) = - 3$ . \nOne way to understand differential entropy is to realize that all real-valued quantities can only be represented to finite precision. It can be shown [CT91, p228] that the entropy of an $n$ -bit quantization of a continuous random variable $X$ is approximately $h ( X ) + n$ . For example, suppose $X sim U ( 0 , frac { 1 } { 8 } )$ . Then in a binary representation of $X$ , the first 3 bits to the right of the binary point must be 0 (since the number is $leq 1 / 8$ ). So to describe $X$ to $n$ bits of accuracy only requires $n - 3$ bits, which agrees with $h ( X ) = - 3$ calculated above. \n6.1.6.1 Example: Entropy of a Gaussian \nThe entropy of a $d$ -dimensional Gaussian is \nIn the 1d case, this becomes \n6.1.6.2 Connection with variance \nThe entropy of a Gaussian increases monotonically as the variance increases. However, this is not always the case. For example, consider a mixture of two 1d Gaussians centered at -1 and $+ 1$ . As we move the means further apart, say to -10 and $+ 1 0$ , the variance increases (since the average distance from the overall mean gets larger). However, the entropy remains more or less the same, since we are still uncertain about where a sample might fall, even if we know that it will be near -10 or $+ 1 0$ . (The exact entropy of a GMM is hard to compute, but a method to compute upper and lower bounds is presented in [Hub+08].) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Entropy",
        "subsubsection": "Perplexity"
    },
    {
        "content": "See [JM08, p96] for further discussion of perplexity and its uses in language models. \n6.1.6 Differential entropy for continuous random variables * \nIf $X$ is a continuous random variable with pdf $p ( x )$ , we define the differential entropy as \nassuming this integral exists. For example, suppose $X sim U ( 0 , a )$ . Then \nNote that, unlike the discrete case, differential entropy can be negative. This is because pdf’s can be bigger than 1. For example if $X sim U ( 0 , 1 / 8 )$ , we have $h ( X ) = log _ { 2 } ( 1 / 8 ) = - 3$ . \nOne way to understand differential entropy is to realize that all real-valued quantities can only be represented to finite precision. It can be shown [CT91, p228] that the entropy of an $n$ -bit quantization of a continuous random variable $X$ is approximately $h ( X ) + n$ . For example, suppose $X sim U ( 0 , frac { 1 } { 8 } )$ . Then in a binary representation of $X$ , the first 3 bits to the right of the binary point must be 0 (since the number is $leq 1 / 8$ ). So to describe $X$ to $n$ bits of accuracy only requires $n - 3$ bits, which agrees with $h ( X ) = - 3$ calculated above. \n6.1.6.1 Example: Entropy of a Gaussian \nThe entropy of a $d$ -dimensional Gaussian is \nIn the 1d case, this becomes \n6.1.6.2 Connection with variance \nThe entropy of a Gaussian increases monotonically as the variance increases. However, this is not always the case. For example, consider a mixture of two 1d Gaussians centered at -1 and $+ 1$ . As we move the means further apart, say to -10 and $+ 1 0$ , the variance increases (since the average distance from the overall mean gets larger). However, the entropy remains more or less the same, since we are still uncertain about where a sample might fall, even if we know that it will be near -10 or $+ 1 0$ . (The exact entropy of a GMM is hard to compute, but a method to compute upper and lower bounds is presented in [Hub+08].) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n6.1.6.3 Discretization \nIn general, computing the differential entropy for a continuous random variable can be difficult. A simple approximation is to discretize or quantize the variables. There are various methods for this (see e.g., [DKS95; KK06] for a summary), but a simple approach is to bin the distribution based on its empirical quantiles. The critical question is how many bins to use [LM04]. Scott [Sco79] suggested the following heuristic: \nwhere $sigma ( mathcal { D } )$ is the empirical standard deviation of the data, and $N _ { mathit { D } } = | mathcal { D } |$ is the number of datapoints in the empirical distribution. However, the technique of discretization does not scale well if $X$ is a multi-dimensional random vector, due to the curse of dimensionality. \n6.2 Relative entropy (KL divergence) * \nGiven two distributions $p$ and $q$ , it is often useful to define a distance metric to measure how “close” or “similar” they are. In fact, we will be more general and consider a divergence measure $D ( p , q )$ which quantifies how far $q$ is from $p$ , without requiring that $D$ be a metric. More precisely, we say that $D$ is a divergence if $D ( p , q ) geq 0$ with equality iff $p = q$ , whereas a metric also requires that $D$ be symmetric and satisfy the triangle inequality, $D ( p , r ) leq D ( p , q ) + D ( q , r )$ . There are many possible divergence measures we can use. In this section, we focus on the Kullback-Leibler divergence or KL divergence, also known as the information gain or relative entropy, between two distributions $p$ and $q$ . \n6.2.1 Definition \nFor discrete distributions, the KL divergence is defined as follows: \nThis naturally extends to continuous distributions as well: \n6.2.2 Interpretation \nWe can rewrite the KL as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Entropy",
        "subsubsection": "Differential entropy for continuous random variables *"
    },
    {
        "content": "6.1.6.3 Discretization \nIn general, computing the differential entropy for a continuous random variable can be difficult. A simple approximation is to discretize or quantize the variables. There are various methods for this (see e.g., [DKS95; KK06] for a summary), but a simple approach is to bin the distribution based on its empirical quantiles. The critical question is how many bins to use [LM04]. Scott [Sco79] suggested the following heuristic: \nwhere $sigma ( mathcal { D } )$ is the empirical standard deviation of the data, and $N _ { mathit { D } } = | mathcal { D } |$ is the number of datapoints in the empirical distribution. However, the technique of discretization does not scale well if $X$ is a multi-dimensional random vector, due to the curse of dimensionality. \n6.2 Relative entropy (KL divergence) * \nGiven two distributions $p$ and $q$ , it is often useful to define a distance metric to measure how “close” or “similar” they are. In fact, we will be more general and consider a divergence measure $D ( p , q )$ which quantifies how far $q$ is from $p$ , without requiring that $D$ be a metric. More precisely, we say that $D$ is a divergence if $D ( p , q ) geq 0$ with equality iff $p = q$ , whereas a metric also requires that $D$ be symmetric and satisfy the triangle inequality, $D ( p , r ) leq D ( p , q ) + D ( q , r )$ . There are many possible divergence measures we can use. In this section, we focus on the Kullback-Leibler divergence or KL divergence, also known as the information gain or relative entropy, between two distributions $p$ and $q$ . \n6.2.1 Definition \nFor discrete distributions, the KL divergence is defined as follows: \nThis naturally extends to continuous distributions as well: \n6.2.2 Interpretation \nWe can rewrite the KL as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Relative entropy (KL divergence) *",
        "subsubsection": "Definition"
    },
    {
        "content": "6.1.6.3 Discretization \nIn general, computing the differential entropy for a continuous random variable can be difficult. A simple approximation is to discretize or quantize the variables. There are various methods for this (see e.g., [DKS95; KK06] for a summary), but a simple approach is to bin the distribution based on its empirical quantiles. The critical question is how many bins to use [LM04]. Scott [Sco79] suggested the following heuristic: \nwhere $sigma ( mathcal { D } )$ is the empirical standard deviation of the data, and $N _ { mathit { D } } = | mathcal { D } |$ is the number of datapoints in the empirical distribution. However, the technique of discretization does not scale well if $X$ is a multi-dimensional random vector, due to the curse of dimensionality. \n6.2 Relative entropy (KL divergence) * \nGiven two distributions $p$ and $q$ , it is often useful to define a distance metric to measure how “close” or “similar” they are. In fact, we will be more general and consider a divergence measure $D ( p , q )$ which quantifies how far $q$ is from $p$ , without requiring that $D$ be a metric. More precisely, we say that $D$ is a divergence if $D ( p , q ) geq 0$ with equality iff $p = q$ , whereas a metric also requires that $D$ be symmetric and satisfy the triangle inequality, $D ( p , r ) leq D ( p , q ) + D ( q , r )$ . There are many possible divergence measures we can use. In this section, we focus on the Kullback-Leibler divergence or KL divergence, also known as the information gain or relative entropy, between two distributions $p$ and $q$ . \n6.2.1 Definition \nFor discrete distributions, the KL divergence is defined as follows: \nThis naturally extends to continuous distributions as well: \n6.2.2 Interpretation \nWe can rewrite the KL as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe recognize the first term as the negative entropy, and the second term as the cross entropy. It can be shown that the cross entropy $mathbb { H } ( p , q )$ is a lower bound on the number of bits needed to compress data coming from distribution $p$ if your code is designed based on distribution $q$ ; thus we can interpret the KL divergence as the “extra number of bits” you need to pay when compressing data samples if you use the incorrect distribution $q$ as the basis of your coding scheme compared to the true distribution $p$ . \nThere are various other interpretations of KL divergence. See the sequel to this book, [Mur23], for more information. \n6.2.3 Example: KL divergence between two Gaussians \nFor example, one can show that the KL divergence between two multivariate Gaussian distributions is given by \nIn the scalar case, this becomes \n6.2.4 Non-negativity of KL \nIn this section, we prove that the KL divergence is always non-negative. \nTo do this, we use Jensen’s inequality. This states that, for any convex function $f$ , we have that \nwhere $lambda _ { i } geq 0$ and $begin{array} { r } { sum _ { i = 1 } ^ { n } lambda _ { i } = 1 } end{array}$ . In words, this result says that $f$ of the average is less than the average of the $f$ ’s. This is clearly true for $n = 2$ , since a convex function curves up above a straight line connecting the two end points (see Section 8.1.3). To prove for general $n$ , we can use induction. For example, if $f ( x ) = log ( x )$ , which is a concave function, we have \nWe use this result below. \nTheorem 6.2.1. (Information inequality) $D _ { mathbb { K L } } left( p parallel q right) ge 0$ with equality iff $p = q$ . \nProof. We now prove the theorem following [CT06, p28]. Let $A = { x : p ( x ) > 0 }$ be the support of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Relative entropy (KL divergence) *",
        "subsubsection": "Interpretation"
    },
    {
        "content": "We recognize the first term as the negative entropy, and the second term as the cross entropy. It can be shown that the cross entropy $mathbb { H } ( p , q )$ is a lower bound on the number of bits needed to compress data coming from distribution $p$ if your code is designed based on distribution $q$ ; thus we can interpret the KL divergence as the “extra number of bits” you need to pay when compressing data samples if you use the incorrect distribution $q$ as the basis of your coding scheme compared to the true distribution $p$ . \nThere are various other interpretations of KL divergence. See the sequel to this book, [Mur23], for more information. \n6.2.3 Example: KL divergence between two Gaussians \nFor example, one can show that the KL divergence between two multivariate Gaussian distributions is given by \nIn the scalar case, this becomes \n6.2.4 Non-negativity of KL \nIn this section, we prove that the KL divergence is always non-negative. \nTo do this, we use Jensen’s inequality. This states that, for any convex function $f$ , we have that \nwhere $lambda _ { i } geq 0$ and $begin{array} { r } { sum _ { i = 1 } ^ { n } lambda _ { i } = 1 } end{array}$ . In words, this result says that $f$ of the average is less than the average of the $f$ ’s. This is clearly true for $n = 2$ , since a convex function curves up above a straight line connecting the two end points (see Section 8.1.3). To prove for general $n$ , we can use induction. For example, if $f ( x ) = log ( x )$ , which is a concave function, we have \nWe use this result below. \nTheorem 6.2.1. (Information inequality) $D _ { mathbb { K L } } left( p parallel q right) ge 0$ with equality iff $p = q$ . \nProof. We now prove the theorem following [CT06, p28]. Let $A = { x : p ( x ) > 0 }$ be the support of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Relative entropy (KL divergence) *",
        "subsubsection": "Example: KL divergence between two Gaussians"
    },
    {
        "content": "We recognize the first term as the negative entropy, and the second term as the cross entropy. It can be shown that the cross entropy $mathbb { H } ( p , q )$ is a lower bound on the number of bits needed to compress data coming from distribution $p$ if your code is designed based on distribution $q$ ; thus we can interpret the KL divergence as the “extra number of bits” you need to pay when compressing data samples if you use the incorrect distribution $q$ as the basis of your coding scheme compared to the true distribution $p$ . \nThere are various other interpretations of KL divergence. See the sequel to this book, [Mur23], for more information. \n6.2.3 Example: KL divergence between two Gaussians \nFor example, one can show that the KL divergence between two multivariate Gaussian distributions is given by \nIn the scalar case, this becomes \n6.2.4 Non-negativity of KL \nIn this section, we prove that the KL divergence is always non-negative. \nTo do this, we use Jensen’s inequality. This states that, for any convex function $f$ , we have that \nwhere $lambda _ { i } geq 0$ and $begin{array} { r } { sum _ { i = 1 } ^ { n } lambda _ { i } = 1 } end{array}$ . In words, this result says that $f$ of the average is less than the average of the $f$ ’s. This is clearly true for $n = 2$ , since a convex function curves up above a straight line connecting the two end points (see Section 8.1.3). To prove for general $n$ , we can use induction. For example, if $f ( x ) = log ( x )$ , which is a concave function, we have \nWe use this result below. \nTheorem 6.2.1. (Information inequality) $D _ { mathbb { K L } } left( p parallel q right) ge 0$ with equality iff $p = q$ . \nProof. We now prove the theorem following [CT06, p28]. Let $A = { x : p ( x ) > 0 }$ be the support of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n$p ( x )$ . Using the concavity of the log function and Jensen’s inequality (Section 6.2.4), we have that \nSince $log ( x )$ is a strictly concave function $( - log ( x )$ is convex), we have equality in Equation (6.37) iff $p ( x ) = c q ( x )$ for some $c$ that tracks the fraction of the whole space $mathcal { X }$ contained in $A$ . We have equality in Equation (6.38) iff $begin{array} { r } { sum _ { x in A } q ( x ) = sum _ { x in mathcal { X } } q ( x ) = 1 } end{array}$ , which implies $c = 1$ . Hence $D _ { mathbb { K L } } left( p parallel q right) = 0$ iff $p ( x ) = q ( x )$ for all $x$ . □ \nThis theorem has many important implications, as we will see throughout the book. For example, we can show that the uniform distribution is the one that maximizes the entropy: \nCorollary 6.2.1. (Uniform distribution maximizes the entropy) $mathbb { H } left( X right) leq log left| mathcal { X } right|$ , where $| { mathcal { X } } |$ is the number of states for $X$ , with equality iff $p ( x )$ is uniform. \nProof. Let $u ( x ) = 1 / vert x vert$ . Then \n6.2.5 KL divergence and MLE \nSuppose we want to find the distribution $q$ that is as close as possible to $p$ , as measured by KL divergence: \nNow suppose $p$ is the empirical distribution, which puts a probability atom on the observed training data and zero mass everywhere else: \nUsing the sifting property of delta functions we get \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $begin{array} { r } { C = int p ( x ) log p ( x ) d x } end{array}$ is a constant independent of $q$ . This is called the cross entropy objective, and is equal to the average negative log likelihood of $q$ on the training set. Thus we see that minimizing KL divergence to the empirical distribution is equivalent to maximizing likelihood.",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Relative entropy (KL divergence) *",
        "subsubsection": "Non-negativity of KL"
    },
    {
        "content": "$p ( x )$ . Using the concavity of the log function and Jensen’s inequality (Section 6.2.4), we have that \nSince $log ( x )$ is a strictly concave function $( - log ( x )$ is convex), we have equality in Equation (6.37) iff $p ( x ) = c q ( x )$ for some $c$ that tracks the fraction of the whole space $mathcal { X }$ contained in $A$ . We have equality in Equation (6.38) iff $begin{array} { r } { sum _ { x in A } q ( x ) = sum _ { x in mathcal { X } } q ( x ) = 1 } end{array}$ , which implies $c = 1$ . Hence $D _ { mathbb { K L } } left( p parallel q right) = 0$ iff $p ( x ) = q ( x )$ for all $x$ . □ \nThis theorem has many important implications, as we will see throughout the book. For example, we can show that the uniform distribution is the one that maximizes the entropy: \nCorollary 6.2.1. (Uniform distribution maximizes the entropy) $mathbb { H } left( X right) leq log left| mathcal { X } right|$ , where $| { mathcal { X } } |$ is the number of states for $X$ , with equality iff $p ( x )$ is uniform. \nProof. Let $u ( x ) = 1 / vert x vert$ . Then \n6.2.5 KL divergence and MLE \nSuppose we want to find the distribution $q$ that is as close as possible to $p$ , as measured by KL divergence: \nNow suppose $p$ is the empirical distribution, which puts a probability atom on the observed training data and zero mass everywhere else: \nUsing the sifting property of delta functions we get \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $begin{array} { r } { C = int p ( x ) log p ( x ) d x } end{array}$ is a constant independent of $q$ . This is called the cross entropy objective, and is equal to the average negative log likelihood of $q$ on the training set. Thus we see that minimizing KL divergence to the empirical distribution is equivalent to maximizing likelihood. \n\nThis perspective points out the flaw with likelihood-based training, namely that it puts too much weight on the training set. In most applications, we do not really believe that the empirical distribution is a good representation of the true distribution, since it just puts “spikes” on a finite set of points, and zero density everywhere else. Even if the dataset is large (say 1M images), the universe from which the data is sampled is usually even larger (e.g., the set of “all natural images” is much larger than 1M). We could smooth the empirical distribution using kernel density estimation (Section 16.3), but that would require a similar kernel on the space of images. An alternative, algorithmic approach is to use data augmentation, which is a way of perturbing the observed data samples in way that we believe reflects plausible “natural variation”. Applying MLE on this augmented dataset often yields superior results, especially when fitting models with many parameters (see Section 19.1). \n6.2.6 Forward vs reverse KL \nSuppose we want to approximate a distribution $p$ using a simpler distribution $q$ . We can do this by minimizing $D _ { mathbb { K L } } left( q parallel p right)$ or $D _ { mathbb { K L } } left( p parallel q right)$ . This gives rise to different behavior, as we discuss below. First we consider the forwards KL, also called the inclusive KL, defined by \nMinimizing this wrt $q$ is known as an M-projection or moment projection. \nWe can gain an understanding of the optimal $q$ by considering inputs $x$ for which $p ( x ) > 0$ but $q ( x ) = 0$ . In this case, the term $log p ( x ) / q ( x )$ will be infinite. Thus minimizing the KL will force $q$ to include all the areas of space for which $p$ has non-zero probability. Put another way, $q$ will be zero-avoiding or mode-covering, and will typically over-estimate the support of $p$ . Figure 6.3(a) illustrates mode covering where $p$ is a bimodal distribution but $q$ is unimodal. \nNow consider the reverse $mathbf { K L }$ , also called the exclusive KL: \nMinimizing this wrt $q$ is known as an I-projection or information projection. \nWe can gain an understanding of the optimal $q$ by consider inputs $x$ for which $p ( x ) = 0$ but $q ( x ) > 0$ . In this case, the term $log { q ( x ) } / p ( x )$ will be infinite. Thus minimizing the exclusive KL will force $q$ to exclude all the areas of space for which $p$ has zero probability. One way to do this is for $q$ to put probability mass in very few parts of space; this is called zero-forcing or mode-seeking behavior. In this case, $q$ will typically under-estimate the support of $p$ . We illustrate mode seeking when $p$ is bimodal but $q$ is unimodal in Figure 6.3(b-c). \n6.3 Mutual information * \nThe KL divergence gave us a way to measure how similar two distributions were. How should we measure how dependant two random variables are? One thing we could do is turn the question \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Relative entropy (KL divergence) *",
        "subsubsection": "KL divergence and MLE"
    },
    {
        "content": "This perspective points out the flaw with likelihood-based training, namely that it puts too much weight on the training set. In most applications, we do not really believe that the empirical distribution is a good representation of the true distribution, since it just puts “spikes” on a finite set of points, and zero density everywhere else. Even if the dataset is large (say 1M images), the universe from which the data is sampled is usually even larger (e.g., the set of “all natural images” is much larger than 1M). We could smooth the empirical distribution using kernel density estimation (Section 16.3), but that would require a similar kernel on the space of images. An alternative, algorithmic approach is to use data augmentation, which is a way of perturbing the observed data samples in way that we believe reflects plausible “natural variation”. Applying MLE on this augmented dataset often yields superior results, especially when fitting models with many parameters (see Section 19.1). \n6.2.6 Forward vs reverse KL \nSuppose we want to approximate a distribution $p$ using a simpler distribution $q$ . We can do this by minimizing $D _ { mathbb { K L } } left( q parallel p right)$ or $D _ { mathbb { K L } } left( p parallel q right)$ . This gives rise to different behavior, as we discuss below. First we consider the forwards KL, also called the inclusive KL, defined by \nMinimizing this wrt $q$ is known as an M-projection or moment projection. \nWe can gain an understanding of the optimal $q$ by considering inputs $x$ for which $p ( x ) > 0$ but $q ( x ) = 0$ . In this case, the term $log p ( x ) / q ( x )$ will be infinite. Thus minimizing the KL will force $q$ to include all the areas of space for which $p$ has non-zero probability. Put another way, $q$ will be zero-avoiding or mode-covering, and will typically over-estimate the support of $p$ . Figure 6.3(a) illustrates mode covering where $p$ is a bimodal distribution but $q$ is unimodal. \nNow consider the reverse $mathbf { K L }$ , also called the exclusive KL: \nMinimizing this wrt $q$ is known as an I-projection or information projection. \nWe can gain an understanding of the optimal $q$ by consider inputs $x$ for which $p ( x ) = 0$ but $q ( x ) > 0$ . In this case, the term $log { q ( x ) } / p ( x )$ will be infinite. Thus minimizing the exclusive KL will force $q$ to exclude all the areas of space for which $p$ has zero probability. One way to do this is for $q$ to put probability mass in very few parts of space; this is called zero-forcing or mode-seeking behavior. In this case, $q$ will typically under-estimate the support of $p$ . We illustrate mode seeking when $p$ is bimodal but $q$ is unimodal in Figure 6.3(b-c). \n6.3 Mutual information * \nThe KL divergence gave us a way to measure how similar two distributions were. How should we measure how dependant two random variables are? One thing we could do is turn the question \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Relative entropy (KL divergence) *",
        "subsubsection": "Forward vs reverse KL"
    },
    {
        "content": "of measuring the dependence of two random variables into a question about the similarity of their distributions. This gives rise to the notion of mutual information (MI) between two random variables, which we define below. \n6.3.1 Definition \nThe mutual information between rv’s $X$ and $Y$ is defined as follows: \n(We write $mathbb { I } ( X ; Y )$ instead of $mathbb { I } ( X , Y )$ , in case $X$ and/or $Y$ represent sets of variables; for example, we can write $mathbb { I } ( X ; Y , Z )$ to represent the MI between $X$ and $( Y , Z )$ .) For continuous random variables, we just replace sums with integrals. \nIt is easy to see that MI is always non-negative, even for continuous random variables, since \nWe achieve the bound of $0$ iff $p ( x , y ) = p ( x ) p ( y )$ . \n6.3.2 Interpretation \nKnowing that the mutual information is a KL divergence between the joint and factored marginal distributions tells us that the MI measures the information gain if we update from a model that treats the two variables as independent $p ( x ) p ( y )$ to one that models their true joint density $p ( x , y )$ . \nTo gain further insight into the meaning of MI, it helps to re-express it in terms of joint and conditional entropies, as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Definition"
    },
    {
        "content": "of measuring the dependence of two random variables into a question about the similarity of their distributions. This gives rise to the notion of mutual information (MI) between two random variables, which we define below. \n6.3.1 Definition \nThe mutual information between rv’s $X$ and $Y$ is defined as follows: \n(We write $mathbb { I } ( X ; Y )$ instead of $mathbb { I } ( X , Y )$ , in case $X$ and/or $Y$ represent sets of variables; for example, we can write $mathbb { I } ( X ; Y , Z )$ to represent the MI between $X$ and $( Y , Z )$ .) For continuous random variables, we just replace sums with integrals. \nIt is easy to see that MI is always non-negative, even for continuous random variables, since \nWe achieve the bound of $0$ iff $p ( x , y ) = p ( x ) p ( y )$ . \n6.3.2 Interpretation \nKnowing that the mutual information is a KL divergence between the joint and factored marginal distributions tells us that the MI measures the information gain if we update from a model that treats the two variables as independent $p ( x ) p ( y )$ to one that models their true joint density $p ( x , y )$ . \nTo gain further insight into the meaning of MI, it helps to re-express it in terms of joint and conditional entropies, as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThus we can interpret the MI between $X$ and $Y$ as the reduction in uncertainty about $X$ after observing $Y$ , or, by symmetry, the reduction in uncertainty about $Y$ after observing $X$ . Incidentally, this result gives an alternative proof that conditioning, on average, reduces entropy. In particular, we have $0 le mathbb { I } left( X ; Y right) = mathbb { H } left( X right) - mathbb { H } left( X | Y right)$ , and hence $mathbb { H } left( X | Y right) leq mathbb { H } left( X right)$ . \nWe can also obtain a different interpretation. One can show that \nFinally, one can show that \nSee Figure 6.4 for a summary of these equations in terms of an information diagram. (Formally, this is a signed measure mapping set expressions to their information-theoretic counterparts [Yeu91].) \n6.3.3 Example \nAs an example, let us reconsider the example concerning prime and even numbers from Section 6.1.3. Recall that $mathbb { H } left( X right) = mathbb { H } left( Y right) = 1$ . The conditional distribution $p ( { Y } | { X } )$ is given by normalizing each row: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Interpretation"
    },
    {
        "content": "Thus we can interpret the MI between $X$ and $Y$ as the reduction in uncertainty about $X$ after observing $Y$ , or, by symmetry, the reduction in uncertainty about $Y$ after observing $X$ . Incidentally, this result gives an alternative proof that conditioning, on average, reduces entropy. In particular, we have $0 le mathbb { I } left( X ; Y right) = mathbb { H } left( X right) - mathbb { H } left( X | Y right)$ , and hence $mathbb { H } left( X | Y right) leq mathbb { H } left( X right)$ . \nWe can also obtain a different interpretation. One can show that \nFinally, one can show that \nSee Figure 6.4 for a summary of these equations in terms of an information diagram. (Formally, this is a signed measure mapping set expressions to their information-theoretic counterparts [Yeu91].) \n6.3.3 Example \nAs an example, let us reconsider the example concerning prime and even numbers from Section 6.1.3. Recall that $mathbb { H } left( X right) = mathbb { H } left( Y right) = 1$ . The conditional distribution $p ( { Y } | { X } )$ is given by normalizing each row: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHence the conditional entropy is \nand the mutual information is \nYou can easily verify that \n6.3.4 Conditional mutual information \nWe can define the conditional mutual information in the obvious way \nThe last equation tells us that the conditional MI is the extra (residual) information that $X$ tells us about $Y$ , excluding what we already knew about $Y$ given $Z$ alone. \nWe can rewrite Equation (6.61) as follows: \nGeneralizing to $N$ variables, we get the chain rule for mutual information: \n6.3.5 MI as a “generalized correlation coefficient” \nSuppose that $( x , y )$ are jointly Gaussian: \nWe now show how to compute the mutual information between $X$ and $Y$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Example"
    },
    {
        "content": "Hence the conditional entropy is \nand the mutual information is \nYou can easily verify that \n6.3.4 Conditional mutual information \nWe can define the conditional mutual information in the obvious way \nThe last equation tells us that the conditional MI is the extra (residual) information that $X$ tells us about $Y$ , excluding what we already knew about $Y$ given $Z$ alone. \nWe can rewrite Equation (6.61) as follows: \nGeneralizing to $N$ variables, we get the chain rule for mutual information: \n6.3.5 MI as a “generalized correlation coefficient” \nSuppose that $( x , y )$ are jointly Gaussian: \nWe now show how to compute the mutual information between $X$ and $Y$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Conditional mutual information"
    },
    {
        "content": "Hence the conditional entropy is \nand the mutual information is \nYou can easily verify that \n6.3.4 Conditional mutual information \nWe can define the conditional mutual information in the obvious way \nThe last equation tells us that the conditional MI is the extra (residual) information that $X$ tells us about $Y$ , excluding what we already knew about $Y$ given $Z$ alone. \nWe can rewrite Equation (6.61) as follows: \nGeneralizing to $N$ variables, we get the chain rule for mutual information: \n6.3.5 MI as a “generalized correlation coefficient” \nSuppose that $( x , y )$ are jointly Gaussian: \nWe now show how to compute the mutual information between $X$ and $Y$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nUsing Equation (6.26), we find that the entropy is \nSince $X$ and $Y$ are individually normal with variance $sigma ^ { 2 }$ , we have \nHence \nWe now discuss some interesting special cases. \n1. $rho = 1$ . In this case, $X = Y$ , and $I ( X , Y ) = infty$ , which makes sense. Observing $Y$ tells us an infinite amount of information about $X$ (as we know its real value exactly).   \n2. $rho = 0$ . In this case, $X$ and $Y$ are independent, and $I ( X , Y ) = 0$ , which makes sense. Observing $Y$ tells us nothing about $X$ .   \n3. $rho = - 1$ . In this case, $X = - Y$ , and $I ( X , Y ) = infty$ , which again makes sense. Observing $Y$ allows us to predict $X$ to infinite precision. \nNow consider the case where $X$ and $Y$ are scalar, but not jointly Gaussian. In general it can be difficult to compute the mutual information between continuous random variables, because we have to estimate the joint density $p ( X , Y )$ . For scalar variables, a simple approximation is to discretize or quantize them, by dividing the ranges of each variable into bins, and computing how many values fall in each histogram bin [Sco79]. We can then easily compute the MI using the empirical pmf. \nUnfortunately, the number of bins used, and the location of the bin boundaries, can have a significant effect on the results. One way to avoid this is to use $K$ -nearest neighbor distances to estimate densities in a non-parametric, adaptive way. This is the basis of the KSG estimator for MI proposed in [KSG04]. This is implemented in the sklearn.feature_selection.mutual_info_regression function. For papers related to this estimator, see [GOV18; HN19]. \n6.3.6 Normalized mutual information \nFor some applications, it is useful to have a normalized measure of dependence, between 0 and 1. We now discuss one way to construct such a measure. \nFirst, note that \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "MI as a ``generalized correlation coefficient''"
    },
    {
        "content": "Using Equation (6.26), we find that the entropy is \nSince $X$ and $Y$ are individually normal with variance $sigma ^ { 2 }$ , we have \nHence \nWe now discuss some interesting special cases. \n1. $rho = 1$ . In this case, $X = Y$ , and $I ( X , Y ) = infty$ , which makes sense. Observing $Y$ tells us an infinite amount of information about $X$ (as we know its real value exactly).   \n2. $rho = 0$ . In this case, $X$ and $Y$ are independent, and $I ( X , Y ) = 0$ , which makes sense. Observing $Y$ tells us nothing about $X$ .   \n3. $rho = - 1$ . In this case, $X = - Y$ , and $I ( X , Y ) = infty$ , which again makes sense. Observing $Y$ allows us to predict $X$ to infinite precision. \nNow consider the case where $X$ and $Y$ are scalar, but not jointly Gaussian. In general it can be difficult to compute the mutual information between continuous random variables, because we have to estimate the joint density $p ( X , Y )$ . For scalar variables, a simple approximation is to discretize or quantize them, by dividing the ranges of each variable into bins, and computing how many values fall in each histogram bin [Sco79]. We can then easily compute the MI using the empirical pmf. \nUnfortunately, the number of bins used, and the location of the bin boundaries, can have a significant effect on the results. One way to avoid this is to use $K$ -nearest neighbor distances to estimate densities in a non-parametric, adaptive way. This is the basis of the KSG estimator for MI proposed in [KSG04]. This is implemented in the sklearn.feature_selection.mutual_info_regression function. For papers related to this estimator, see [GOV18; HN19]. \n6.3.6 Normalized mutual information \nFor some applications, it is useful to have a normalized measure of dependence, between 0 and 1. We now discuss one way to construct such a measure. \nFirst, note that \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTherefore we can define the normalized mutual information as follows: \nThis normalized mutual information ranges from 0 to 1. When $N M I ( X , Y ) = 0$ , we have $mathbb { I } left( X ; Y right) = 0$ , so $X$ and $Y$ are independent. When $N M I ( X , Y ) = 1$ , and $mathbb { H } left( X right) < mathbb { H } left( Y right)$ , we have \nand so $X$ is a deterministic function of $Y$ . For example, suppose $X$ is a discrete random variable with pmf [0.5, 0.25, 0.25]. We have $M I ( X , X ) = 1 . 5$ (using log base 2), and $H ( X ) = 1 . 5$ , so the normalized MI is $^ { 1 }$ , as is to be expected. \nFor continuous random variables, it is harder to normalize the mutual information, because of the need to estimate the differential entropy, which is sensitive to the level of quantization. See Section 6.3.7 for further discussion. \n6.3.7 Maximal information coefficient \nAs we discussed in Section 6.3.6, it is useful to have a normalized estimate of the mutual information, but this can be tricky to compute for real-valued data. One approach, known as the maximal information coefficient (MIC) [Res+11], is to define the following quantity: \nwhere $G$ is the set of 2d grids, and $( X , Y ) | _ { G }$ represents a discretization of the variables onto this grid, and $| | G | |$ is $operatorname* { m i n } ( G _ { x } , G _ { y } )$ , where $G _ { x }$ is the number of grid cells in the $x$ direction, and $G _ { y }$ is the number of grid cells in the $y$ direction. (The maximum grid resolution depends on the sample size $n$ ; they suggest restricting grids so that $G _ { x } G _ { y } leq B ( n )$ , where $B ( n ) = n ^ { alpha }$ , where $alpha = 0 . 6$ .) The denominator is the entropy of a uniform joint distribution; dividing by this ensures $0 leq mathrm { M I C } leq 1$ . \nThe intuition behind this statistic is the following: if there is a relationship between $X$ and $Y$ , then there should be some discrete gridding of the 2d input space that captures this. Since we don’t know the correct grid to use, MIC searches over different grid resolutions (e.g., 2x2, 2x3, etc), as well as over locations of the grid boundaries. Given a grid, it is easy to quantize the data and compute MI. We define the characteristic matrix $M ( k , l )$ to be the maximum MI achievable by any grid of size $( k , l )$ , normalized by $log ( operatorname* { m i n } ( k , l ) )$ . The MIC is then the maximum entry in this matrix, $mathrm { m a x } _ { k l le B ( n ) } M ( k , l )$ . See Figure 6.5 for a visualization of this process. \nIn [Res+11], they show that this quantity exhibits a property known as equitability, which means that it gives similar scores to equally noisy relationships, regardless of the type of relationship (e.g., linear, non-linear, non-functional). \nIn [Res+16], they present an improved estimator, called MICe, which is more efficient to compute, and only requires optimizing over 1d grids, which can be done in $O ( n )$ time using dynamic programming. They also present another quantity, called TICe (total information content), that has higher \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license power to detect relationships from small sample sizes, but lower equitability. This is defined to be $begin{array} { r } { sum _ { k l leq B ( n ) } M ( k , l ) } end{array}$ . They recommend using TICe to screen a large number of candidate relationships, and then using MICe to quantify the strength of the relationship. For an efficient implementation of both of these metrics, see [Alb+18].",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Normalized mutual information"
    },
    {
        "content": "Therefore we can define the normalized mutual information as follows: \nThis normalized mutual information ranges from 0 to 1. When $N M I ( X , Y ) = 0$ , we have $mathbb { I } left( X ; Y right) = 0$ , so $X$ and $Y$ are independent. When $N M I ( X , Y ) = 1$ , and $mathbb { H } left( X right) < mathbb { H } left( Y right)$ , we have \nand so $X$ is a deterministic function of $Y$ . For example, suppose $X$ is a discrete random variable with pmf [0.5, 0.25, 0.25]. We have $M I ( X , X ) = 1 . 5$ (using log base 2), and $H ( X ) = 1 . 5$ , so the normalized MI is $^ { 1 }$ , as is to be expected. \nFor continuous random variables, it is harder to normalize the mutual information, because of the need to estimate the differential entropy, which is sensitive to the level of quantization. See Section 6.3.7 for further discussion. \n6.3.7 Maximal information coefficient \nAs we discussed in Section 6.3.6, it is useful to have a normalized estimate of the mutual information, but this can be tricky to compute for real-valued data. One approach, known as the maximal information coefficient (MIC) [Res+11], is to define the following quantity: \nwhere $G$ is the set of 2d grids, and $( X , Y ) | _ { G }$ represents a discretization of the variables onto this grid, and $| | G | |$ is $operatorname* { m i n } ( G _ { x } , G _ { y } )$ , where $G _ { x }$ is the number of grid cells in the $x$ direction, and $G _ { y }$ is the number of grid cells in the $y$ direction. (The maximum grid resolution depends on the sample size $n$ ; they suggest restricting grids so that $G _ { x } G _ { y } leq B ( n )$ , where $B ( n ) = n ^ { alpha }$ , where $alpha = 0 . 6$ .) The denominator is the entropy of a uniform joint distribution; dividing by this ensures $0 leq mathrm { M I C } leq 1$ . \nThe intuition behind this statistic is the following: if there is a relationship between $X$ and $Y$ , then there should be some discrete gridding of the 2d input space that captures this. Since we don’t know the correct grid to use, MIC searches over different grid resolutions (e.g., 2x2, 2x3, etc), as well as over locations of the grid boundaries. Given a grid, it is easy to quantize the data and compute MI. We define the characteristic matrix $M ( k , l )$ to be the maximum MI achievable by any grid of size $( k , l )$ , normalized by $log ( operatorname* { m i n } ( k , l ) )$ . The MIC is then the maximum entry in this matrix, $mathrm { m a x } _ { k l le B ( n ) } M ( k , l )$ . See Figure 6.5 for a visualization of this process. \nIn [Res+11], they show that this quantity exhibits a property known as equitability, which means that it gives similar scores to equally noisy relationships, regardless of the type of relationship (e.g., linear, non-linear, non-functional). \nIn [Res+16], they present an improved estimator, called MICe, which is more efficient to compute, and only requires optimizing over 1d grids, which can be done in $O ( n )$ time using dynamic programming. They also present another quantity, called TICe (total information content), that has higher \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license power to detect relationships from small sample sizes, but lower equitability. This is defined to be $begin{array} { r } { sum _ { k l leq B ( n ) } M ( k , l ) } end{array}$ . They recommend using TICe to screen a large number of candidate relationships, and then using MICe to quantify the strength of the relationship. For an efficient implementation of both of these metrics, see [Alb+18]. \n\nWe can interpret MIC of $0$ to mean there is no relationship between the variables, and 1 to represent a noise-free relationship of any form. This is illustrated in Figure 6.6. Unlike correlation coefficients, MIC is not restricted to finding linear relationships. For this reason, the MIC has been called “a correlation for the 21st century” [Spe11]. \nIn Figure 6.7, we give a more interesting example, from [Res+11]. The data consists of 357 variables measuring a variety of social, economic, health and political indicators, collected by the World Health Organization (WHO). On the left of the figure, we see the correlation coefficient (CC) plotted against \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the MIC for all 63,546 variable pairs. On the right of the figure, we see scatter plots for particular pairs of variables, which we now discuss: \n\n• The point marked C (near 0,0 on the plot) has a low CC and a low MIC. The corresponding scatter plot makes it clear that there is no relationship between these two variables (percentage of lives lost to injury and density of dentists in the population).   \n• The points marked D and H have high CC (in absolute value) and high MIC, because they represent nearly linear relationships.   \n• The points marked E, F, and G have low CC but high MIC. This is because they correspond to non-linear (and sometimes, as in the case of $mathrm { E }$ and F, non-functional, i.e., one-to-many) relationships between the variables. \n6.3.8 Data processing inequality \nSuppose we have an unknown variable $X$ , and we observe a noisy function of it, call it $Y$ . If we process the noisy observations in some way to create a new variable $Z$ , it should be intuitively obvious that we cannot increase the amount of information we have about the unknown quantity, $X$ . This is known as the data processing inequality. We now state this more formally, and then prove it. \nTheorem 6.3.1. Suppose $X  Y  Z$ forms a Markov chain, so that $X perp Z | Y$ . Then $mathbb { I } left( X ; Y right) geq$ $mathbb { I } left( X ; Z right)$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Maximal information coefficient"
    },
    {
        "content": "• The point marked C (near 0,0 on the plot) has a low CC and a low MIC. The corresponding scatter plot makes it clear that there is no relationship between these two variables (percentage of lives lost to injury and density of dentists in the population).   \n• The points marked D and H have high CC (in absolute value) and high MIC, because they represent nearly linear relationships.   \n• The points marked E, F, and G have low CC but high MIC. This is because they correspond to non-linear (and sometimes, as in the case of $mathrm { E }$ and F, non-functional, i.e., one-to-many) relationships between the variables. \n6.3.8 Data processing inequality \nSuppose we have an unknown variable $X$ , and we observe a noisy function of it, call it $Y$ . If we process the noisy observations in some way to create a new variable $Z$ , it should be intuitively obvious that we cannot increase the amount of information we have about the unknown quantity, $X$ . This is known as the data processing inequality. We now state this more formally, and then prove it. \nTheorem 6.3.1. Suppose $X  Y  Z$ forms a Markov chain, so that $X perp Z | Y$ . Then $mathbb { I } left( X ; Y right) geq$ $mathbb { I } left( X ; Z right)$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nProof. By the chain rule for mutual information (Equation (6.62)), we can expand the mutual information in two different ways: \nSince $X perp Z | Y$ , we have $mathbb { I } left( X ; Z | Y right) = 0$ , so \nSince $mathbb { I } left( X ; Y | Z right) ge 0$ , we have $mathbb { I } left( X ; Y right) ge mathbb { I } left( X ; Z right)$ . Similarly one can prove that $mathbb { I } left( Y ; Z right) ge mathbb { I } left( X ; Z right)$ . □ \n6.3.9 Sufficient Statistics \nAn important consequence of the DPI is the following. Suppose we have the chain $theta to { mathcal { D } } to s ( { mathcal { D } } )$ . Then \nIf this holds with equality, then we say that $s ( mathcal D )$ is a sufficient statistic of the data $mathcal { D }$ for the purposes of inferring $theta$ . In this case, we can equivalently write $theta  :   : s ( mathcal { D } )  mathcal { D }$ , since we can reconstruct the data from knowing $s ( mathcal D )$ just as accurately as from knowing $theta$ . \nAn example of a sufficient statistic is the data itself, $s ( mathcal { D } ) = mathcal { D }$ , but this is not very useful, since it doesn’t summarize the data at all. Hence we define a minimal sufficient statistic $s ( mathcal D )$ as one which is sufficient, and which contains no extra information about $theta$ ; thus $s ( mathcal D )$ maximally compresses the data $mathcal { D }$ without losing information which is relevant to predicting $theta$ . More formally, we say $s$ is a minimal sufficient statistic for $mathcal { D }$ if for all sufficient statistics $s ^ { prime } ( mathcal { D } )$ there is some function $f$ such that $s ( mathcal D ) = f ( s ^ { prime } ( mathcal D ) )$ . We can summarize the situation as follows: \nHere $s ^ { prime } ( mathcal { D } )$ takes $s ( mathcal D )$ and adds redundant information to it, thus creating a one-to-many mapping. For example, a minimal sufficient statistic for a set of $N$ Bernoulli trials is simply $N$ and $N _ { 1 } =$ $begin{array} { r } { sum _ { n } mathbb { I } left( X _ { n } = 1 right) } end{array}$ , i.e., the number of successes. In other words, we don’t need to keep track of the entire sequence of heads and tails and their ordering, we only need to keep track of the total number of heads and tails. Similarly, for inferring the mean of a Gaussian distribution with known variance we only need to know the empirical mean and number of samples. \n6.3.10 Fano’s inequality * \nA common method for feature selection is to pick input features $X _ { d }$ which have high mutual information with the response variable $Y$ . Below we justify why this is a reasonable thing to do. In particular, we state a result, known as Fano’s inequality, which bounds the probability of misclassification (for any method) in terms of the mutual information between the features $X$ and the class label $Y$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Data processing inequality"
    },
    {
        "content": "Proof. By the chain rule for mutual information (Equation (6.62)), we can expand the mutual information in two different ways: \nSince $X perp Z | Y$ , we have $mathbb { I } left( X ; Z | Y right) = 0$ , so \nSince $mathbb { I } left( X ; Y | Z right) ge 0$ , we have $mathbb { I } left( X ; Y right) ge mathbb { I } left( X ; Z right)$ . Similarly one can prove that $mathbb { I } left( Y ; Z right) ge mathbb { I } left( X ; Z right)$ . □ \n6.3.9 Sufficient Statistics \nAn important consequence of the DPI is the following. Suppose we have the chain $theta to { mathcal { D } } to s ( { mathcal { D } } )$ . Then \nIf this holds with equality, then we say that $s ( mathcal D )$ is a sufficient statistic of the data $mathcal { D }$ for the purposes of inferring $theta$ . In this case, we can equivalently write $theta  :   : s ( mathcal { D } )  mathcal { D }$ , since we can reconstruct the data from knowing $s ( mathcal D )$ just as accurately as from knowing $theta$ . \nAn example of a sufficient statistic is the data itself, $s ( mathcal { D } ) = mathcal { D }$ , but this is not very useful, since it doesn’t summarize the data at all. Hence we define a minimal sufficient statistic $s ( mathcal D )$ as one which is sufficient, and which contains no extra information about $theta$ ; thus $s ( mathcal D )$ maximally compresses the data $mathcal { D }$ without losing information which is relevant to predicting $theta$ . More formally, we say $s$ is a minimal sufficient statistic for $mathcal { D }$ if for all sufficient statistics $s ^ { prime } ( mathcal { D } )$ there is some function $f$ such that $s ( mathcal D ) = f ( s ^ { prime } ( mathcal D ) )$ . We can summarize the situation as follows: \nHere $s ^ { prime } ( mathcal { D } )$ takes $s ( mathcal D )$ and adds redundant information to it, thus creating a one-to-many mapping. For example, a minimal sufficient statistic for a set of $N$ Bernoulli trials is simply $N$ and $N _ { 1 } =$ $begin{array} { r } { sum _ { n } mathbb { I } left( X _ { n } = 1 right) } end{array}$ , i.e., the number of successes. In other words, we don’t need to keep track of the entire sequence of heads and tails and their ordering, we only need to keep track of the total number of heads and tails. Similarly, for inferring the mean of a Gaussian distribution with known variance we only need to know the empirical mean and number of samples. \n6.3.10 Fano’s inequality * \nA common method for feature selection is to pick input features $X _ { d }$ which have high mutual information with the response variable $Y$ . Below we justify why this is a reasonable thing to do. In particular, we state a result, known as Fano’s inequality, which bounds the probability of misclassification (for any method) in terms of the mutual information between the features $X$ and the class label $Y$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Sufficient Statistics"
    },
    {
        "content": "Proof. By the chain rule for mutual information (Equation (6.62)), we can expand the mutual information in two different ways: \nSince $X perp Z | Y$ , we have $mathbb { I } left( X ; Z | Y right) = 0$ , so \nSince $mathbb { I } left( X ; Y | Z right) ge 0$ , we have $mathbb { I } left( X ; Y right) ge mathbb { I } left( X ; Z right)$ . Similarly one can prove that $mathbb { I } left( Y ; Z right) ge mathbb { I } left( X ; Z right)$ . □ \n6.3.9 Sufficient Statistics \nAn important consequence of the DPI is the following. Suppose we have the chain $theta to { mathcal { D } } to s ( { mathcal { D } } )$ . Then \nIf this holds with equality, then we say that $s ( mathcal D )$ is a sufficient statistic of the data $mathcal { D }$ for the purposes of inferring $theta$ . In this case, we can equivalently write $theta  :   : s ( mathcal { D } )  mathcal { D }$ , since we can reconstruct the data from knowing $s ( mathcal D )$ just as accurately as from knowing $theta$ . \nAn example of a sufficient statistic is the data itself, $s ( mathcal { D } ) = mathcal { D }$ , but this is not very useful, since it doesn’t summarize the data at all. Hence we define a minimal sufficient statistic $s ( mathcal D )$ as one which is sufficient, and which contains no extra information about $theta$ ; thus $s ( mathcal D )$ maximally compresses the data $mathcal { D }$ without losing information which is relevant to predicting $theta$ . More formally, we say $s$ is a minimal sufficient statistic for $mathcal { D }$ if for all sufficient statistics $s ^ { prime } ( mathcal { D } )$ there is some function $f$ such that $s ( mathcal D ) = f ( s ^ { prime } ( mathcal D ) )$ . We can summarize the situation as follows: \nHere $s ^ { prime } ( mathcal { D } )$ takes $s ( mathcal D )$ and adds redundant information to it, thus creating a one-to-many mapping. For example, a minimal sufficient statistic for a set of $N$ Bernoulli trials is simply $N$ and $N _ { 1 } =$ $begin{array} { r } { sum _ { n } mathbb { I } left( X _ { n } = 1 right) } end{array}$ , i.e., the number of successes. In other words, we don’t need to keep track of the entire sequence of heads and tails and their ordering, we only need to keep track of the total number of heads and tails. Similarly, for inferring the mean of a Gaussian distribution with known variance we only need to know the empirical mean and number of samples. \n6.3.10 Fano’s inequality * \nA common method for feature selection is to pick input features $X _ { d }$ which have high mutual information with the response variable $Y$ . Below we justify why this is a reasonable thing to do. In particular, we state a result, known as Fano’s inequality, which bounds the probability of misclassification (for any method) in terms of the mutual information between the features $X$ and the class label $Y$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTheorem 6.3.2. (Fano’s inequality) Consider an estimator ${ hat { Y } } = f ( X )$ such that $Y  X  { hat { Y } }$ forms a Markov chain. Let $E$ be the event $hat { Y } neq Y$ , indicating that an error occured, and let $P _ { e } = P ( Y neq hat { Y } )$ be the probability of error. Then we have \nSince $mathbb { H } left( E right) leq 1$ , as we saw in Figure 6.1, we can weaken this result to get \nand hence \nThus minimizing $mathbb { H } left( Y | X right)$ (which can be done by maximizing $mathbb { I } ( X ; Y )$ ) will also minimize the lower bound on $P _ { e }$ . \nProof. (From [CT06, p38].) Using the chain rule for entropy, we have \nSince conditioning reduces entropy (see Section 6.2.4), we have $mathbb { H } left( E | hat { Y } right) leq mathbb { H } left( E right)$ . The final term can be bounded as follows: \nHence \nFinally, by the data processing inequality, we have $mathbb { I } ( Y ; hat { Y } ) leq mathbb { I } ( Y ; X )$ , so $mathbb { H } left( Y | X right) leq mathbb { H } left( Y | hat { Y } right)$ , which establishes Equation (6.82). □ \n6.4 Exercises \nExercise 6.1 [Expressing mutual information in terms of entropies *] Prove the following identities: \nand \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Mutual information *",
        "subsubsection": "Fano's inequality *"
    },
    {
        "content": "Theorem 6.3.2. (Fano’s inequality) Consider an estimator ${ hat { Y } } = f ( X )$ such that $Y  X  { hat { Y } }$ forms a Markov chain. Let $E$ be the event $hat { Y } neq Y$ , indicating that an error occured, and let $P _ { e } = P ( Y neq hat { Y } )$ be the probability of error. Then we have \nSince $mathbb { H } left( E right) leq 1$ , as we saw in Figure 6.1, we can weaken this result to get \nand hence \nThus minimizing $mathbb { H } left( Y | X right)$ (which can be done by maximizing $mathbb { I } ( X ; Y )$ ) will also minimize the lower bound on $P _ { e }$ . \nProof. (From [CT06, p38].) Using the chain rule for entropy, we have \nSince conditioning reduces entropy (see Section 6.2.4), we have $mathbb { H } left( E | hat { Y } right) leq mathbb { H } left( E right)$ . The final term can be bounded as follows: \nHence \nFinally, by the data processing inequality, we have $mathbb { I } ( Y ; hat { Y } ) leq mathbb { I } ( Y ; X )$ , so $mathbb { H } left( Y | X right) leq mathbb { H } left( Y | hat { Y } right)$ , which establishes Equation (6.82). □ \n6.4 Exercises \nExercise 6.1 [Expressing mutual information in terms of entropies *] Prove the following identities: \nand \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nExercise 6.2 [Relationship between $D ( p | | q )$ and $chi ^ { 2 }$ statistic] (Source: [CT91, Q12.2].) \nShow that, if $p ( x ) approx q ( x )$ , then \nwhere \nHint: write \nand use the Taylor series expansion for $log ( 1 + x )$ . \nfor $- 1 < x leq 1$ . \nExercise 6.3 [Fun with entropies *] \n(Source: Mackay.) Consider the joint distribution $p ( X , Y )$ \na. What is the joint entropy $H ( X , Y )$ ? \nb. What are the marginal entropies $H ( X )$ and $H ( Y )$ ? c. The entropy of $X$ conditioned on a specific value of $y$ is defined as \nCompute $H ( X | y )$ for each value of $y$ . Does the posterior entropy on $X$ ever increase given an observation of $Y$ ? \nd. The conditional entropy is defined as \nCompute this. Does the posterior entropy on $X$ increase or decrease when averaged over the possible values of $Y$ ? \ne. What is the mutual information between $X$ and $Y$ ? \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nExercise 6.4 [Forwards vs reverse KL divergence] \n(Source: Exercise 33.7 of [Mac03].) Consider a factored approximation $q ( x , y ) = q ( x ) q ( y )$ to a joint distribution $p ( x , y )$ . Show that to minimize the forwards KL $D _ { mathbb { K L } } left( p parallel q right)$ we should set $q ( x ) = p ( x )$ and $q ( y ) = p ( y )$ , i.e., the optimal approximation is a product of marginals \nNow consider the following joint distribution, where the rows represent $y$ and the columns $x$ . \nShow that the reverse KL $D _ { mathbb { K L } } left( q parallel p right)$ for this $p$ has three distinct minima. Identify those minima and evaluate $D _ { mathbb { K L } } left( q parallel p right)$ at each of them. What is the value of $D _ { mathbb { K L } } left( q parallel p right)$ if we set $q ( x , y ) = p ( x ) p ( y ) ^ { * }$ ? \n7 Linear Algebra \nThis chapter is co-authored with Zico Kolter. \n7.1 Introduction \nLinear algebra is the study of matrices and vectors. In this chapter, we summarize the key material that we will need throughout the book. Much more information can be found in other sources, such as [Str09; Ips09; Kle13; Mol04; TB97; Axl15; Tho17; Agg20]. \n7.1.1 Notation \nIn this section, we define some notation. \n7.1.1.1 Vectors \nA vector $pmb { x } in mathbb { R } ^ { n }$ is a list of $n$ numbers, usually written as a column vector \nThe vector of all ones is denoted 1. The vector of all zeros is denoted 0. The unit vector $e _ { i }$ is a vector of all $0$ ’s, except entry $i$ , which has value 1: \nThis is also called a one-hot vector. \n7.1.1.2 Matrices \nA matrix $mathbf { A } in mathbb { R } ^ { m times n }$ with $m$ rows and $n$ columns is a 2d array of numbers, arranged as follows:",
        "chapter": "I Foundations",
        "section": "Information Theory",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "7 Linear Algebra \nThis chapter is co-authored with Zico Kolter. \n7.1 Introduction \nLinear algebra is the study of matrices and vectors. In this chapter, we summarize the key material that we will need throughout the book. Much more information can be found in other sources, such as [Str09; Ips09; Kle13; Mol04; TB97; Axl15; Tho17; Agg20]. \n7.1.1 Notation \nIn this section, we define some notation. \n7.1.1.1 Vectors \nA vector $pmb { x } in mathbb { R } ^ { n }$ is a list of $n$ numbers, usually written as a column vector \nThe vector of all ones is denoted 1. The vector of all zeros is denoted 0. The unit vector $e _ { i }$ is a vector of all $0$ ’s, except entry $i$ , which has value 1: \nThis is also called a one-hot vector. \n7.1.1.2 Matrices \nA matrix $mathbf { A } in mathbb { R } ^ { m times n }$ with $m$ rows and $n$ columns is a 2d array of numbers, arranged as follows: \nIf $m = n$ , the matrix is said to be square. \nWe use the notation $A _ { i j }$ or $A _ { i , j }$ to denote the entry of $mathbf { A }$ in the $i$ th row and $j$ th column. We use the notation $mathbf { A } _ { i } ,$ : to denote the $i$ ’th row and A: $, j$ to denote the $j$ ’th column. We treat all vectors as column vectors by default (so $mathbf { A } _ { i } ,$ : is viewed as a column vector with $n$ entries). We use bold upper case letters to denote matrices, bold lower case letters to denote vectors, and non-bold letters to denote scalars. \nWe can view a matrix as a set of columns stacked along the horizontal axis: \nFor brevity, we will denote this by \nWe can also view a matrix as a set of rows stacked along the vertical axis: \nFor brevity, we will denote this by \n(Note the use of a semicolon.) \nThe transpose of a matrix results from “flipping” the rows and columns. Given a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ , its transpose, written $mathbf { A } ^ { top } in mathbb { R } ^ { n times m }$ , is defined as \nThe following properties of transposes are easily verified: \nIf a square matrix satisfies $mathbf { A } = mathbf { A } ^ { mathsf { I } }$ , it is called symmetric. We denote the set of all symmetric matrices of size $n$ as $mathbb { S } ^ { n }$ . \n7.1.1.3 Tensors \nA tensor (in machine learning terminology) is just a generalization of a 2d array to more than 2 dimensions, as illustrated in Figure 7.1. For example, the entries of a 3d tensor are denoted by $A _ { i j k }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThe number of dimensions is known as the order or rank of the tensor.1 In mathematics, tensors can be viewed as a way to define multilinear maps, just as matrices can be used to define linear functions, although we will not need to use this interpretation. \nWe can reshape a matrix into a vector by stacking its columns on top of each other, as shown in Figure 7.1. This is denoted by \nConversely, we can reshape a vector into a matrix. There are two choices for how to do this, known as row-major order (used by languages such as Python and C++) and column-major order (used by languages such as Julia, Matlab, R and Fortran). See Figure 7.2 for an illustration of the difference. \n7.1.2 Vector spaces \nIn this section, we discuss some fundamental concepts in linear algebra. \n7.1.2.1 Vector addition and scaling \nWe can view a vector $pmb { x } in mathbb { R } ^ { n }$ as defining a point in $n$ -dimensional Euclidean space. A vector space is a collection of such vectors, which can be added together, and scaled by scalars (1-dimensional numbers), in order to create new points. These operations are defined to operate elementwise, in the obvious way, namely $pmb { x } + pmb { y } = ( x _ { 1 } + y _ { 1 } , dots , x _ { n } + y _ { n } )$ and $c { pmb x } = ( c x _ { 1 } , ldots , c x _ { n } )$ , where $c in mathbb { R }$ . See Figure 7.3a for an illustration. \n7.1.2.2 Linear independence, spans and basis sets \nA set of vectors ${ pmb { x } _ { 1 } , pmb { x } _ { 2 } , ldots pmb { x } _ { n } }$ is said to be (linearly) independent if no vector can be represented as a linear combination of the remaining vectors. Conversely, a vector which can be represented as a linear combination of the remaining vectors is said to be (linearly) dependent. For example, if \nfor some ${ alpha _ { 1 } , dotsc , alpha _ { n - 1 } }$ then ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ is dependent on ${ pmb { x } _ { 1 } , ldots , pmb { x } _ { n - 1 } }$ ; otherwise, it is independent of ${ pmb { x } _ { 1 } , dotsc , pmb { x } _ { n - 1 } }$ . \nThe span of a set of vectors ${ pmb { x } _ { 1 } , pmb { x } _ { 2 } , dots , pmb { x } _ { n } }$ is the set of all vectors that can be expressed as a linear combination of ${ pmb { x } _ { 1 } , ldots , pmb { x } _ { n } }$ . That is, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Introduction",
        "subsubsection": "Notation"
    },
    {
        "content": "7.1.2 Vector spaces \nIn this section, we discuss some fundamental concepts in linear algebra. \n7.1.2.1 Vector addition and scaling \nWe can view a vector $pmb { x } in mathbb { R } ^ { n }$ as defining a point in $n$ -dimensional Euclidean space. A vector space is a collection of such vectors, which can be added together, and scaled by scalars (1-dimensional numbers), in order to create new points. These operations are defined to operate elementwise, in the obvious way, namely $pmb { x } + pmb { y } = ( x _ { 1 } + y _ { 1 } , dots , x _ { n } + y _ { n } )$ and $c { pmb x } = ( c x _ { 1 } , ldots , c x _ { n } )$ , where $c in mathbb { R }$ . See Figure 7.3a for an illustration. \n7.1.2.2 Linear independence, spans and basis sets \nA set of vectors ${ pmb { x } _ { 1 } , pmb { x } _ { 2 } , ldots pmb { x } _ { n } }$ is said to be (linearly) independent if no vector can be represented as a linear combination of the remaining vectors. Conversely, a vector which can be represented as a linear combination of the remaining vectors is said to be (linearly) dependent. For example, if \nfor some ${ alpha _ { 1 } , dotsc , alpha _ { n - 1 } }$ then ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ is dependent on ${ pmb { x } _ { 1 } , ldots , pmb { x } _ { n - 1 } }$ ; otherwise, it is independent of ${ pmb { x } _ { 1 } , dotsc , pmb { x } _ { n - 1 } }$ . \nThe span of a set of vectors ${ pmb { x } _ { 1 } , pmb { x } _ { 2 } , dots , pmb { x } _ { n } }$ is the set of all vectors that can be expressed as a linear combination of ${ pmb { x } _ { 1 } , ldots , pmb { x } _ { n } }$ . That is, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIt can be shown that if ${ pmb { x } _ { 1 } , hdots , pmb { x } _ { n } }$ is a set of $n$ linearly independent vectors, where each $pmb { x } _ { i } in mathbb { R } ^ { n }$ , then $operatorname { s p a n } ( { pmb { x } _ { 1 } , ldots , pmb { x } _ { n } } ) = mathbb { R } ^ { n }$ . In other words, any vector $pmb { v } in mathbb { R } ^ { n }$ can be written as a linear combination of ${ bf { x } } _ { 1 }$ through ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . \nA basis $boldsymbol { B }$ is a set of linearly independent vectors that spans the whole space, meaning that $operatorname { s p a n } ( B ) = mathbb { R } ^ { n }$ . There are often multiple bases to choose from, as illustrated in Figure 7.3b. The standard basis uses the coordinate vectors $pmb { e } _ { 1 } = ( 1 , 0 , dots , 0 )$ , up to $pmb { e } _ { n } = ( 0 , 0 , ldots , 0 , 1 )$ . This lets us translate back and forth between viewing a vector in $mathbb { R } ^ { 2 }$ as an either an “arrow in the plane”, rooted at the origin, or as an ordered list of numbers (corresponding to the coefficients for each basis vector). \n7.1.2.3 Linear maps and matrices \nA linear map or linear transformation is any function $f : mathcal { V } to mathcal { W }$ such that $f ( { pmb v } + { pmb w } ) =$ $f ( pmb { v } ) + f ( pmb { w } )$ and $f ( a  v ) = a  f ( { pmb v } )$ for all $pmb { v } , pmb { w } in mathcal { V }$ . Once the basis of $nu$ is chosen, a linear map $f : mathcal { V } to mathcal { W }$ is completely determined by specifying the images of the basis vectors, because any element of $nu$ can be expressed uniquely as a linear combination of them. \nSuppose $nu = mathbb { R } ^ { n }$ and $mathcal { W } = mathbb { R } ^ { m }$ . We can compute $f ( pmb { v } _ { i } ) in mathbb { R } ^ { m }$ for each basis vector in $nu$ , and store these along the columns of an $m times n$ matrix A. We can then compute $pmb { y } = f ( pmb { x } ) in mathbb { R } ^ { m }$ for any $pmb { x } in mathbb { R } ^ { n }$ as follows: \nThis corresponds to multiplying the vector $_ { x }$ by the matrix A: \nSee Section 7.2 for more details. \nIf the function is invertible, we can write \nSee Section 7.3 for details. \n7.1.2.4 Range and nullspace of a matrix \nSuppose we view a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ as a set of $n$ vectors in $mathbb { R } ^ { m }$ . The range (sometimes also called the column space) of this matrix is the span of the columns of A. In other words, \nThis can be thought of as the set of vectors that can be “reached” or “generated” by A; it is a subspace of $mathbb { R } ^ { m }$ whose dimensionality is given by the rank of $mathbf { A }$ (see Section 7.1.4.3). The nullspace of a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ is the set of all vectors that get mapped to the null vector when multiplied by $mathbf { A }$ , i.e., \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe span of the rows of A is the complement to the nullspace of A \nSee Figure 7.4 for an illustration of the range and nullspace of a matrix. We shall discuss how to compute the range and nullspace of a matrix numerically in Section 7.5.4 below. \n7.1.2.5 Linear projection \nThe projection of a vector $pmb { y } in mathbb { R } ^ { m }$ onto the span of ${ pmb { x } _ { 1 } , ldots , pmb { x } _ { n } }$ (here we assume $pmb { x } _ { i } in mathbb { R } ^ { m }$ ) is the vector ${ pmb v } in operatorname { s p a n } ( { { pmb x } _ { 1 } , dots , { pmb x } _ { n } } )$ , such that $_ { v }$ is as close as possible to $textbf {  { y } }$ , as measured by the Euclidean norm $| { pmb v } - { pmb y } | _ { 2 }$ . We denote the projection as $operatorname { P r o j } ( pmb { y } ; { pmb { x } _ { 1 } , dots , pmb { x } _ { n } } )$ and can define it formally as \nGiven a (full rank) matrix $mathbf { A } in mathbb { R } ^ { m times n }$ with $m geq n$ , we can define the projection of a vector $pmb { y } in mathbb { R } ^ { m }$ onto the range of $mathbf { A }$ as follows: \nThese are the same as the normal equations from Section 11.2.2.2. \n7.1.3 Norms of a vector and matrix \nIn this section, we discuss ways of measuring the “size” of a vector and matrix. \n7.1.3.1 Vector norms \nA norm of a vector $| pmb { x } |$ is, informally, a measure of the “length” of the vector. More formally, a norm is any function $f : mathbb { R } ^ { n }  mathbb { R }$ that satisfies 4 properties: \n• For all $pmb { x } in mathbb { R } ^ { n }$ , $f ( { pmb x } ) geq 0$ (non-negativity). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Introduction",
        "subsubsection": "Vector spaces"
    },
    {
        "content": "The span of the rows of A is the complement to the nullspace of A \nSee Figure 7.4 for an illustration of the range and nullspace of a matrix. We shall discuss how to compute the range and nullspace of a matrix numerically in Section 7.5.4 below. \n7.1.2.5 Linear projection \nThe projection of a vector $pmb { y } in mathbb { R } ^ { m }$ onto the span of ${ pmb { x } _ { 1 } , ldots , pmb { x } _ { n } }$ (here we assume $pmb { x } _ { i } in mathbb { R } ^ { m }$ ) is the vector ${ pmb v } in operatorname { s p a n } ( { { pmb x } _ { 1 } , dots , { pmb x } _ { n } } )$ , such that $_ { v }$ is as close as possible to $textbf {  { y } }$ , as measured by the Euclidean norm $| { pmb v } - { pmb y } | _ { 2 }$ . We denote the projection as $operatorname { P r o j } ( pmb { y } ; { pmb { x } _ { 1 } , dots , pmb { x } _ { n } } )$ and can define it formally as \nGiven a (full rank) matrix $mathbf { A } in mathbb { R } ^ { m times n }$ with $m geq n$ , we can define the projection of a vector $pmb { y } in mathbb { R } ^ { m }$ onto the range of $mathbf { A }$ as follows: \nThese are the same as the normal equations from Section 11.2.2.2. \n7.1.3 Norms of a vector and matrix \nIn this section, we discuss ways of measuring the “size” of a vector and matrix. \n7.1.3.1 Vector norms \nA norm of a vector $| pmb { x } |$ is, informally, a measure of the “length” of the vector. More formally, a norm is any function $f : mathbb { R } ^ { n }  mathbb { R }$ that satisfies 4 properties: \n• For all $pmb { x } in mathbb { R } ^ { n }$ , $f ( { pmb x } ) geq 0$ (non-negativity). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n• $f ( { pmb x } ) = 0$ if and only if $mathbf { boldsymbol { x } } = mathbf { boldsymbol { 0 } }$ (definiteness). \nFor all $pmb { x } in mathbb { R } ^ { n }$ , $t in mathbb R$ , $f ( t { mathbf { } } x ) = | t | f ( x )$ (absolute value homogeneity). \n• For all $pmb { x } , pmb { y } in mathbb { R } ^ { n }$ , $f ( { pmb x } + { pmb y } ) leq f ( { pmb x } ) + f ( { pmb y } )$ (triangle inequality). \nConsider the following common examples: \np-norm $begin{array} { r } { | pmb { x } | _ { p } = big ( sum _ { i = 1 } ^ { n } | x _ { i } | ^ { p } big ) ^ { 1 / p } } end{array}$ , for $p geq 1$ . \n2-norm $begin{array} { r } { | pmb { x } | _ { 2 } = sqrt { sum _ { i = 1 } ^ { n } x _ { i } ^ { 2 } } } end{array}$ , also called Euclidean norm. Note that $| pmb { x } | _ { 2 } ^ { 2 } = pmb { x } ^ { 1 } pmb { x }$ . \n1-norm $begin{array} { r } { | pmb { x } | _ { 1 } = sum _ { i = 1 } ^ { n } left| x _ { i } right| } end{array}$ . \nMax-norm $| pmb { x } | _ { infty } = operatorname* { m a x } _ { i } | x _ { i } |$ . \n0-norm $begin{array} { r } { | pmb { x } | _ { 0 } = sum _ { i = 1 } ^ { n } mathbb { I } left( | x _ { i } | > 0 right) } end{array}$ . This is a pseudo norm, since it does not satisfy homogeneity. It counts the number of non-zero elements in $_ { x }$ . If we define $0 ^ { 0 } = 0$ , we can write this as $begin{array} { r } { | pmb { x } | _ { 0 } = sum _ { i = 1 } ^ { n } x _ { i } ^ { 0 } } end{array}$ . \n7.1.3.2 Matrix norms \nSuppose we think of a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ as defining a linear function $f ( pmb { x } ) = mathbf { A } pmb { x }$ . We define the induced norm of $mathbf { A }$ as the maximum amount by which $f$ can lengthen any unit-norm input: \nTypically $p = 2$ , in which case \nwhere $sigma _ { i }$ is the $i$ ’th singular value. \nThe nuclear norm, also called the trace norm, is defined as \nwhere $sqrt { mathbf { A } ^ { mathsf { T } } mathbf { A } }$ is the matrix square root. Since the singular values are always non-negative, we have \nUsing this as a regularizer encourages many singular values to become zero, resulting in a low rank matrix. More generally, we can define the Schatten $p$ -norm as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf we think of a matrix as a vector, we can define the matrix norm in terms of a vector norm, $| | mathbf { A } | | = | | mathrm { v e c } ( mathbf { A } ) | |$ . If the vector norm is the 2-norm, the corresponding matrix norm is the Frobenius norm: \nIf $mathbf { A }$ is expensive to evaluate, but ${ bf A } v$ is cheap (for a random vector $_ { v }$ ), we can create a stochastic approximation to the Frobenius norm by using the Hutchinson trace estimator from Equation (7.37) as follows: \nwhere $pmb { v } sim mathcal { N } ( mathbf { 0 } , mathbf { I } )$ . \n7.1.4 Properties of a matrix \nIn this section, we discuss various scalar properties of matrices. \n7.1.4.1 Trace of a square matrix \nThe trace of a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ , denoted $operatorname { t r } ( mathbf { A } )$ , is the sum of diagonal elements in the matrix: \nThe trace has the following properties, where $c in mathbb { R }$ is a scalar, and $mathbf { A } , mathbf { B } in mathbb { R } ^ { n times n }$ are square matrices: \nWe also have the following important cyclic permutation property: For $mathbf { A } , mathbf { B } , mathbf { C }$ such that ABC is square, \nFrom this, we can derive the trace trick, which rewrites the scalar inner product ${ pmb x } ^ { 1 } { bf A } { pmb x }$ as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Introduction",
        "subsubsection": "Norms of a vector and matrix"
    },
    {
        "content": "If we think of a matrix as a vector, we can define the matrix norm in terms of a vector norm, $| | mathbf { A } | | = | | mathrm { v e c } ( mathbf { A } ) | |$ . If the vector norm is the 2-norm, the corresponding matrix norm is the Frobenius norm: \nIf $mathbf { A }$ is expensive to evaluate, but ${ bf A } v$ is cheap (for a random vector $_ { v }$ ), we can create a stochastic approximation to the Frobenius norm by using the Hutchinson trace estimator from Equation (7.37) as follows: \nwhere $pmb { v } sim mathcal { N } ( mathbf { 0 } , mathbf { I } )$ . \n7.1.4 Properties of a matrix \nIn this section, we discuss various scalar properties of matrices. \n7.1.4.1 Trace of a square matrix \nThe trace of a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ , denoted $operatorname { t r } ( mathbf { A } )$ , is the sum of diagonal elements in the matrix: \nThe trace has the following properties, where $c in mathbb { R }$ is a scalar, and $mathbf { A } , mathbf { B } in mathbb { R } ^ { n times n }$ are square matrices: \nWe also have the following important cyclic permutation property: For $mathbf { A } , mathbf { B } , mathbf { C }$ such that ABC is square, \nFrom this, we can derive the trace trick, which rewrites the scalar inner product ${ pmb x } ^ { 1 } { bf A } { pmb x }$ as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn some cases, it may be expensive to evaluate the matrix $mathbf { A }$ , but we may be able to cheaply evaluate matrix-vector products $mathbf { A } v$ . Suppose $mathbf { nabla } _ { mathbf { v } }$ is a random vector such that $mathbb { E } leftlfloor { boldsymbol { v } } { boldsymbol { v } } ^ { mathsf { T } } rightrfloor = mathbf { I }$ . In this case, we can create a Monte Carlo approximation to $operatorname { t r } ( mathbf { A } )$ using the following identity: \nThis is called the Hutchinson trace estimator [Hut90]. \n7.1.4.2 Determinant of a square matrix \nThe determinant of a square matrix, denoted $operatorname* { d e t } ( mathbf { A } )$ or $| mathbf { A } |$ , is a measure of how much it changes a unit volume when viewed as a linear transformation. (The formal definition is rather complex and is not needed here.) \nThe determinant operator satisfies these properties, where $mathbf { A } , mathbf { B } in mathbb { R } ^ { n times n }$ \nFor a positive definite matrix $mathbf { A }$ , we can write $mathbf { A } = mathbf { L L } ^ { mathsf { I } }$ , where $mathbf { L }$ is the lower triangular Cholesky decomposition. In this case, we have \nso \n7.1.4.3 Rank of a matrix \nThe column rank of a matrix A is the dimension of the space spanned by its columns, and the row rank is the dimension of the space spanned by its rows. It is a basic fact of linear algebra (that can be shown using the SVD, discussed in Section 7.5) that for any matrix A, columnrank $mathbf { dot { eta } } ( mathbf { A } ) = operatorname { r o w r a n k } ( mathbf { A } )$ , and so this quantity is simply referred to as the rank of $mathbf { A }$ , denoted as rank(A). The following are some basic properties of the rank: \n• For $mathbf { A } in mathbb { R } ^ { m times n }$ , $operatorname { r a n k } ( mathbf { A } ) leq operatorname* { m i n } ( m , n )$ . If $operatorname { r a n k } ( mathbf { A } ) = operatorname* { m i n } ( m , n )$ , then $mathbf { A }$ is said to be full rank, otherwise it is called rank deficient.   \n• For $mathbf { A } in mathbb { R } ^ { m times n }$ $begin{array} { r l } & { mathrm { , ~ r a n k } ( mathbf { A } ) = mathrm { r a n k } ( mathbf { A } ^ { mathsf { T } } ) = mathrm { r a n k } ( mathbf { A } ^ { mathsf { T } } mathbf { A } ) = mathrm { r a n k } ( mathbf { A } mathbf { A } ^ { mathsf { T } } ) mathrm { . } }  & { mathrm { , ~ } mathbf { B } in mathbb { R } ^ { n times p } mathrm { , ~ r a n k } ( mathbf { A } mathbf { B } ) le operatorname* { m i n } ( mathrm { r a n k } ( mathbf { A } ) mathrm { , r a n k } ( mathbf { B } ) ) mathrm { . } } end{array}$   \n• For $mathbf { A } in mathbb { R } ^ { m times n }$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license • For $mathbf { A } , mathbf { B } in mathbb { R } ^ { m times n }$ , $operatorname { r a n k } ( mathbf { A } + mathbf { B } ) leq operatorname { r a n k } ( mathbf { A } ) + operatorname { r a n k } ( mathbf { B } ) .$ \n\nOne can show that a square matrix is invertible iff it is full rank. \n7.1.4.4 Condition numbers \nThe condition number of a matrix A is a measure of how numerically stable any computations involving A will be. It is defined as follows: \nwhere $| | mathbf { A } | |$ is the norm of the matrix. We can show that $kappa ( mathbf { A } ) geq 1$ . (The condition number depends on which norm we use; we will assume the $ell _ { 2 }$ -norm unless stated otherwise.) \nWe say A is well-conditioned if $kappa ( mathbf { A } )$ is small (close to 1), and ill-conditioned if $kappa ( mathbf { A } )$ is large. A large condition number means $mathbf { A }$ is nearly singular. This is a better measure of nearness to singularity than the size of the determinant. For example, suppose $mathbf { A } = 0 . 1 mathbf { I } _ { 1 0 0 times 1 0 0 }$ . Then $operatorname* { d e t } ( mathbf { A } ) = 1 0 ^ { - 1 0 0 }$ , which suggests $mathbf { A }$ is nearly singular, but $kappa ( mathbf { A } ) = 1$ , which means $mathbf { A }$ is well-conditioned, reflecting the fact that ${ bf A } x$ simply scales the entries of $_ { x }$ by 0.1. \nTo get a better understanding of condition numbers, consider the linear system of equations $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = mathbf { mathit { b } }$ . If $mathbf { A }$ is non-singular, the unique solution is ${ pmb x } = { bf A } ^ { - 1 } { pmb b }$ . Suppose we change $^ { b }$ to $boldsymbol { b } + Delta boldsymbol { b }$ ; what effect will that have on $_ { x }$ ? The new solution must satisify \nwhere \nWe say that $mathbf { A }$ is well-conditioned if a small $Delta boldsymbol { b }$ results in a small $Delta { } x$ ; otherwise we say that $mathbf { A }$ is ill-conditioned. \nFor example, suppose \nThe solution for $pmb { b } = ( 1 , 1 )$ is ${ pmb x } = ( 1 , 1 )$ . If we change $^ { b }$ by $Delta boldsymbol { b }$ , the solution changes to \nSo a small change in $^ { b }$ can lead to an extremely large change in $_ { x }$ , because $mathbf { A }$ is ill-conditioned ( $kappa ( mathbf { A } ) = 2 times 1 0 ^ { 1 0 }$ ). \nIn the case of the $ell _ { 2 }$ -norm, the condition number is equal to the ratio of the largest to smallest singular values (defined in Section 7.5); furthermore, the singular values of $mathbf { A }$ are the square roots of the eigenvalues of $mathbf { A } ^ { 1 } mathbf { A }$ , and so \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe can gain further insight into condition numbers by considering a quadratic objective function $f ( pmb { x } ) = pmb { x } ^ { top } mathbf { A } pmb { x }$ . If we plot the level set of this function, it will be elliptical, as shown in Section 7.4.4. As we increase the condition number of A, the ellipses become more and more elongated along certain directions, corresponding to a very narrow valley in function space. If $kappa = 1$ (the minimum possible value), the level set will be circular. \n7.1.5 Special types of matrices \nIn this section, we will list some common kinds of matrices with various forms of structure. \n7.1.5.1 Diagonal matrix \nA diagonal matrix is a matrix where all non-diagonal elements are 0. This is typically denoted $mathbf { D } = mathrm { d i a g } ( d _ { 1 } , d _ { 2 } , ldots , d _ { n } )$ , with \nThe identity matrix, denoted $mathbf { I } in mathbb { R } ^ { n times n }$ , is a square matrix with ones on the diagonal and zeros everywhere else, $mathbf { I } = mathrm { d i a g } ( 1 , 1 , dots , 1 )$ . It has the property that for all $mathbf { A } in mathbb { R } ^ { n times n }$ , \nwhere the size of $mathbf { I }$ is determined by the dimensions of $mathbf { A }$ so that matrix multiplication is possible. \nWe can extract the diagonal vector from a matrix using $pmb { d } = mathrm { d i a g } ( mathbf { D } )$ . We can convert a vector into a diagonal matrix by writing $mathbf { D } = mathrm { d i a g } ( pmb { d } )$ . \nA block diagonal matrix is one which contains matrices on its main diagonal, and is $0$ everywhere else, e.g., \nA band-diagonal matrix only has non-zero entries along the diagonal, and on $k$ sides of the diagonal, where $k$ is the bandwidth. For example, a tridiagonal $6 times 6$ matrix looks like this: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Introduction",
        "subsubsection": "Properties of a matrix"
    },
    {
        "content": "We can gain further insight into condition numbers by considering a quadratic objective function $f ( pmb { x } ) = pmb { x } ^ { top } mathbf { A } pmb { x }$ . If we plot the level set of this function, it will be elliptical, as shown in Section 7.4.4. As we increase the condition number of A, the ellipses become more and more elongated along certain directions, corresponding to a very narrow valley in function space. If $kappa = 1$ (the minimum possible value), the level set will be circular. \n7.1.5 Special types of matrices \nIn this section, we will list some common kinds of matrices with various forms of structure. \n7.1.5.1 Diagonal matrix \nA diagonal matrix is a matrix where all non-diagonal elements are 0. This is typically denoted $mathbf { D } = mathrm { d i a g } ( d _ { 1 } , d _ { 2 } , ldots , d _ { n } )$ , with \nThe identity matrix, denoted $mathbf { I } in mathbb { R } ^ { n times n }$ , is a square matrix with ones on the diagonal and zeros everywhere else, $mathbf { I } = mathrm { d i a g } ( 1 , 1 , dots , 1 )$ . It has the property that for all $mathbf { A } in mathbb { R } ^ { n times n }$ , \nwhere the size of $mathbf { I }$ is determined by the dimensions of $mathbf { A }$ so that matrix multiplication is possible. \nWe can extract the diagonal vector from a matrix using $pmb { d } = mathrm { d i a g } ( mathbf { D } )$ . We can convert a vector into a diagonal matrix by writing $mathbf { D } = mathrm { d i a g } ( pmb { d } )$ . \nA block diagonal matrix is one which contains matrices on its main diagonal, and is $0$ everywhere else, e.g., \nA band-diagonal matrix only has non-zero entries along the diagonal, and on $k$ sides of the diagonal, where $k$ is the bandwidth. For example, a tridiagonal $6 times 6$ matrix looks like this: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.1.5.2 Triangular matrices \nAn upper triangular matrix only has non-zero entries on and above the diagonal. A lower triangular matrix only has non-zero entries on and below the diagonal. \nTriangular matrices have the useful property that the diagonal entries of $mathbf { A }$ are the eigenvalues of A, and hence the determinant is the product of diagonal entries: $begin{array} { r } { operatorname* { d e t } ( mathbf { A } ) = prod _ { i } A _ { i i } } end{array}$ . \n7.1.5.3 Positive definite matrices \nGiven a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ and a vector $pmb { x } in mathbb { R } ^ { n }$ , the scalar value ${ pmb x } ^ { 1 } { bf A } { pmb x }$ is called a quadratic form. Written explicitly, we see that \nNote that, \nFor this reason, we often implicitly assume that the matrices appearing in a quadratic form are symmetric. \nWe give the following definitions: \n• A symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ is positive definite iff for all non-zero vectors $pmb { x } in mathbb { R } ^ { n }$ , ${ pmb x } ^ { 1 } { bf A } { pmb x } > 0$ . This is usually denoted $mathbf A succ 0$ (or just $mathbf A > 0$ ). If it is possible that ${ pmb x } ^ { 1 } { bf A } { pmb x } = 0$ , we say the matrix is positive semidefinite or psd. We denote the set of all positive definite matrices by $mathbb { S } _ { + + } ^ { n }$ . • A symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ is negative definite, denoted $mathbf A prec 0$ (or just $mathbf A < 0$ ) iff for all non-zero $pmb { x } in mathbb { R } ^ { n }$ , $pmb { x } ^ { top } mathbf { A } pmb { x } < 0$ . If it is possible that ${ pmb x } ^ { 1 } { bf A } { pmb x } = 0$ , we say the matrix is negative semidefinite. • A symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ is indefinite, if it is neither positive semidefinite nor negative semidefinite — i.e., if there exists $pmb { x } _ { 1 } , pmb { x } _ { 2 } in mathbb { R } ^ { n }$ such that $x _ { 1 } ^ { mathsf { T } } mathbf { A } x _ { 1 } > 0$ and $pmb { x } _ { 2 } ^ { top } mathbf { A } pmb { x } _ { 2 } < 0$ . \nIt should be obvious that if $mathbf { A }$ is positive definite, then $- mathbf { A }$ is negative definite and vice versa. Likewise, if $mathbf { A }$ is positive semidefinite then $- mathbf { A }$ is negative semidefinite and vice versa. If A is indefinite, then so is $- mathbf { A }$ . It can also be shown that positive definite and negative definite matrices are always invertible. \nIn Section 7.4.3.1, we show that a symmetric matrix is positive definite iff its eigenvalues are positive. Note that if all elements of $mathbf { A }$ are positive, it does not mean A is necessarily positive definite. For example, $mathbf { A } = { left( begin{array} { l l } { 4 } & { 3 }  { 3 } & { 2 } end{array} right) }$ is not positive definite. Conversely, a positive definite matrix can have negative entries e.g., $mathbf { A } = { binom { 2 } { - 1 } } quad { binom { - 1 } { 2 } }$ \nA sufficient condition for a (real, symmetric) matrix to be positive definite is that it is diagonally dominant, i.e., if in every row of the matrix, the magnitude of the diagonal entry in that row is \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 larger than the sum of the magnitudes of all the other (non-diagonal) entries in that row. More precisely, \n\nIn 2d, any real, symmetric $2 times 2$ matrix $textstyle { binom { a quad b } { b } }$ is positive definite iff $a > 0$ , $d > 0$ and $a d > b ^ { 2 }$ . Finally, there is one type of positive definite matrix that comes up frequently, and so deserves some special mention. Given any matrix $mathbf { A } in mathbb { R } ^ { m times n }$ (not necessarily symmetric or even square), the Gram matrix $mathbf { G } = mathbf { A } ^ { mathsf { I } } mathbf { A }$ is always positive semidefinite. Further, if $m geq n$ (and we assume for convenience that A is full rank), then $mathbf { G } = mathbf { A } ^ { 1 } mathbf { A }$ is positive definite. \n7.1.5.4 Orthogonal matrices \nTwo vectors $pmb { x } , pmb { y } in mathbb { R } ^ { n }$ are orthogonal if $pmb { x } ^ { 1 } pmb { y } = 0$ . A vector $pmb { x } in mathbb { R } ^ { n }$ is normalized if $| { pmb x } | _ { 2 } = 1$ . A set of vectors that is pairwise orthogonal and normalized is called orthonormal. A square matrix $mathbf { U } in mathbb { R } ^ { n times n }$ is orthogonal if all its columns are orthonormal. (Note the different meaning of the term orthogonal when talking about vectors versus matrices.) If the entries of $mathbf { U }$ are complex valued, we use the term unitary instead of orthogonal. \nIt follows immediately from the definition of orthogonality and normality that $mathbf { U }$ is orthogonal iff \nIn other words, the inverse of an orthogonal matrix is its transpose. Note that if $mathbf { U }$ is not square i.e., $mathbf { U } in mathbb { R } ^ { m times n } , n < m$ — but its columns are still orthonormal, then $mathbf { U } ^ { mathsf { T } } mathbf { U } = mathbf { I }$ , but $mathbf { U } mathbf { U } ^ { mathsf { I } } neq I$ . We generally only use the term orthogonal to describe the previous case, where $mathbf { U }$ is square. \nAn example of an orthogonal matrix is a rotation matrix (see Exercise 7.1). For example, a rotation in 3d by angle $alpha$ about the $z$ axis is given by \nIf $alpha = 4 5 ^ { circ }$ , this becomes \nwhere $textstyle { frac { 1 } { sqrt { 2 } } } = 0 . 7 0 7 1$ . We see that $mathbf { R } ( - alpha ) = mathbf { R } ( alpha ) ^ { - 1 } = mathbf { R } ( alpha ) ^ { 1 }$ , so this is an orthogonal matrix. \nOne nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix will not change its Euclidean norm, i.e., \nfor any nonzero $pmb { x } in mathbb { R } ^ { n }$ , and orthogonal $mathbf { U } in mathbb { R } ^ { n times n }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nSimilarly, one can show that the angle between two vectors is preserved after they are transformed by an orthogonal matrix. The cosine of the angle between $_ { x }$ and $pmb { y }$ is given by \nso \nIn summary, transformations by orthogonal matrices are generalizations of rotations (if $operatorname* { d e t } ( mathbf { U } ) = 1$ ) and reflections (if $operatorname* { d e t } ( mathbf { U } ) = - 1$ ), since they preserve lengths and angles. \nNote that there is technique called Gram Schmidt orthogonalization which is a way to make any square matrix orthogonal, but we will not cover it here. \n7.2 Matrix multiplication \nThe product of two matrices $mathbf { A } in mathbb { R } ^ { m times n }$ and $mathbf { B } in mathbb { R } ^ { n times p }$ is the matrix \nwhere \nNote that in order for the matrix product to exist, the number of columns in A must equal the number of rows in $mathbf { B }$ . \nMatrix multiplication generally takes $O ( m n p )$ time, although faster methods exist. In addition, specialized hardware, such as GPUs and TPUs, can be leveraged to speed up matrix multiplication significantly, by performing operations across the rows (or columns) in parallel. \nIt is useful to know a few basic properties of matrix multiplication: \n• Matrix multiplication is associative: $( mathbf { A B } ) mathbf { C } = mathbf { A } ( mathbf { B C } )$ .   \n• Matrix multiplication is distributive: $mathbf { A } ( mathbf { B } + mathbf { C } ) = mathbf { A B } + mathbf { A C }$ .   \n• Matrix multiplication is, in general, not commutative; that is, it can be the case that $mathbf { A B } neq mathbf { B A }$ . \n(In each of the above cases, we are assuming that the dimensions match.) There are many important special cases of matrix multiplication, as we discuss below. \n7.2.1 Vector–vector products \nGiven two vectors $pmb { x } , pmb { y } in mathbb { R } ^ { n }$ , the quantity $x ^ { 1 } y$ , called the inner product, dot product or scalar product of the vectors, is a real number given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Introduction",
        "subsubsection": "Special types of matrices"
    },
    {
        "content": "Similarly, one can show that the angle between two vectors is preserved after they are transformed by an orthogonal matrix. The cosine of the angle between $_ { x }$ and $pmb { y }$ is given by \nso \nIn summary, transformations by orthogonal matrices are generalizations of rotations (if $operatorname* { d e t } ( mathbf { U } ) = 1$ ) and reflections (if $operatorname* { d e t } ( mathbf { U } ) = - 1$ ), since they preserve lengths and angles. \nNote that there is technique called Gram Schmidt orthogonalization which is a way to make any square matrix orthogonal, but we will not cover it here. \n7.2 Matrix multiplication \nThe product of two matrices $mathbf { A } in mathbb { R } ^ { m times n }$ and $mathbf { B } in mathbb { R } ^ { n times p }$ is the matrix \nwhere \nNote that in order for the matrix product to exist, the number of columns in A must equal the number of rows in $mathbf { B }$ . \nMatrix multiplication generally takes $O ( m n p )$ time, although faster methods exist. In addition, specialized hardware, such as GPUs and TPUs, can be leveraged to speed up matrix multiplication significantly, by performing operations across the rows (or columns) in parallel. \nIt is useful to know a few basic properties of matrix multiplication: \n• Matrix multiplication is associative: $( mathbf { A B } ) mathbf { C } = mathbf { A } ( mathbf { B C } )$ .   \n• Matrix multiplication is distributive: $mathbf { A } ( mathbf { B } + mathbf { C } ) = mathbf { A B } + mathbf { A C }$ .   \n• Matrix multiplication is, in general, not commutative; that is, it can be the case that $mathbf { A B } neq mathbf { B A }$ . \n(In each of the above cases, we are assuming that the dimensions match.) There are many important special cases of matrix multiplication, as we discuss below. \n7.2.1 Vector–vector products \nGiven two vectors $pmb { x } , pmb { y } in mathbb { R } ^ { n }$ , the quantity $x ^ { 1 } y$ , called the inner product, dot product or scalar product of the vectors, is a real number given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nNote that it is always the case that $mathbf { x } ^ { mathsf { Pi } } mathbf { y } = mathbf { y } ^ { mathsf { Pi } } mathbf { x }$ . \nGiven vectors $pmb { x } in mathbb { R } ^ { r n }$ , $ b { y } in mathbb { R } ^ { n }$ (they no longer have to be the same size), $boldsymbol { x } boldsymbol { y } ^ { intercal }$ is called the outer product of the vectors. It is a matrix whose entries are given by $( { pmb x } { pmb y } ^ { top } ) _ { i j } = x _ { i } y _ { j }$ , i.e., \n7.2.2 Matrix–vector products \nGiven a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ and a vector $pmb { x } in mathbb { R } ^ { n }$ , their product is a vector $pmb { y } = mathbf { A } pmb { x } in mathbb { R } ^ { m }$ . There are a couple ways of looking at matrix-vector multiplication, and we will look at them both. \nIf we write $mathbf { A }$ by rows, then we can express $mathbf { boldsymbol { y } } = mathbf { boldsymbol { A } } mathbf { boldsymbol { x } }$ as follows: \nIn other words, the $i$ th entry of $textbf {  { y } }$ is equal to the inner product of the $i$ th $r o w$ of $mathbf { A }$ and $_ { x }$ , $y _ { i } = pmb { a } _ { i } ^ { 1 } pmb { x }$ . Alternatively, let’s write $mathbf { A }$ in column form. In this case we see that \nIn other words, $pmb { y }$ is a linear combination of the columns of A, where the coefficients of the linear combination are given by the entries of $_ { x }$ . We can view the columns of $mathbf { A }$ as a set of basis vectors defining a linear subspace. We can contstruct vectors in this subspace by taking linear combinations of the basis vectors. See Section 7.1.2 for details. \n7.2.3 Matrix–matrix products \nBelow we look at four different (but, of course, equivalent) ways of viewing the matrix-matrix multiplication $mathbf { C } = mathbf { A } mathbf { B }$ . \nFirst we can view matrix-matrix multiplication as a set of vector-vector products. The most obvious viewpoint, which follows immediately from the definition, is that the $i , j$ entry of $mathbf { C }$ is equal to the inner product of the $i$ th row of $mathbf { A }$ and the $j$ th column of $mathbf { B }$ . Symbolically, this looks like the following, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix multiplication",
        "subsubsection": "Vector–vector products"
    },
    {
        "content": "Note that it is always the case that $mathbf { x } ^ { mathsf { Pi } } mathbf { y } = mathbf { y } ^ { mathsf { Pi } } mathbf { x }$ . \nGiven vectors $pmb { x } in mathbb { R } ^ { r n }$ , $ b { y } in mathbb { R } ^ { n }$ (they no longer have to be the same size), $boldsymbol { x } boldsymbol { y } ^ { intercal }$ is called the outer product of the vectors. It is a matrix whose entries are given by $( { pmb x } { pmb y } ^ { top } ) _ { i j } = x _ { i } y _ { j }$ , i.e., \n7.2.2 Matrix–vector products \nGiven a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ and a vector $pmb { x } in mathbb { R } ^ { n }$ , their product is a vector $pmb { y } = mathbf { A } pmb { x } in mathbb { R } ^ { m }$ . There are a couple ways of looking at matrix-vector multiplication, and we will look at them both. \nIf we write $mathbf { A }$ by rows, then we can express $mathbf { boldsymbol { y } } = mathbf { boldsymbol { A } } mathbf { boldsymbol { x } }$ as follows: \nIn other words, the $i$ th entry of $textbf {  { y } }$ is equal to the inner product of the $i$ th $r o w$ of $mathbf { A }$ and $_ { x }$ , $y _ { i } = pmb { a } _ { i } ^ { 1 } pmb { x }$ . Alternatively, let’s write $mathbf { A }$ in column form. In this case we see that \nIn other words, $pmb { y }$ is a linear combination of the columns of A, where the coefficients of the linear combination are given by the entries of $_ { x }$ . We can view the columns of $mathbf { A }$ as a set of basis vectors defining a linear subspace. We can contstruct vectors in this subspace by taking linear combinations of the basis vectors. See Section 7.1.2 for details. \n7.2.3 Matrix–matrix products \nBelow we look at four different (but, of course, equivalent) ways of viewing the matrix-matrix multiplication $mathbf { C } = mathbf { A } mathbf { B }$ . \nFirst we can view matrix-matrix multiplication as a set of vector-vector products. The most obvious viewpoint, which follows immediately from the definition, is that the $i , j$ entry of $mathbf { C }$ is equal to the inner product of the $i$ th row of $mathbf { A }$ and the $j$ th column of $mathbf { B }$ . Symbolically, this looks like the following, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix multiplication",
        "subsubsection": "Matrix–vector products"
    },
    {
        "content": "Note that it is always the case that $mathbf { x } ^ { mathsf { Pi } } mathbf { y } = mathbf { y } ^ { mathsf { Pi } } mathbf { x }$ . \nGiven vectors $pmb { x } in mathbb { R } ^ { r n }$ , $ b { y } in mathbb { R } ^ { n }$ (they no longer have to be the same size), $boldsymbol { x } boldsymbol { y } ^ { intercal }$ is called the outer product of the vectors. It is a matrix whose entries are given by $( { pmb x } { pmb y } ^ { top } ) _ { i j } = x _ { i } y _ { j }$ , i.e., \n7.2.2 Matrix–vector products \nGiven a matrix $mathbf { A } in mathbb { R } ^ { m times n }$ and a vector $pmb { x } in mathbb { R } ^ { n }$ , their product is a vector $pmb { y } = mathbf { A } pmb { x } in mathbb { R } ^ { m }$ . There are a couple ways of looking at matrix-vector multiplication, and we will look at them both. \nIf we write $mathbf { A }$ by rows, then we can express $mathbf { boldsymbol { y } } = mathbf { boldsymbol { A } } mathbf { boldsymbol { x } }$ as follows: \nIn other words, the $i$ th entry of $textbf {  { y } }$ is equal to the inner product of the $i$ th $r o w$ of $mathbf { A }$ and $_ { x }$ , $y _ { i } = pmb { a } _ { i } ^ { 1 } pmb { x }$ . Alternatively, let’s write $mathbf { A }$ in column form. In this case we see that \nIn other words, $pmb { y }$ is a linear combination of the columns of A, where the coefficients of the linear combination are given by the entries of $_ { x }$ . We can view the columns of $mathbf { A }$ as a set of basis vectors defining a linear subspace. We can contstruct vectors in this subspace by taking linear combinations of the basis vectors. See Section 7.1.2 for details. \n7.2.3 Matrix–matrix products \nBelow we look at four different (but, of course, equivalent) ways of viewing the matrix-matrix multiplication $mathbf { C } = mathbf { A } mathbf { B }$ . \nFirst we can view matrix-matrix multiplication as a set of vector-vector products. The most obvious viewpoint, which follows immediately from the definition, is that the $i , j$ entry of $mathbf { C }$ is equal to the inner product of the $i$ th row of $mathbf { A }$ and the $j$ th column of $mathbf { B }$ . Symbolically, this looks like the following, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nRemember that since $mathbf { A } in mathbb { R } ^ { m times n }$ and $mathbf { B } in mathbb { R } ^ { n times p }$ , $mathbf { pmb { a } } _ { i } in mathbb { R } ^ { n }$ and $boldsymbol { b } _ { j } in mathbb { R } ^ { n }$ , so these inner products all make sense. This is the most “natural” representation when we represent $mathbf { A }$ by rows and $mathbf { B }$ by columns. See Figure 7.5 for an illustration. \nAlternatively, we can represent $mathbf { A }$ by columns, and $mathbf { B }$ by rows, which leads to the interpretation of AB as a sum of outer products. Symbolically, \nPut another way, AB is equal to the sum, over all $i$ , of the outer product of the $i$ th column of A and the $i$ th row of $mathbf { B }$ . Since, in this case, $mathbf { pmb { a } } _ { i } in mathbb { R } ^ { m }$ and $mathbf { Delta } b _ { i } in mathbb { R } ^ { p }$ , the dimension of the outer product $mathbf { alpha } _ { mathbf { } mathbf { } mathbf { } a _ { i } mathbf { delta } _ { i } }$ is $m times p$ , which coincides with the dimension of $mathbf { C }$ . \nWe can also view matrix-matrix multiplication as a set of matrix-vector products. Specifically, if we represent $mathbf { B }$ by columns, we can view the columns of $mathbf { C }$ as matrix-vector products between $mathbf { A }$ and the columns of $mathbf { B }$ . Symbolically, \nHere the $i$ th column of $mathbf { C }$ is given by the matrix-vector product with the vector on the right, $pmb { c } _ { i } = mathbf { A } b _ { i }$ These matrix-vector products can in turn be interpreted using both viewpoints given in the previous subsection. \nFinally, we have the analogous viewpoint, where we represent A by rows, and view the rows of $mathbf { C }$ as the matrix-vector product between the rows of $mathbf { A }$ and the matrix $mathbf { B }$ . Symbolically, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHere the $i$ th row of $mathbf { C }$ is given by the matrix-vector product with the vector on the left, $mathbf { c } _ { i } ^ { mathsf { I } } = mathbf { a } _ { i } ^ { mathsf { I } } mathbf { B }$ . It may seem like overkill to dissect matrix multiplication to such a large degree, especially when all these viewpoints follow immediately from the initial definition we gave (in about a line of math) at the beginning of this section. However, virtually all of linear algebra deals with matrix multiplications of some kind, and it is worthwhile to spend some time trying to develop an intuitive understanding of the viewpoints presented here. \nFinally, a word on notation. We write ${ mathbf { A } } ^ { 2 }$ as shorthand for AA, which is the matrix product. To denote elementwise squaring of the elements of a matrix, we write $mathbf { A } ^ { odot 2 } = [ A _ { i j } ^ { 2 } ]$ . (If A is diagonal, then $mathbf { A } ^ { 2 } = mathbf { A } ^ { odot 2 }$ .) \nWe can also define the inverse of ${ mathbf A } ^ { 2 }$ using the matrix square root: we say $mathbf { A } = { sqrt { mathbf { M } } }$ if $mathbf { A } ^ { 2 } = mathbf { M }$ . To denote elementwise square root of the elements of a matrix, we write $[ sqrt { M _ { i j } } ]$ . \n7.2.4 Application: manipulating data matrices \nAs an application of the above results, consider the case where $mathbf { X }$ is the $N times D$ design matrix, whose rows are the data cases. There are various common preprocessing operations that we apply to this matrix, which we summarize below. (Writing these operations in matrix form is useful because it is notationally compact, and it allows us to implement the methods quickly using fast matrix code.) \n7.2.4.1 Summing slices of the matrix \nSuppose $mathbf { X }$ is an $N times D$ matrix. We can sum across the rows by premultiplying by a $1 times N$ matrix of ones to create a $1 times D$ matrix: \nHence the mean of the data vectors is given by \nWe can sum across the columns by postmultiplying by a $D times 1$ matrix of ones to create a $N times 1$ matrix: \nWe can sum all entries in a matrix by pre and post multiplying by a vector of 1s: \nHence the overall mean is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix multiplication",
        "subsubsection": "Matrix–matrix products"
    },
    {
        "content": "Here the $i$ th row of $mathbf { C }$ is given by the matrix-vector product with the vector on the left, $mathbf { c } _ { i } ^ { mathsf { I } } = mathbf { a } _ { i } ^ { mathsf { I } } mathbf { B }$ . It may seem like overkill to dissect matrix multiplication to such a large degree, especially when all these viewpoints follow immediately from the initial definition we gave (in about a line of math) at the beginning of this section. However, virtually all of linear algebra deals with matrix multiplications of some kind, and it is worthwhile to spend some time trying to develop an intuitive understanding of the viewpoints presented here. \nFinally, a word on notation. We write ${ mathbf { A } } ^ { 2 }$ as shorthand for AA, which is the matrix product. To denote elementwise squaring of the elements of a matrix, we write $mathbf { A } ^ { odot 2 } = [ A _ { i j } ^ { 2 } ]$ . (If A is diagonal, then $mathbf { A } ^ { 2 } = mathbf { A } ^ { odot 2 }$ .) \nWe can also define the inverse of ${ mathbf A } ^ { 2 }$ using the matrix square root: we say $mathbf { A } = { sqrt { mathbf { M } } }$ if $mathbf { A } ^ { 2 } = mathbf { M }$ . To denote elementwise square root of the elements of a matrix, we write $[ sqrt { M _ { i j } } ]$ . \n7.2.4 Application: manipulating data matrices \nAs an application of the above results, consider the case where $mathbf { X }$ is the $N times D$ design matrix, whose rows are the data cases. There are various common preprocessing operations that we apply to this matrix, which we summarize below. (Writing these operations in matrix form is useful because it is notationally compact, and it allows us to implement the methods quickly using fast matrix code.) \n7.2.4.1 Summing slices of the matrix \nSuppose $mathbf { X }$ is an $N times D$ matrix. We can sum across the rows by premultiplying by a $1 times N$ matrix of ones to create a $1 times D$ matrix: \nHence the mean of the data vectors is given by \nWe can sum across the columns by postmultiplying by a $D times 1$ matrix of ones to create a $N times 1$ matrix: \nWe can sum all entries in a matrix by pre and post multiplying by a vector of 1s: \nHence the overall mean is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.2.4.2 Scaling rows and columns of a matrix \nWe often want to scale rows or columns of a data matrix (e.g., to standardize them). We now show how to write this in matrix notation. \nIf we pre-multiply $mathbf { X }$ by a diagonal matrix $mathbf { S } = mathrm { d i a g } ( pmb { s } )$ , where $s$ is an $N$ -vector, then we just scale each row of $mathbf { X }$ by the corresponding scale factor in $pmb { s }$ : \nIf we post-multiply $mathbf { X }$ by a diagonal matrix $mathbf { S } = mathrm { d i a g } ( pmb { s } )$ , where $pmb { s }$ is a $D$ -vector, then we just scale each column of $mathbf { X }$ by the corresponding element in $pmb { s }$ . \nThus we can rewrite the standardization operation from Section 10.2.8 in matrix form as follows: \nwhere $pmb { mu } = overline { { pmb { x } } }$ is the empirical mean, and $pmb { sigma }$ is a vector of the empirical standard deviations. \n7.2.4.3 Sum of squares and scatter matrix \nThe sum of squares matrix is $D times D$ matrix defined by \nThe scatter matrix is a $D times D$ matrix defined by \nWe see that this is the sum of squares matrix applied to the mean-centered data. More precisely, define $tilde { mathbf { X } }$ to be a version of $mathbf { X }$ where we subtract the mean $begin{array} { r } { overline { { pmb { x } } } = frac { 1 } { N } mathbf { X } ^ { mathsf { T } } mathbf { 1 } _ { N } } end{array}$ off every row. Hence we can compute the centered data matrix using \nwhere \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nis the centering matrix, and $mathbf { J } _ { N } = mathbf { 1 } _ { N } mathbf { 1 } _ { N } ^ { sf }$ is a matrix of all 1s. The scatter matrix can now be computed as follows: \nwhere we exploited the fact that $mathbf { C } _ { N }$ is symmetric and idempotent, i.e., ${ bf C } _ { N } ^ { k } = { bf C } _ { N }$ for $k = 1 , 2 , ldots$ (since once we subtract the mean, subtracting it again has no effect). \n7.2.4.4 Gram matrix \nThe $N times N$ matrix $mathbf { X X ^ { parallel } }$ is a matrix of inner products called the Gram matrix: \nSometimes we want to compute the inner products of the mean-centered data vectors, $tilde { mathbf { K } } = tilde { mathbf { X } } tilde { mathbf { X } } ^ { top }$ . However, if we are working with a feature similarity matrix instead of raw features, we will only have access to $mathbf { K }$ , not $mathbf { X }$ . (We will see examples of this in Section 20.4.4 and Section 20.4.6.) Fortunately, we can compute $tilde { bf K }$ from $mathbf { K }$ using the double centering trick: \nThis subtracts the row means and column means from $mathbf { K }$ , and adds back the global mean that gets subtracted twice, so that both row means and column means of $tilde { bf K }$ are equal to zero. \nTo see why Equation (7.89) is true, consider the scalar form: \n7.2.4.5 Distance matrix \nLet $mathbf { X }$ be $N _ { x } times D$ datamatrix, and $mathbf { Y }$ be another $N _ { y } times D$ datamatrix. We can compute the squared pairwise distances between these using \nLet us now write this in matrix form. Let $hat { pmb x } = [ | | { pmb x } _ { 1 } | | ^ { 2 } ; cdot cdot cdot ; | | { pmb x } _ { N _ { x } } | | ^ { 2 } ] = mathrm { d i a g } ( { pmb X } { pmb X } ^ { 1 } )$ be a vector where each element is the squared norm of the examples in $mathbf { X }$ , and define $hat { boldsymbol y }$ similarly. Then we have \nIn the case that $mathbf { X } = mathbf { Y }$ , we have \nThis vectorized computation is often much faster than using for loops. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.2.5 Kronecker products * \nIf $mathbf { A }$ is an $m times n$ matrix and $mathbf { B }$ is a $p times q$ matrix, then the Kronecker product $mathbf { A } otimes mathbf { B }$ is the $m p times n q$ block matrix \nFor example, \nHere are some useful identities: \nwhere vec(M) stacks the columns of M. (If we stack along the rows, we get $( mathbf { A } otimes mathbf { B } ) mathrm { v e c } ( mathbf { C } ) =$ $mathrm { v e c } ( mathbf { A C B } ^ { mathsf { T } } )$ .) See [Loa00] for a list of other useful properties. \n7.2.6 Einstein summation * \nEinstein summation, or einsum for short, is a notational shortcut for working with tensors. The convention was introduced by Einstein [Ein16, sec 5], who later joked to a friend, “I have made a great discovery in mathematics; I have suppressed the summation sign every time that the summation must be made over an index which occurs twice...” [Pai05, p.216]. For example, instead of writing matrix multiplication as $begin{array} { r } { C _ { i j } = sum _ { k } A _ { i k } B _ { k j } } end{array}$ , we can just write it as $C _ { i j } = A _ { i k } B _ { k j }$ , where we drop the $scriptstyle sum _ { k }$ . \nAs a more complex example, suppose we have a 3d tensor $S _ { n t k }$ where $n$ indexes examples in the batch, $t$ indexes locations in the sequence, and $k$ indexes words in a one-hot representation. Let $W _ { k d }$ be an embedding matrix that maps sparse one-hot vectors $mathbb { R } ^ { k }$ to dense vectors in $mathbb { R } ^ { d }$ . We can convert the batch of sequences of one-hots to a batch of sequences of embeddings as follows: \nWe can compute the sum of the embedding vectors for each sequence (to get a global representation of each bag of words) as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix multiplication",
        "subsubsection": "Application: manipulating data matrices"
    },
    {
        "content": "7.2.5 Kronecker products * \nIf $mathbf { A }$ is an $m times n$ matrix and $mathbf { B }$ is a $p times q$ matrix, then the Kronecker product $mathbf { A } otimes mathbf { B }$ is the $m p times n q$ block matrix \nFor example, \nHere are some useful identities: \nwhere vec(M) stacks the columns of M. (If we stack along the rows, we get $( mathbf { A } otimes mathbf { B } ) mathrm { v e c } ( mathbf { C } ) =$ $mathrm { v e c } ( mathbf { A C B } ^ { mathsf { T } } )$ .) See [Loa00] for a list of other useful properties. \n7.2.6 Einstein summation * \nEinstein summation, or einsum for short, is a notational shortcut for working with tensors. The convention was introduced by Einstein [Ein16, sec 5], who later joked to a friend, “I have made a great discovery in mathematics; I have suppressed the summation sign every time that the summation must be made over an index which occurs twice...” [Pai05, p.216]. For example, instead of writing matrix multiplication as $begin{array} { r } { C _ { i j } = sum _ { k } A _ { i k } B _ { k j } } end{array}$ , we can just write it as $C _ { i j } = A _ { i k } B _ { k j }$ , where we drop the $scriptstyle sum _ { k }$ . \nAs a more complex example, suppose we have a 3d tensor $S _ { n t k }$ where $n$ indexes examples in the batch, $t$ indexes locations in the sequence, and $k$ indexes words in a one-hot representation. Let $W _ { k d }$ be an embedding matrix that maps sparse one-hot vectors $mathbb { R } ^ { k }$ to dense vectors in $mathbb { R } ^ { d }$ . We can convert the batch of sequences of one-hots to a batch of sequences of embeddings as follows: \nWe can compute the sum of the embedding vectors for each sequence (to get a global representation of each bag of words) as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix multiplication",
        "subsubsection": "Kronecker products *"
    },
    {
        "content": "7.2.5 Kronecker products * \nIf $mathbf { A }$ is an $m times n$ matrix and $mathbf { B }$ is a $p times q$ matrix, then the Kronecker product $mathbf { A } otimes mathbf { B }$ is the $m p times n q$ block matrix \nFor example, \nHere are some useful identities: \nwhere vec(M) stacks the columns of M. (If we stack along the rows, we get $( mathbf { A } otimes mathbf { B } ) mathrm { v e c } ( mathbf { C } ) =$ $mathrm { v e c } ( mathbf { A C B } ^ { mathsf { T } } )$ .) See [Loa00] for a list of other useful properties. \n7.2.6 Einstein summation * \nEinstein summation, or einsum for short, is a notational shortcut for working with tensors. The convention was introduced by Einstein [Ein16, sec 5], who later joked to a friend, “I have made a great discovery in mathematics; I have suppressed the summation sign every time that the summation must be made over an index which occurs twice...” [Pai05, p.216]. For example, instead of writing matrix multiplication as $begin{array} { r } { C _ { i j } = sum _ { k } A _ { i k } B _ { k j } } end{array}$ , we can just write it as $C _ { i j } = A _ { i k } B _ { k j }$ , where we drop the $scriptstyle sum _ { k }$ . \nAs a more complex example, suppose we have a 3d tensor $S _ { n t k }$ where $n$ indexes examples in the batch, $t$ indexes locations in the sequence, and $k$ indexes words in a one-hot representation. Let $W _ { k d }$ be an embedding matrix that maps sparse one-hot vectors $mathbb { R } ^ { k }$ to dense vectors in $mathbb { R } ^ { d }$ . We can convert the batch of sequences of one-hots to a batch of sequences of embeddings as follows: \nWe can compute the sum of the embedding vectors for each sequence (to get a global representation of each bag of words) as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nFinally we can pass each sequence’s vector representation through another linear transform $V _ { d c }$ to map to the logits over a classifier with $c$ labels: \nIn einsum notation, we write $L _ { n c } = S _ { n t k } W _ { k d } V _ { d c }$ . We sum over $k$ and $d$ because those indices occur twice on the RHS. We sum over $t$ because that index does not occur on the LHS. \nEinsum is implemented in NumPy, Tensorflow, PyTorch, etc. What makes it particularly useful is that it can perform the relevant tensor multiplications in complex expressions in an optimal order, so as to minimize time and intermediate memory allocation.2 The library is best illustrated by the examples in einsum_demo.ipynb. \nNote that the speed of einsum depends on the order in which the operations are performed, which depends on the shapes of the relevant arguments. The optimal ordering minimizes the treewidth of the resulting computation graph, as explained in [GASG18]. In general, the time to compute the optimal ordering is exponential in the number of arguments, so it is common to use a greedy approximation. However, if we expect to repeat the same calculation many times, using tensors of the same shape but potentially different content, we can compute the optimal ordering once and reuse it multiple times. \n7.3 Matrix inversion \nIn this section, we discuss how to invert different kinds of matrices. \n7.3.1 The inverse of a square matrix \nThe inverse of a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ is denoted $mathbf { A } ^ { - 1 }$ , and is the unique matrix such that \nNote that ${ { bf A } ^ { - 1 } }$ exists if and only if $operatorname* { d e t } ( mathbf { A } ) neq 0$ . If $operatorname* { d e t } ( mathbf { A } ) = 0$ , it is called a singular matrix. The following are properties of the inverse; all assume that $mathbf { A } , mathbf { B } in mathbb { R } ^ { n times n }$ are non-singular: \nFor the case of a $2 times 2$ matrix, the expression for $mathbf { A } ^ { - 1 }$ is simple enough to give explicitly. We have \nFor a block diagonal matrix, the inverse is obtained by simply inverting each block separately, e.g., \n2. These optimizations are implemented in the opt-einsum library [GASG18]. Its core functionality is included in NumPy and JAX einsum functions, provided you set optimize $ c =$ True parameter. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix multiplication",
        "subsubsection": "Einstein summation *"
    },
    {
        "content": "Finally we can pass each sequence’s vector representation through another linear transform $V _ { d c }$ to map to the logits over a classifier with $c$ labels: \nIn einsum notation, we write $L _ { n c } = S _ { n t k } W _ { k d } V _ { d c }$ . We sum over $k$ and $d$ because those indices occur twice on the RHS. We sum over $t$ because that index does not occur on the LHS. \nEinsum is implemented in NumPy, Tensorflow, PyTorch, etc. What makes it particularly useful is that it can perform the relevant tensor multiplications in complex expressions in an optimal order, so as to minimize time and intermediate memory allocation.2 The library is best illustrated by the examples in einsum_demo.ipynb. \nNote that the speed of einsum depends on the order in which the operations are performed, which depends on the shapes of the relevant arguments. The optimal ordering minimizes the treewidth of the resulting computation graph, as explained in [GASG18]. In general, the time to compute the optimal ordering is exponential in the number of arguments, so it is common to use a greedy approximation. However, if we expect to repeat the same calculation many times, using tensors of the same shape but potentially different content, we can compute the optimal ordering once and reuse it multiple times. \n7.3 Matrix inversion \nIn this section, we discuss how to invert different kinds of matrices. \n7.3.1 The inverse of a square matrix \nThe inverse of a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ is denoted $mathbf { A } ^ { - 1 }$ , and is the unique matrix such that \nNote that ${ { bf A } ^ { - 1 } }$ exists if and only if $operatorname* { d e t } ( mathbf { A } ) neq 0$ . If $operatorname* { d e t } ( mathbf { A } ) = 0$ , it is called a singular matrix. The following are properties of the inverse; all assume that $mathbf { A } , mathbf { B } in mathbb { R } ^ { n times n }$ are non-singular: \nFor the case of a $2 times 2$ matrix, the expression for $mathbf { A } ^ { - 1 }$ is simple enough to give explicitly. We have \nFor a block diagonal matrix, the inverse is obtained by simply inverting each block separately, e.g., \n2. These optimizations are implemented in the opt-einsum library [GASG18]. Its core functionality is included in NumPy and JAX einsum functions, provided you set optimize $ c =$ True parameter. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.3.2 Schur complements * \nIn this section, we review some useful results concerning block structured matrices. \nTheorem 7.3.1 (Inverse of a partitioned matrix). Consider a general partitioned matrix \nwhere we assume $mathbf { E }$ and $mathbf { H }$ are invertible. We have \nwhere \nWe say that $mathbf { M } / mathbf { H }$ is the Schur complement of M wrt $mathbf { H }$ , and M/E is the Schur complement of M wrt $mathbf { E }$ . \nEquation (7.109) and Equation (7.110) are called the partitioned inverse formulae. \nProof. If we could block diagonalize $mathbf { M }$ , it would be easier to invert. To zero out the top right block of M we can pre-multiply as follows \nSimilarly, to zero out the bottom left we can post-multiply as follows \nPutting it all together we get \nTaking the inverse of both sides yields \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix inversion",
        "subsubsection": "The inverse of a square matrix"
    },
    {
        "content": "7.3.2 Schur complements * \nIn this section, we review some useful results concerning block structured matrices. \nTheorem 7.3.1 (Inverse of a partitioned matrix). Consider a general partitioned matrix \nwhere we assume $mathbf { E }$ and $mathbf { H }$ are invertible. We have \nwhere \nWe say that $mathbf { M } / mathbf { H }$ is the Schur complement of M wrt $mathbf { H }$ , and M/E is the Schur complement of M wrt $mathbf { E }$ . \nEquation (7.109) and Equation (7.110) are called the partitioned inverse formulae. \nProof. If we could block diagonalize $mathbf { M }$ , it would be easier to invert. To zero out the top right block of M we can pre-multiply as follows \nSimilarly, to zero out the bottom left we can post-multiply as follows \nPutting it all together we get \nTaking the inverse of both sides yields \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nSubstituting in the definitions we get \nAlternatively, we could have decomposed the matrix $mathbf { M }$ in terms of $mathbf { E }$ and $mathbf { M } / mathbf { E } = ( mathbf { H } - mathbf { G } mathbf { E } ^ { - 1 } mathbf { F } )$ , yielding \n7.3.3 The matrix inversion lemma * \nEquating the top left block of the first matrix in Equation (7.119) with the top left block of the matrix in Equation (7.121) \nThis is known as the matrix inversion lemma or the Sherman-Morrison-Woodbury formula. A typical application in machine learning is the following. Let $mathbf { X }$ be an $N times D$ data matrix, and $pmb { Sigma }$ be $N times N$ diagonal matrix. Then we have (using the substitutions $mathbf { E } = Sigma$ , $mathbf { F } = mathbf { G } ^ { sf I } = mathbf { X }$ , and $mathbf { H } ^ { - 1 } = - mathbf { I }$ ) the following result: \nThe LHS takes $O ( N ^ { 3 } )$ time to compute, the RHS takes time $O ( D ^ { 3 } )$ to compute. \nAnother application concerns computing a rank one update of an inverse matrix. Let $mathbf { E } = mathbf { A }$ , ${ bf F } = { pmb u }$ , $mathbf { G } = v ^ { mathsf { I } }$ , and $H = - 1$ . Then we have \nThis is known as the Sherman-Morrison formula. \n7.3.4 Matrix determinant lemma * \nWe now use the above results to derive an efficient way to compute the determinant of a blockstructured matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix inversion",
        "subsubsection": "Schur complements *"
    },
    {
        "content": "Substituting in the definitions we get \nAlternatively, we could have decomposed the matrix $mathbf { M }$ in terms of $mathbf { E }$ and $mathbf { M } / mathbf { E } = ( mathbf { H } - mathbf { G } mathbf { E } ^ { - 1 } mathbf { F } )$ , yielding \n7.3.3 The matrix inversion lemma * \nEquating the top left block of the first matrix in Equation (7.119) with the top left block of the matrix in Equation (7.121) \nThis is known as the matrix inversion lemma or the Sherman-Morrison-Woodbury formula. A typical application in machine learning is the following. Let $mathbf { X }$ be an $N times D$ data matrix, and $pmb { Sigma }$ be $N times N$ diagonal matrix. Then we have (using the substitutions $mathbf { E } = Sigma$ , $mathbf { F } = mathbf { G } ^ { sf I } = mathbf { X }$ , and $mathbf { H } ^ { - 1 } = - mathbf { I }$ ) the following result: \nThe LHS takes $O ( N ^ { 3 } )$ time to compute, the RHS takes time $O ( D ^ { 3 } )$ to compute. \nAnother application concerns computing a rank one update of an inverse matrix. Let $mathbf { E } = mathbf { A }$ , ${ bf F } = { pmb u }$ , $mathbf { G } = v ^ { mathsf { I } }$ , and $H = - 1$ . Then we have \nThis is known as the Sherman-Morrison formula. \n7.3.4 Matrix determinant lemma * \nWe now use the above results to derive an efficient way to compute the determinant of a blockstructured matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix inversion",
        "subsubsection": "The matrix inversion lemma *"
    },
    {
        "content": "Substituting in the definitions we get \nAlternatively, we could have decomposed the matrix $mathbf { M }$ in terms of $mathbf { E }$ and $mathbf { M } / mathbf { E } = ( mathbf { H } - mathbf { G } mathbf { E } ^ { - 1 } mathbf { F } )$ , yielding \n7.3.3 The matrix inversion lemma * \nEquating the top left block of the first matrix in Equation (7.119) with the top left block of the matrix in Equation (7.121) \nThis is known as the matrix inversion lemma or the Sherman-Morrison-Woodbury formula. A typical application in machine learning is the following. Let $mathbf { X }$ be an $N times D$ data matrix, and $pmb { Sigma }$ be $N times N$ diagonal matrix. Then we have (using the substitutions $mathbf { E } = Sigma$ , $mathbf { F } = mathbf { G } ^ { sf I } = mathbf { X }$ , and $mathbf { H } ^ { - 1 } = - mathbf { I }$ ) the following result: \nThe LHS takes $O ( N ^ { 3 } )$ time to compute, the RHS takes time $O ( D ^ { 3 } )$ to compute. \nAnother application concerns computing a rank one update of an inverse matrix. Let $mathbf { E } = mathbf { A }$ , ${ bf F } = { pmb u }$ , $mathbf { G } = v ^ { mathsf { I } }$ , and $H = - 1$ . Then we have \nThis is known as the Sherman-Morrison formula. \n7.3.4 Matrix determinant lemma * \nWe now use the above results to derive an efficient way to compute the determinant of a blockstructured matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFrom Equation (7.115), we have \nSo we can see that $mathbf { M } / mathbf { H }$ acts somewhat like a division operator (hence the notation). Furthermore, we have \nHence (setting $mathbf { E } = mathbf { A }$ , $mathbf { F } = - pmb { u }$ , $mathbf { G } = v ^ { top }$ , $mathbf { H } = 1$ ) we have \nThis is known as the matrix determinant lemma. \n7.3.5 Application: deriving the conditionals of an MVN * \nConsider a joint Gaussian of the form $p ( { pmb x } _ { 1 } , { pmb x } _ { 2 } ) = mathcal { N } ( { pmb x } | { pmb mu } , { pmb Sigma } )$ , where \nIn Section 3.2.3, we claimed that \nIn this section, we derive this result using Schur complenents. \nLet us factor the joint $p ( pmb { x } _ { 1 } , pmb { x } _ { 2 } )$ as $p ( pmb { x } _ { 2 } ) p ( pmb { x } _ { 1 } | pmb { x } _ { 2 } )$ as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix inversion",
        "subsubsection": "Matrix determinant lemma *"
    },
    {
        "content": "From Equation (7.115), we have \nSo we can see that $mathbf { M } / mathbf { H }$ acts somewhat like a division operator (hence the notation). Furthermore, we have \nHence (setting $mathbf { E } = mathbf { A }$ , $mathbf { F } = - pmb { u }$ , $mathbf { G } = v ^ { top }$ , $mathbf { H } = 1$ ) we have \nThis is known as the matrix determinant lemma. \n7.3.5 Application: deriving the conditionals of an MVN * \nConsider a joint Gaussian of the form $p ( { pmb x } _ { 1 } , { pmb x } _ { 2 } ) = mathcal { N } ( { pmb x } | { pmb mu } , { pmb Sigma } )$ , where \nIn Section 3.2.3, we claimed that \nIn this section, we derive this result using Schur complenents. \nLet us factor the joint $p ( pmb { x } _ { 1 } , pmb { x } _ { 2 } )$ as $p ( pmb { x } _ { 2 } ) p ( pmb { x } _ { 1 } | pmb { x } _ { 2 } )$ as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nUsing Equation (7.118) the above exponent becomes \nThis is of the form \nHence we have successfully factorized the joint as \nwhere the parameters of the conditional distribution can be read off from the above equations using \nWe can also use the fact that $| mathbf { M } | = | mathbf { M } / mathbf { H } | | mathbf { H } |$ to check the normalization constants are correct: \nwhere $d _ { 1 } = dim ( { pmb x } _ { 1 } )$ and $d _ { 2 } = dim ( x _ { 2 } )$ . \n7.4 Eigenvalue decomposition (EVD) \nIn this section, we review some standard material on the eigenvalue decomposition or EVD of square (real-valued) matrices. \n7.4.1 Basics \nGiven a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ , we say that $lambda in mathbb R$ is an eigenvalue of $mathbf { A }$ and $pmb { u } in mathbb { R } ^ { n }$ is the corresponding eigenvector if \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix inversion",
        "subsubsection": "Application: deriving the conditionals of an MVN *"
    },
    {
        "content": "Using Equation (7.118) the above exponent becomes \nThis is of the form \nHence we have successfully factorized the joint as \nwhere the parameters of the conditional distribution can be read off from the above equations using \nWe can also use the fact that $| mathbf { M } | = | mathbf { M } / mathbf { H } | | mathbf { H } |$ to check the normalization constants are correct: \nwhere $d _ { 1 } = dim ( { pmb x } _ { 1 } )$ and $d _ { 2 } = dim ( x _ { 2 } )$ . \n7.4 Eigenvalue decomposition (EVD) \nIn this section, we review some standard material on the eigenvalue decomposition or EVD of square (real-valued) matrices. \n7.4.1 Basics \nGiven a square matrix $mathbf { A } in mathbb { R } ^ { n times n }$ , we say that $lambda in mathbb R$ is an eigenvalue of $mathbf { A }$ and $pmb { u } in mathbb { R } ^ { n }$ is the corresponding eigenvector if \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIntuitively, this definition means that multiplying $mathbf { A }$ by the vector $mathbf { Delta } _ { mathbf { u } }$ results in a new vector that points in the same direction as $mathbf { Delta } _ { mathbf { u } }$ , but is scaled by a factor $lambda$ . For example, if $mathbf { A }$ is a rotation matrix, then $mathbf { Delta } _ { mathbf { u } }$ is the axis of rotation and $lambda = 1$ . \nNote that for any eigenvector $pmb { u } in mathbb { R } ^ { n }$ , and scalar $c in mathbb { R }$ , \nHence $c u$ is also an eigenvector. For this reason when we talk about “the” eigenvector associated with $lambda$ , we usually assume that the eigenvector is normalized to have length 1 (this still creates some ambiguity, since $mathbf { Delta } _ { mathbf { u } }$ and $^ { - }$ will both be eigenvectors, but we will have to live with this). \nWe can rewrite the equation above to state that $( lambda , pmb { x } )$ is an eigenvalue-eigenvector pair of $mathbf { A }$ if \nNow $( lambda mathbf { I } - mathbf { A } ) pmb { u } = mathbf { 0 }$ has a non-zero solution to $mathbf { Delta } _ { mathbf { u } }$ if and only if $( lambda mathbf { I } - mathbf { A } )$ has a non-empty nullspace, which is only the case if $( lambda mathbf { I } - mathbf { A } )$ is singular, i.e., \nThis is called the characteristic equation of A. (See Exercise 7.2.) The $n$ solutions of this equation are the $n$ (possibly complex-valued) eigenvalues $lambda _ { i }$ , and ${ pmb u } _ { i }$ are the corresponding eigenvectors. It is standard to sort the eigenvectors in order of their eigenvalues, with the largest magnitude ones first. The following are properties of eigenvalues and eigenvectors. \nThe trace of a matrix is equal to the sum of its eigenvalues, \n• The determinant of $mathbf { A }$ is equal to the product of its eigenvalues, \n• The rank of $mathbf { A }$ is equal to the number of non-zero eigenvalues of A. \n• If A is non-singular then $1 / lambda _ { i }$ is an eigenvalue of $mathbf { A } ^ { - 1 }$ with associated eigenvector $mathbf { delta } mathbf { u } _ { i }$ , i.e., $mathbf { A } ^ { - 1 } pmb { u } _ { i } = ( 1 / lambda _ { i } ) pmb { u } _ { i }$ . \n• The eigenvalues of a diagonal or triangular matrix are just the diagonal entries. \n7.4.2 Diagonalization \nWe can write all the eigenvector equations simultaneously as \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Basics"
    },
    {
        "content": "Intuitively, this definition means that multiplying $mathbf { A }$ by the vector $mathbf { Delta } _ { mathbf { u } }$ results in a new vector that points in the same direction as $mathbf { Delta } _ { mathbf { u } }$ , but is scaled by a factor $lambda$ . For example, if $mathbf { A }$ is a rotation matrix, then $mathbf { Delta } _ { mathbf { u } }$ is the axis of rotation and $lambda = 1$ . \nNote that for any eigenvector $pmb { u } in mathbb { R } ^ { n }$ , and scalar $c in mathbb { R }$ , \nHence $c u$ is also an eigenvector. For this reason when we talk about “the” eigenvector associated with $lambda$ , we usually assume that the eigenvector is normalized to have length 1 (this still creates some ambiguity, since $mathbf { Delta } _ { mathbf { u } }$ and $^ { - }$ will both be eigenvectors, but we will have to live with this). \nWe can rewrite the equation above to state that $( lambda , pmb { x } )$ is an eigenvalue-eigenvector pair of $mathbf { A }$ if \nNow $( lambda mathbf { I } - mathbf { A } ) pmb { u } = mathbf { 0 }$ has a non-zero solution to $mathbf { Delta } _ { mathbf { u } }$ if and only if $( lambda mathbf { I } - mathbf { A } )$ has a non-empty nullspace, which is only the case if $( lambda mathbf { I } - mathbf { A } )$ is singular, i.e., \nThis is called the characteristic equation of A. (See Exercise 7.2.) The $n$ solutions of this equation are the $n$ (possibly complex-valued) eigenvalues $lambda _ { i }$ , and ${ pmb u } _ { i }$ are the corresponding eigenvectors. It is standard to sort the eigenvectors in order of their eigenvalues, with the largest magnitude ones first. The following are properties of eigenvalues and eigenvectors. \nThe trace of a matrix is equal to the sum of its eigenvalues, \n• The determinant of $mathbf { A }$ is equal to the product of its eigenvalues, \n• The rank of $mathbf { A }$ is equal to the number of non-zero eigenvalues of A. \n• If A is non-singular then $1 / lambda _ { i }$ is an eigenvalue of $mathbf { A } ^ { - 1 }$ with associated eigenvector $mathbf { delta } mathbf { u } _ { i }$ , i.e., $mathbf { A } ^ { - 1 } pmb { u } _ { i } = ( 1 / lambda _ { i } ) pmb { u } _ { i }$ . \n• The eigenvalues of a diagonal or triangular matrix are just the diagonal entries. \n7.4.2 Diagonalization \nWe can write all the eigenvector equations simultaneously as \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere the columns of $mathbf { U } in mathbb { R } ^ { n times n }$ are the eigenvectors of $mathbf { A }$ and $pmb { Lambda }$ is a diagonal matrix whose entries are the eigenvalues of $mathbf { A }$ , i.e., \nIf the eigenvectors of $mathbf { A }$ are linearly independent, then the matrix $mathbf { U }$ will be invertible, so \nA matrix that can be written in this form is called diagonalizable. \n7.4.3 Eigenvalues and eigenvectors of symmetric matrices \nWhen A is real and symmetric, it can be shown that all the eigenvalues are real, and the eigenvectors are orthonormal, i.e., $pmb { u } _ { i } ^ { 1 } pmb { u } _ { j } = 0$ if $i neq j$ , and $pmb { u } _ { i } ^ { mathsf { I } } pmb { u } _ { i } = 1$ , where $mathbf { Delta } mathbf { u } _ { i }$ are the eigenvectors. In matrix form, this becomes $mathbf { U } ^ { mathsf { T } } mathbf { U } = mathbf { U } mathbf { U } ^ { mathsf { T } } = mathbf { I }$ ; hence we see that $mathbf { U }$ is an orthogonal matrix. \nWe can therefore represent $mathbf { A }$ as \nThus multiplying by any symmetric matrix A can be interpreted as multiplying by a rotation matrix $mathbf { U } ^ { mathsf { I } }$ , a scaling matrix $pmb { Lambda }$ , followed by an inverse rotation $mathbf { U }$ . \nOnce we have diagonalized a matrix, it is easy to invert. Since $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { I } }$ , where ${ bf U } ^ { parallel } = { bf U } ^ { - 1 }$ , we have \nThis corresponds to rotating, unscaling, and then rotating back. \n7.4.3.1 Checking for positive definiteness \nWe can also use the diagonalization property to show that a symmetric matrix is positive definite iff all its eigenvalues are positive. To see this, note that \nwhere $pmb { y } = mathbf { U } ^ { top } pmb { x }$ . Because $y _ { i } ^ { 2 }$ is always nonnegative, the sign of this expression depends entirely on the $lambda _ { i }$ ’s. If all $lambda _ { i } > 0$ , then the matrix is positive definite; if all $lambda _ { i } geq 0$ , it is positive semidefinite. Likewise, if all $lambda _ { i } < 0$ or $lambda _ { i } leq 0$ , then A is negative definite or negative semidefinite respectively. Finally, if $mathbf { A }$ has both positive and negative eigenvalues, it is indefinite. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Diagonalization"
    },
    {
        "content": "where the columns of $mathbf { U } in mathbb { R } ^ { n times n }$ are the eigenvectors of $mathbf { A }$ and $pmb { Lambda }$ is a diagonal matrix whose entries are the eigenvalues of $mathbf { A }$ , i.e., \nIf the eigenvectors of $mathbf { A }$ are linearly independent, then the matrix $mathbf { U }$ will be invertible, so \nA matrix that can be written in this form is called diagonalizable. \n7.4.3 Eigenvalues and eigenvectors of symmetric matrices \nWhen A is real and symmetric, it can be shown that all the eigenvalues are real, and the eigenvectors are orthonormal, i.e., $pmb { u } _ { i } ^ { 1 } pmb { u } _ { j } = 0$ if $i neq j$ , and $pmb { u } _ { i } ^ { mathsf { I } } pmb { u } _ { i } = 1$ , where $mathbf { Delta } mathbf { u } _ { i }$ are the eigenvectors. In matrix form, this becomes $mathbf { U } ^ { mathsf { T } } mathbf { U } = mathbf { U } mathbf { U } ^ { mathsf { T } } = mathbf { I }$ ; hence we see that $mathbf { U }$ is an orthogonal matrix. \nWe can therefore represent $mathbf { A }$ as \nThus multiplying by any symmetric matrix A can be interpreted as multiplying by a rotation matrix $mathbf { U } ^ { mathsf { I } }$ , a scaling matrix $pmb { Lambda }$ , followed by an inverse rotation $mathbf { U }$ . \nOnce we have diagonalized a matrix, it is easy to invert. Since $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { I } }$ , where ${ bf U } ^ { parallel } = { bf U } ^ { - 1 }$ , we have \nThis corresponds to rotating, unscaling, and then rotating back. \n7.4.3.1 Checking for positive definiteness \nWe can also use the diagonalization property to show that a symmetric matrix is positive definite iff all its eigenvalues are positive. To see this, note that \nwhere $pmb { y } = mathbf { U } ^ { top } pmb { x }$ . Because $y _ { i } ^ { 2 }$ is always nonnegative, the sign of this expression depends entirely on the $lambda _ { i }$ ’s. If all $lambda _ { i } > 0$ , then the matrix is positive definite; if all $lambda _ { i } geq 0$ , it is positive semidefinite. Likewise, if all $lambda _ { i } < 0$ or $lambda _ { i } leq 0$ , then A is negative definite or negative semidefinite respectively. Finally, if $mathbf { A }$ has both positive and negative eigenvalues, it is indefinite. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.4.4 Geometry of quadratic forms \nA quadratic form is a function that can be written as \nwhere $boldsymbol { x } in mathbb { R } ^ { n }$ and $mathbf { A }$ is a positive definite, symmetric $n$ -by- $n$ matrix. Let $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { T } }$ be a diagonalization of $mathbf { A }$ (see Section 7.4.3). Hence we can write \nwhere $y _ { i } = pmb { x } ^ { top } pmb { u } _ { i }$ and ${ { lambda } _ { i } } > 0$ (since $mathbf { A }$ is positive definite). The level sets of $f ( { pmb x } )$ define hyper-ellipsoids. For example, in 2d, we have \nwhich is the equation of a 2d ellipse. This is illustrated in Figure 7.6. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elongated it is. \n7.4.5 Standardizing and whitening data \nSuppose we have a dataset $mathbf { X } in mathbb { R } ^ { N times D }$ . It is common to preprocess the data so that each column has zero mean and unit variance. This is called standardizing the data, as we discuss in Section 10.2.8. Although standardizing forces the variance to be 1, it does not remove correlation between the columns. To do that, we must whiten the data. To define this, let the empirical covariance matrix be $begin{array} { r } { pmb { Sigma } = frac { 1 } { N } pmb { mathrm { X } } ^ { top } pmb { mathrm { X } } } end{array}$ , and let $begin{array} { r } { pmb { Sigma } = mathbf { E } mathbf { D } mathbf { E } ^ { top } } end{array}$ be its diagonalization. Equivalently, let [U, S, V] be the SVD of $mathbf { X }$ (so $mathbf { E } = mathbf { V }$ and $mathbf { D } = mathbf { S } ^ { 2 }$ , as we discuss in Section 20.1.3.3.) Now define \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Eigenvalues and eigenvectors of symmetric matrices"
    },
    {
        "content": "7.4.4 Geometry of quadratic forms \nA quadratic form is a function that can be written as \nwhere $boldsymbol { x } in mathbb { R } ^ { n }$ and $mathbf { A }$ is a positive definite, symmetric $n$ -by- $n$ matrix. Let $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { T } }$ be a diagonalization of $mathbf { A }$ (see Section 7.4.3). Hence we can write \nwhere $y _ { i } = pmb { x } ^ { top } pmb { u } _ { i }$ and ${ { lambda } _ { i } } > 0$ (since $mathbf { A }$ is positive definite). The level sets of $f ( { pmb x } )$ define hyper-ellipsoids. For example, in 2d, we have \nwhich is the equation of a 2d ellipse. This is illustrated in Figure 7.6. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elongated it is. \n7.4.5 Standardizing and whitening data \nSuppose we have a dataset $mathbf { X } in mathbb { R } ^ { N times D }$ . It is common to preprocess the data so that each column has zero mean and unit variance. This is called standardizing the data, as we discuss in Section 10.2.8. Although standardizing forces the variance to be 1, it does not remove correlation between the columns. To do that, we must whiten the data. To define this, let the empirical covariance matrix be $begin{array} { r } { pmb { Sigma } = frac { 1 } { N } pmb { mathrm { X } } ^ { top } pmb { mathrm { X } } } end{array}$ , and let $begin{array} { r } { pmb { Sigma } = mathbf { E } mathbf { D } mathbf { E } ^ { top } } end{array}$ be its diagonalization. Equivalently, let [U, S, V] be the SVD of $mathbf { X }$ (so $mathbf { E } = mathbf { V }$ and $mathbf { D } = mathbf { S } ^ { 2 }$ , as we discuss in Section 20.1.3.3.) Now define \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Geometry of quadratic forms"
    },
    {
        "content": "7.4.4 Geometry of quadratic forms \nA quadratic form is a function that can be written as \nwhere $boldsymbol { x } in mathbb { R } ^ { n }$ and $mathbf { A }$ is a positive definite, symmetric $n$ -by- $n$ matrix. Let $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { T } }$ be a diagonalization of $mathbf { A }$ (see Section 7.4.3). Hence we can write \nwhere $y _ { i } = pmb { x } ^ { top } pmb { u } _ { i }$ and ${ { lambda } _ { i } } > 0$ (since $mathbf { A }$ is positive definite). The level sets of $f ( { pmb x } )$ define hyper-ellipsoids. For example, in 2d, we have \nwhich is the equation of a 2d ellipse. This is illustrated in Figure 7.6. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elongated it is. \n7.4.5 Standardizing and whitening data \nSuppose we have a dataset $mathbf { X } in mathbb { R } ^ { N times D }$ . It is common to preprocess the data so that each column has zero mean and unit variance. This is called standardizing the data, as we discuss in Section 10.2.8. Although standardizing forces the variance to be 1, it does not remove correlation between the columns. To do that, we must whiten the data. To define this, let the empirical covariance matrix be $begin{array} { r } { pmb { Sigma } = frac { 1 } { N } pmb { mathrm { X } } ^ { top } pmb { mathrm { X } } } end{array}$ , and let $begin{array} { r } { pmb { Sigma } = mathbf { E } mathbf { D } mathbf { E } ^ { top } } end{array}$ be its diagonalization. Equivalently, let [U, S, V] be the SVD of $mathbf { X }$ (so $mathbf { E } = mathbf { V }$ and $mathbf { D } = mathbf { S } ^ { 2 }$ , as we discuss in Section 20.1.3.3.) Now define \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis is called the PCA whitening matrix. (We discuss PCA in Section 20.1.) Let $pmb { y } = mathbf { W } _ { p c a } pmb { x }$ be a transformed vector. We can check that its covariance is white as follows: \nThe whitening matrix is not unique, since any rotation of it, $mathbf { W } = mathbf { R } mathbf { W } _ { p c a }$ , will still maintain the whitening property, i.e., $mathbf { W } ^ { mathsf { T } } mathbf { W } = pmb { Sigma } ^ { - 1 }$ . For example, if we take $mathbf R = mathbf E$ , we get \nThis is called Mahalanobis whitening or ZCA. (ZCA stands for “zero-phase component analysis”, and was introduced in [BS97].) The advantage of ZCA whitening over PCA whitening is that the resulting transformed data is as close as possible to the original data (in the least squares sense) [Amo17]. This is illustrated in Figure 7.7. When applied to images, the ZCA transformed data \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license vectors still look like images. This is useful when the method is used inside a deep learning system [KH09]. \n\n7.4.6 Power method \nWe now describe a simple iterative method for computing the eigenvector corresponding to the largest eigenvalue of a real, symmetric matrix; this is called the power method. This can be useful when the matrix is very large but sparse. For example, it is used by Google’s PageRank to compute the stationary distribution of the transition matrix of the world wide web (a matrix of size about 3 billion by 3 billion!). In Section 7.4.7, we will see how to use this method to compute subsequent eigenvectors and values. \nLet $mathbf { A }$ be a matrix with orthonormal eigenvectors $mathbf { Delta } mathbf { u } _ { i }$ and eigenvalues $| lambda _ { 1 } | > | lambda _ { 2 } | geq cdots geq | lambda _ { m } | geq 0$ , so $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { I } }$ . Let be an arbitrary vector in the range of $mathbf { A }$ , so $mathbf { A } pmb { x } = pmb { v } _ { ( 0 ) }$ for some . Hence $mathbf { boldsymbol { v } } _ { ( 0 ) }$ $_ { ast }$ we can write ${ pmb v } _ { ( 0 ) }$ as \nfor some constants $a _ { i }$ . We can now repeatedly multiply $_ { v }$ by $mathbf { A }$ and renormalize: \n(We normalize at each iteration for numerical stability.) Since ${ mathbf { } } { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } { mathbf } ^ { mathbf { } } { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf } { mathbf { } } ^ { mathbf { } } { mathbf } { mathbf { } } ^ { mathbf { } } { mathbf } { mathbf { } } ^ { mathbf { } mathbf { } } { mathbf } { mathbf } { } ^ { mathbf { } mathbf { } } { mathbf } { mathbf } { mathbf } { mathbf } ^ { } { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } { } ^ { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf mathbf { } mathbf mathbf { } mathbf mathbf mathbf { } mathbf mathbf { mathbf } mathbf mathbf { mathbf } mathbf mathbf { } mathbf mathbf mathbf { mathbf } mathbf mathbf { mathbf } mathbf mathbf mathbf { mathbf } mathbf mathbf mathbf { mathbf mathbf } mathbf mathbf { mathbf mathbf mathbf } mathbf mathbf { mathbf mathbf mathbf mathbf } mathbf mathbf  mathbf mathbf mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf mathbf mathbf mathbf { mathbf } mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf  mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf  mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf $ is a multiple of $mathbf { A } ^ { t } pmb { v } _ { 0 }$ , we have \nsince $frac { | lambda _ { k } | } { | lambda _ { 1 } | } < 1$ for $k > 1$ (assuming the eigenvalues are sorted in descending order). So we see that this converges to $mathbf { Delta } mathbf { u } _ { 1 }$ , although not very quickly (the error is reduced by approximately $| lambda _ { 2 } / lambda _ { 1 } |$ at each iteration). The only requirement is that the initial guess satisfy $pmb { v } _ { 0 } ^ { 1 } pmb { u } _ { 1 } neq 0$ , which will be true for a random ${ pmb v } _ { 0 }$ with high probability. \nWe now discuss how to compute the corresponding eigenvalue, $lambda _ { 1 }$ . Define the Rayleigh quotient to be \nHence \nThus we can easily compute $lambda _ { 1 }$ from $mathbf { Delta } mathbf { u } _ { 1 }$ and $mathbf { A }$ . See power_method_demo.ipynb for a demo. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Standardizing and whitening data"
    },
    {
        "content": "7.4.6 Power method \nWe now describe a simple iterative method for computing the eigenvector corresponding to the largest eigenvalue of a real, symmetric matrix; this is called the power method. This can be useful when the matrix is very large but sparse. For example, it is used by Google’s PageRank to compute the stationary distribution of the transition matrix of the world wide web (a matrix of size about 3 billion by 3 billion!). In Section 7.4.7, we will see how to use this method to compute subsequent eigenvectors and values. \nLet $mathbf { A }$ be a matrix with orthonormal eigenvectors $mathbf { Delta } mathbf { u } _ { i }$ and eigenvalues $| lambda _ { 1 } | > | lambda _ { 2 } | geq cdots geq | lambda _ { m } | geq 0$ , so $mathbf { A } = mathbf { U } mathbf { A } mathbf { U } ^ { mathsf { I } }$ . Let be an arbitrary vector in the range of $mathbf { A }$ , so $mathbf { A } pmb { x } = pmb { v } _ { ( 0 ) }$ for some . Hence $mathbf { boldsymbol { v } } _ { ( 0 ) }$ $_ { ast }$ we can write ${ pmb v } _ { ( 0 ) }$ as \nfor some constants $a _ { i }$ . We can now repeatedly multiply $_ { v }$ by $mathbf { A }$ and renormalize: \n(We normalize at each iteration for numerical stability.) Since ${ mathbf { } } { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf { } } { mathbf } ^ { mathbf { } } { mathbf { } } { mathbf { } } ^ { mathbf { } } { mathbf } { mathbf { } } ^ { mathbf { } } { mathbf } { mathbf { } } ^ { mathbf { } } { mathbf } { mathbf { } } ^ { mathbf { } mathbf { } } { mathbf } { mathbf } { } ^ { mathbf { } mathbf { } } { mathbf } { mathbf } { mathbf } { mathbf } ^ { } { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } { } ^ { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } { mathbf } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf { } mathbf mathbf mathbf { } mathbf mathbf { } mathbf mathbf mathbf { } mathbf mathbf { mathbf } mathbf mathbf { mathbf } mathbf mathbf { } mathbf mathbf mathbf { mathbf } mathbf mathbf { mathbf } mathbf mathbf mathbf { mathbf } mathbf mathbf mathbf { mathbf mathbf } mathbf mathbf { mathbf mathbf mathbf } mathbf mathbf { mathbf mathbf mathbf mathbf } mathbf mathbf  mathbf mathbf mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf mathbf mathbf mathbf { mathbf } mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf  mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf  mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf $ is a multiple of $mathbf { A } ^ { t } pmb { v } _ { 0 }$ , we have \nsince $frac { | lambda _ { k } | } { | lambda _ { 1 } | } < 1$ for $k > 1$ (assuming the eigenvalues are sorted in descending order). So we see that this converges to $mathbf { Delta } mathbf { u } _ { 1 }$ , although not very quickly (the error is reduced by approximately $| lambda _ { 2 } / lambda _ { 1 } |$ at each iteration). The only requirement is that the initial guess satisfy $pmb { v } _ { 0 } ^ { 1 } pmb { u } _ { 1 } neq 0$ , which will be true for a random ${ pmb v } _ { 0 }$ with high probability. \nWe now discuss how to compute the corresponding eigenvalue, $lambda _ { 1 }$ . Define the Rayleigh quotient to be \nHence \nThus we can easily compute $lambda _ { 1 }$ from $mathbf { Delta } mathbf { u } _ { 1 }$ and $mathbf { A }$ . See power_method_demo.ipynb for a demo. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n7.4.7 Deflation \nSuppose we have computed the first eigenvector and value $mathbf { delta } u _ { 1 } , lambda _ { 1 }$ by the power method. We now describe how to compute subsequent eigenvectors and values. Since the eigenvectors are orthonormal, and the eigenvalues are real, we can project out the $mathbf { delta u } _ { 1 }$ component from the matrix as follows: \nThis is called matrix deflation. We can then apply the power method to $mathbf { A } ^ { ( 2 ) }$ , which will find the largest eigenvector/value in the subspace orthogonal to $mathbf { Delta } mathbf { u } _ { 1 }$ . \nIn Section 20.1.2, we show that the optimal estimate $hat { bf W }$ for the PCA model (described in Section 20.1) is given by the first $K$ eigenvectors of the empirical covariance matrix. Hence deflation can be used to implement PCA. It can also be modified to implement sparse PCA [Mac09]. \n7.4.8 Eigenvectors optimize quadratic forms \nWe can use matrix calculus to solve an optimization problem in a way that leads directly to eigenvalue/eigenvector analysis. Consider the following, equality constrained optimization problem: \nfor a symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ . A standard way of solving optimization problems with equality constraints is by forming the Lagrangian, an objective function that includes the equality constraints (see Section 8.5.1). The Lagrangian in this case can be given by \nwhere $lambda$ is called the Lagrange multiplier associated with the equality constraint. It can be established that for $x ^ { * }$ to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at $x ^ { * }$ (this is not the only condition, but it is required). That is, \nNotice that this is just the linear equation $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = lambda { boldsymbol { mathbf { mathit { x } } } }$ . This shows that the only points which can possibly maximize (or minimize) ${ pmb x } ^ { 1 } { bf A } { pmb x }$ assuming $pmb { x } ^ { 1 } pmb { x } = 1$ are the eigenvectors of $mathbf { A }$ . \n7.5 Singular value decomposition (SVD) \nWe now discuss the SVD, which generalizes EVD to rectangular matrices. \n7.5.1 Basics \nAny (real) $m times n$ matrix A can be decomposed as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathbf { U }$ is an $m times m$ whose columns are orthornormal (so $mathbf { U } ^ { mathsf { I } } mathbf { U } = mathbf { I } _ { m }$ ), $mathbf { V }$ is $n times n$ matrix whose rows and columns are orthonormal (so $mathbf { V } ^ { mathsf { T } } mathbf { V } = mathbf { V } mathbf { V } ^ { mathsf { T } } = mathbf { I } _ { n }$ ), and $mathbf { s }$ is a $m times n$ matrix containing the $r = operatorname* { m i n } ( m , n )$ singular values $sigma _ { i } geq 0$ on the main diagonal, with 0s filling the rest of the matrix. The columns of U are the left singular vectors, and the columns of $mathbf { V }$ are the right singular vectors. This is called the singular value decomposition or SVD of the matrix. See Figure 7.8 for an example.",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Power method"
    },
    {
        "content": "7.4.7 Deflation \nSuppose we have computed the first eigenvector and value $mathbf { delta } u _ { 1 } , lambda _ { 1 }$ by the power method. We now describe how to compute subsequent eigenvectors and values. Since the eigenvectors are orthonormal, and the eigenvalues are real, we can project out the $mathbf { delta u } _ { 1 }$ component from the matrix as follows: \nThis is called matrix deflation. We can then apply the power method to $mathbf { A } ^ { ( 2 ) }$ , which will find the largest eigenvector/value in the subspace orthogonal to $mathbf { Delta } mathbf { u } _ { 1 }$ . \nIn Section 20.1.2, we show that the optimal estimate $hat { bf W }$ for the PCA model (described in Section 20.1) is given by the first $K$ eigenvectors of the empirical covariance matrix. Hence deflation can be used to implement PCA. It can also be modified to implement sparse PCA [Mac09]. \n7.4.8 Eigenvectors optimize quadratic forms \nWe can use matrix calculus to solve an optimization problem in a way that leads directly to eigenvalue/eigenvector analysis. Consider the following, equality constrained optimization problem: \nfor a symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ . A standard way of solving optimization problems with equality constraints is by forming the Lagrangian, an objective function that includes the equality constraints (see Section 8.5.1). The Lagrangian in this case can be given by \nwhere $lambda$ is called the Lagrange multiplier associated with the equality constraint. It can be established that for $x ^ { * }$ to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at $x ^ { * }$ (this is not the only condition, but it is required). That is, \nNotice that this is just the linear equation $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = lambda { boldsymbol { mathbf { mathit { x } } } }$ . This shows that the only points which can possibly maximize (or minimize) ${ pmb x } ^ { 1 } { bf A } { pmb x }$ assuming $pmb { x } ^ { 1 } pmb { x } = 1$ are the eigenvectors of $mathbf { A }$ . \n7.5 Singular value decomposition (SVD) \nWe now discuss the SVD, which generalizes EVD to rectangular matrices. \n7.5.1 Basics \nAny (real) $m times n$ matrix A can be decomposed as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathbf { U }$ is an $m times m$ whose columns are orthornormal (so $mathbf { U } ^ { mathsf { I } } mathbf { U } = mathbf { I } _ { m }$ ), $mathbf { V }$ is $n times n$ matrix whose rows and columns are orthonormal (so $mathbf { V } ^ { mathsf { T } } mathbf { V } = mathbf { V } mathbf { V } ^ { mathsf { T } } = mathbf { I } _ { n }$ ), and $mathbf { s }$ is a $m times n$ matrix containing the $r = operatorname* { m i n } ( m , n )$ singular values $sigma _ { i } geq 0$ on the main diagonal, with 0s filling the rest of the matrix. The columns of U are the left singular vectors, and the columns of $mathbf { V }$ are the right singular vectors. This is called the singular value decomposition or SVD of the matrix. See Figure 7.8 for an example.",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Deflation"
    },
    {
        "content": "7.4.7 Deflation \nSuppose we have computed the first eigenvector and value $mathbf { delta } u _ { 1 } , lambda _ { 1 }$ by the power method. We now describe how to compute subsequent eigenvectors and values. Since the eigenvectors are orthonormal, and the eigenvalues are real, we can project out the $mathbf { delta u } _ { 1 }$ component from the matrix as follows: \nThis is called matrix deflation. We can then apply the power method to $mathbf { A } ^ { ( 2 ) }$ , which will find the largest eigenvector/value in the subspace orthogonal to $mathbf { Delta } mathbf { u } _ { 1 }$ . \nIn Section 20.1.2, we show that the optimal estimate $hat { bf W }$ for the PCA model (described in Section 20.1) is given by the first $K$ eigenvectors of the empirical covariance matrix. Hence deflation can be used to implement PCA. It can also be modified to implement sparse PCA [Mac09]. \n7.4.8 Eigenvectors optimize quadratic forms \nWe can use matrix calculus to solve an optimization problem in a way that leads directly to eigenvalue/eigenvector analysis. Consider the following, equality constrained optimization problem: \nfor a symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ . A standard way of solving optimization problems with equality constraints is by forming the Lagrangian, an objective function that includes the equality constraints (see Section 8.5.1). The Lagrangian in this case can be given by \nwhere $lambda$ is called the Lagrange multiplier associated with the equality constraint. It can be established that for $x ^ { * }$ to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at $x ^ { * }$ (this is not the only condition, but it is required). That is, \nNotice that this is just the linear equation $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = lambda { boldsymbol { mathbf { mathit { x } } } }$ . This shows that the only points which can possibly maximize (or minimize) ${ pmb x } ^ { 1 } { bf A } { pmb x }$ assuming $pmb { x } ^ { 1 } pmb { x } = 1$ are the eigenvectors of $mathbf { A }$ . \n7.5 Singular value decomposition (SVD) \nWe now discuss the SVD, which generalizes EVD to rectangular matrices. \n7.5.1 Basics \nAny (real) $m times n$ matrix A can be decomposed as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathbf { U }$ is an $m times m$ whose columns are orthornormal (so $mathbf { U } ^ { mathsf { I } } mathbf { U } = mathbf { I } _ { m }$ ), $mathbf { V }$ is $n times n$ matrix whose rows and columns are orthonormal (so $mathbf { V } ^ { mathsf { T } } mathbf { V } = mathbf { V } mathbf { V } ^ { mathsf { T } } = mathbf { I } _ { n }$ ), and $mathbf { s }$ is a $m times n$ matrix containing the $r = operatorname* { m i n } ( m , n )$ singular values $sigma _ { i } geq 0$ on the main diagonal, with 0s filling the rest of the matrix. The columns of U are the left singular vectors, and the columns of $mathbf { V }$ are the right singular vectors. This is called the singular value decomposition or SVD of the matrix. See Figure 7.8 for an example.",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Eigenvalue decomposition (EVD)",
        "subsubsection": "Eigenvectors optimize quadratic forms"
    },
    {
        "content": "7.4.7 Deflation \nSuppose we have computed the first eigenvector and value $mathbf { delta } u _ { 1 } , lambda _ { 1 }$ by the power method. We now describe how to compute subsequent eigenvectors and values. Since the eigenvectors are orthonormal, and the eigenvalues are real, we can project out the $mathbf { delta u } _ { 1 }$ component from the matrix as follows: \nThis is called matrix deflation. We can then apply the power method to $mathbf { A } ^ { ( 2 ) }$ , which will find the largest eigenvector/value in the subspace orthogonal to $mathbf { Delta } mathbf { u } _ { 1 }$ . \nIn Section 20.1.2, we show that the optimal estimate $hat { bf W }$ for the PCA model (described in Section 20.1) is given by the first $K$ eigenvectors of the empirical covariance matrix. Hence deflation can be used to implement PCA. It can also be modified to implement sparse PCA [Mac09]. \n7.4.8 Eigenvectors optimize quadratic forms \nWe can use matrix calculus to solve an optimization problem in a way that leads directly to eigenvalue/eigenvector analysis. Consider the following, equality constrained optimization problem: \nfor a symmetric matrix $mathbf { A } in mathbb { S } ^ { n }$ . A standard way of solving optimization problems with equality constraints is by forming the Lagrangian, an objective function that includes the equality constraints (see Section 8.5.1). The Lagrangian in this case can be given by \nwhere $lambda$ is called the Lagrange multiplier associated with the equality constraint. It can be established that for $x ^ { * }$ to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at $x ^ { * }$ (this is not the only condition, but it is required). That is, \nNotice that this is just the linear equation $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = lambda { boldsymbol { mathbf { mathit { x } } } }$ . This shows that the only points which can possibly maximize (or minimize) ${ pmb x } ^ { 1 } { bf A } { pmb x }$ assuming $pmb { x } ^ { 1 } pmb { x } = 1$ are the eigenvectors of $mathbf { A }$ . \n7.5 Singular value decomposition (SVD) \nWe now discuss the SVD, which generalizes EVD to rectangular matrices. \n7.5.1 Basics \nAny (real) $m times n$ matrix A can be decomposed as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathbf { U }$ is an $m times m$ whose columns are orthornormal (so $mathbf { U } ^ { mathsf { I } } mathbf { U } = mathbf { I } _ { m }$ ), $mathbf { V }$ is $n times n$ matrix whose rows and columns are orthonormal (so $mathbf { V } ^ { mathsf { T } } mathbf { V } = mathbf { V } mathbf { V } ^ { mathsf { T } } = mathbf { I } _ { n }$ ), and $mathbf { s }$ is a $m times n$ matrix containing the $r = operatorname* { m i n } ( m , n )$ singular values $sigma _ { i } geq 0$ on the main diagonal, with 0s filling the rest of the matrix. The columns of U are the left singular vectors, and the columns of $mathbf { V }$ are the right singular vectors. This is called the singular value decomposition or SVD of the matrix. See Figure 7.8 for an example. \n\nAs is apparent from Figure 7.8a, if $m > n$ , there are at most $n$ singular values, so the last $m - n$ columns of $mathbf { U }$ are irrelevant (since they will be multiplied by 0). The economy sized SVD, also called a thin SVD, avoids computing these unnecessary elements. In other words, if we write the $mathbf { U }$ matrix as $mathbf { U } = [ mathbf { U } _ { 1 } , mathbf { U } _ { 2 } ]$ , we only compute $mathbf { U } _ { 1 }$ . Figure 7.8b shows the opposite case, where $m < n$ , where we represent $mathbf { V } = [ mathbf { V } _ { 1 } ; mathbf { V } _ { 2 } ]$ , and only compute $mathbf { V } _ { 1 }$ . \nThe cost of computing the SVD is $O ( operatorname* { m i n } ( m n ^ { 2 } , m ^ { 2 } n ) )$ . Details on how it works can be found in standard linear algebra textbooks. \n7.5.2 Connection between SVD and EVD \nIf $mathbf { A }$ is real, symmetric and positive definite, then the singular values are equal to the eigenvalues, and the left and right singular vectors are equal to the eigenvectors (up to a sign change): \nNote, however, that NumPy always returns the singular values in decreasing order, whereas the eigenvalues need not necessarily be sorted. \nIn general, for an arbitrary real matrix $mathbf { A }$ , if $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { parallel }$ , we have \nHence \nso the eigenvectors of $mathbf { A } ^ { mathsf { T } } mathbf { A }$ are equal to $mathbf { V }$ , the right singular vectors of $mathbf { A }$ , and the eigenvalues of $mathbf { A } ^ { mathsf { T } } mathbf { A }$ are equal to ${ mathbf D } _ { n } = { mathbf S } ^ { mathsf T } { mathbf S }$ , which is an $n times n$ diagonal matrix containing the squared singular values. Similarly \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Singular value decomposition (SVD)",
        "subsubsection": "Basics"
    },
    {
        "content": "As is apparent from Figure 7.8a, if $m > n$ , there are at most $n$ singular values, so the last $m - n$ columns of $mathbf { U }$ are irrelevant (since they will be multiplied by 0). The economy sized SVD, also called a thin SVD, avoids computing these unnecessary elements. In other words, if we write the $mathbf { U }$ matrix as $mathbf { U } = [ mathbf { U } _ { 1 } , mathbf { U } _ { 2 } ]$ , we only compute $mathbf { U } _ { 1 }$ . Figure 7.8b shows the opposite case, where $m < n$ , where we represent $mathbf { V } = [ mathbf { V } _ { 1 } ; mathbf { V } _ { 2 } ]$ , and only compute $mathbf { V } _ { 1 }$ . \nThe cost of computing the SVD is $O ( operatorname* { m i n } ( m n ^ { 2 } , m ^ { 2 } n ) )$ . Details on how it works can be found in standard linear algebra textbooks. \n7.5.2 Connection between SVD and EVD \nIf $mathbf { A }$ is real, symmetric and positive definite, then the singular values are equal to the eigenvalues, and the left and right singular vectors are equal to the eigenvectors (up to a sign change): \nNote, however, that NumPy always returns the singular values in decreasing order, whereas the eigenvalues need not necessarily be sorted. \nIn general, for an arbitrary real matrix $mathbf { A }$ , if $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { parallel }$ , we have \nHence \nso the eigenvectors of $mathbf { A } ^ { mathsf { T } } mathbf { A }$ are equal to $mathbf { V }$ , the right singular vectors of $mathbf { A }$ , and the eigenvalues of $mathbf { A } ^ { mathsf { T } } mathbf { A }$ are equal to ${ mathbf D } _ { n } = { mathbf S } ^ { mathsf T } { mathbf S }$ , which is an $n times n$ diagonal matrix containing the squared singular values. Similarly \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nso the eigenvectors of $mathbf { A A } ^ { 1 }$ are equal to $mathbf { U }$ , the left singular vectors of $mathbf { A }$ , and the eigenvalues of $mathbf { A A } ^ { mathsf { T } }$ are equal to $mathbf { D } _ { m } = mathbf { S } mathbf { S } ^ { mathsf { I } }$ , which is an $m times m$ diagonal matrix containing the squared singular values. In summary, \nIf we just use the computed (non-zero) parts in the economy-sized SVD, then we can define \nNote also that an EVD does not always exist, even for square $mathbf { A }$ , whereas an SVD always exists. \n7.5.3 Pseudo inverse \nThe Moore-Penrose pseudo-inverse of $mathbf { A }$ , pseudo inverse denoted $mathbf { A } ^ { dagger }$ , is defined as the unique matrix that satisfies the following 4 properties: \nIf $mathbf { A }$ is square and non-singular, then ${ bf A } ^ { dagger } = { bf A } ^ { - 1 }$ . \nIf $m > n$ (tall, skinny) and the columns of $mathbf { A }$ are linearly independent (so $mathbf { A }$ is full rank), then \nwhich is the same expression as arises in the normal equations (see Section 11.2.2.1). In this case, $mathbf { A } ^ { dagger }$ is a left inverse of $mathbf { A }$ because \nbut is not a right inverse because \nonly has rank $n$ , and so cannot be the $m times m$ identity matrix. \nIf $m < n$ (short, fat) and the rows of $mathbf { A }$ are linearly independent (so $mathbf { A } ^ { 1 }$ is full rank), then the pseudo inverse is \nIn this case, $mathbf { A } ^ { dagger }$ is a right inverse of $mathbf { A }$ . \nWe can compute the pseudo inverse using the SVD decomposition $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { parallel }$ . In particular, one can show that \nwhere $r$ is the rank of the matrix, and where we define $mathbf { S } ^ { - 1 } = mathrm { d i a g } ( sigma _ { 1 } ^ { - 1 } , ldots , sigma _ { r } ^ { - 1 } , 0 , ldots , 0 )$ . Indeed if the matrices were square and full rank we would have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Singular value decomposition (SVD)",
        "subsubsection": "Connection between SVD and EVD"
    },
    {
        "content": "so the eigenvectors of $mathbf { A A } ^ { 1 }$ are equal to $mathbf { U }$ , the left singular vectors of $mathbf { A }$ , and the eigenvalues of $mathbf { A A } ^ { mathsf { T } }$ are equal to $mathbf { D } _ { m } = mathbf { S } mathbf { S } ^ { mathsf { I } }$ , which is an $m times m$ diagonal matrix containing the squared singular values. In summary, \nIf we just use the computed (non-zero) parts in the economy-sized SVD, then we can define \nNote also that an EVD does not always exist, even for square $mathbf { A }$ , whereas an SVD always exists. \n7.5.3 Pseudo inverse \nThe Moore-Penrose pseudo-inverse of $mathbf { A }$ , pseudo inverse denoted $mathbf { A } ^ { dagger }$ , is defined as the unique matrix that satisfies the following 4 properties: \nIf $mathbf { A }$ is square and non-singular, then ${ bf A } ^ { dagger } = { bf A } ^ { - 1 }$ . \nIf $m > n$ (tall, skinny) and the columns of $mathbf { A }$ are linearly independent (so $mathbf { A }$ is full rank), then \nwhich is the same expression as arises in the normal equations (see Section 11.2.2.1). In this case, $mathbf { A } ^ { dagger }$ is a left inverse of $mathbf { A }$ because \nbut is not a right inverse because \nonly has rank $n$ , and so cannot be the $m times m$ identity matrix. \nIf $m < n$ (short, fat) and the rows of $mathbf { A }$ are linearly independent (so $mathbf { A } ^ { 1 }$ is full rank), then the pseudo inverse is \nIn this case, $mathbf { A } ^ { dagger }$ is a right inverse of $mathbf { A }$ . \nWe can compute the pseudo inverse using the SVD decomposition $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { parallel }$ . In particular, one can show that \nwhere $r$ is the rank of the matrix, and where we define $mathbf { S } ^ { - 1 } = mathrm { d i a g } ( sigma _ { 1 } ^ { - 1 } , ldots , sigma _ { r } ^ { - 1 } , 0 , ldots , 0 )$ . Indeed if the matrices were square and full rank we would have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.5.4 SVD and the range and null space of a matrix * \nIn this section, we show that the left and right singular vectors form an orthonormal basis for the range and null space. \nFrom Equation (7.178) we have \nwhere $r$ is the rank of $mathbf { A }$ . Thus any ${ bf A } x$ can be written as a linear combination of the left singular vectors $mathbf { u } _ { 1 } , ldots , mathbf { u } _ { r }$ , so the range of $mathbf { A }$ is given by \nwith dimension $r$ . \nTo find a basis for the null space, let us now define a second vector $boldsymbol { y } in mathbb { R } ^ { n }$ that is a linear combination solely of the right singular vectors for the zero singular values, \nSince the ${ pmb v } _ { j }$ ’s are orthonormal, we have \nHence the right singular vectors form an orthonormal basis for the null space: \nwith dimension $n - r$ . We see that \nIn words, this is often written as \nThis is called the rank-nullity theorem. It follows from this that the rank of a matrix is the number of nonzero singular values. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Singular value decomposition (SVD)",
        "subsubsection": "Pseudo inverse"
    },
    {
        "content": "7.5.4 SVD and the range and null space of a matrix * \nIn this section, we show that the left and right singular vectors form an orthonormal basis for the range and null space. \nFrom Equation (7.178) we have \nwhere $r$ is the rank of $mathbf { A }$ . Thus any ${ bf A } x$ can be written as a linear combination of the left singular vectors $mathbf { u } _ { 1 } , ldots , mathbf { u } _ { r }$ , so the range of $mathbf { A }$ is given by \nwith dimension $r$ . \nTo find a basis for the null space, let us now define a second vector $boldsymbol { y } in mathbb { R } ^ { n }$ that is a linear combination solely of the right singular vectors for the zero singular values, \nSince the ${ pmb v } _ { j }$ ’s are orthonormal, we have \nHence the right singular vectors form an orthonormal basis for the null space: \nwith dimension $n - r$ . We see that \nIn words, this is often written as \nThis is called the rank-nullity theorem. It follows from this that the rank of a matrix is the number of nonzero singular values. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.5.5 Truncated SVD \nLet $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { T }$ be the SVD of $mathbf { A }$ , and let $bar { mathbf { A } } _ { K } = mathbf { U } _ { K } mathbf { S } _ { K } mathbf { V } _ { K } ^ { top }$ , where we use the first $K$ columns of $mathbf { U }$ and $mathbf { V }$ . This can be shown to be the optimal rank $K$ approximation, in the sense that it minimizes $| | mathbf { A } - hat { mathbf { A } } _ { K } | | _ { F } ^ { 2 }$ . \nIf $K = r = mathrm { r a n k } ( mathbf { A } )$ , there is no error introduced by this decomposition. But if $K < r$ , we incur some error. This is called a truncated SVD. If the singular values die off quickly, as is typical in natural data (see e.g., Figure 7.10), the error will be small. The total number of parameters needed to represent an $N times D$ matrix using a rank $K$ approximation is \nAs an example, consider the $2 0 0 times 3 2 0$ pixel image in Figure 7.9(top left). This has 64,000 numbers in it. We see that a rank 20 approximation, with only $( 2 0 0 + 3 2 0 + 1 ) times 2 0 = 1 0 , 4 2 0$ numbers is a very good approximation. \nOne can show that the error in this rank- $K$ approximation is given by \nwhere $sigma _ { k }$ is the $k$ ’th singular value of $mathbf { A }$ . \n7.6 Other matrix decompositions * \nIn this section, we briefly review some other useful matrix decompositions. \n7.6.1 LU factorization \nWe can factorize any square matrix A into a product of a lower triangular matrix $mathbf { L }$ and an upper triangular matrix U. For example, \nIn general we may need to permute the entries in the matrix before creating this decomposition. To see this, suppose $a _ { 1 1 } = 0$ . Since $a _ { 1 1 } = l _ { 1 1 } u _ { 1 1 }$ , this means either $l _ { 1 1 }$ or $u _ { 1 1 }$ or both must be zero, but that would imply $mathbf { L }$ or $mathbf { U }$ are singular. To avoid this, the first step of the algorithm can simply reorder the rows so that the first element is nonzero. This is repeated for subsequent steps. We can denote this process by \nwhere $mathbf { P }$ is a permutation matrix, i.e., a square binary matrix where $P _ { i j } = 1$ if row $j$ gets permuted to row $i$ . This is called partial pivoting. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Singular value decomposition (SVD)",
        "subsubsection": "SVD and the range and null space of a matrix *"
    },
    {
        "content": "7.5.5 Truncated SVD \nLet $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { T }$ be the SVD of $mathbf { A }$ , and let $bar { mathbf { A } } _ { K } = mathbf { U } _ { K } mathbf { S } _ { K } mathbf { V } _ { K } ^ { top }$ , where we use the first $K$ columns of $mathbf { U }$ and $mathbf { V }$ . This can be shown to be the optimal rank $K$ approximation, in the sense that it minimizes $| | mathbf { A } - hat { mathbf { A } } _ { K } | | _ { F } ^ { 2 }$ . \nIf $K = r = mathrm { r a n k } ( mathbf { A } )$ , there is no error introduced by this decomposition. But if $K < r$ , we incur some error. This is called a truncated SVD. If the singular values die off quickly, as is typical in natural data (see e.g., Figure 7.10), the error will be small. The total number of parameters needed to represent an $N times D$ matrix using a rank $K$ approximation is \nAs an example, consider the $2 0 0 times 3 2 0$ pixel image in Figure 7.9(top left). This has 64,000 numbers in it. We see that a rank 20 approximation, with only $( 2 0 0 + 3 2 0 + 1 ) times 2 0 = 1 0 , 4 2 0$ numbers is a very good approximation. \nOne can show that the error in this rank- $K$ approximation is given by \nwhere $sigma _ { k }$ is the $k$ ’th singular value of $mathbf { A }$ . \n7.6 Other matrix decompositions * \nIn this section, we briefly review some other useful matrix decompositions. \n7.6.1 LU factorization \nWe can factorize any square matrix A into a product of a lower triangular matrix $mathbf { L }$ and an upper triangular matrix U. For example, \nIn general we may need to permute the entries in the matrix before creating this decomposition. To see this, suppose $a _ { 1 1 } = 0$ . Since $a _ { 1 1 } = l _ { 1 1 } u _ { 1 1 }$ , this means either $l _ { 1 1 }$ or $u _ { 1 1 }$ or both must be zero, but that would imply $mathbf { L }$ or $mathbf { U }$ are singular. To avoid this, the first step of the algorithm can simply reorder the rows so that the first element is nonzero. This is repeated for subsequent steps. We can denote this process by \nwhere $mathbf { P }$ is a permutation matrix, i.e., a square binary matrix where $P _ { i j } = 1$ if row $j$ gets permuted to row $i$ . This is called partial pivoting. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Singular value decomposition (SVD)",
        "subsubsection": "Truncated SVD"
    },
    {
        "content": "7.5.5 Truncated SVD \nLet $mathbf { A } = mathbf { U } mathbf { S } mathbf { V } ^ { T }$ be the SVD of $mathbf { A }$ , and let $bar { mathbf { A } } _ { K } = mathbf { U } _ { K } mathbf { S } _ { K } mathbf { V } _ { K } ^ { top }$ , where we use the first $K$ columns of $mathbf { U }$ and $mathbf { V }$ . This can be shown to be the optimal rank $K$ approximation, in the sense that it minimizes $| | mathbf { A } - hat { mathbf { A } } _ { K } | | _ { F } ^ { 2 }$ . \nIf $K = r = mathrm { r a n k } ( mathbf { A } )$ , there is no error introduced by this decomposition. But if $K < r$ , we incur some error. This is called a truncated SVD. If the singular values die off quickly, as is typical in natural data (see e.g., Figure 7.10), the error will be small. The total number of parameters needed to represent an $N times D$ matrix using a rank $K$ approximation is \nAs an example, consider the $2 0 0 times 3 2 0$ pixel image in Figure 7.9(top left). This has 64,000 numbers in it. We see that a rank 20 approximation, with only $( 2 0 0 + 3 2 0 + 1 ) times 2 0 = 1 0 , 4 2 0$ numbers is a very good approximation. \nOne can show that the error in this rank- $K$ approximation is given by \nwhere $sigma _ { k }$ is the $k$ ’th singular value of $mathbf { A }$ . \n7.6 Other matrix decompositions * \nIn this section, we briefly review some other useful matrix decompositions. \n7.6.1 LU factorization \nWe can factorize any square matrix A into a product of a lower triangular matrix $mathbf { L }$ and an upper triangular matrix U. For example, \nIn general we may need to permute the entries in the matrix before creating this decomposition. To see this, suppose $a _ { 1 1 } = 0$ . Since $a _ { 1 1 } = l _ { 1 1 } u _ { 1 1 }$ , this means either $l _ { 1 1 }$ or $u _ { 1 1 }$ or both must be zero, but that would imply $mathbf { L }$ or $mathbf { U }$ are singular. To avoid this, the first step of the algorithm can simply reorder the rows so that the first element is nonzero. This is repeated for subsequent steps. We can denote this process by \nwhere $mathbf { P }$ is a permutation matrix, i.e., a square binary matrix where $P _ { i j } = 1$ if row $j$ gets permuted to row $i$ . This is called partial pivoting. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n7.6.2 QR decomposition \nSuppose we have $mathbf { A } in mathbb { R } ^ { m times n }$ representing a set of linearly independent basis vectors (so $m geq n$ ), and we want to find a series of orthonormal vectors $q _ { 1 } , q _ { 2 } , . . .$ that span the successive subspaces of $operatorname { s p a n } ( pmb { a } _ { 1 } )$ , $mathrm { s p a n } ( pmb { a } _ { 1 } , pmb { a } _ { 2 } )$ , etc. In other words, we want to find vectors ${ pmb q } _ { j }$ and coefficients $r _ { i j }$ such that \nWe can write this \nso we see $pmb q _ { 1 }$ spans the space of $mathbf { delta } _ { mathbf { u } _ { 1 } }$ , and $pmb q _ { 1 }$ and $mathbf { q } _ { 2 }$ span the space of ${ a _ { 1 } , a _ { 2 } }$ , etc. In matrix notation, we have \nwhere $hat { mathbf { Q } }$ is $m times n$ with orthonormal columns and $hat { mathbf { R } }$ is $n times n$ and upper triangular. This is called a reduced QR or economy sized QR factorization of $mathbf { A }$ ; see Figure 7.11. \nA full QR factorization appends an additional $m - n$ orthonormal columns to $hat { mathbf { Q } }$ so it becomes a square, orthogonal matrix $mathbf { Q }$ , which satisfies $mathbf { Q Q } ^ { mathsf { T } } = mathbf { Q } ^ { mathsf { T } } mathbf { Q } = mathbf { I }$ . Also, we append rows made of zero to $hat { textbf { R } }$ so it becomes an $m times n$ matrix that is still upper triangular, called $mathbf { R }$ : see Figure 7.11. The zero entries in $mathbf { R }$ “kill off” the new columns in $mathbf { Q }$ , so the result is the same as $hat { mathbf { Q } } hat { mathbf { R } }$ . \nQR decomposition is commonly used to solve systems of linear equations, as we discuss in Section 11.2.2.3. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Other matrix decompositions *",
        "subsubsection": "LU factorization"
    },
    {
        "content": "7.6.2 QR decomposition \nSuppose we have $mathbf { A } in mathbb { R } ^ { m times n }$ representing a set of linearly independent basis vectors (so $m geq n$ ), and we want to find a series of orthonormal vectors $q _ { 1 } , q _ { 2 } , . . .$ that span the successive subspaces of $operatorname { s p a n } ( pmb { a } _ { 1 } )$ , $mathrm { s p a n } ( pmb { a } _ { 1 } , pmb { a } _ { 2 } )$ , etc. In other words, we want to find vectors ${ pmb q } _ { j }$ and coefficients $r _ { i j }$ such that \nWe can write this \nso we see $pmb q _ { 1 }$ spans the space of $mathbf { delta } _ { mathbf { u } _ { 1 } }$ , and $pmb q _ { 1 }$ and $mathbf { q } _ { 2 }$ span the space of ${ a _ { 1 } , a _ { 2 } }$ , etc. In matrix notation, we have \nwhere $hat { mathbf { Q } }$ is $m times n$ with orthonormal columns and $hat { mathbf { R } }$ is $n times n$ and upper triangular. This is called a reduced QR or economy sized QR factorization of $mathbf { A }$ ; see Figure 7.11. \nA full QR factorization appends an additional $m - n$ orthonormal columns to $hat { mathbf { Q } }$ so it becomes a square, orthogonal matrix $mathbf { Q }$ , which satisfies $mathbf { Q Q } ^ { mathsf { T } } = mathbf { Q } ^ { mathsf { T } } mathbf { Q } = mathbf { I }$ . Also, we append rows made of zero to $hat { textbf { R } }$ so it becomes an $m times n$ matrix that is still upper triangular, called $mathbf { R }$ : see Figure 7.11. The zero entries in $mathbf { R }$ “kill off” the new columns in $mathbf { Q }$ , so the result is the same as $hat { mathbf { Q } } hat { mathbf { R } }$ . \nQR decomposition is commonly used to solve systems of linear equations, as we discuss in Section 11.2.2.3. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n7.6.3 Cholesky decomposition \nAny symmetric positive definite matrix can be factorized as $mathbf { A } = mathbf { R } ^ { mathsf { I } } mathbf { R }$ , where $mathbf { R }$ is upper triangular with real, positive diagonal elements. (This can also be written as $mathbf { A } = mathbf { L L } ^ { mathsf { I } }$ , where $mathbf { L } = mathbf { R } ^ { mathsf { I } }$ is lower triangular.) This is called a Cholesky factorization or matrix square root. In NumPy, this is implemented by np.linalg.cholesky. The computational complexity of this operation is $O ( V ^ { 3 } )$ , where $V$ is the number of variables, but can be less for sparse matrices. Below we give some applications of this factorization. \n7.6.3.1 Application: Sampling from an MVN \nThe Cholesky decomposition of a covariance matrix can be used to sample from a multivariate Gaussian. Let $pmb { y } sim mathcal { N } ( pmb { mu } , pmb { Sigma } )$ and $pmb { Sigma } = mathbf { L } mathbf { L } ^ { mathsf { I } }$ . We first sample $pmb { x } sim mathcal { N } ( mathbf { 0 } , mathbf { I } )$ , which is easy because it just requires sampling from $d$ separate 1d Gaussians. We then set $pmb { y } = mathbf { L } pmb { x } + pmb { mu }$ . This is valid since \nSee cholesky_demo.ipynb for some code. \n7.7 Solving systems of linear equations * \nAn important application of linear algebra is the study of systems of linear equations. For example, consider the following set of 3 equations: \nWe can represent this in matrix-vector form as follows: \nwhere \nThe solution is $pmb { x } = [ 1 , - 2 , - 2 ]$ . \nIn general, if we have $m$ equations and $n$ unknowns, then A will be a $m times n$ matrix, and $^ { b }$ will be a $m times 1$ vector. If $m = n$ (and $mathbf { A }$ is full rank), there is a single unique solution. If $m < n$ , the system is underdetermined, so there is not a unique solution. If $m > n$ , the system is overdetermined, since there are more constraints than unknowns, and not all the lines intersect at the same point. See Figure 7.12 for an illustration. We discuss how to compute solutions in each of these cases below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Other matrix decompositions *",
        "subsubsection": "QR decomposition"
    },
    {
        "content": "7.6.3 Cholesky decomposition \nAny symmetric positive definite matrix can be factorized as $mathbf { A } = mathbf { R } ^ { mathsf { I } } mathbf { R }$ , where $mathbf { R }$ is upper triangular with real, positive diagonal elements. (This can also be written as $mathbf { A } = mathbf { L L } ^ { mathsf { I } }$ , where $mathbf { L } = mathbf { R } ^ { mathsf { I } }$ is lower triangular.) This is called a Cholesky factorization or matrix square root. In NumPy, this is implemented by np.linalg.cholesky. The computational complexity of this operation is $O ( V ^ { 3 } )$ , where $V$ is the number of variables, but can be less for sparse matrices. Below we give some applications of this factorization. \n7.6.3.1 Application: Sampling from an MVN \nThe Cholesky decomposition of a covariance matrix can be used to sample from a multivariate Gaussian. Let $pmb { y } sim mathcal { N } ( pmb { mu } , pmb { Sigma } )$ and $pmb { Sigma } = mathbf { L } mathbf { L } ^ { mathsf { I } }$ . We first sample $pmb { x } sim mathcal { N } ( mathbf { 0 } , mathbf { I } )$ , which is easy because it just requires sampling from $d$ separate 1d Gaussians. We then set $pmb { y } = mathbf { L } pmb { x } + pmb { mu }$ . This is valid since \nSee cholesky_demo.ipynb for some code. \n7.7 Solving systems of linear equations * \nAn important application of linear algebra is the study of systems of linear equations. For example, consider the following set of 3 equations: \nWe can represent this in matrix-vector form as follows: \nwhere \nThe solution is $pmb { x } = [ 1 , - 2 , - 2 ]$ . \nIn general, if we have $m$ equations and $n$ unknowns, then A will be a $m times n$ matrix, and $^ { b }$ will be a $m times 1$ vector. If $m = n$ (and $mathbf { A }$ is full rank), there is a single unique solution. If $m < n$ , the system is underdetermined, so there is not a unique solution. If $m > n$ , the system is overdetermined, since there are more constraints than unknowns, and not all the lines intersect at the same point. See Figure 7.12 for an illustration. We discuss how to compute solutions in each of these cases below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Other matrix decompositions *",
        "subsubsection": "Cholesky decomposition"
    },
    {
        "content": "7.7.1 Solving square systems \nIn the case where $m = n$ , we can solve for $_ { x }$ by computing an LU decomposition, $mathbf { A } = mathbf { L U }$ , and then proceeding as follows: \nThe crucial point is that $mathbf { L }$ and $mathbf { U }$ are both triangular matrices, so we can avoid taking matrix inverses, and use a method known as backsubstitution instead. \nIn particular, we can solve $pmb { y } = mathbf L ^ { - 1 } pmb { b }$ without taking inverses as follows. First we write \nWe start by solving $L _ { 1 1 } y _ { 1 } = b _ { 1 }$ to find $y _ { 1 }$ and then substitute this in to solve \nfor $y _ { 2 }$ . We repeat this recursively. This process is often denoted by the backslash operator, $pmb { y } = mathbf { L } setminus pmb { b }$ . Once we have $pmb { y }$ , we can solve ${ pmb x } = { bf U } ^ { - 1 } { pmb y }$ using backsubstitution in a similar manner. \n7.7.2 Solving underconstrained systems (least norm estimation) \nIn this section, we consider the underconstrained setting, where $m < n$ .3 We assume the rows are linearly independent, so A is full rank.",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Solving systems of linear equations *",
        "subsubsection": "Solving square systems"
    },
    {
        "content": "7.7.1 Solving square systems \nIn the case where $m = n$ , we can solve for $_ { x }$ by computing an LU decomposition, $mathbf { A } = mathbf { L U }$ , and then proceeding as follows: \nThe crucial point is that $mathbf { L }$ and $mathbf { U }$ are both triangular matrices, so we can avoid taking matrix inverses, and use a method known as backsubstitution instead. \nIn particular, we can solve $pmb { y } = mathbf L ^ { - 1 } pmb { b }$ without taking inverses as follows. First we write \nWe start by solving $L _ { 1 1 } y _ { 1 } = b _ { 1 }$ to find $y _ { 1 }$ and then substitute this in to solve \nfor $y _ { 2 }$ . We repeat this recursively. This process is often denoted by the backslash operator, $pmb { y } = mathbf { L } setminus pmb { b }$ . Once we have $pmb { y }$ , we can solve ${ pmb x } = { bf U } ^ { - 1 } { pmb y }$ using backsubstitution in a similar manner. \n7.7.2 Solving underconstrained systems (least norm estimation) \nIn this section, we consider the underconstrained setting, where $m < n$ .3 We assume the rows are linearly independent, so A is full rank. \nWhen $m < n$ , there are multiple possible solutions, which have the form \nwhere ${ boldsymbol { mathbf { mathit { x } } } } _ { p }$ is any particular solution. It is standard to pick the particular solution with minimal $ell _ { 2 }$ norm, i.e., \nWe can compute the minimal norm solution using the right pseudo inverse: \n(See Section 7.5.3 for more details.) To see this, suppose $scriptstyle { mathbf {  { x } } }$ is some other solution, so $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = mathbf { mathit { b } }$ , and $mathbf { A } ( pmb { x } - pmb { x } _ { mathrm { p i n v } } ) = mathbf { 0 }$ . Thus \nand hence $( pmb { x } - pmb { x } _ { mathrm { p i n v } } ) perp pmb { x } _ { mathrm { p i n v } }$ . By Pythagoras’s theorem, the norm of $_ { x }$ is \nThus any solution apart from $pmb { x } _ { mathrm { p i n v } }$ has larger norm. \nWe can also solve the constrained optimization problem in Equation (7.222) by minimizing the following unconstrained objective \nFrom Section 8.5.1, the optimality conditions are \nFrom the first condition we have $pmb { x } = - mathbf { A } ^ { 1 } lambda / 2$ . Subsituting into the second we get \nwhich implies $lambda = - 2 ( mathbf { A } mathbf { A } ^ { intercal } ) ^ { - 1 } b$ . Hence $pmb { x } = mathbf { A } ^ { 1 } ( mathbf { A } mathbf { A } ^ { 1 } ) ^ { - 1 } pmb { b }$ , which is the right pseudo inverse solution. \n7.7.3 Solving overconstrained systems (least squares estimation) \nIf $m > n$ , we have an overdetermined solution, which typically does not have an exact solution, but we will try to find the solution that gets as close as possible to satisfying all of the constraints specified by $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = mathbf { boldsymbol { mathbf { b } } }$ . We can do this by minimizing the following cost function, known as the least squares objective:4",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Solving systems of linear equations *",
        "subsubsection": "Solving underconstrained systems (least norm estimation)"
    },
    {
        "content": "When $m < n$ , there are multiple possible solutions, which have the form \nwhere ${ boldsymbol { mathbf { mathit { x } } } } _ { p }$ is any particular solution. It is standard to pick the particular solution with minimal $ell _ { 2 }$ norm, i.e., \nWe can compute the minimal norm solution using the right pseudo inverse: \n(See Section 7.5.3 for more details.) To see this, suppose $scriptstyle { mathbf {  { x } } }$ is some other solution, so $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = mathbf { mathit { b } }$ , and $mathbf { A } ( pmb { x } - pmb { x } _ { mathrm { p i n v } } ) = mathbf { 0 }$ . Thus \nand hence $( pmb { x } - pmb { x } _ { mathrm { p i n v } } ) perp pmb { x } _ { mathrm { p i n v } }$ . By Pythagoras’s theorem, the norm of $_ { x }$ is \nThus any solution apart from $pmb { x } _ { mathrm { p i n v } }$ has larger norm. \nWe can also solve the constrained optimization problem in Equation (7.222) by minimizing the following unconstrained objective \nFrom Section 8.5.1, the optimality conditions are \nFrom the first condition we have $pmb { x } = - mathbf { A } ^ { 1 } lambda / 2$ . Subsituting into the second we get \nwhich implies $lambda = - 2 ( mathbf { A } mathbf { A } ^ { intercal } ) ^ { - 1 } b$ . Hence $pmb { x } = mathbf { A } ^ { 1 } ( mathbf { A } mathbf { A } ^ { 1 } ) ^ { - 1 } pmb { b }$ , which is the right pseudo inverse solution. \n7.7.3 Solving overconstrained systems (least squares estimation) \nIf $m > n$ , we have an overdetermined solution, which typically does not have an exact solution, but we will try to find the solution that gets as close as possible to satisfying all of the constraints specified by $mathbf { A } { boldsymbol { mathbf { mathit { x } } } } = mathbf { boldsymbol { mathbf { b } } }$ . We can do this by minimizing the following cost function, known as the least squares objective:4 \nUsing matrix calculus results from Section 7.8 we have that the gradient is given by \nThe optimum can be found by solving $pmb { g } ( pmb { x } ) = mathbf { 0 }$ . This gives \nThese are known as the normal equations, since, at the optimal solution, $mathbf { delta } _ { b mathrm { ~ - ~ } } mathbf { A } x$ is normal (orthogonal) to the range of $mathbf { A }$ , as we explain in Section 11.2.2.2. The corresponding solution $hat { textbf { textit { x } } }$ is the ordinary least squares (OLS) solution, which is given by \nThe quantity $mathbf { A } ^ { dagger } = ( mathbf { A } ^ { mathsf { T } } mathbf { A } ) ^ { - 1 } mathbf { A } ^ { mathsf { T } }$ is the left pseudo inverse of the (non-square) matrix A (see Section 7.5.3 for more details). \nWe can check that the solution is unique by showing that the Hessian is positive definite. In this case, the Hessian is given by \nIf $mathbf { A }$ is full rank (so the columns of $mathbf { A }$ are linearly independent), then $mathbf { H }$ is positive definite, since for any $v > 0$ , we have \nHence in the full rank case, the least squares objective has a unique global minimum. \n7.8 Matrix calculus \nThe topic of calculus concerns computing “rates of change” of functions as we vary their inputs. It is of vital importance to machine learning, as well as almost every other numerical discipline. In this section, we review some standard results. In some cases, we use some concepts and notation from matrix algebra, which we cover in Chapter 7. For more details on these results from a deep learning perspective, see [PH18]. \n7.8.1 Derivatives \nConsider a scalar-argument function $f : mathbb { R } to mathbb { R }$ . We define its derivative at a point $x$ to be the quantity \nassuming the limit exists. This measures how quickly the output changes when we move a small distance in input space away from $x$ (i.e., the “rate of change” of the function). We can interpret $f ^ { prime } ( x )$ as the slope of the tangent line at $f ( x )$ , and hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Solving systems of linear equations *",
        "subsubsection": "Solving overconstrained systems (least squares estimation)"
    },
    {
        "content": "Using matrix calculus results from Section 7.8 we have that the gradient is given by \nThe optimum can be found by solving $pmb { g } ( pmb { x } ) = mathbf { 0 }$ . This gives \nThese are known as the normal equations, since, at the optimal solution, $mathbf { delta } _ { b mathrm { ~ - ~ } } mathbf { A } x$ is normal (orthogonal) to the range of $mathbf { A }$ , as we explain in Section 11.2.2.2. The corresponding solution $hat { textbf { textit { x } } }$ is the ordinary least squares (OLS) solution, which is given by \nThe quantity $mathbf { A } ^ { dagger } = ( mathbf { A } ^ { mathsf { T } } mathbf { A } ) ^ { - 1 } mathbf { A } ^ { mathsf { T } }$ is the left pseudo inverse of the (non-square) matrix A (see Section 7.5.3 for more details). \nWe can check that the solution is unique by showing that the Hessian is positive definite. In this case, the Hessian is given by \nIf $mathbf { A }$ is full rank (so the columns of $mathbf { A }$ are linearly independent), then $mathbf { H }$ is positive definite, since for any $v > 0$ , we have \nHence in the full rank case, the least squares objective has a unique global minimum. \n7.8 Matrix calculus \nThe topic of calculus concerns computing “rates of change” of functions as we vary their inputs. It is of vital importance to machine learning, as well as almost every other numerical discipline. In this section, we review some standard results. In some cases, we use some concepts and notation from matrix algebra, which we cover in Chapter 7. For more details on these results from a deep learning perspective, see [PH18]. \n7.8.1 Derivatives \nConsider a scalar-argument function $f : mathbb { R } to mathbb { R }$ . We define its derivative at a point $x$ to be the quantity \nassuming the limit exists. This measures how quickly the output changes when we move a small distance in input space away from $x$ (i.e., the “rate of change” of the function). We can interpret $f ^ { prime } ( x )$ as the slope of the tangent line at $f ( x )$ , and hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nfor small $h$ . \nWe can compute a finite difference approximation to the derivative by using a finite step size $h$ , as follows: \nThe smaller the step size $h$ , the better the estimate, although if $h$ is too small, there can be errors due to numerical cancellation. \nWe can think of differentiation as an operator that maps functions to functions, $D ( f ) = f ^ { prime }$ , where $f ^ { prime } ( x )$ computes the derivative at $x$ (assuming the derivative exists at that point). The use of the prime symbol $f ^ { prime }$ to denote the derivative is called Lagrange notation. The second derivative function, which measures how quickly the gradient is changing, is denoted by $f ^ { prime prime }$ . The $n$ ’th derivative function is denoted $f ^ { ( n ) }$ . \nAlternatively, we can use Leibniz notation, in which we denote the function by $y = f ( x )$ , and its derivative by $textstyle { frac { d y } { d x } }$ or $textstyle { frac { d } { d x } } f ( x )$ . To denote the evaluation of the derivative at a point $a$ , we write $scriptstyle { frac { d f } { d x } } left| _ { x = a } . { } right.$ \n7.8.2 Gradients \nWe can extend the notion of derivatives to handle vector-argument functions, $f : mathbb { R } ^ { n }  mathbb { R }$ , by defining the partial derivative of $f$ with respect to $x _ { i }$ to be \nwhere $e _ { i }$ is the $i$ ’th unit vector. \nThe gradient of a function at a point $_ { x }$ is the vector of its partial derivatives: \nTo emphasize the point at which the gradient is evaluated, we can write \nWe see that the operator $nabla$ (pronounced “nabla”) maps a function $f : mathbb { R } ^ { n }  mathbb { R }$ to another function $g : mathbb { R } ^ { n }  mathbb { R } ^ { n }$ . Since $g (  u )$ is a vector-valued function, it is known as a vector field. By contrast, the derivative function $f ^ { prime }$ is a scalar field. \n7.8.3 Directional derivative \nThe directional derivative measures how much the function $f : mathbb { R } ^ { n } to mathbb { R }$ changes along a direction $_ { v }$ in space. It is defined as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Derivatives"
    },
    {
        "content": "for small $h$ . \nWe can compute a finite difference approximation to the derivative by using a finite step size $h$ , as follows: \nThe smaller the step size $h$ , the better the estimate, although if $h$ is too small, there can be errors due to numerical cancellation. \nWe can think of differentiation as an operator that maps functions to functions, $D ( f ) = f ^ { prime }$ , where $f ^ { prime } ( x )$ computes the derivative at $x$ (assuming the derivative exists at that point). The use of the prime symbol $f ^ { prime }$ to denote the derivative is called Lagrange notation. The second derivative function, which measures how quickly the gradient is changing, is denoted by $f ^ { prime prime }$ . The $n$ ’th derivative function is denoted $f ^ { ( n ) }$ . \nAlternatively, we can use Leibniz notation, in which we denote the function by $y = f ( x )$ , and its derivative by $textstyle { frac { d y } { d x } }$ or $textstyle { frac { d } { d x } } f ( x )$ . To denote the evaluation of the derivative at a point $a$ , we write $scriptstyle { frac { d f } { d x } } left| _ { x = a } . { } right.$ \n7.8.2 Gradients \nWe can extend the notion of derivatives to handle vector-argument functions, $f : mathbb { R } ^ { n }  mathbb { R }$ , by defining the partial derivative of $f$ with respect to $x _ { i }$ to be \nwhere $e _ { i }$ is the $i$ ’th unit vector. \nThe gradient of a function at a point $_ { x }$ is the vector of its partial derivatives: \nTo emphasize the point at which the gradient is evaluated, we can write \nWe see that the operator $nabla$ (pronounced “nabla”) maps a function $f : mathbb { R } ^ { n }  mathbb { R }$ to another function $g : mathbb { R } ^ { n }  mathbb { R } ^ { n }$ . Since $g (  u )$ is a vector-valued function, it is known as a vector field. By contrast, the derivative function $f ^ { prime }$ is a scalar field. \n7.8.3 Directional derivative \nThe directional derivative measures how much the function $f : mathbb { R } ^ { n } to mathbb { R }$ changes along a direction $_ { v }$ in space. It is defined as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Gradients"
    },
    {
        "content": "for small $h$ . \nWe can compute a finite difference approximation to the derivative by using a finite step size $h$ , as follows: \nThe smaller the step size $h$ , the better the estimate, although if $h$ is too small, there can be errors due to numerical cancellation. \nWe can think of differentiation as an operator that maps functions to functions, $D ( f ) = f ^ { prime }$ , where $f ^ { prime } ( x )$ computes the derivative at $x$ (assuming the derivative exists at that point). The use of the prime symbol $f ^ { prime }$ to denote the derivative is called Lagrange notation. The second derivative function, which measures how quickly the gradient is changing, is denoted by $f ^ { prime prime }$ . The $n$ ’th derivative function is denoted $f ^ { ( n ) }$ . \nAlternatively, we can use Leibniz notation, in which we denote the function by $y = f ( x )$ , and its derivative by $textstyle { frac { d y } { d x } }$ or $textstyle { frac { d } { d x } } f ( x )$ . To denote the evaluation of the derivative at a point $a$ , we write $scriptstyle { frac { d f } { d x } } left| _ { x = a } . { } right.$ \n7.8.2 Gradients \nWe can extend the notion of derivatives to handle vector-argument functions, $f : mathbb { R } ^ { n }  mathbb { R }$ , by defining the partial derivative of $f$ with respect to $x _ { i }$ to be \nwhere $e _ { i }$ is the $i$ ’th unit vector. \nThe gradient of a function at a point $_ { x }$ is the vector of its partial derivatives: \nTo emphasize the point at which the gradient is evaluated, we can write \nWe see that the operator $nabla$ (pronounced “nabla”) maps a function $f : mathbb { R } ^ { n }  mathbb { R }$ to another function $g : mathbb { R } ^ { n }  mathbb { R } ^ { n }$ . Since $g (  u )$ is a vector-valued function, it is known as a vector field. By contrast, the derivative function $f ^ { prime }$ is a scalar field. \n7.8.3 Directional derivative \nThe directional derivative measures how much the function $f : mathbb { R } ^ { n } to mathbb { R }$ changes along a direction $_ { v }$ in space. It is defined as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe can approximate this numerically using 2 function calls to $f$ , regardless of $n$ . By contrast, a numerical approximation to the standard gradient vector takes $n + 1$ calls (or $2 n$ if using central differences). \nNote that the directional derivative along $mathbf { nabla } _ { mathbf { v } }$ is the scalar product of the gradient $pmb { g }$ and the vector $_ { v }$ : \n7.8.4 Total derivative * \nSuppose that some of the arguments to the function depend on each other. Concretely, suppose the function has the form $f ( t , x ( t ) , y ( t ) )$ . We define the total derivative of $f$ wrt $t$ as follows: \nIf we multiply both sides by the differential $d t$ , we get the total differential \nThis measures how much $f$ changes when we change $t$ , both via the direct effect of $t$ on $f$ , but also indirectly, via the effects of $t$ on $x$ and $y$ . \n7.8.5 Jacobian \nConsider a function that maps a vector to another vector, $f : mathbb { R } ^ { n }  mathbb { R } ^ { m }$ . The Jacobian matrix of this function is an $m times n$ matrix of partial derivatives: \nNote that we lay out the results in the same orientation as the output $f$ ; this is sometimes called numerator layout or the Jacobian formulation.5 \n7.8.5.1 Multiplying Jacobians and vectors \nThe Jacobian vector product or JVP is defined to be the operation that corresponds to rightmultiplying the Jacobian matrix $mathbf { J } in mathbb { R } ^ { m times n }$ by a vector $pmb { v } in mathbb { R } ^ { n }$ : \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Directional derivative"
    },
    {
        "content": "We can approximate this numerically using 2 function calls to $f$ , regardless of $n$ . By contrast, a numerical approximation to the standard gradient vector takes $n + 1$ calls (or $2 n$ if using central differences). \nNote that the directional derivative along $mathbf { nabla } _ { mathbf { v } }$ is the scalar product of the gradient $pmb { g }$ and the vector $_ { v }$ : \n7.8.4 Total derivative * \nSuppose that some of the arguments to the function depend on each other. Concretely, suppose the function has the form $f ( t , x ( t ) , y ( t ) )$ . We define the total derivative of $f$ wrt $t$ as follows: \nIf we multiply both sides by the differential $d t$ , we get the total differential \nThis measures how much $f$ changes when we change $t$ , both via the direct effect of $t$ on $f$ , but also indirectly, via the effects of $t$ on $x$ and $y$ . \n7.8.5 Jacobian \nConsider a function that maps a vector to another vector, $f : mathbb { R } ^ { n }  mathbb { R } ^ { m }$ . The Jacobian matrix of this function is an $m times n$ matrix of partial derivatives: \nNote that we lay out the results in the same orientation as the output $f$ ; this is sometimes called numerator layout or the Jacobian formulation.5 \n7.8.5.1 Multiplying Jacobians and vectors \nThe Jacobian vector product or JVP is defined to be the operation that corresponds to rightmultiplying the Jacobian matrix $mathbf { J } in mathbb { R } ^ { m times n }$ by a vector $pmb { v } in mathbb { R } ^ { n }$ : \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Total derivative *"
    },
    {
        "content": "We can approximate this numerically using 2 function calls to $f$ , regardless of $n$ . By contrast, a numerical approximation to the standard gradient vector takes $n + 1$ calls (or $2 n$ if using central differences). \nNote that the directional derivative along $mathbf { nabla } _ { mathbf { v } }$ is the scalar product of the gradient $pmb { g }$ and the vector $_ { v }$ : \n7.8.4 Total derivative * \nSuppose that some of the arguments to the function depend on each other. Concretely, suppose the function has the form $f ( t , x ( t ) , y ( t ) )$ . We define the total derivative of $f$ wrt $t$ as follows: \nIf we multiply both sides by the differential $d t$ , we get the total differential \nThis measures how much $f$ changes when we change $t$ , both via the direct effect of $t$ on $f$ , but also indirectly, via the effects of $t$ on $x$ and $y$ . \n7.8.5 Jacobian \nConsider a function that maps a vector to another vector, $f : mathbb { R } ^ { n }  mathbb { R } ^ { m }$ . The Jacobian matrix of this function is an $m times n$ matrix of partial derivatives: \nNote that we lay out the results in the same orientation as the output $f$ ; this is sometimes called numerator layout or the Jacobian formulation.5 \n7.8.5.1 Multiplying Jacobians and vectors \nThe Jacobian vector product or JVP is defined to be the operation that corresponds to rightmultiplying the Jacobian matrix $mathbf { J } in mathbb { R } ^ { m times n }$ by a vector $pmb { v } in mathbb { R } ^ { n }$ : \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nSo we can see that we can approximate this numerically using just 2 calls to $f$ . \nThe vector Jacobian product or VJP is defined to be the operation that corresponds to left-multiplying the Jacobian matrix $mathbf { J } in mathbb { R } ^ { m times n }$ by a vector $pmb { u } in mathbb { R } ^ { m }$ : \nThe JVP is more efficient if $m geq n$ , and the VJP is more efficient if $m leq n$ . See Section 13.3 for details on how this can be used to perform automatic differentiation in a computation graph such as a DNN. \n7.8.5.2 Jacobian of a composition \nSometimes it is useful to take the Jacobian of the composition of two functions. Let $h ( { pmb x } ) = g ( f ( { pmb x } ) )$ . By the chain rule of calculus, we have \nFor example, suppose $f : mathbb { R } to mathbb { R } ^ { 2 }$ and $g : mathbb { R } ^ { 2 }  mathbb { R } ^ { 2 }$ . We have \n7.8.6 Hessian \nFor a function $f : mathbb { R } ^ { n } to mathbb { R }$ that is twice differentiable, we define the Hessian matrix as the (symmetric) $n times n$ matrix of second partial derivatives: \nWe see that the Hessian is the Jacobian of the gradient. \n7.8.7 Gradients of commonly used functions \nIn this section, we list without proof the gradients of certain widely used functions. \n7.8.7.1 Functions that map scalars to scalars \nConsider a differentiable function $f : mathbb { R } to mathbb { R }$ . Here are some useful identities from scalar calculus, which you should already be familiar with. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Jacobian"
    },
    {
        "content": "So we can see that we can approximate this numerically using just 2 calls to $f$ . \nThe vector Jacobian product or VJP is defined to be the operation that corresponds to left-multiplying the Jacobian matrix $mathbf { J } in mathbb { R } ^ { m times n }$ by a vector $pmb { u } in mathbb { R } ^ { m }$ : \nThe JVP is more efficient if $m geq n$ , and the VJP is more efficient if $m leq n$ . See Section 13.3 for details on how this can be used to perform automatic differentiation in a computation graph such as a DNN. \n7.8.5.2 Jacobian of a composition \nSometimes it is useful to take the Jacobian of the composition of two functions. Let $h ( { pmb x } ) = g ( f ( { pmb x } ) )$ . By the chain rule of calculus, we have \nFor example, suppose $f : mathbb { R } to mathbb { R } ^ { 2 }$ and $g : mathbb { R } ^ { 2 }  mathbb { R } ^ { 2 }$ . We have \n7.8.6 Hessian \nFor a function $f : mathbb { R } ^ { n } to mathbb { R }$ that is twice differentiable, we define the Hessian matrix as the (symmetric) $n times n$ matrix of second partial derivatives: \nWe see that the Hessian is the Jacobian of the gradient. \n7.8.7 Gradients of commonly used functions \nIn this section, we list without proof the gradients of certain widely used functions. \n7.8.7.1 Functions that map scalars to scalars \nConsider a differentiable function $f : mathbb { R } to mathbb { R }$ . Here are some useful identities from scalar calculus, which you should already be familiar with. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Hessian"
    },
    {
        "content": "So we can see that we can approximate this numerically using just 2 calls to $f$ . \nThe vector Jacobian product or VJP is defined to be the operation that corresponds to left-multiplying the Jacobian matrix $mathbf { J } in mathbb { R } ^ { m times n }$ by a vector $pmb { u } in mathbb { R } ^ { m }$ : \nThe JVP is more efficient if $m geq n$ , and the VJP is more efficient if $m leq n$ . See Section 13.3 for details on how this can be used to perform automatic differentiation in a computation graph such as a DNN. \n7.8.5.2 Jacobian of a composition \nSometimes it is useful to take the Jacobian of the composition of two functions. Let $h ( { pmb x } ) = g ( f ( { pmb x } ) )$ . By the chain rule of calculus, we have \nFor example, suppose $f : mathbb { R } to mathbb { R } ^ { 2 }$ and $g : mathbb { R } ^ { 2 }  mathbb { R } ^ { 2 }$ . We have \n7.8.6 Hessian \nFor a function $f : mathbb { R } ^ { n } to mathbb { R }$ that is twice differentiable, we define the Hessian matrix as the (symmetric) $n times n$ matrix of second partial derivatives: \nWe see that the Hessian is the Jacobian of the gradient. \n7.8.7 Gradients of commonly used functions \nIn this section, we list without proof the gradients of certain widely used functions. \n7.8.7.1 Functions that map scalars to scalars \nConsider a differentiable function $f : mathbb { R } to mathbb { R }$ . Here are some useful identities from scalar calculus, which you should already be familiar with. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nEquation (7.261) is known as the chain rule of calculus. \n7.8.7.2 Functions that map vectors to scalars \nConsider a differentiable function $f : mathbb { R } ^ { n }  mathbb { R }$ . Here are some useful identities:6 \nIt is fairly easy to prove these identities by expanding out the quadratic form, and applying scalar calculus. \n7.8.7.3 Functions that map matrices to scalars \nConsider a function $f : mathbb { R } ^ { m times n }  mathbb { R }$ which maps a matrix to a scalar. We are using the following natural layout for the derivative matrix: \nBelow are some useful identities. \nIdentities involving quadratic forms \nOne can show the following results. \nIdentities involving matrix trace \nOne can show the following results. \nIdentities involving matrix determinant \nOne can show the following results. \n7.9 Exercises \nExercise 7.1 [Orthogonal matrices] \na. A rotation in 3d by angle $alpha$ about the $z$ axis is given by the following matrix: \nProve that $mathbf { R }$ is an orthogonal matrix, i.e., $mathbf { R } ^ { T } mathbf { R } = mathbf { I }$ , for any $alpha$ . \nb. What is the only eigenvector $_ v$ of $mathbf { R }$ with an eigenvalue of 1.0 and of unit norm (i.e., $| | pmb { v } | | ^ { 2 } = 1$ )? (Your answer should be the same for any $alpha$ .) Hint: think about the geometrical interpretation of eigenvectors. \nExercise 7.2 [Eigenvectors by hand *] Find the eigenvalues and eigenvectors of the following matrix \nCompute your result by hand and check it with Python. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Matrix calculus",
        "subsubsection": "Gradients of commonly used functions"
    },
    {
        "content": "Identities involving quadratic forms \nOne can show the following results. \nIdentities involving matrix trace \nOne can show the following results. \nIdentities involving matrix determinant \nOne can show the following results. \n7.9 Exercises \nExercise 7.1 [Orthogonal matrices] \na. A rotation in 3d by angle $alpha$ about the $z$ axis is given by the following matrix: \nProve that $mathbf { R }$ is an orthogonal matrix, i.e., $mathbf { R } ^ { T } mathbf { R } = mathbf { I }$ , for any $alpha$ . \nb. What is the only eigenvector $_ v$ of $mathbf { R }$ with an eigenvalue of 1.0 and of unit norm (i.e., $| | pmb { v } | | ^ { 2 } = 1$ )? (Your answer should be the same for any $alpha$ .) Hint: think about the geometrical interpretation of eigenvectors. \nExercise 7.2 [Eigenvectors by hand *] Find the eigenvalues and eigenvectors of the following matrix \nCompute your result by hand and check it with Python. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n8 Optimization \nParts of this chapter were written by Frederik Kunstner, Si Yi Meng, Aaron Mishkin, Sharan Vaswani, and Mark Schmidt. \n8.1 Introduction \nWe saw in Chapter 4 that the core problem in machine learning is parameter estimation (aka model fitting). This requires solving an optimization problem, where we try to find the values for a set of variables $pmb theta in Theta$ , that minimize a scalar-valued loss function or cost function $mathcal { L } : Theta to mathbb { R }$ : \nWe will assume that the parameter space is given by $Theta subseteq mathbb { R } ^ { D }$ , where $D$ is the number of variables being optimized over. Thus we are focusing on continuous optimization, rather than discrete optimization. \nIf we want to maximize a score function or reward function $R ( pmb theta )$ , we can equivalently minimize $mathcal { L } ( pmb { theta } ) = - R ( pmb { theta } )$ . We will use the term objective function to refer generically to a function we want to maximize or minimize. An algorithm that can find an optimum of an objective function is often called a solver. \nIn the rest of this chapter, we discuss different kinds of solvers for different kinds of objective functions, with a focus on methods used in the machine learning community. For more details on optimization, please consult some of the many excellent textbooks, such as [KW19b; BV04; NW06; Ber15; Ber16] as well as various review articles, such as [BCN18; Sun+19b; PPS18; Pey20]. \n8.1.1 Local vs global optimization \nA point that satisfies Equation (8.1) is called a global optimum. Finding such a point is called global optimization. \nIn general, finding global optima is computationally intractable [Neu04]. In such cases, we will just try to find a local optimum. For continuous problems, this is defined to be a point $pmb { theta } ^ { * }$ which has lower (or equal) cost than “nearby” points. Formally, we say $pmb { theta } ^ { * }$ is a local minimum if",
        "chapter": "I Foundations",
        "section": "Linear Algebra",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "8 Optimization \nParts of this chapter were written by Frederik Kunstner, Si Yi Meng, Aaron Mishkin, Sharan Vaswani, and Mark Schmidt. \n8.1 Introduction \nWe saw in Chapter 4 that the core problem in machine learning is parameter estimation (aka model fitting). This requires solving an optimization problem, where we try to find the values for a set of variables $pmb theta in Theta$ , that minimize a scalar-valued loss function or cost function $mathcal { L } : Theta to mathbb { R }$ : \nWe will assume that the parameter space is given by $Theta subseteq mathbb { R } ^ { D }$ , where $D$ is the number of variables being optimized over. Thus we are focusing on continuous optimization, rather than discrete optimization. \nIf we want to maximize a score function or reward function $R ( pmb theta )$ , we can equivalently minimize $mathcal { L } ( pmb { theta } ) = - R ( pmb { theta } )$ . We will use the term objective function to refer generically to a function we want to maximize or minimize. An algorithm that can find an optimum of an objective function is often called a solver. \nIn the rest of this chapter, we discuss different kinds of solvers for different kinds of objective functions, with a focus on methods used in the machine learning community. For more details on optimization, please consult some of the many excellent textbooks, such as [KW19b; BV04; NW06; Ber15; Ber16] as well as various review articles, such as [BCN18; Sun+19b; PPS18; Pey20]. \n8.1.1 Local vs global optimization \nA point that satisfies Equation (8.1) is called a global optimum. Finding such a point is called global optimization. \nIn general, finding global optima is computationally intractable [Neu04]. In such cases, we will just try to find a local optimum. For continuous problems, this is defined to be a point $pmb { theta } ^ { * }$ which has lower (or equal) cost than “nearby” points. Formally, we say $pmb { theta } ^ { * }$ is a local minimum if \nA local minimum could be surrounded by other local minima with the same objective value; this is known as a flat local minimum. A point is said to be a strict local minimum if its cost is strictly lower than those of neighboring points: \nWe can define a (strict) local maximum analogously. See Figure 8.1a for an illustration. \nA final note on terminology; if an algorithm is guaranteed to converge to a stationary point from any starting point, it is called globally convergent. However, this does not mean (rather confusingly) that it will converge to a global optimum; instead, it just means it will converge to some stationary point. \n8.1.1.1 Optimality conditions for local vs global optima \nFor continuous, twice differentiable functions, we can precisely characterize the points which correspond to local minima. Let $begin{array} { r }  begin{array} { r l } { mathbf { boldsymbol { mathsf { sigma } } } } & { { } mathbf { boldsymbol { mathsf { sigma } } } mathbf { boldsymbol { mathsf { sigma } } } end{array} } end{array}$ be the gradient vector, and $mathbf { H } ( pmb theta ) = nabla ^ { 2 } mathcal { L } ( pmb theta )$ be the Hessian matrix. (See Section 7.8 for a refresher on these concepts, if necessary.) Consider a point $pmb { theta } ^ { ast } in mathbb { R } ^ { D }$ , and let $g ^ { * } = g ( theta ) | _ { theta ^ { * } }$ be the gradient at that point, and $mathbf { H } ^ { * } = mathbf { H } ( pmb { theta } ) | _ { pmb { theta } ^ { * } }$ be the corresponding Hessian. One can show that the following conditions characterize every local minimum: \n• Necessary condition: If $pmb { theta } ^ { * }$ is a local minimum, then we must have $mathbf { boldsymbol { g } } ^ { * } = mathbf { 0 }$ (i.e., $theta ^ { * }$ must be a stationary point), and $mathbf { H } ^ { * }$ must be positive semi-definite. • Sufficient condition: If $mathbf { boldsymbol { g } } ^ { * } = mathbf { 0 }$ and $mathbf { H } ^ { * }$ is positive definite, then $theta ^ { * }$ is a local optimum. \nTo see why the first condition is necessary, suppose we were at a point $pmb { theta } ^ { * }$ at which the gradient is non-zero: at such a point, we could decrease the function by following the negative gradient a small distance, so this would not be optimal. So the gradient must be zero. (In the case of nonsmooth \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 functions, the necessary condition is that the zero is a local subgradient at the minimum.) To see why a zero gradient is not sufficient, note that the stationary point could be a local minimum, maximum or saddle point, which is a point where some directions point downhill, and some uphill (see Figure 8.1b). More precisely, at a saddle point, the eigenvalues of the Hessian will be both positive and negative. However, if the Hessian at a point is positive semi-definite, then some directions may point uphill, while others are flat. Moreover, if the Hessian is strictly positive definite, then we are at the bottom of a “bowl”, and all directions point uphill, which is sufficient for this to be a minimum. \n\n8.1.2 Constrained vs unconstrained optimization \nIn unconstrained optimization, we define the optimization task as finding any value in the parameter space $Theta$ that minimizes the loss. However, we often have a set of constraints on the allowable values. It is standard to partition the set of constraints $boldsymbol { mathcal { C } }$ into inequality constraints, $g _ { j } ( pmb { theta } ) leq 0$ for $j in mathcal { I }$ and equality constraints, $h _ { k } ( pmb theta ) = 0$ for $k in mathcal { E }$ . For example, we can represent a sum-to-one constraint as an equality constraint $begin{array} { r } { h ( pmb { theta } ) = ( 1 - sum _ { i = 1 } ^ { D } theta _ { i } ) = 0 } end{array}$ , and we can represent a nonnegativity constraint on the parameters by using $D$ inequality constraints of the form $g _ { i } ( pmb { theta } ) = - theta _ { i } le 0$ \nWe define the feasible set as the subset of the parameter space that satisfies the constraints: \nOur constrained optimization problem now becomes \nIf $mathcal { C } = mathbb { R } ^ { D }$ , it is called unconstrained optimization. \nThe addition of constraints can change the number of optima of a function. For example, a function that was previously unbounded (and hence had no well-defined global maximum or minimum) can “acquire” multiple maxima or minima when we add constraints, as illustrated in Figure 8.2. However, if we add too many constraints, we may find that the feasible set becomes empty. The task of finding any point (regardless of its cost) in the feasible set is called a feasibility problem; this can be a hard subproblem in itself. \nA common strategy for solving constrained problems is to create penalty terms that measure how much we violate each constraint. We then add these terms to the objective and solve an unconstrained optimization problem. The Lagrangian is a special case of such a combined objective (see Section 8.5 for details). \n8.1.3 Convex vs nonconvex optimization \nIn convex optimization, we require the objective to be a convex function defined over a convex set (we define these terms below). In such problems, every local minimum is also a global minimum. Thus many models are designed so that their training objectives are convex. \n8.1.3.1 Convex sets \nWe say $boldsymbol { S }$ is a convex set if, for any ${ pmb x } , { pmb x } ^ { prime } in mathcal { S }$ , we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Introduction",
        "subsubsection": "Local vs global optimization"
    },
    {
        "content": "8.1.2 Constrained vs unconstrained optimization \nIn unconstrained optimization, we define the optimization task as finding any value in the parameter space $Theta$ that minimizes the loss. However, we often have a set of constraints on the allowable values. It is standard to partition the set of constraints $boldsymbol { mathcal { C } }$ into inequality constraints, $g _ { j } ( pmb { theta } ) leq 0$ for $j in mathcal { I }$ and equality constraints, $h _ { k } ( pmb theta ) = 0$ for $k in mathcal { E }$ . For example, we can represent a sum-to-one constraint as an equality constraint $begin{array} { r } { h ( pmb { theta } ) = ( 1 - sum _ { i = 1 } ^ { D } theta _ { i } ) = 0 } end{array}$ , and we can represent a nonnegativity constraint on the parameters by using $D$ inequality constraints of the form $g _ { i } ( pmb { theta } ) = - theta _ { i } le 0$ \nWe define the feasible set as the subset of the parameter space that satisfies the constraints: \nOur constrained optimization problem now becomes \nIf $mathcal { C } = mathbb { R } ^ { D }$ , it is called unconstrained optimization. \nThe addition of constraints can change the number of optima of a function. For example, a function that was previously unbounded (and hence had no well-defined global maximum or minimum) can “acquire” multiple maxima or minima when we add constraints, as illustrated in Figure 8.2. However, if we add too many constraints, we may find that the feasible set becomes empty. The task of finding any point (regardless of its cost) in the feasible set is called a feasibility problem; this can be a hard subproblem in itself. \nA common strategy for solving constrained problems is to create penalty terms that measure how much we violate each constraint. We then add these terms to the objective and solve an unconstrained optimization problem. The Lagrangian is a special case of such a combined objective (see Section 8.5 for details). \n8.1.3 Convex vs nonconvex optimization \nIn convex optimization, we require the objective to be a convex function defined over a convex set (we define these terms below). In such problems, every local minimum is also a global minimum. Thus many models are designed so that their training objectives are convex. \n8.1.3.1 Convex sets \nWe say $boldsymbol { S }$ is a convex set if, for any ${ pmb x } , { pmb x } ^ { prime } in mathcal { S }$ , we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Introduction",
        "subsubsection": "Constrained vs unconstrained optimization"
    },
    {
        "content": "8.1.2 Constrained vs unconstrained optimization \nIn unconstrained optimization, we define the optimization task as finding any value in the parameter space $Theta$ that minimizes the loss. However, we often have a set of constraints on the allowable values. It is standard to partition the set of constraints $boldsymbol { mathcal { C } }$ into inequality constraints, $g _ { j } ( pmb { theta } ) leq 0$ for $j in mathcal { I }$ and equality constraints, $h _ { k } ( pmb theta ) = 0$ for $k in mathcal { E }$ . For example, we can represent a sum-to-one constraint as an equality constraint $begin{array} { r } { h ( pmb { theta } ) = ( 1 - sum _ { i = 1 } ^ { D } theta _ { i } ) = 0 } end{array}$ , and we can represent a nonnegativity constraint on the parameters by using $D$ inequality constraints of the form $g _ { i } ( pmb { theta } ) = - theta _ { i } le 0$ \nWe define the feasible set as the subset of the parameter space that satisfies the constraints: \nOur constrained optimization problem now becomes \nIf $mathcal { C } = mathbb { R } ^ { D }$ , it is called unconstrained optimization. \nThe addition of constraints can change the number of optima of a function. For example, a function that was previously unbounded (and hence had no well-defined global maximum or minimum) can “acquire” multiple maxima or minima when we add constraints, as illustrated in Figure 8.2. However, if we add too many constraints, we may find that the feasible set becomes empty. The task of finding any point (regardless of its cost) in the feasible set is called a feasibility problem; this can be a hard subproblem in itself. \nA common strategy for solving constrained problems is to create penalty terms that measure how much we violate each constraint. We then add these terms to the objective and solve an unconstrained optimization problem. The Lagrangian is a special case of such a combined objective (see Section 8.5 for details). \n8.1.3 Convex vs nonconvex optimization \nIn convex optimization, we require the objective to be a convex function defined over a convex set (we define these terms below). In such problems, every local minimum is also a global minimum. Thus many models are designed so that their training objectives are convex. \n8.1.3.1 Convex sets \nWe say $boldsymbol { S }$ is a convex set if, for any ${ pmb x } , { pmb x } ^ { prime } in mathcal { S }$ , we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThat is, if we draw a line from $_ { x }$ to $mathbf { { x } ^ { prime } }$ , all points on the line lie inside the set. See Figure 8.3 for some illustrations of convex and non-convex sets. \n8.1.3.2 Convex functions \nWe say $f$ is a convex function if its epigraph (the set of points above the function, illustrated in Figure 8.4a) defines a convex set. Equivalently, a function $f ( { pmb x } )$ is called convex if it is defined on a convex set and if, for any $mathbf { { boldsymbol { x } } } , mathbf { { boldsymbol { y } } } in S$ , and for any $0 leq lambda leq 1$ , we have \nSee Figure 8.5(a) for a 1d example of a convex function. A function is called strictly convex if the inequality is strict. A function $f ( { pmb x } )$ is concave if $- f ( { pmb x } )$ is convex, and strictly concave if $- f ( { pmb x } )$ is strictly convex. See Figure 8.5(b) for a 1d example of a function that is neither convex nor concave. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHere are some examples of 1d convex functions: \n8.1.3.3 Characterization of convex functions \nIntuitively, a convex function is shaped like a bowl. Formally, one can prove the following important result: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTheorem 8.1.1. Suppose $f : mathbb { R } ^ { n }  mathbb { R }$ is twice differentiable over its domain. Then $f$ is convex iff ${ bf H } = nabla ^ { 2 } f ( { pmb x } )$ is positive semi definite (Section 7.1.5.3) for all $pmb { x } in operatorname { d o m } ( f )$ . Furthermore, $f$ is strictly convex if $mathbf { H }$ is positive definite. \nFor example, consider the quadratic form \nThis is convex if $mathbf { A }$ is positive semi definite, and is strictly convex if $mathbf { A }$ is positive definite. It is neither convex nor concave if $mathbf { A }$ has eigenvalues of mixed sign. See Figure 8.6. \n8.1.3.4 Strongly convex functions \nWe say a function $f$ is strongly convex with parameter $m > 0$ if the following holds for all $_ { x }$ , $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ in $f$ ’s domain: \nA strongly convex function is also strictly convex, but not vice versa. \nIf the function $f$ is twice continuously differentiable, then it is strongly convex with parameter $m$ if and only if $nabla ^ { 2 } f ( { pmb x } ) succeq m { bf I }$ for all $_ { x }$ in the domain, where $mathbf { I }$ is the identity and $nabla ^ { 2 } f$ is the Hessian matrix, and the inequality $succeq$ means that $nabla ^ { 2 } f ( { pmb x } ) - m { bf I }$ is positive semi-definite. This is equivalent \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 to requiring that the minimum eigenvalue of $nabla ^ { 2 } f ( { pmb x } )$ be at least $m$ for all $_ { x }$ . If the domain is just the real line, then $nabla ^ { 2 } f ( x )$ is just the second derivative $f ^ { prime prime } ( x )$ , so the condition becomes $f ^ { prime prime } ( x ) geq m$ . If $m = 0$ , then this means the Hessian is positive semidefinite (or if the domain is the real line, it means that $f ^ { prime prime } ( x ) geq 0$ ), which implies the function is convex, and perhaps strictly convex, but not strongly convex. \n\nThe distinction between convex, strictly convex, and strongly convex is rather subtle. To better understand this, consider the case where $f$ is twice continuously differentiable and the domain is the real line. Then we can characterize the differences as follows: \n$bullet$ $f$ is convex if and only if $f ^ { prime prime } ( x ) geq 0$ for all $x$ .   \n• $f$ is strictly convex if $f ^ { prime prime } ( x ) > 0$ for all $x$ (note: this is sufficient, but not necessary).   \n• $f$ is strongly convex if and only if $f ^ { prime prime } ( x ) geq m > 0$ for all $x$ .   \nNote that it can be shown that a function $f$ is strongly convex with parameter $m$ iff the function \nis convex. \n8.1.4 Smooth vs nonsmooth optimization \nIn smooth optimization, the objective and constraints are continuously differentiable functions. For smooth functions, we can quantify the degree of smoothness using the Lipschitz constant. In the 1d case, this is defined as any constant $L geq 0$ such that, for all real $x _ { 1 }$ and $x _ { 2 }$ , we have \nThis is illustrated in Figure 8.8: for a given constant $L$ , the function output cannot change by more than $L$ if we change the function input by 1 unit. This can be generalized to vector inputs using a suitable norm. \nIn nonsmooth optimization, there are at least some points where the gradient of the objective function or the constraints is not well-defined. See Figure 8.7 for an example. In some optimization \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Introduction",
        "subsubsection": "Convex vs nonconvex optimization"
    },
    {
        "content": "The distinction between convex, strictly convex, and strongly convex is rather subtle. To better understand this, consider the case where $f$ is twice continuously differentiable and the domain is the real line. Then we can characterize the differences as follows: \n$bullet$ $f$ is convex if and only if $f ^ { prime prime } ( x ) geq 0$ for all $x$ .   \n• $f$ is strictly convex if $f ^ { prime prime } ( x ) > 0$ for all $x$ (note: this is sufficient, but not necessary).   \n• $f$ is strongly convex if and only if $f ^ { prime prime } ( x ) geq m > 0$ for all $x$ .   \nNote that it can be shown that a function $f$ is strongly convex with parameter $m$ iff the function \nis convex. \n8.1.4 Smooth vs nonsmooth optimization \nIn smooth optimization, the objective and constraints are continuously differentiable functions. For smooth functions, we can quantify the degree of smoothness using the Lipschitz constant. In the 1d case, this is defined as any constant $L geq 0$ such that, for all real $x _ { 1 }$ and $x _ { 2 }$ , we have \nThis is illustrated in Figure 8.8: for a given constant $L$ , the function output cannot change by more than $L$ if we change the function input by 1 unit. This can be generalized to vector inputs using a suitable norm. \nIn nonsmooth optimization, there are at least some points where the gradient of the objective function or the constraints is not well-defined. See Figure 8.7 for an example. In some optimization \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nproblems, we can partition the objective into a part that only contains smooth terms, and a part that contains the nonsmooth terms: \nwhere $mathcal { L } _ { s }$ is smooth (differentiable), and $mathcal { L } _ { r }$ is nonsmooth (“rough”). This is often referred to as a composite objective. In machine learning applications, $mathcal { L } _ { s }$ is usually the training set loss, and $mathcal { L } _ { r }$ is a regularizer, such as the $ell _ { 1 }$ norm of $pmb theta$ . This composite structure can be exploited by various algorithms. \n8.1.4.1 Subgradients \nIn this section, we generalize the notion of a derivative to work with functions which have local discontinuities. In particular, for a convex function of several variables, $f : mathbb { R } ^ { n }  mathbb { R }$ , we say that $mathbf { boldsymbol { g } } in mathbb { R } ^ { n }$ is a subgradient of $f$ at $pmb { x } in operatorname { d o m } ( f )$ if for all $z in operatorname { d o m } ( f )$ , \nNote that a subgradient can exist even when $f$ is not differentiable at a point, as shown in Figure 8.9. A function $f$ is called subdifferentiable at $_ { x }$ if there is at least one subgradient at $_ { x }$ . The set of such subgradients is called the subdifferential of $f$ at $_ { x }$ , and is denoted $partial f ( { pmb x } )$ . \nFor example, consider the absolute value function $f ( x ) = left| x right|$ . Its subdifferential is given by \nwhere the notation $[ - 1 , 1 ]$ means any value between -1 and $^ { 1 }$ inclusive. See Figure 8.10 for an illustration. \n8.2 First-order methods \nIn this section, we consider iterative optimization methods that leverage first-order derivatives of the objective function, i.e., they compute which directions point “downhill”, but they ignore curvature \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Introduction",
        "subsubsection": "Smooth vs nonsmooth optimization"
    },
    {
        "content": "8.2.1 Descent direction \nWe say that a direction $textbf { em d }$ is a descent direction if there is a small enough (but nonzero) amount $eta$ we can move in direction $textbf { em d }$ and be guaranteed to decrease the function value. Formally, we require that there exists an $eta _ { mathrm { m a x } } > 0$ such that \nfor all $0 < eta < eta _ { mathrm { m a x } }$ . The gradient at the current iterate, $pmb { theta } _ { t }$ , is given by \nThis points in the direction of maximal increase in $f$ , so the negative gradient is a descent direction. It can be shown that any direction $textbf { em d }$ is also a descent direction if the angle $theta$ between $textbf { em d }$ and $mathbf { - } pmb { g } _ { t }$ is less than 90 degrees and satisfies \nIt seems that the best choice would be to pick $d _ { t } = - g _ { t }$ . This is known as the direction of steepest descent. However, this can be quite slow. We consider faster versions later. \n8.2.2 Step size (learning rate) \nIn machine learning, the sequence of step sizes ${ eta _ { t } }$ is called the learning rate schedule. There are several widely used methods for picking this, some of which we discuss below. (See also Section 8.4.3, where we discuss schedules for stochastic optimization.) \n8.2.2.1 Constant step size \nThe simplest method is to use a constant step size, $eta _ { t } = eta$ . However, if it is too large, the method may fail to converge, and if it is too small, the method will converge but very slowly. For example, consider the convex function \nLet us pick as our descent direction $d _ { t } = - g _ { t }$ . Figure 8.11 shows what happens if we use this descent direction with a fixed step size, starting from $( 0 , 0 )$ . In Figure 8.11(a), we use a small step size of $eta = 0 . 1$ ; we see that the iterates move slowly along the valley. In Figure 8.11(b), we use a larger step size $eta = 0 . 6$ ; we see that the iterates start oscillating up and down the sides of the valley and never converge to the optimum, even though this is a convex problem. \nIn some cases, we can derive a theoretical upper bound on the maximum step size we can use. For example, consider a quadratic objective, $begin{array} { r } { mathcal { L } ( pmb { theta } ) = frac { 1 } { 2 } pmb { theta } ^ { mathrm { 1 } } mathbf { A } pmb { theta } + b ^ { mathrm { 1 } } pmb { theta } + c } end{array}$ with $mathbf { A } succeq mathbf { 0 }$ . One can show that steepest descent will have global convergence iff the step size satisfies \nwhere $lambda _ { operatorname* { m a x } } ( mathbf { A } )$ is the largest eigenvalue of A. The intuitive reason for this can be understood by thinking of a ball rolling down a valley. We want to make sure it doesn’t take a step that is larger than the slope of the steepest direction, which is what the largest eigenvalue measures (see Section 3.2.2). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "First-order methods",
        "subsubsection": "Descent direction"
    },
    {
        "content": "8.2.1 Descent direction \nWe say that a direction $textbf { em d }$ is a descent direction if there is a small enough (but nonzero) amount $eta$ we can move in direction $textbf { em d }$ and be guaranteed to decrease the function value. Formally, we require that there exists an $eta _ { mathrm { m a x } } > 0$ such that \nfor all $0 < eta < eta _ { mathrm { m a x } }$ . The gradient at the current iterate, $pmb { theta } _ { t }$ , is given by \nThis points in the direction of maximal increase in $f$ , so the negative gradient is a descent direction. It can be shown that any direction $textbf { em d }$ is also a descent direction if the angle $theta$ between $textbf { em d }$ and $mathbf { - } pmb { g } _ { t }$ is less than 90 degrees and satisfies \nIt seems that the best choice would be to pick $d _ { t } = - g _ { t }$ . This is known as the direction of steepest descent. However, this can be quite slow. We consider faster versions later. \n8.2.2 Step size (learning rate) \nIn machine learning, the sequence of step sizes ${ eta _ { t } }$ is called the learning rate schedule. There are several widely used methods for picking this, some of which we discuss below. (See also Section 8.4.3, where we discuss schedules for stochastic optimization.) \n8.2.2.1 Constant step size \nThe simplest method is to use a constant step size, $eta _ { t } = eta$ . However, if it is too large, the method may fail to converge, and if it is too small, the method will converge but very slowly. For example, consider the convex function \nLet us pick as our descent direction $d _ { t } = - g _ { t }$ . Figure 8.11 shows what happens if we use this descent direction with a fixed step size, starting from $( 0 , 0 )$ . In Figure 8.11(a), we use a small step size of $eta = 0 . 1$ ; we see that the iterates move slowly along the valley. In Figure 8.11(b), we use a larger step size $eta = 0 . 6$ ; we see that the iterates start oscillating up and down the sides of the valley and never converge to the optimum, even though this is a convex problem. \nIn some cases, we can derive a theoretical upper bound on the maximum step size we can use. For example, consider a quadratic objective, $begin{array} { r } { mathcal { L } ( pmb { theta } ) = frac { 1 } { 2 } pmb { theta } ^ { mathrm { 1 } } mathbf { A } pmb { theta } + b ^ { mathrm { 1 } } pmb { theta } + c } end{array}$ with $mathbf { A } succeq mathbf { 0 }$ . One can show that steepest descent will have global convergence iff the step size satisfies \nwhere $lambda _ { operatorname* { m a x } } ( mathbf { A } )$ is the largest eigenvalue of A. The intuitive reason for this can be understood by thinking of a ball rolling down a valley. We want to make sure it doesn’t take a step that is larger than the slope of the steepest direction, which is what the largest eigenvalue measures (see Section 3.2.2). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nMore generally, setting $eta < 2 / L$ , where $L$ is the Lipschitz constant of the gradient (Section 8.1.4), ensures convergence. Since this constant is generally unknown, we usually need to adapt the step size, as we discuss below. \n8.2.2.2 Line search \nThe optimal step size can be found by finding the value that maximally decreases the objective along the chosen direction by solving the 1d minimization problem \nThis is known as line search, since we are searching along the line defined by $scriptstyle d _ { t }$ . \nIf the loss is convex, this subproblem is also convex, because $phi _ { t } ( eta ) = mathcal { L } ( pmb { theta } _ { t } + eta pmb { d } _ { t } )$ is a convex function of an affine function of $eta$ , for fixed $pmb { theta } _ { t }$ and $scriptstyle d _ { t }$ . For example, consider the quadratic loss \nComputing the derivative of $phi$ gives \nSolving for $begin{array} { r } { frac { d phi ( eta ) } { d eta } = 0 } end{array}$ gives \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nUsing the optimal step size is known as exact line search. However, it is not usually necessary to be so precise. There are several methods, such as the Armijo backtracking method, that try to ensure sufficient reduction in the objective function without spending too much time trying to solve Equation (8.21). In particular, we can start with the current stepsize (or some maximum value), and then reduce it by a factor $0 < beta < 1$ at each step until we satisfy the following condition, known as the Armijo-Goldstein test: \nwhere $c in [ 0 , 1 ]$ is a constant, typically $c = 1 0 ^ { - 4 }$ . In practice, the initialization of the line-search and how to backtrack can significantly affect performance. See [NW06, Sec 3.1] for details. \n8.2.3 Convergence rates \nWe want to find optimization algorithms that converge quickly to a (local) optimum. For certain convex problems, with a gradient with bounded Lipschitz constant, one can show that gradient descent converges at a linear rate. This means that there exists a number $0 < mu < 1$ such that \nHere $mu$ is called the rate of convergence. \nFor some simple problems, we can derive the convergence rate explicitly, For example, consider a quadratic objective $begin{array} { r } { mathcal { L } ( pmb { theta } ) = frac { 1 } { 2 } pmb { theta } ^ { top } mathbf { A } pmb { theta } + b ^ { top } pmb { theta } + c } end{array}$ with $mathbf A succ 0$ . Suppose we use steepest descent with exact line search. One can show (see e.g., [Ber15]) that the convergence rate is given by \nwhere $lambda _ { mathrm { m a x } }$ is the largest eigenvalue of $mathbf { A }$ and $lambda _ { mathrm { m i n } }$ is the smallest eigenvalue. We can rewrite this as $begin{array} { r } { mu = left( frac { kappa - 1 } { kappa _ { - } + 1 } right) ^ { 2 } } end{array}$ , where $begin{array} { r } { kappa = frac { lambda _ { mathrm { m a x } } } { lambda _ { mathrm { m i n } } } } end{array}$ is the condition number of A. Intuitively, the condition number measures how “skewed” the space is, in the sense of being far from a symmetrical “bowl”. (See Section 7.1.4.4 for more information on condition numbers.) \nFigure 8.12 illustrates the effect of the condition number on the convergence rate. On the left we show an example where $mathbf { A } = lfloor 2 0 , 5 ; 5 , 2 rfloor$ , $pmb { b } = [ - 1 4 ; - 6 ]$ and $c = 1 0$ , so $kappa ( mathbf { A } ) = 3 0 . 2 3 4$ . On the right we show an example where $mathbf { A } = lfloor 2 0 , 5 ; 5 , 1 6 rfloor$ , $pmb { b } = [ - 1 4 ; - 6 ]$ and $c = 1 0$ , so $kappa ( mathbf { A } ) = 1 . 8 5 4 1$ . We see that steepest descent converges much more quickly for the problem with the smaller condition number. \nIn the more general case of non-quadratic functions, the objective will often be locally quadratic around a local optimum. Hence the convergence rate depends on the condition number of the Hessian, $kappa ( { bf H } )$ , at that point. We can often improve the convergence speed by optimizing a surrogate objective (or model) at each step which has a Hessian that is close to the Hessian of the objective function as we discuss in Section 8.3. \nAlthough line search works well, we see from Figure 8.12 that the path of steepest descent with an exact line-search exhibits a characteristic zig-zag behavior, which is inefficient. This problem can be overcome using a method called conjugate gradient descent (see e.g., [She94]). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "First-order methods",
        "subsubsection": "Step size (learning rate)"
    },
    {
        "content": "Using the optimal step size is known as exact line search. However, it is not usually necessary to be so precise. There are several methods, such as the Armijo backtracking method, that try to ensure sufficient reduction in the objective function without spending too much time trying to solve Equation (8.21). In particular, we can start with the current stepsize (or some maximum value), and then reduce it by a factor $0 < beta < 1$ at each step until we satisfy the following condition, known as the Armijo-Goldstein test: \nwhere $c in [ 0 , 1 ]$ is a constant, typically $c = 1 0 ^ { - 4 }$ . In practice, the initialization of the line-search and how to backtrack can significantly affect performance. See [NW06, Sec 3.1] for details. \n8.2.3 Convergence rates \nWe want to find optimization algorithms that converge quickly to a (local) optimum. For certain convex problems, with a gradient with bounded Lipschitz constant, one can show that gradient descent converges at a linear rate. This means that there exists a number $0 < mu < 1$ such that \nHere $mu$ is called the rate of convergence. \nFor some simple problems, we can derive the convergence rate explicitly, For example, consider a quadratic objective $begin{array} { r } { mathcal { L } ( pmb { theta } ) = frac { 1 } { 2 } pmb { theta } ^ { top } mathbf { A } pmb { theta } + b ^ { top } pmb { theta } + c } end{array}$ with $mathbf A succ 0$ . Suppose we use steepest descent with exact line search. One can show (see e.g., [Ber15]) that the convergence rate is given by \nwhere $lambda _ { mathrm { m a x } }$ is the largest eigenvalue of $mathbf { A }$ and $lambda _ { mathrm { m i n } }$ is the smallest eigenvalue. We can rewrite this as $begin{array} { r } { mu = left( frac { kappa - 1 } { kappa _ { - } + 1 } right) ^ { 2 } } end{array}$ , where $begin{array} { r } { kappa = frac { lambda _ { mathrm { m a x } } } { lambda _ { mathrm { m i n } } } } end{array}$ is the condition number of A. Intuitively, the condition number measures how “skewed” the space is, in the sense of being far from a symmetrical “bowl”. (See Section 7.1.4.4 for more information on condition numbers.) \nFigure 8.12 illustrates the effect of the condition number on the convergence rate. On the left we show an example where $mathbf { A } = lfloor 2 0 , 5 ; 5 , 2 rfloor$ , $pmb { b } = [ - 1 4 ; - 6 ]$ and $c = 1 0$ , so $kappa ( mathbf { A } ) = 3 0 . 2 3 4$ . On the right we show an example where $mathbf { A } = lfloor 2 0 , 5 ; 5 , 1 6 rfloor$ , $pmb { b } = [ - 1 4 ; - 6 ]$ and $c = 1 0$ , so $kappa ( mathbf { A } ) = 1 . 8 5 4 1$ . We see that steepest descent converges much more quickly for the problem with the smaller condition number. \nIn the more general case of non-quadratic functions, the objective will often be locally quadratic around a local optimum. Hence the convergence rate depends on the condition number of the Hessian, $kappa ( { bf H } )$ , at that point. We can often improve the convergence speed by optimizing a surrogate objective (or model) at each step which has a Hessian that is close to the Hessian of the objective function as we discuss in Section 8.3. \nAlthough line search works well, we see from Figure 8.12 that the path of steepest descent with an exact line-search exhibits a characteristic zig-zag behavior, which is inefficient. This problem can be overcome using a method called conjugate gradient descent (see e.g., [She94]). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n8.2.4 Momentum methods \nGradient descent can move very slowly along flat regions of the loss landscape, as we illustrated in Figure 8.11. We discuss some solutions to this below. \n8.2.4.1 Momentum \nOne simple heuristic, known as the heavy ball or momentum method [Ber99], is to move faster along directions that were previously good, and to slow down along directions where the gradient has suddenly changed, just like a ball rolling downhill. This can be implemented as follows: \nwhere ${ mathbf { } } ^ { mathbf { mathit { m } } _ { t } }$ is the momentum (mass times velocity) and $0 < beta < 1$ . A typical value of $beta$ is 0.9. For $beta = 0$ , the method reduces to gradient descent. \nWe see that ${ mathbf { } } m _ { t }$ is like an exponentially weighted moving average of the past gradients (see Section 4.4.2.2): \nIf all the past gradients are a constant, say $pmb { g }$ , this simplifies to \nThe scaling factor is a geometric series, whose infinite sum is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "First-order methods",
        "subsubsection": "Convergence rates"
    },
    {
        "content": "8.2.4 Momentum methods \nGradient descent can move very slowly along flat regions of the loss landscape, as we illustrated in Figure 8.11. We discuss some solutions to this below. \n8.2.4.1 Momentum \nOne simple heuristic, known as the heavy ball or momentum method [Ber99], is to move faster along directions that were previously good, and to slow down along directions where the gradient has suddenly changed, just like a ball rolling downhill. This can be implemented as follows: \nwhere ${ mathbf { } } ^ { mathbf { mathit { m } } _ { t } }$ is the momentum (mass times velocity) and $0 < beta < 1$ . A typical value of $beta$ is 0.9. For $beta = 0$ , the method reduces to gradient descent. \nWe see that ${ mathbf { } } m _ { t }$ is like an exponentially weighted moving average of the past gradients (see Section 4.4.2.2): \nIf all the past gradients are a constant, say $pmb { g }$ , this simplifies to \nThe scaling factor is a geometric series, whose infinite sum is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThus in the limit, we multiply the gradient by $1 / ( 1 - beta )$ . For example, if $beta = 0 . 9$ , we scale the gradient up by 10. \nSince we update the parameters using the gradient average ${ mathbf { } } ^ { m } t - 1$ , rather than just the most recent gradient, $mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { mathbf { delta } _ { delta } _ { mathbf { delta } _ { delta } _ { mathbf { delta } _ { delta } _ { mathbf { delta } _ { delta } _ { mathbf { delta } _ { delta } _ { mathbf delta } _ { delta } } } } } } } } } } } } } }$ , we see that past gradients can exhibit some influence on the present. Furthermore, when momentum is combined with SGD, discussed in Section 8.4, we will see that it can simulate the effects of a larger minibatch, without the computational cost. \n8.2.4.2 Nesterov momentum \nOne problem with the standard momentum method is that it may not slow down enough at the bottom of a valley, causing oscillation. The Nesterov accelerated gradient method of [Nes04] instead modifies the gradient descent to include an extrapolation step, as follows: \nThis is essentially a form of one-step “look ahead”, that can reduce the amount of oscillation, as illustrated in Figure 8.13. \nNesterov accelerated gradient can also be rewritten in the same format as standard momentum. In this case, the momentum term is updated using the gradient at the predicted new location, \nThis explains why the Nesterov accelerated gradient method is sometimes called Nesterov momentum. It also shows how this method can be faster than standard momentum: the momentum vector is already roughly pointing in the right direction, so measuring the gradient at the new location, $pmb { theta } _ { t } + beta pmb { m } _ { t }$ , rather than the current location, $pmb { theta } _ { t }$ , can be more accurate. \nThe Nesterov accelerated gradient method is provably faster than steepest descent for convex functions when $beta$ and $eta _ { t }$ are chosen appropriately. It is called “accelerated” because of this improved \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 convergence rate, which is optimal for gradient-based methods using only first-order information when the objective function is convex and has Lipschitz-continuous gradients. In practice, however, using Nesterov momentum can be slower than steepest descent, and can even unstable if $beta$ or $eta _ { t }$ are misspecified. \n\n8.3 Second-order methods \nOptimization algorithms that only use the gradient are called first-order methods. They have the advantage that the gradient is cheap to compute and to store, but they do not model the curvature of the space, and hence they can be slow to converge, as we have seen in Figure 8.12. Second-order optimization methods incorporate curvature in various ways (e.g., via the Hessian), which may yield faster convergence. We discuss some of these methods below. \n8.3.1 Newton’s method \nThe classic second-order method is Newton’s method. This consists of updates of the form \nwhere \nis assumed to be positive-definite to ensure the update is well-defined. The pseudo-code for Newton’s method is given in Algorithm 1. The intuition for why this is faster than gradient descent is that the matrix inverse ${ bf H } ^ { - 1 }$ “undoes” any skew in the local curvature, converting a topology like Figure 8.12a to one like Figure 8.12b. \n1 Initialize $pmb { theta } _ { 0 }$   \n2 for $t = 1 , 2 , ldots$ . until convergence do   \n3 Evaluate $begin{array} { r } { pmb { g } _ { t } = nabla mathcal { L } ( pmb { theta } _ { t } ) } end{array}$   \n4 Evaluate $mathbf { H } _ { t } = nabla ^ { 2 } mathcal { L } ( pmb { theta } _ { t } )$   \n5 Solve $mathbf { H } _ { t } pmb { d } _ { t } = - pmb { g } _ { t }$ for $scriptstyle d _ { t }$   \n6 Use line search to find stepsize $eta _ { t }$ along $scriptstyle d _ { t }$   \n7 $pmb { theta } _ { t + 1 } = pmb { theta } _ { t } + eta _ { t } pmb { d } _ { t }$ \nThis algorithm can be derived as follows. Consider making a second-order Taylor series approximation of ${ mathcal { L } } ( theta )$ around $pmb { theta } _ { t }$ : \nThe minimum of ${ mathcal { L } } _ { mathrm { q u a d } }$ is at \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "First-order methods",
        "subsubsection": "Momentum methods"
    },
    {
        "content": "8.3 Second-order methods \nOptimization algorithms that only use the gradient are called first-order methods. They have the advantage that the gradient is cheap to compute and to store, but they do not model the curvature of the space, and hence they can be slow to converge, as we have seen in Figure 8.12. Second-order optimization methods incorporate curvature in various ways (e.g., via the Hessian), which may yield faster convergence. We discuss some of these methods below. \n8.3.1 Newton’s method \nThe classic second-order method is Newton’s method. This consists of updates of the form \nwhere \nis assumed to be positive-definite to ensure the update is well-defined. The pseudo-code for Newton’s method is given in Algorithm 1. The intuition for why this is faster than gradient descent is that the matrix inverse ${ bf H } ^ { - 1 }$ “undoes” any skew in the local curvature, converting a topology like Figure 8.12a to one like Figure 8.12b. \n1 Initialize $pmb { theta } _ { 0 }$   \n2 for $t = 1 , 2 , ldots$ . until convergence do   \n3 Evaluate $begin{array} { r } { pmb { g } _ { t } = nabla mathcal { L } ( pmb { theta } _ { t } ) } end{array}$   \n4 Evaluate $mathbf { H } _ { t } = nabla ^ { 2 } mathcal { L } ( pmb { theta } _ { t } )$   \n5 Solve $mathbf { H } _ { t } pmb { d } _ { t } = - pmb { g } _ { t }$ for $scriptstyle d _ { t }$   \n6 Use line search to find stepsize $eta _ { t }$ along $scriptstyle d _ { t }$   \n7 $pmb { theta } _ { t + 1 } = pmb { theta } _ { t } + eta _ { t } pmb { d } _ { t }$ \nThis algorithm can be derived as follows. Consider making a second-order Taylor series approximation of ${ mathcal { L } } ( theta )$ around $pmb { theta } _ { t }$ : \nThe minimum of ${ mathcal { L } } _ { mathrm { q u a d } }$ is at \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nSo if the quadratic approximation is a good one, we should pick ${ pmb { d } } _ { t } = - { bf { H } } _ { t } ^ { - 1 } { pmb { g } } _ { t }$ as our descent direction. See Figure 8.14(a) for an illustration. Note that, in a “pure” Newton method, we use $eta _ { t } = 1$ as our stepsize. However, we can also use linesearch to find the best stepsize; this tends to be more robust as using $eta _ { t } = 1$ may not always converge globally. \nIf we apply this method to linear regression, we get to the optimum in one step, since (as we shown in Section 11.2.2.1) we have $mathbf { H } = mathbf { X } ^ { mathsf { I } } mathbf { X }$ and $begin{array} { r } { mathbf { Delta } mathbf { g } = mathbf { X } ^ { top } mathbf { X } mathbf { Delta } w - mathbf { X } ^ { top } mathbf { Delta } y } end{array}$ , so the Newton update becomes \nwhich is the OLS estimate. However, when we apply this method to logistic regression, it may take multiple iterations to converge to the global optimum, as we discuss in Section 10.2.6. \n8.3.2 BFGS and other quasi-Newton methods \nQuasi-Newton methods, sometimes called variable metric methods, iteratively build up an approximation to the Hessian using information gleaned from the gradient vector at each step. The most common method is called BFGS (named after its simultaneous inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to the Hessian $mathbf { B } _ { t } approx mathbf { H } _ { t }$ as follows: \nThis is a rank-two update to the matrix. If $mathbf { B } _ { 0 }$ is positive-definite, and the step size $eta$ is chosen via line search satisfying both the Armijo condition in Equation (8.27) and the following curvature \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Second-order methods",
        "subsubsection": "Newton's method"
    },
    {
        "content": "So if the quadratic approximation is a good one, we should pick ${ pmb { d } } _ { t } = - { bf { H } } _ { t } ^ { - 1 } { pmb { g } } _ { t }$ as our descent direction. See Figure 8.14(a) for an illustration. Note that, in a “pure” Newton method, we use $eta _ { t } = 1$ as our stepsize. However, we can also use linesearch to find the best stepsize; this tends to be more robust as using $eta _ { t } = 1$ may not always converge globally. \nIf we apply this method to linear regression, we get to the optimum in one step, since (as we shown in Section 11.2.2.1) we have $mathbf { H } = mathbf { X } ^ { mathsf { I } } mathbf { X }$ and $begin{array} { r } { mathbf { Delta } mathbf { g } = mathbf { X } ^ { top } mathbf { X } mathbf { Delta } w - mathbf { X } ^ { top } mathbf { Delta } y } end{array}$ , so the Newton update becomes \nwhich is the OLS estimate. However, when we apply this method to logistic regression, it may take multiple iterations to converge to the global optimum, as we discuss in Section 10.2.6. \n8.3.2 BFGS and other quasi-Newton methods \nQuasi-Newton methods, sometimes called variable metric methods, iteratively build up an approximation to the Hessian using information gleaned from the gradient vector at each step. The most common method is called BFGS (named after its simultaneous inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to the Hessian $mathbf { B } _ { t } approx mathbf { H } _ { t }$ as follows: \nThis is a rank-two update to the matrix. If $mathbf { B } _ { 0 }$ is positive-definite, and the step size $eta$ is chosen via line search satisfying both the Armijo condition in Equation (8.27) and the following curvature \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \ncondition \nthen $mathbf { B } _ { t + 1 }$ will remain positive definite. The constant $c _ { 2 }$ is chosen within $( c , 1 )$ where $c$ is the tunable parameter in Equation (8.27). The two step size conditions are together known as the Wolfe conditions. We typically start with a diagonal approximation, $mathbf { B } _ { 0 } = mathbf { I }$ . Thus BFGS can be thought of as a “diagonal plus low-rank” approximation to the Hessian. \nAlternatively, BFGS can iteratively update an approximation to the inverse Hessian, $mathbf { C } _ { t } approx mathbf { H } _ { t } ^ { - 1 }$ , as follows: \nSince storing the Hessian approximation still takes $O ( D ^ { 2 } )$ space, for very large problems, one can use limited memory BFGS, or L-BFGS, where we control the rank of the approximation by only using the $M$ most recent $left( boldsymbol { s } _ { t } , boldsymbol { y } _ { t } right)$ pairs while ignoring older information. Rather than storing $mathbf { B } _ { t }$ explicitly, we just store these vectors in memory, and then approximate $mathbf { H } _ { t } ^ { - 1 } { boldsymbol { g } } _ { t }$ by performing a sequence of inner products with the stored $mathbf { boldsymbol { s } } _ { t }$ and ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla nabla mathbf  nabla nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { nabla nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf } nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ vectors. The storage requirements are therefore $O ( M D )$ . Typically choosing $M$ to be between 5–20 suffices for good performance [NW06, p177]. \nNote that sklearn uses LBFGS as its default solver for logistic regression. \n8.3.3 Trust region methods \nIf the objective function is nonconvex, then the Hessian $mathbf { H } _ { t }$ may not be positive definite, so $mathbf { } d _ { t } =$ $- mathbf { H } _ { t } ^ { - 1 } g _ { t }$ may not be a descent direction. This is illustrated in 1d in Figure 8.14(b), which shows that Newton’s method can end up in a local maximum rather than a local minimum. \nIn general, any time the quadratic approximation made by Newton’s method becomes invalid, we are in trouble. However, there is usually a local region around the current iterate where we can safely",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Second-order methods",
        "subsubsection": "BFGS and other quasi-Newton methods"
    },
    {
        "content": "condition \nthen $mathbf { B } _ { t + 1 }$ will remain positive definite. The constant $c _ { 2 }$ is chosen within $( c , 1 )$ where $c$ is the tunable parameter in Equation (8.27). The two step size conditions are together known as the Wolfe conditions. We typically start with a diagonal approximation, $mathbf { B } _ { 0 } = mathbf { I }$ . Thus BFGS can be thought of as a “diagonal plus low-rank” approximation to the Hessian. \nAlternatively, BFGS can iteratively update an approximation to the inverse Hessian, $mathbf { C } _ { t } approx mathbf { H } _ { t } ^ { - 1 }$ , as follows: \nSince storing the Hessian approximation still takes $O ( D ^ { 2 } )$ space, for very large problems, one can use limited memory BFGS, or L-BFGS, where we control the rank of the approximation by only using the $M$ most recent $left( boldsymbol { s } _ { t } , boldsymbol { y } _ { t } right)$ pairs while ignoring older information. Rather than storing $mathbf { B } _ { t }$ explicitly, we just store these vectors in memory, and then approximate $mathbf { H } _ { t } ^ { - 1 } { boldsymbol { g } } _ { t }$ by performing a sequence of inner products with the stored $mathbf { boldsymbol { s } } _ { t }$ and ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla nabla mathbf  nabla nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { nabla nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf } nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ vectors. The storage requirements are therefore $O ( M D )$ . Typically choosing $M$ to be between 5–20 suffices for good performance [NW06, p177]. \nNote that sklearn uses LBFGS as its default solver for logistic regression. \n8.3.3 Trust region methods \nIf the objective function is nonconvex, then the Hessian $mathbf { H } _ { t }$ may not be positive definite, so $mathbf { } d _ { t } =$ $- mathbf { H } _ { t } ^ { - 1 } g _ { t }$ may not be a descent direction. This is illustrated in 1d in Figure 8.14(b), which shows that Newton’s method can end up in a local maximum rather than a local minimum. \nIn general, any time the quadratic approximation made by Newton’s method becomes invalid, we are in trouble. However, there is usually a local region around the current iterate where we can safely \napproximate the objective by a quadratic. Let us call this region $mathcal { R } _ { t }$ , and let us call $M ( pmb delta )$ the model (or approximation) to the objective, where $pmb { delta } = pmb { theta } - pmb { theta } _ { t }$ . Then at each step we can solve \nThis is called trust-region optimization. (This can be seen as the “opposite” of line search, in the sense that we pick a distance we want to travel, determined by $mathcal { R } _ { t }$ , and then solve for the optimal direction, rather than picking the direction and then solving for the optimal distance.) \nWe usually assume that $M _ { t } ( delta )$ is a quadratic approximation: \nwhere $begin{array} { r } { pmb { g } _ { t } = nabla _ { pmb { theta } } mathcal { L } ( pmb { theta } ) | _ { pmb { theta } _ { t } } } end{array}$ is the gradient, and $mathbf { H } _ { t } = nabla _ { pmb { theta } } ^ { 2 } mathcal { L } ( pmb { theta } ) | _ { pmb { theta } _ { t } }$ is the Hessian. Furthermore, it is common to assume that $mathcal { R } _ { t }$ is a ball of radius $r$ , i.e., $mathcal { R } _ { t } = { delta : | | delta | | _ { 2 } leq r }$ . Using this, we can convert the constrained problem into an unconstrained one as follows: \nfor some Lagrange multiplier $lambda > 0$ which depends on the radius $r$ (see Section 8.5.1 for a discussion of Lagrange multipliers). We can solve this using \nThis is called Tikhonov damping or Tikhonov regularization. See Figure 8.15 for an illustration. Note that adding a sufficiently large $lambda mathbf { I }$ to $mathbf { H }$ ensures the resulting matrix is always positive definite. As $lambda  0$ , this trust method reduces to Newton’s method, but for $lambda$ large enough, it will make all the negative eigenvalues positive (and all the 0 eigenvalues become equal to $lambda$ ). \n8.4 Stochastic gradient descent \nIn this section, we consider stochastic optimization, where the goal is to minimize the average value of a function: \nwhere $boldsymbol { z }$ is a random input to the objective. This could be a “noise” term, coming from the environment, or it could be a training example drawn randomly from the training set, as we explain below. \nAt each iteration, we assume we observe $mathcal { L } _ { t } ( pmb { theta } ) = mathcal { L } ( pmb { theta } , pmb { z } _ { t } )$ , where $z _ { t } sim q$ . We also assume a way to compute an unbiased estimate of the gradient of $mathcal { L }$ . If the distribution $q ( z )$ is independent of the parameters we are optimizing, we can use $pmb { g } _ { t } = nabla _ { pmb { theta } } mathcal { L } _ { t } ( pmb { theta } _ { t } )$ . In this case, The resulting algorithm can be written as follows: \nThis method is known as stochastic gradient descent or SGD. As long as the gradient estimate is unbiased, then this method will converge to a stationary point, providing we decay the step size $mathit { Delta } ^ { prime } mathit { I } _ { t }$ at a certain rate, as we discuss in Section 8.4.3. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Second-order methods",
        "subsubsection": "Trust region methods"
    },
    {
        "content": "8.4.1 Application to finite sum problems \nSGD is very widely used in machine learning. To see why, recall from Section 4.3 that many model fitting procedures are based on empirical risk minimization, which involve minimizing the following loss: \nThis is called a finite sum problem. The gradient of this objective has the form \nThis requires summing over all $N$ training examples, and thus can be slow if $N$ is large. Fortunately we can approximate this by sampling a minibatch of $B ll N$ samples to get \nwhere $B _ { t }$ is a set of randomly chosen examples to use at iteration $t$ .2 This is an unbiased approximation to the empirical average in Equation (8.56). Hence we can safely use this with SGD. \nAlthough the theoretical rate of convergence of SGD is slower than batch GD (in particular, SGD has a sublinear convergence rate), in practice SGD is often faster, since the per-step time is much lower [BB08; BB11]. To see why SGD can make faster progress than full batch GD, suppose we have a dataset consisting of a single example duplicated $K$ times. Batch training will be (at least) $K$ times slower than SGD, since it will waste time computing the gradient for the repeated examples. Even if there are no duplicates, batch training can be wasteful, since early on in training the parameters are not well estimated, so it is not worth carefully evaluating the gradient. \n8.4.2 Example: SGD for fitting linear regression \nIn this section, we show how to use SGD to fit a linear regression model. Recall from Section 4.2.7 that the objective has the form \nThe gradient is",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Stochastic gradient descent",
        "subsubsection": "Application to finite sum problems"
    },
    {
        "content": "8.4.1 Application to finite sum problems \nSGD is very widely used in machine learning. To see why, recall from Section 4.3 that many model fitting procedures are based on empirical risk minimization, which involve minimizing the following loss: \nThis is called a finite sum problem. The gradient of this objective has the form \nThis requires summing over all $N$ training examples, and thus can be slow if $N$ is large. Fortunately we can approximate this by sampling a minibatch of $B ll N$ samples to get \nwhere $B _ { t }$ is a set of randomly chosen examples to use at iteration $t$ .2 This is an unbiased approximation to the empirical average in Equation (8.56). Hence we can safely use this with SGD. \nAlthough the theoretical rate of convergence of SGD is slower than batch GD (in particular, SGD has a sublinear convergence rate), in practice SGD is often faster, since the per-step time is much lower [BB08; BB11]. To see why SGD can make faster progress than full batch GD, suppose we have a dataset consisting of a single example duplicated $K$ times. Batch training will be (at least) $K$ times slower than SGD, since it will waste time computing the gradient for the repeated examples. Even if there are no duplicates, batch training can be wasteful, since early on in training the parameters are not well estimated, so it is not worth carefully evaluating the gradient. \n8.4.2 Example: SGD for fitting linear regression \nIn this section, we show how to use SGD to fit a linear regression model. Recall from Section 4.2.7 that the objective has the form \nThe gradient is \nNow consider using SGD with a minibatch size of $B = 1$ . The update becomes \nwhere $n = n ( t )$ is the index of the example chosen at iteration $t$ . The overall algorithm is called the least mean squares (LMS) algorithm, and is also known as the delta rule, or the Widrow-Hoff rule. \nFigure 8.16 shows the results of applying this algorithm to the data shown in Figure 11.2. We start at $pmb { theta } = ( - 0 . 5 , 2 )$ and converge (in the sense that $| | pmb { theta } _ { t } - pmb { theta } _ { t - 1 } | | _ { 2 } ^ { 2 }$ drops below a threshold of $1 0 ^ { - 2 }$ ) in about 26 iterations. Note that SGD (and hence LMS) may require multiple passes through the data to find the optimum. \n8.4.3 Choosing the step size (learning rate) \nWhen using SGD, we need to be careful in how we choose the learning rate in order to achieve convergence. For example, in Figure 8.17 we plot the loss vs the learning rate when we apply SGD to a deep neural network classifier (see Chapter 13 for details). We see a U-shaped curve, where an overly small learning rate results in underfitting, and overly large learning rate results in instability of the model (c.f., Figure 8.11(b)); in both cases, we fail to converge to a local optimum. \nOne heuristic for choosing a good learning rate, proposed in [Smi18], is to start with a small learning rate and gradually increase it, evaluating performance using a small number of minibatches. We then make a plot like the one in Figure 8.17, and pick the learning rate with the lowest loss. (In practice, it is better to pick a rate that is slightly smaller than (i.e., to the left of) the one with the lowest loss, to ensure stability.) \nRather than choosing a single constant learning rate, we can use a learning rate schedule, in which we adjust the step size over time. Theoretically, a sufficient condition for SGD to achieve \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 convergence is if the learning rate schedule satisfies the Robbins-Monro conditions:",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Stochastic gradient descent",
        "subsubsection": "Example: SGD for fitting linear regression"
    },
    {
        "content": "Now consider using SGD with a minibatch size of $B = 1$ . The update becomes \nwhere $n = n ( t )$ is the index of the example chosen at iteration $t$ . The overall algorithm is called the least mean squares (LMS) algorithm, and is also known as the delta rule, or the Widrow-Hoff rule. \nFigure 8.16 shows the results of applying this algorithm to the data shown in Figure 11.2. We start at $pmb { theta } = ( - 0 . 5 , 2 )$ and converge (in the sense that $| | pmb { theta } _ { t } - pmb { theta } _ { t - 1 } | | _ { 2 } ^ { 2 }$ drops below a threshold of $1 0 ^ { - 2 }$ ) in about 26 iterations. Note that SGD (and hence LMS) may require multiple passes through the data to find the optimum. \n8.4.3 Choosing the step size (learning rate) \nWhen using SGD, we need to be careful in how we choose the learning rate in order to achieve convergence. For example, in Figure 8.17 we plot the loss vs the learning rate when we apply SGD to a deep neural network classifier (see Chapter 13 for details). We see a U-shaped curve, where an overly small learning rate results in underfitting, and overly large learning rate results in instability of the model (c.f., Figure 8.11(b)); in both cases, we fail to converge to a local optimum. \nOne heuristic for choosing a good learning rate, proposed in [Smi18], is to start with a small learning rate and gradually increase it, evaluating performance using a small number of minibatches. We then make a plot like the one in Figure 8.17, and pick the learning rate with the lowest loss. (In practice, it is better to pick a rate that is slightly smaller than (i.e., to the left of) the one with the lowest loss, to ensure stability.) \nRather than choosing a single constant learning rate, we can use a learning rate schedule, in which we adjust the step size over time. Theoretically, a sufficient condition for SGD to achieve \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 convergence is if the learning rate schedule satisfies the Robbins-Monro conditions: \n\nSome common examples of learning rate schedules are listed below: \nIn the piecewise constant schedule, $t _ { i }$ are a set of time points at which we adjust the learning rate to a specified value. For example, we may set $eta _ { i } = eta _ { 0 } gamma ^ { i }$ , which reduces the initial learning rate by a factor of $gamma$ for each threshold (or milestone) that we pass. Figure 8.18a illustrates this for $eta _ { 0 } = 1$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license and $gamma = 0 . 9$ . This is called step decay. Sometimes the thresholds times are computed adaptively, by estimating when the train or validation loss has plateaued; this is called reduce-on-plateau. Exponential decay is typically too fast, as illustrated in Figure 8.18b. A common choice is polynomial decay, with $alpha = 0 . 5$ and $beta = 1$ , as illustrated in Figure 8.18c; this corresponds to a square-root schedule, ηt = η0 √t1+1 . \n\nIn the deep learning community, another common schedule is to quickly increase the learning rate and then gradually decrease it again, as shown in Figure 8.19a. This is called learning rate warmup, or the one-cycle learning rate schedule [Smi18]. The motivation for this is the following: initially the parameters may be in a part of the loss landscape that is poorly conditioned, so a large step size will “bounce around” too much (c.f., Figure 8.11(b)) and fail to make progress downhill. However, with a slow learning rate, the algorithm can discover flatter regions of space, where a larger step size can be used. Once there, fast progress can be made. However, to ensure convergence to a point, we must reduce the learning rate to 0. See [Got+19; Gil+21] for more details. \nIt is also possible to increase and decrease the learning rate multiple times, in a cyclical fashion. This is called a cyclical learning rate [Smi18], and was popularized by the fast.ai course. See Figure 8.19b for an illustration using triangular shapes. The motivation behind this approach is to escape local minima. The minimum and maximum learning rates can be found based on the initial “dry run” described above, and the half-cycle can be chosen based on how many restarts you want to do with your training budget. A related approach, known as stochastic gradient descent with warm restarts, was proposed in [LH17]; they proposed storing all the checkpoints visited after each cool down, and using all of them as members of a model ensemble. (See Section 18.2 for a discussion of ensemble learning.) \nAn alternative to using heuristics for estimating the learning rate is to use line search (Section 8.2.2.2). This is tricky when using SGD, because the noisy gradients make the computation of the Armijo condition difficult [CS20]. However, [Vas+19] show that it can be made to work if the variance of the gradient noise goes to zero over time. This can happen if the model is sufficiently flexible that it can perfectly interpolate the training set. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n8.4.4 Iterate averaging \nThe parameter estimates produced by SGD can be very unstable over time. To reduce the variance of the estimate, we can compute the average using \nwhere $pmb { theta } _ { t }$ are the usual SGD iterates. This is called iterate averaging or Polyak-Ruppert averaging [Rup88]. \nIn [PJ92], they prove that the estimate ${ overline { { theta } } } _ { t }$ achieves the best possible asymptotic convergence rate among SGD algorithms, matching that of variants using second-order information, such as Hessians. \nThis averaging can also have statistical benefits. For example, in [NR18], they prove that, in the case of linear regression, this method is equivalent to $ell _ { 2 }$ regularization (i.e., ridge regression). \nRather than an exponential moving average of SGD iterates, Stochastic Weight Averaging (SWA) [Izm+18] uses an equal average in conjunction with a modified learning rate schedule. In contrast to standard Polyak-Ruppert averaging, which was motivated for faster convergence rates, SWA exploits the flatness in objectives used to train deep neural networks, to find solutions which provide better generalization. \n8.4.5 Variance reduction * \nIn this section, we discuss various ways to reduce the variance in SGD. In some cases, this can improve the theoretical convergence rate from sublinear to linear (i.e., the same as full-batch gradient descent) [SLRB17; JZ13; DBLJ14]. These methods reduce the variance of the gradients, rather than the parameters themselves and are designed to work for finite sum problems. \n8.4.5.1 SVRG \nThe basic idea of stochastic variance reduced gradient (SVRG) [JZ13] is to use a control variate, in which we estimate a baseline value of the gradient based on the full batch, which we then use to compare the stochastic gradients to. \nMore precisely, ever so often (e.g., once per epoch), we compute the full gradient at a “snapshot” of the model parameters $tilde { pmb { theta } }$ ; the corresponding “exact” gradient is therefore $nabla { mathcal { L } } ( { tilde { boldsymbol { theta } } } )$ . At step $t$ , we compute the usual stochastic gradient at the current parameters, $nabla { mathcal { L } } _ { t } ( theta _ { t } )$ , but also at the snapshot parameters, $nabla { mathcal { L } } _ { t } ( { bar { pmb { theta } } } )$ , which we use as a baseline. We can then use the following improved gradient estimate \nto compute $pmb { theta } _ { t + 1 }$ . This is unbiased because $mathbb { E } left[ nabla mathcal { L } _ { t } ( tilde { { boldsymbol { theta } } } ) right] = nabla mathcal { L } ( tilde { { boldsymbol { theta } } } )$ . Furthermore, the update only involves two gradient computations, since we can compute $nabla { mathcal { L } } ( { bar { boldsymbol { theta } } } )$ once per epoch. At the end of the epoch, we update the snapshot parameters, $tilde { pmb { theta } }$ , based on the most recent value of $pmb { theta } _ { t }$ , or a running average of the iterates, and update the expected baseline. (We can compute snapshots less often, but then the baseline will not be correlated with the objective and can hurt performance, as shown in [DB18].) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Stochastic gradient descent",
        "subsubsection": "Choosing the step size (learning rate)"
    },
    {
        "content": "8.4.4 Iterate averaging \nThe parameter estimates produced by SGD can be very unstable over time. To reduce the variance of the estimate, we can compute the average using \nwhere $pmb { theta } _ { t }$ are the usual SGD iterates. This is called iterate averaging or Polyak-Ruppert averaging [Rup88]. \nIn [PJ92], they prove that the estimate ${ overline { { theta } } } _ { t }$ achieves the best possible asymptotic convergence rate among SGD algorithms, matching that of variants using second-order information, such as Hessians. \nThis averaging can also have statistical benefits. For example, in [NR18], they prove that, in the case of linear regression, this method is equivalent to $ell _ { 2 }$ regularization (i.e., ridge regression). \nRather than an exponential moving average of SGD iterates, Stochastic Weight Averaging (SWA) [Izm+18] uses an equal average in conjunction with a modified learning rate schedule. In contrast to standard Polyak-Ruppert averaging, which was motivated for faster convergence rates, SWA exploits the flatness in objectives used to train deep neural networks, to find solutions which provide better generalization. \n8.4.5 Variance reduction * \nIn this section, we discuss various ways to reduce the variance in SGD. In some cases, this can improve the theoretical convergence rate from sublinear to linear (i.e., the same as full-batch gradient descent) [SLRB17; JZ13; DBLJ14]. These methods reduce the variance of the gradients, rather than the parameters themselves and are designed to work for finite sum problems. \n8.4.5.1 SVRG \nThe basic idea of stochastic variance reduced gradient (SVRG) [JZ13] is to use a control variate, in which we estimate a baseline value of the gradient based on the full batch, which we then use to compare the stochastic gradients to. \nMore precisely, ever so often (e.g., once per epoch), we compute the full gradient at a “snapshot” of the model parameters $tilde { pmb { theta } }$ ; the corresponding “exact” gradient is therefore $nabla { mathcal { L } } ( { tilde { boldsymbol { theta } } } )$ . At step $t$ , we compute the usual stochastic gradient at the current parameters, $nabla { mathcal { L } } _ { t } ( theta _ { t } )$ , but also at the snapshot parameters, $nabla { mathcal { L } } _ { t } ( { bar { pmb { theta } } } )$ , which we use as a baseline. We can then use the following improved gradient estimate \nto compute $pmb { theta } _ { t + 1 }$ . This is unbiased because $mathbb { E } left[ nabla mathcal { L } _ { t } ( tilde { { boldsymbol { theta } } } ) right] = nabla mathcal { L } ( tilde { { boldsymbol { theta } } } )$ . Furthermore, the update only involves two gradient computations, since we can compute $nabla { mathcal { L } } ( { bar { boldsymbol { theta } } } )$ once per epoch. At the end of the epoch, we update the snapshot parameters, $tilde { pmb { theta } }$ , based on the most recent value of $pmb { theta } _ { t }$ , or a running average of the iterates, and update the expected baseline. (We can compute snapshots less often, but then the baseline will not be correlated with the objective and can hurt performance, as shown in [DB18].) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Stochastic gradient descent",
        "subsubsection": "Iterate averaging"
    },
    {
        "content": "8.4.4 Iterate averaging \nThe parameter estimates produced by SGD can be very unstable over time. To reduce the variance of the estimate, we can compute the average using \nwhere $pmb { theta } _ { t }$ are the usual SGD iterates. This is called iterate averaging or Polyak-Ruppert averaging [Rup88]. \nIn [PJ92], they prove that the estimate ${ overline { { theta } } } _ { t }$ achieves the best possible asymptotic convergence rate among SGD algorithms, matching that of variants using second-order information, such as Hessians. \nThis averaging can also have statistical benefits. For example, in [NR18], they prove that, in the case of linear regression, this method is equivalent to $ell _ { 2 }$ regularization (i.e., ridge regression). \nRather than an exponential moving average of SGD iterates, Stochastic Weight Averaging (SWA) [Izm+18] uses an equal average in conjunction with a modified learning rate schedule. In contrast to standard Polyak-Ruppert averaging, which was motivated for faster convergence rates, SWA exploits the flatness in objectives used to train deep neural networks, to find solutions which provide better generalization. \n8.4.5 Variance reduction * \nIn this section, we discuss various ways to reduce the variance in SGD. In some cases, this can improve the theoretical convergence rate from sublinear to linear (i.e., the same as full-batch gradient descent) [SLRB17; JZ13; DBLJ14]. These methods reduce the variance of the gradients, rather than the parameters themselves and are designed to work for finite sum problems. \n8.4.5.1 SVRG \nThe basic idea of stochastic variance reduced gradient (SVRG) [JZ13] is to use a control variate, in which we estimate a baseline value of the gradient based on the full batch, which we then use to compare the stochastic gradients to. \nMore precisely, ever so often (e.g., once per epoch), we compute the full gradient at a “snapshot” of the model parameters $tilde { pmb { theta } }$ ; the corresponding “exact” gradient is therefore $nabla { mathcal { L } } ( { tilde { boldsymbol { theta } } } )$ . At step $t$ , we compute the usual stochastic gradient at the current parameters, $nabla { mathcal { L } } _ { t } ( theta _ { t } )$ , but also at the snapshot parameters, $nabla { mathcal { L } } _ { t } ( { bar { pmb { theta } } } )$ , which we use as a baseline. We can then use the following improved gradient estimate \nto compute $pmb { theta } _ { t + 1 }$ . This is unbiased because $mathbb { E } left[ nabla mathcal { L } _ { t } ( tilde { { boldsymbol { theta } } } ) right] = nabla mathcal { L } ( tilde { { boldsymbol { theta } } } )$ . Furthermore, the update only involves two gradient computations, since we can compute $nabla { mathcal { L } } ( { bar { boldsymbol { theta } } } )$ once per epoch. At the end of the epoch, we update the snapshot parameters, $tilde { pmb { theta } }$ , based on the most recent value of $pmb { theta } _ { t }$ , or a running average of the iterates, and update the expected baseline. (We can compute snapshots less often, but then the baseline will not be correlated with the objective and can hurt performance, as shown in [DB18].) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIterations of SVRG are computationally faster than those of full-batch GD, but SVRG can still match the theoretical convergence rate of GD. \n8.4.5.2 SAGA \nIn this section, we describe the stochastic averaged gradient accelerated (SAGA) algorithm of [DBLJ14]. Unlike SVRG, it only requires one full batch gradient computation, at the start of the algorithm. However, it “pays” for this saving in time by using more memory. In particular, it must store $N$ gradient vectors. This enables the method to maintain an approximation of the global gradient by removing the old local gradient from the overall sum and replacing it with the new local gradient. This is called an aggregated gradient method. \n$begin{array} { r } { { pmb g } ^ { mathrm { a v g } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } { pmb g } _ { n } ^ { mathrm { l o c a l } } } end{array}$ More precisely, we first initialize by computing . Then, at iteration $t$ , we use the gradient estimate $pmb { g } _ { n } ^ { mathrm { l o c a l } } = nabla mathcal { L } _ { n } ( pmb { theta } _ { 0 } )$ for all $n$ , and the average, \nwhere $n sim operatorname { U n i f } { 1 , dots , N }$ is the example index sampled at iteration $t$ . We then update $begin{array} { l } { { g _ { n } ^ { mathrm { l o c a l } } = } } end{array}$ $nabla { mathcal { L } } _ { n } ( theta _ { t } )$ and $g ^ { mathrm { a v g } }$ by replacing the old $mathbf { Psi } _ { g _ { n } } ^ { mathrm { l o c a l } }$ by its new value. \nThis has an advantage over SVRG since it only has to do one full batch sweep at the start. (In fact, the initial sweep is not necessary, since we can compute ${ pmb g } ^ { mathrm { a v g } }$ “lazily”, by only incorporating gradients we have seen so far.) The downside is the large extra memory cost. However, if the features (and hence gradients) are sparse, the memory cost can be reasonable. Indeed, the SAGA algorithm is recommended for use in the sklearn logistic regression code when $N$ is large and $_ { x }$ is sparse.3 \n8.4.5.3 Application to deep learning \nVariance reduction methods are widely used for fitting ML models with convex objectives, such as linear models. However, there are various difficulties associated with using SVRG with conventional deep learning training practices. For example, the use of batch normalization (Section 14.2.4.1), data augmentation (Section 19.1) and dropout (Section 13.5.4) all break the assumptions of the method, since the loss will differ randomly in ways that depend not just on the parameters and the data index $n$ . For more details, see e.g., [DB18; Arn+19]. \n8.4.6 Preconditioned SGD \nIn this section, we consider preconditioned SGD, which involves the following update: \nwhere $mathbf { M } _ { t }$ is a preconditioning matrix, or simply the preconditioner, typically chosen to be positive-definite. Unfortunately the noise in the gradient estimates make it difficult to reliably estimate the Hessian, which makes it difficult to use the methods from Section 8.3. In addition, it is expensive to solve for the update direction with a full preconditioning matrix. Therefore most practitioners use a diagonal preconditioner $mathbf { M } _ { t }$ . Such preconditioners do not necessarily use second-order information, but often result in speedups compared to vanilla SGD. See also [Roo+21] for a probabilitic interpretation of these heuristics, and sgd_comparison.ipynb for an empirical comparison on some simple datasets.",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Stochastic gradient descent",
        "subsubsection": "Variance reduction *"
    },
    {
        "content": "Iterations of SVRG are computationally faster than those of full-batch GD, but SVRG can still match the theoretical convergence rate of GD. \n8.4.5.2 SAGA \nIn this section, we describe the stochastic averaged gradient accelerated (SAGA) algorithm of [DBLJ14]. Unlike SVRG, it only requires one full batch gradient computation, at the start of the algorithm. However, it “pays” for this saving in time by using more memory. In particular, it must store $N$ gradient vectors. This enables the method to maintain an approximation of the global gradient by removing the old local gradient from the overall sum and replacing it with the new local gradient. This is called an aggregated gradient method. \n$begin{array} { r } { { pmb g } ^ { mathrm { a v g } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } { pmb g } _ { n } ^ { mathrm { l o c a l } } } end{array}$ More precisely, we first initialize by computing . Then, at iteration $t$ , we use the gradient estimate $pmb { g } _ { n } ^ { mathrm { l o c a l } } = nabla mathcal { L } _ { n } ( pmb { theta } _ { 0 } )$ for all $n$ , and the average, \nwhere $n sim operatorname { U n i f } { 1 , dots , N }$ is the example index sampled at iteration $t$ . We then update $begin{array} { l } { { g _ { n } ^ { mathrm { l o c a l } } = } } end{array}$ $nabla { mathcal { L } } _ { n } ( theta _ { t } )$ and $g ^ { mathrm { a v g } }$ by replacing the old $mathbf { Psi } _ { g _ { n } } ^ { mathrm { l o c a l } }$ by its new value. \nThis has an advantage over SVRG since it only has to do one full batch sweep at the start. (In fact, the initial sweep is not necessary, since we can compute ${ pmb g } ^ { mathrm { a v g } }$ “lazily”, by only incorporating gradients we have seen so far.) The downside is the large extra memory cost. However, if the features (and hence gradients) are sparse, the memory cost can be reasonable. Indeed, the SAGA algorithm is recommended for use in the sklearn logistic regression code when $N$ is large and $_ { x }$ is sparse.3 \n8.4.5.3 Application to deep learning \nVariance reduction methods are widely used for fitting ML models with convex objectives, such as linear models. However, there are various difficulties associated with using SVRG with conventional deep learning training practices. For example, the use of batch normalization (Section 14.2.4.1), data augmentation (Section 19.1) and dropout (Section 13.5.4) all break the assumptions of the method, since the loss will differ randomly in ways that depend not just on the parameters and the data index $n$ . For more details, see e.g., [DB18; Arn+19]. \n8.4.6 Preconditioned SGD \nIn this section, we consider preconditioned SGD, which involves the following update: \nwhere $mathbf { M } _ { t }$ is a preconditioning matrix, or simply the preconditioner, typically chosen to be positive-definite. Unfortunately the noise in the gradient estimates make it difficult to reliably estimate the Hessian, which makes it difficult to use the methods from Section 8.3. In addition, it is expensive to solve for the update direction with a full preconditioning matrix. Therefore most practitioners use a diagonal preconditioner $mathbf { M } _ { t }$ . Such preconditioners do not necessarily use second-order information, but often result in speedups compared to vanilla SGD. See also [Roo+21] for a probabilitic interpretation of these heuristics, and sgd_comparison.ipynb for an empirical comparison on some simple datasets. \n\n8.4.6.1 AdaGrad \nThe AdaGrad (short for “adaptive gradient”) method of [DHS11] was originally designed for optimizing convex objectives where many elements of the gradient vector are zero; these might correspond to features that are rarely present in the input, such as rare words. The update has the following form \nwhere $d = 1 : D$ indexes the dimensions of the parameter vector, and \nis the sum of the squared gradients and $epsilon > 0$ is a small term to avoid dividing by zero. Equivalently we can write the update in vector form as follows: \nwhere the square root and division is performed elementwise. Viewed as preconditioned SGD, this is equivalent to taking ${ bf M } _ { t } = mathrm { d i a g } ( s _ { t } + epsilon ) ^ { 1 / 2 }$ . This is an example of an adaptive learning rate; the overall stepsize $eta _ { t }$ still needs to be chosen, but the results are less sensitive to it compared to vanilla GD. In particular, we usually fix $eta _ { t } = eta _ { 0 }$ . \n8.4.6.2 RMSProp and AdaDelta \nA defining feature of AdaGrad is that the term in the denominator gets larger over time, so the effective learning rate drops. While it is necessary to ensure convergence, it might hurt performance as the denominator gets large too fast. \nAn alternative is to use an exponentially weighted moving average (EWMA, Section 4.4.2.2) of the past squared gradients, rather than their sum: \nIn practice we usually use $beta sim 0 . 9$ , which puts more weight on recent examples. In this case, \nwhere RMS stands for “root mean squared”. Hence this method, (which is based on the earlier RPROP method of [RB93]) is known as RMSProp [Hin14]. The overall update of RMSProp is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe AdaDelta method was independently introduced in [Zei12], and is similar to RMSprop. However, in addition to accumulating an EWMA of the gradients in $hat { boldsymbol { s } }$ , it also keeps an EWMA of the updates $delta _ { t }$ to obtain an update of the form \nwhere \nand $mathbf { } s _ { t }$ is the same as in RMSProp. This has the advantage that the “units” of the numerator and denominator cancel, so we are just elementwise-multiplying the gradient by a scalar. This eliminates the need to tune the learning rate $eta _ { t }$ , which means one can simply set $eta _ { t } = 1$ , although popular implementations of AdaDelta still keep $eta _ { t }$ as a tunable hyperparameter. However, since these adaptive learning rates need not decrease with time (unless we choose $eta _ { t }$ to explicitly do so), these methods are not guaranteed to converge to a solution. \n8.4.6.3 Adam \nIt is possible to combine RMSProp with momentum. In particular, let us compute an EWMA of the gradients (as in momentum) and squared gradients (as in RMSProp) \nWe then perform the following update: \nThe resulting method is known as Adam, which stands for “adaptive moment estimation” [KB15]. The standard values for the various constants are $beta _ { 1 } = 0 . 9$ , $beta _ { 2 } = 0 . 9 9 9$ and $epsilon = 1 0 ^ { - 6 }$ . (If we set $beta _ { 1 } = 0$ and no bias-correction, we recover RMSProp, which does not use momentum.) For the overall learning rate, it is common to use a fixed value such as $eta _ { t } = 0 . 0 0 1$ . Again, as the adaptive learning rate may not decrease over time, convergence is not guaranteed (see Section 8.4.6.4). \nIf we initialize with $pmb { m } _ { 0 } = pmb { s } _ { 0 } = mathbf { 0 }$ , then initial estimates will be biased towards small values. The authors therefore recommend using the bias-corrected moments, which increase the values early in the optimization process. These estimates are given by \nThe advantage of bias-correction is shown in Figure 4.3. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n8.4.6.4 Issues with adaptive learning rates \nWhen using diagonal scaling methods, the overall learning rate is determined by $eta _ { 0 } mathbf { M } _ { t } ^ { - 1 }$ , which changes with time. Hence these methods are often called adaptive learning rate methods. However, they still require setting the base learning rate $eta _ { 0 }$ . \nSince the EWMA methods are typically used in the stochastic setting where the gradient estimates are noisy, their learning rate adaptation can result in non-convergence even on convex problems [RKK18]. Various solutions to this problem have been proposed, including AMSGrad [RKK18], Padam [CG18; Zho+18], and Yogi [Zah+18]. For example, the Yogi update modifies Adam by replacing \nwith \n8.4.6.5 Non-diagonal preconditioning matrices \nAlthough the methods we have discussed above can adapt the learning rate of each parameter, they do not solve the more fundamental problem of ill-conditioning due to correlation of the parameters, and hence do not always provide as much of a speed boost over vanilla SGD as one may hope. \nOne way to get faster convergence is to use the following preconditioning matrix, known as full-matrix Adagrad [DHS11]: \nwhere \nHere $pmb { g } _ { i } = nabla _ { pmb { psi } } c ( pmb { psi } _ { i } )$ is the $D$ -dimensional gradient vector computed at step $i$ . Unfortunately, $mathbf { M } _ { t }$ is a $boldsymbol { D } times boldsymbol { D }$ matrix, which is expensive to store and invert. \nThe Shampoo algorithm [GKS18] makes a block diagonal approximation to $mathbf { M }$ , one per layer of the model, and then exploits Kronecker product structure to efficiently invert it. (It is called “shampoo” because it uses a conditioner.) Recently, [Ani+20] scaled this method up to fit very large deep models in record time. \n8.5 Constrained optimization \nIn this section, we consider the following constrained optimization problem: \nwhere the feasible set, or constraint set, is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $boldsymbol { xi }$ is the set of equality constraints, and $mathcal { T }$ is the set of inequality constraints.",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Stochastic gradient descent",
        "subsubsection": "Preconditioned SGD"
    },
    {
        "content": "For example, suppose we have a quadratic objective, ${ mathcal L } ( pmb { theta } ) = { theta } _ { 1 } ^ { 2 } + { theta } _ { 2 } ^ { 2 }$ , subject to a linear equality constraint, $h ( pmb theta ) = 1 - theta _ { 1 } - theta _ { 2 } = 0$ . Figure 8.20(a) plots the level sets of $mathcal { L }$ , as well as the constraint surface. What we are trying to do is find the point $pmb { theta } ^ { * }$ that lives on the line, but which is closest to the origin. It is clear from the geometry that the optimal solution is $pmb { theta } = ( 0 . 5 , 0 . 5 )$ , indicated by the solid black dot. \nIn the following sections, we briefly describe some of the theory and algorithms underlying constrained optimization. More details can be found in other books, such as [BV04; NW06; Ber15; Ber16]. \n8.5.1 Lagrange multipliers \nIn this section, we discuss how to solve equality contrained optimization problems. We initially assume that we have just one equality constraint, $h ( pmb theta ) = 0$ . \nFirst note that for any point on the constraint surface, $nabla h ( pmb theta )$ will be orthogonal to the constraint surface. To see why, consider another point nearby, $theta + epsilon$ , that also lies on the surface. If we make a first-order Taylor expansion around $pmb theta$ we have \nSince both $pmb theta$ and $theta + epsilon$ are on the constraint surface, we must have $h ( pmb theta ) = h ( pmb theta + pmb epsilon )$ and hence $epsilon ^ { mathsf { I } } nabla h ( pmb { theta } ) approx mathrm { 0 }$ . Since $epsilon$ is parallel to the constraint surface, $nabla h ( pmb theta )$ must be perpendicular to it. \nWe seek a point $pmb { theta } ^ { * }$ on the constraint surface such that ${ mathcal { L } } ( theta )$ is minimized. We just showed that it must satisfy the condition that $nabla h ( theta ^ { * } )$ is orthogonal to the constraint surface. In addition, such a point must have the property that $nabla { mathcal { L } } ( pmb theta )$ is also orthogonal to the constraint surface, as otherwise we could decrease ${ mathcal { L } } ( theta )$ by moving a short distance along the constraint surface. Since both $nabla h ( pmb theta )$ and $nabla { mathcal { L } } ( theta )$ are orthogonal to the constraint surface at $theta ^ { * }$ , they must be parallel (or anti-parallel) to each other. Hence there must exist a constant $lambda ^ { ast } in mathbb R$ such that \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n(We cannot just equate the gradient vectors, since they may have different magnitudes.) The constant $lambda ^ { * }$ is called a Lagrange multiplier, and can be positive, negative, or zero. This latter case occurs when $nabla { mathcal { L } } ( theta ^ { * } ) = 0$ . \nWe can convert Equation (8.89) into an objective, known as the Lagrangian, that we should find a stationary point of the following: \nAt a stationary point of the Lagrangian, we have \nThis is called a critical point, and satisfies the original constraint $h ( pmb theta ) = 0$ and Equation (8.89) If we have $m > 1$ constraints, we can form a new constraint function by addition, as follows: \nWe now have $D + m$ equations in $D + m$ unknowns and we can use standard unconstrained optimization methods to find a stationary point. We give some examples below. \n8.5.1.1 Example: 2d Quadratic objective with one linear equality constraint \nConsider minimizing ${ mathcal L } ( pmb { theta } ) = { theta } _ { 1 } ^ { 2 } + { theta } _ { 2 } ^ { 2 }$ subject to the constraint that $theta _ { 1 } + theta _ { 2 } = 1$ . (This is the problem illustrated in Figure 8.20(a).) The Lagrangian is \nWe have the following conditions for a stationary point: \nFrom Equations 8.94 and 8.95 we find $2 theta _ { 1 } = - lambda = 2 theta _ { 2 }$ , so $theta _ { 1 } = theta _ { 2 }$ . Also, from Equation (8.96), we find $2 theta _ { 1 } = 1$ . So $pmb { theta } ^ { * } = ( 0 . 5 , 0 . 5 )$ , as we claimed earlier. Furthermore, this is the global minimum since the objective is convex and the constraint is affine. \n8.5.2 The KKT conditions \nIn this section, we generalize the concept of Lagrange multipliers to additionally handle inequality constraints. \nFirst consider the case where we have a single inequality constraint $g ( pmb theta ) le 0$ . To find the optimum, one approach would be to consider an unconstrained problem where we add the penalty as an infinite step function: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Constrained optimization",
        "subsubsection": "Lagrange multipliers"
    },
    {
        "content": "(We cannot just equate the gradient vectors, since they may have different magnitudes.) The constant $lambda ^ { * }$ is called a Lagrange multiplier, and can be positive, negative, or zero. This latter case occurs when $nabla { mathcal { L } } ( theta ^ { * } ) = 0$ . \nWe can convert Equation (8.89) into an objective, known as the Lagrangian, that we should find a stationary point of the following: \nAt a stationary point of the Lagrangian, we have \nThis is called a critical point, and satisfies the original constraint $h ( pmb theta ) = 0$ and Equation (8.89) If we have $m > 1$ constraints, we can form a new constraint function by addition, as follows: \nWe now have $D + m$ equations in $D + m$ unknowns and we can use standard unconstrained optimization methods to find a stationary point. We give some examples below. \n8.5.1.1 Example: 2d Quadratic objective with one linear equality constraint \nConsider minimizing ${ mathcal L } ( pmb { theta } ) = { theta } _ { 1 } ^ { 2 } + { theta } _ { 2 } ^ { 2 }$ subject to the constraint that $theta _ { 1 } + theta _ { 2 } = 1$ . (This is the problem illustrated in Figure 8.20(a).) The Lagrangian is \nWe have the following conditions for a stationary point: \nFrom Equations 8.94 and 8.95 we find $2 theta _ { 1 } = - lambda = 2 theta _ { 2 }$ , so $theta _ { 1 } = theta _ { 2 }$ . Also, from Equation (8.96), we find $2 theta _ { 1 } = 1$ . So $pmb { theta } ^ { * } = ( 0 . 5 , 0 . 5 )$ , as we claimed earlier. Furthermore, this is the global minimum since the objective is convex and the constraint is affine. \n8.5.2 The KKT conditions \nIn this section, we generalize the concept of Lagrange multipliers to additionally handle inequality constraints. \nFirst consider the case where we have a single inequality constraint $g ( pmb theta ) le 0$ . To find the optimum, one approach would be to consider an unconstrained problem where we add the penalty as an infinite step function: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHowever, this is a discontinuous function that is hard to optimize. \nInstead, we create a lower bound of the form $mu g ( pmb theta )$ , where $mu geq 0$ . This gives us the following Lagrangian: \nNote that the step function can be recovered using \nThus our optimization problem becomes \nNow consider the general case where we have multiple inequality constraints, $mathbf { delta } _ { g ( pmb { theta } ) } le mathbf { 0 }$ , and multiple equality constraints, $mathbf { delta } h ( theta ) = mathbf { 0 }$ . The generalized Lagrangian becomes \n(We are free to change $- lambda _ { j } h _ { j }$ to $+ lambda _ { j } h _ { j }$ since the sign is arbitrary.) Our optimization problem becomes \nWhen $mathcal { L }$ and $g$ are convex, then all critical points of this problem must satisfy the following criteria (under some conditions [BV04, Sec.5.2.3]): \nAll constraints are satisfied (this is called feasibility): \n• The solution is a stationary point: \n• The penalty for the inequality constraint points in the right direction (this is called dual feasibility): \n• The Lagrange multipliers pick up any slack in the inactive constraints, i.e., either $mu _ { i } = 0$ or $g _ { i } ( theta ^ { * } ) = 0$ , so \nThis is called complementary slackness. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo see why the last condition holds, consider (for simplicity) the case of a single inequality constraint, $g ( pmb theta ) le 0$ . Either it is active, meaning $g ( pmb theta ) = 0$ , or it is inactive, meaning $g ( pmb theta ) < 0$ . In the active case, the solution lies on the constraint boundary, and $g ( pmb theta ) = 0$ becomes an equality constraint; then we have $nabla { mathcal { L } } = mu nabla g$ for some constant $mu neq 0$ , because of Equation (8.89). In the inactive case, the solution is not on the constraint boundary; we still have $nabla { mathcal { L } } = mu nabla g$ , but now µ = 0. \nThese are called called the Karush-Kuhn-Tucker (KKT) conditions. If $mathcal { L }$ is a convex function, and the constraints define a convex set, the KKT conditions are sufficient for (global) optimality, as well as necessary. \n8.5.3 Linear programming \nConsider optimizing a linear function subject to linear constraints. When written in standard form, this can be represented as \nThe feasible set defines a convex polytope, which is a convex set defined as the intersection of half spaces. See Figure 8.21(a) for a 2d example. Figure 8.21(b) shows a linear cost function that decreases as we move to the bottom right. We see that the lowest point that is in the feasible set is a vertex. In fact, it can be proved that the optimum point always occurs at a vertex of the polytope, assuming the solution is unique. If there are multiple solutions, the line will be parallel to a face. There may also be no optima inside the feasible set; in this case, the problem is said to be infeasible. \n8.5.3.1 The simplex algorithm \nIt can be shown that the optima of an LP occur at vertices of the polytope defining the feasible set (see Figure 8.21(b) for an example). The simplex algorithm solves LPs by moving from vertex to vertex, each time seeking the edge which most improves the objective. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Constrained optimization",
        "subsubsection": "The KKT conditions"
    },
    {
        "content": "To see why the last condition holds, consider (for simplicity) the case of a single inequality constraint, $g ( pmb theta ) le 0$ . Either it is active, meaning $g ( pmb theta ) = 0$ , or it is inactive, meaning $g ( pmb theta ) < 0$ . In the active case, the solution lies on the constraint boundary, and $g ( pmb theta ) = 0$ becomes an equality constraint; then we have $nabla { mathcal { L } } = mu nabla g$ for some constant $mu neq 0$ , because of Equation (8.89). In the inactive case, the solution is not on the constraint boundary; we still have $nabla { mathcal { L } } = mu nabla g$ , but now µ = 0. \nThese are called called the Karush-Kuhn-Tucker (KKT) conditions. If $mathcal { L }$ is a convex function, and the constraints define a convex set, the KKT conditions are sufficient for (global) optimality, as well as necessary. \n8.5.3 Linear programming \nConsider optimizing a linear function subject to linear constraints. When written in standard form, this can be represented as \nThe feasible set defines a convex polytope, which is a convex set defined as the intersection of half spaces. See Figure 8.21(a) for a 2d example. Figure 8.21(b) shows a linear cost function that decreases as we move to the bottom right. We see that the lowest point that is in the feasible set is a vertex. In fact, it can be proved that the optimum point always occurs at a vertex of the polytope, assuming the solution is unique. If there are multiple solutions, the line will be parallel to a face. There may also be no optima inside the feasible set; in this case, the problem is said to be infeasible. \n8.5.3.1 The simplex algorithm \nIt can be shown that the optima of an LP occur at vertices of the polytope defining the feasible set (see Figure 8.21(b) for an example). The simplex algorithm solves LPs by moving from vertex to vertex, each time seeking the edge which most improves the objective. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIn the worst-case scenario, the simplex algorithm can take time exponential in $D$ , although in practice it is usually very efficient. There are also various polynomial-time algorithms, such as the interior point method, although these are often slower in practice. \n8.5.3.2 Applications \nThere are many applications of linear programming in science, engineering and business. It is also useful in some machine learning problems. For example, Section 11.6.1.1 shows how to use it to solve robust linear regression. It is also useful for state estimation in graphical models (see e.g., [SGJ11]). \n8.5.4 Quadratic programming \nConsider minimizing a quadratic objective subject to linear equality and inequality constraints. This kind of problem is known as a quadratic program or $mathbf { Q P }$ , and can be written as follows: \nIf $mathbf { H }$ is positive semidefinite, then this is a convex optimization problem. \n8.5.4.1 Example: 2d quadratic objective with linear inequality constraints \nAs a concrete example, suppose we want to minimize \nwhere $mathbf { H } = 2 mathbf { I }$ and $pmb { c } = - ( 3 , 1 / 4 )$ , subject to \nSee Figure 8.20(b) for an illustration. \nWe can rewrite the constraints as \nwhich we can write more compactly as \nwhere $mathbf b = mathbf 1$ and \nThis is now in the standard QP form. \nFrom the geometry of the problem, shown in Figure 8.20(b), we see that the constraints corresponding to the two left faces of the diamond) are inactive (since we are trying to get as close to the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 center of the circle as possible, which is outside of, and to the right of, the constrained feasible region). Denoting $g _ { i } ( pmb theta )$ as the inequality constraint corresponding to row $i$ of $mathbf { A }$ , this means $g _ { 3 } ( theta ^ { * } ) > 0$ and $g _ { 4 } ( pmb { theta } ^ { * } ) > 0$ , and hence, by complementarity, $mu _ { 3 } ^ { * } = mu _ { 4 } ^ { * } = 0$ . We can therefore remove these inactive constraints.",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Constrained optimization",
        "subsubsection": "Linear programming"
    },
    {
        "content": "In the worst-case scenario, the simplex algorithm can take time exponential in $D$ , although in practice it is usually very efficient. There are also various polynomial-time algorithms, such as the interior point method, although these are often slower in practice. \n8.5.3.2 Applications \nThere are many applications of linear programming in science, engineering and business. It is also useful in some machine learning problems. For example, Section 11.6.1.1 shows how to use it to solve robust linear regression. It is also useful for state estimation in graphical models (see e.g., [SGJ11]). \n8.5.4 Quadratic programming \nConsider minimizing a quadratic objective subject to linear equality and inequality constraints. This kind of problem is known as a quadratic program or $mathbf { Q P }$ , and can be written as follows: \nIf $mathbf { H }$ is positive semidefinite, then this is a convex optimization problem. \n8.5.4.1 Example: 2d quadratic objective with linear inequality constraints \nAs a concrete example, suppose we want to minimize \nwhere $mathbf { H } = 2 mathbf { I }$ and $pmb { c } = - ( 3 , 1 / 4 )$ , subject to \nSee Figure 8.20(b) for an illustration. \nWe can rewrite the constraints as \nwhich we can write more compactly as \nwhere $mathbf b = mathbf 1$ and \nThis is now in the standard QP form. \nFrom the geometry of the problem, shown in Figure 8.20(b), we see that the constraints corresponding to the two left faces of the diamond) are inactive (since we are trying to get as close to the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 center of the circle as possible, which is outside of, and to the right of, the constrained feasible region). Denoting $g _ { i } ( pmb theta )$ as the inequality constraint corresponding to row $i$ of $mathbf { A }$ , this means $g _ { 3 } ( theta ^ { * } ) > 0$ and $g _ { 4 } ( pmb { theta } ^ { * } ) > 0$ , and hence, by complementarity, $mu _ { 3 } ^ { * } = mu _ { 4 } ^ { * } = 0$ . We can therefore remove these inactive constraints. \n\nFrom the KKT conditions we know that \nUsing these for the actively constrained subproblem, we get \nHence the solution is \nNotice that the optimal value of $pmb theta$ occurs at one of the vertices of the $ell _ { 1 }$ “ball” (the diamond shape). \n8.5.4.2 Applications \nThere are several applications of quadratic programming in ML. For example, in Section 11.4, we discuss the lasso method for sparse linear regression, which amounts to optimizing ${ mathcal { L } } ( w ) =$ $| | mathbf { X } pmb { w } - pmb { y } | | _ { 2 } ^ { 2 } + lambda | | pmb { w } | | _ { 1 }$ , which can be reformulated into a QP. And in Section 17.3, we show how to use QP for SVMs (support vector machines). \n8.5.5 Mixed integer linear programming * \nInteger linear programming or ILP corresponds to minimizing a linear objective, subject to linear constraints, where the optimization variables are discrete integers instead of reals. In standard form, the problem is as follows: \nwhere $mathbb { Z }$ is the set of integers. If some of the optimization variables are real-valued, it is called a mixed ILP, often called a MIP for short. (If all of the variables are real-valued, it becomes a standard LP.) \nMIPs have a large number of applications, such as in vehicle routing, scheduling and packing. They are also useful for some ML applications, such as formally verifying the behavior of certain kinds of deep neural networks [And+18], and proving robustness properties of DNNs to adversarial (worst-case) perturbations [TXT19]. \n8.6 Proximal gradient method * \nWe are often interested in optimizing an objective of the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathcal { L } _ { s }$ is differentiable (smooth), and $mathcal { L } _ { r }$ is convex but not necessarily differentiable (i.e., it may be non-smooth or “rough”). For example, $mathcal { L } _ { s }$ might be the negative log likelihood (NLL), and $mathcal { L } _ { r }$ might be an indicator function that is infinite if a constraint is violated (see Section 8.6.1), or $mathcal { L } _ { r }$ might be the $ell _ { 1 }$ norm of some parameters (see Section 8.6.2), or $mathcal { L } _ { r }$ might measure how far the parameters are from a set of allowed quantized values (see Section 8.6.3).",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Constrained optimization",
        "subsubsection": "Quadratic programming"
    },
    {
        "content": "From the KKT conditions we know that \nUsing these for the actively constrained subproblem, we get \nHence the solution is \nNotice that the optimal value of $pmb theta$ occurs at one of the vertices of the $ell _ { 1 }$ “ball” (the diamond shape). \n8.5.4.2 Applications \nThere are several applications of quadratic programming in ML. For example, in Section 11.4, we discuss the lasso method for sparse linear regression, which amounts to optimizing ${ mathcal { L } } ( w ) =$ $| | mathbf { X } pmb { w } - pmb { y } | | _ { 2 } ^ { 2 } + lambda | | pmb { w } | | _ { 1 }$ , which can be reformulated into a QP. And in Section 17.3, we show how to use QP for SVMs (support vector machines). \n8.5.5 Mixed integer linear programming * \nInteger linear programming or ILP corresponds to minimizing a linear objective, subject to linear constraints, where the optimization variables are discrete integers instead of reals. In standard form, the problem is as follows: \nwhere $mathbb { Z }$ is the set of integers. If some of the optimization variables are real-valued, it is called a mixed ILP, often called a MIP for short. (If all of the variables are real-valued, it becomes a standard LP.) \nMIPs have a large number of applications, such as in vehicle routing, scheduling and packing. They are also useful for some ML applications, such as formally verifying the behavior of certain kinds of deep neural networks [And+18], and proving robustness properties of DNNs to adversarial (worst-case) perturbations [TXT19]. \n8.6 Proximal gradient method * \nWe are often interested in optimizing an objective of the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathcal { L } _ { s }$ is differentiable (smooth), and $mathcal { L } _ { r }$ is convex but not necessarily differentiable (i.e., it may be non-smooth or “rough”). For example, $mathcal { L } _ { s }$ might be the negative log likelihood (NLL), and $mathcal { L } _ { r }$ might be an indicator function that is infinite if a constraint is violated (see Section 8.6.1), or $mathcal { L } _ { r }$ might be the $ell _ { 1 }$ norm of some parameters (see Section 8.6.2), or $mathcal { L } _ { r }$ might measure how far the parameters are from a set of allowed quantized values (see Section 8.6.3).",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Constrained optimization",
        "subsubsection": "Mixed integer linear programming *"
    },
    {
        "content": "One way to tackle such problems is to use the proximal gradient method (see e.g., [PB+14; PSW15]). Roughly speaking, this takes a step of size $eta$ in the direction of the gradient, and then projects the resulting parameter update into a space that respects $mathcal { L } _ { r }$ . More precisely, the update is as follows \nwhere $mathrm { p r o x } _ { eta f } ( pmb { theta } )$ is the proximal operator of $mathcal { L } _ { r }$ (scaled by $eta$ ) evaluated at $pmb theta$ : \n(The factor of $textstyle { frac { 1 } { 2 } }$ is an arbitrary convention.) We can rewrite the proximal operator as solving a constrained opimtization problem, as follows: \nwhere the bound $rho$ depends on the scaling factor $eta$ . Thus we see that the proximal projection minimizes the function while staying close to (i.e., proximal to) the current iterate. We give some examples below. \n8.6.1 Projected gradient descent \nSuppose we want to solve the problem \nwhere $boldsymbol { mathcal { C } }$ is a convex set. For example, we may have the box constraints $mathcal { C } = { pmb theta : l leq pmb theta leq pmb u }$ , where we specify lower and upper bounds on each element. These bounds can be infinite for certain elements if we don’t want to constrain values along that dimension. For example, if we just want to ensure the parameters are non-negative, we set $l _ { d } = 0$ and $u _ { d } = infty$ for each dimension $d$ . \nWe can convert the constrained optimization problem into an unconstrained one by adding a penalty term to the original objective: \nwhere $mathcal { L } _ { r } ( pmb { theta } )$ is the indicator function for the convex set $boldsymbol { mathcal { C } }$ , i.e., \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe can use proximal gradient descent to solve Equation (8.123). The proximal operator for the indicator function is equivalent to projection onto the set $boldsymbol { mathscr { C } }$ : \nThis method is known as projected gradient descent. See Figure 8.22 for an illustration. \nFor example, consider the box constraints $mathcal { C } = { pmb theta : l leq pmb theta leq pmb u }$ . The projection operator in this case can be computed elementwise by simply thresholding at the boundaries: \nFor example, if we want to ensure all elements are non-negative, we can use \nSee Section 11.4.9.2 for an application of this method to sparse linear regression. \n8.6.2 Proximal operator for $ell _ { 1 }$ -norm regularizer \nConsider a linear predictor of the form $textstyle f ( pmb { x } ; pmb { theta } ) = sum _ { d = 1 } ^ { D } theta _ { d } x _ { d }$ . If we have $theta _ { d } = 0$ for any dimension $d$ , we ignore the corresponding feature $x _ { d }$ . This is a form of feature selection, which can be useful both as a way to reduce overfitting as well as way to improve model interpretability. We can encourage weights to be zero (and not just small) by penalizing the $ell _ { 1 }$ norm, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Proximal gradient method *",
        "subsubsection": "Projected gradient descent"
    },
    {
        "content": "We can use proximal gradient descent to solve Equation (8.123). The proximal operator for the indicator function is equivalent to projection onto the set $boldsymbol { mathscr { C } }$ : \nThis method is known as projected gradient descent. See Figure 8.22 for an illustration. \nFor example, consider the box constraints $mathcal { C } = { pmb theta : l leq pmb theta leq pmb u }$ . The projection operator in this case can be computed elementwise by simply thresholding at the boundaries: \nFor example, if we want to ensure all elements are non-negative, we can use \nSee Section 11.4.9.2 for an application of this method to sparse linear regression. \n8.6.2 Proximal operator for $ell _ { 1 }$ -norm regularizer \nConsider a linear predictor of the form $textstyle f ( pmb { x } ; pmb { theta } ) = sum _ { d = 1 } ^ { D } theta _ { d } x _ { d }$ . If we have $theta _ { d } = 0$ for any dimension $d$ , we ignore the corresponding feature $x _ { d }$ . This is a form of feature selection, which can be useful both as a way to reduce overfitting as well as way to improve model interpretability. We can encourage weights to be zero (and not just small) by penalizing the $ell _ { 1 }$ norm, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis is called a sparsity inducing regularizer. \nTo see why this induces sparsity, consider two possible parameter vectors, one which is sparse, $pmb { theta } = ( 1 , 0 )$ , and one which is non-sparse, $pmb { theta } ^ { prime } = ( 1 / sqrt { 2 } , 1 / sqrt { 2 } )$ . Both have the same $ell _ { 2 }$ norm \nHence $ell _ { 2 }$ regularization (Section 4.5.3) will not favor the sparse solution over the dense solution. However, when using $ell _ { 1 }$ regularization, the sparse solution is cheaper, since \nSee Section 11.4 for more details on sparse regression. \nIf we combine this regularizer with our smooth loss, we get \nWe can optimize this objective using proximal gradient descent. The key question is how to compute the prox operator for the function $f ( pmb theta ) = | | pmb theta | | _ { 1 }$ . Since this function decomposes over dimensions $d$ , the proximal projection can be computed componentwise. From Equation (8.120), with $eta = 1$ , we have \nIn Section 11.4.3, we show that the solution to this is given by \nThis is known as the soft thresholding operator, since values less than $lambda$ in absolute value are set to 0 (thresholded), but in a continuous way. Note that soft thresholding can be written more compactly as \nwhere $theta _ { + } = operatorname* { m a x } ( theta , 0 )$ is the positive part of $theta$ . In the vector case, we perform this elementwise: \nSee Section 11.4.9.3 for an application of this method to sparse linear regression. \n8.6.3 Proximal operator for quantization \nIn some applications (e.g., when training deep neural networks to run on memory-limited edge devices, such as mobile phones) we want to ensure that the parameters are quantized. For example, in the extreme case where each parameter can only be -1 or $+ 1$ , the state space becomes $mathcal { C } = { - 1 , + 1 } ^ { D }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Proximal gradient method *",
        "subsubsection": "Proximal operator for 1-norm regularizer"
    },
    {
        "content": "This is called a sparsity inducing regularizer. \nTo see why this induces sparsity, consider two possible parameter vectors, one which is sparse, $pmb { theta } = ( 1 , 0 )$ , and one which is non-sparse, $pmb { theta } ^ { prime } = ( 1 / sqrt { 2 } , 1 / sqrt { 2 } )$ . Both have the same $ell _ { 2 }$ norm \nHence $ell _ { 2 }$ regularization (Section 4.5.3) will not favor the sparse solution over the dense solution. However, when using $ell _ { 1 }$ regularization, the sparse solution is cheaper, since \nSee Section 11.4 for more details on sparse regression. \nIf we combine this regularizer with our smooth loss, we get \nWe can optimize this objective using proximal gradient descent. The key question is how to compute the prox operator for the function $f ( pmb theta ) = | | pmb theta | | _ { 1 }$ . Since this function decomposes over dimensions $d$ , the proximal projection can be computed componentwise. From Equation (8.120), with $eta = 1$ , we have \nIn Section 11.4.3, we show that the solution to this is given by \nThis is known as the soft thresholding operator, since values less than $lambda$ in absolute value are set to 0 (thresholded), but in a continuous way. Note that soft thresholding can be written more compactly as \nwhere $theta _ { + } = operatorname* { m a x } ( theta , 0 )$ is the positive part of $theta$ . In the vector case, we perform this elementwise: \nSee Section 11.4.9.3 for an application of this method to sparse linear regression. \n8.6.3 Proximal operator for quantization \nIn some applications (e.g., when training deep neural networks to run on memory-limited edge devices, such as mobile phones) we want to ensure that the parameters are quantized. For example, in the extreme case where each parameter can only be -1 or $+ 1$ , the state space becomes $mathcal { C } = { - 1 , + 1 } ^ { D }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nLet us define a regularizer that measures distance to the nearest quantized version of the parameter vector: \n(We could also use the $ell _ { 2 }$ norm.) In the case of $mathcal { C } = { - 1 , + 1 } ^ { D }$ , this becomes \nLet us define the corresponding quantization operator to be \nThe core difficulty with quantized learning is that quantization is not a differentiable operation. A popular solution to this is to use the straight-through estimator, which uses the approximation $begin{array} { r } { frac { partial mathcal { L } } { partial q ( theta ) } approx frac { partial mathcal { L } } { partial theta } } end{array}$ (see e.g., [Yin+19]). The corresponding update can be done in two steps: first compute the gradient vector at the quantized version of the current parameters, and then update the unconstrained parameters using this approximate gradient: \nWhen applied to $mathcal { C } = { - 1 , + 1 } ^ { D }$ , this is known as the binary connect method [CBD15]. \nWe can get better results using proximal gradient descent, in which we treat quantization as a regularizer, rather than a hard constraint; this is known as ProxQuant [BWL19]. The update becomes \nIn the case that $mathcal { C } = { - 1 , + 1 } ^ { D }$ , one can show that the proximal operator is a generalization of the soft thresholding operator in Equation (8.135): \nThis can be generalized to other forms of quantization; see [Yin+19] for details. \n8.6.4 Incremental (online) proximal methods \nMany ML problems have an objective function which is a sum of losses, one per example. Such problems can be solved incrementally; this is a special case of online learning. It is possible to extend proximal methods to this setting. For a probabilistic perspective on such methods (in terms of Kalman filtering), see [AEM18; Aky+19]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Proximal gradient method *",
        "subsubsection": "Proximal operator for quantization"
    },
    {
        "content": "Let us define a regularizer that measures distance to the nearest quantized version of the parameter vector: \n(We could also use the $ell _ { 2 }$ norm.) In the case of $mathcal { C } = { - 1 , + 1 } ^ { D }$ , this becomes \nLet us define the corresponding quantization operator to be \nThe core difficulty with quantized learning is that quantization is not a differentiable operation. A popular solution to this is to use the straight-through estimator, which uses the approximation $begin{array} { r } { frac { partial mathcal { L } } { partial q ( theta ) } approx frac { partial mathcal { L } } { partial theta } } end{array}$ (see e.g., [Yin+19]). The corresponding update can be done in two steps: first compute the gradient vector at the quantized version of the current parameters, and then update the unconstrained parameters using this approximate gradient: \nWhen applied to $mathcal { C } = { - 1 , + 1 } ^ { D }$ , this is known as the binary connect method [CBD15]. \nWe can get better results using proximal gradient descent, in which we treat quantization as a regularizer, rather than a hard constraint; this is known as ProxQuant [BWL19]. The update becomes \nIn the case that $mathcal { C } = { - 1 , + 1 } ^ { D }$ , one can show that the proximal operator is a generalization of the soft thresholding operator in Equation (8.135): \nThis can be generalized to other forms of quantization; see [Yin+19] for details. \n8.6.4 Incremental (online) proximal methods \nMany ML problems have an objective function which is a sum of losses, one per example. Such problems can be solved incrementally; this is a special case of online learning. It is possible to extend proximal methods to this setting. For a probabilistic perspective on such methods (in terms of Kalman filtering), see [AEM18; Aky+19]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n8.7 Bound optimization * \nIn this section, we consider a class of algorithms known as bound optimization or MM algorithms. In the context of minimization, MM stands for majorize-minimize. In the context of maximization, MM stands for minorize-maximize. We will discuss a special case of MM, known as expectation maximization or EM, in Section 8.7.2. \n8.7.1 The general algorithm \nIn this section, we give a brief outline of MM methods. (More details can be found in e.g., [HL04; Mai15; SBP17; Nad+19].) To be consistent with the literature, we assume our goal is to maximize some function $ell ( pmb theta )$ , such as the log likelihood, wrt its parameters $pmb theta$ . The basic approach in MM algorithms is to construct a surrogate function $Q ( pmb theta , pmb theta ^ { t } )$ which is a tight lowerbound to $ell ( pmb theta )$ such that $Q ( pmb theta , pmb theta ^ { t } ) leq ell ( pmb theta )$ and $Q ( pmb theta ^ { t } , pmb theta ^ { t } ) = ell ( pmb theta ^ { t } )$ . If these conditions are met, we say that $Q$ minorizes $ell$ . We then perform the following update at each step: \nThis guarantees us monotonic increases in the original objective: \nwhere the first inequality follows since $Q ( pmb theta ^ { t + 1 } , pmb theta ^ { prime } )$ is a lower bound on $ell ( pmb { theta } ^ { t + 1 } )$ for any $pmb { theta } ^ { prime }$ ; the second inequality follows from Equation (8.144); and the final equality follows the tightness property. As a consequence of this result, if you do not observe monotonic increase of the objective, you must have an error in your math and/or code. This is a surprisingly powerful debugging tool. \nThis process is sketched in Figure 8.23. The dashed red curve is the original function (e.g., the log-likelihood of the observed data). The solid blue curve is the lower bound, evaluated at $pmb { theta } ^ { t }$ ; this touches the objective function at $pmb { theta } ^ { t }$ . We then set $pmb { theta } ^ { t + 1 }$ to the maximum of the lower bound (blue curve), and fit a new bound at that point (dotted green curve). The maximum of this new bound becomes $pmb { theta } ^ { t + 2 }$ , etc. \nIf $Q$ is a quadratic lower bound, the overall method is similar to Newton’s method, which repeatedly fits and then optimizes a quadratic approximation, as shown in Figure 8.14(a). The difference is that optimizing $Q$ is guaranteed to lead to an improvement in the objective, even if it is not convex, whereas Newton’s method may overshoot or lead to a decrease in the objective, as shown in Figure 8.24, since it is a quadratic approximation and not a bound. \n8.7.2 The EM algorithm \nIn this section, we discuss the expectation maximization (EM) algorithm [DLR77; MK97], which is a bound optimization algorithm designed to compute the MLE or MAP parameter estimate for probability models that have missing data and/or hidden variables. We let ${ bf { y } } _ { n }$ be the visible data for example $n$ , and $z _ { n }$ be the hidden data. \nThe basic idea behind EM is to alternate between estimating the hidden variables (or missing values) during the $mathbf { E }$ step (expectation step), and then using the fully observed data to compute the MLE during the M step (maximization step). Of course, we need to iterate this process, since the expected values depend on the parameters, but the parameters depend on the expected values. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Proximal gradient method *",
        "subsubsection": "Incremental (online) proximal methods"
    },
    {
        "content": "8.7 Bound optimization * \nIn this section, we consider a class of algorithms known as bound optimization or MM algorithms. In the context of minimization, MM stands for majorize-minimize. In the context of maximization, MM stands for minorize-maximize. We will discuss a special case of MM, known as expectation maximization or EM, in Section 8.7.2. \n8.7.1 The general algorithm \nIn this section, we give a brief outline of MM methods. (More details can be found in e.g., [HL04; Mai15; SBP17; Nad+19].) To be consistent with the literature, we assume our goal is to maximize some function $ell ( pmb theta )$ , such as the log likelihood, wrt its parameters $pmb theta$ . The basic approach in MM algorithms is to construct a surrogate function $Q ( pmb theta , pmb theta ^ { t } )$ which is a tight lowerbound to $ell ( pmb theta )$ such that $Q ( pmb theta , pmb theta ^ { t } ) leq ell ( pmb theta )$ and $Q ( pmb theta ^ { t } , pmb theta ^ { t } ) = ell ( pmb theta ^ { t } )$ . If these conditions are met, we say that $Q$ minorizes $ell$ . We then perform the following update at each step: \nThis guarantees us monotonic increases in the original objective: \nwhere the first inequality follows since $Q ( pmb theta ^ { t + 1 } , pmb theta ^ { prime } )$ is a lower bound on $ell ( pmb { theta } ^ { t + 1 } )$ for any $pmb { theta } ^ { prime }$ ; the second inequality follows from Equation (8.144); and the final equality follows the tightness property. As a consequence of this result, if you do not observe monotonic increase of the objective, you must have an error in your math and/or code. This is a surprisingly powerful debugging tool. \nThis process is sketched in Figure 8.23. The dashed red curve is the original function (e.g., the log-likelihood of the observed data). The solid blue curve is the lower bound, evaluated at $pmb { theta } ^ { t }$ ; this touches the objective function at $pmb { theta } ^ { t }$ . We then set $pmb { theta } ^ { t + 1 }$ to the maximum of the lower bound (blue curve), and fit a new bound at that point (dotted green curve). The maximum of this new bound becomes $pmb { theta } ^ { t + 2 }$ , etc. \nIf $Q$ is a quadratic lower bound, the overall method is similar to Newton’s method, which repeatedly fits and then optimizes a quadratic approximation, as shown in Figure 8.14(a). The difference is that optimizing $Q$ is guaranteed to lead to an improvement in the objective, even if it is not convex, whereas Newton’s method may overshoot or lead to a decrease in the objective, as shown in Figure 8.24, since it is a quadratic approximation and not a bound. \n8.7.2 The EM algorithm \nIn this section, we discuss the expectation maximization (EM) algorithm [DLR77; MK97], which is a bound optimization algorithm designed to compute the MLE or MAP parameter estimate for probability models that have missing data and/or hidden variables. We let ${ bf { y } } _ { n }$ be the visible data for example $n$ , and $z _ { n }$ be the hidden data. \nThe basic idea behind EM is to alternate between estimating the hidden variables (or missing values) during the $mathbf { E }$ step (expectation step), and then using the fully observed data to compute the MLE during the M step (maximization step). Of course, we need to iterate this process, since the expected values depend on the parameters, but the parameters depend on the expected values. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Bound optimization *",
        "subsubsection": "The general algorithm"
    },
    {
        "content": "8.7 Bound optimization * \nIn this section, we consider a class of algorithms known as bound optimization or MM algorithms. In the context of minimization, MM stands for majorize-minimize. In the context of maximization, MM stands for minorize-maximize. We will discuss a special case of MM, known as expectation maximization or EM, in Section 8.7.2. \n8.7.1 The general algorithm \nIn this section, we give a brief outline of MM methods. (More details can be found in e.g., [HL04; Mai15; SBP17; Nad+19].) To be consistent with the literature, we assume our goal is to maximize some function $ell ( pmb theta )$ , such as the log likelihood, wrt its parameters $pmb theta$ . The basic approach in MM algorithms is to construct a surrogate function $Q ( pmb theta , pmb theta ^ { t } )$ which is a tight lowerbound to $ell ( pmb theta )$ such that $Q ( pmb theta , pmb theta ^ { t } ) leq ell ( pmb theta )$ and $Q ( pmb theta ^ { t } , pmb theta ^ { t } ) = ell ( pmb theta ^ { t } )$ . If these conditions are met, we say that $Q$ minorizes $ell$ . We then perform the following update at each step: \nThis guarantees us monotonic increases in the original objective: \nwhere the first inequality follows since $Q ( pmb theta ^ { t + 1 } , pmb theta ^ { prime } )$ is a lower bound on $ell ( pmb { theta } ^ { t + 1 } )$ for any $pmb { theta } ^ { prime }$ ; the second inequality follows from Equation (8.144); and the final equality follows the tightness property. As a consequence of this result, if you do not observe monotonic increase of the objective, you must have an error in your math and/or code. This is a surprisingly powerful debugging tool. \nThis process is sketched in Figure 8.23. The dashed red curve is the original function (e.g., the log-likelihood of the observed data). The solid blue curve is the lower bound, evaluated at $pmb { theta } ^ { t }$ ; this touches the objective function at $pmb { theta } ^ { t }$ . We then set $pmb { theta } ^ { t + 1 }$ to the maximum of the lower bound (blue curve), and fit a new bound at that point (dotted green curve). The maximum of this new bound becomes $pmb { theta } ^ { t + 2 }$ , etc. \nIf $Q$ is a quadratic lower bound, the overall method is similar to Newton’s method, which repeatedly fits and then optimizes a quadratic approximation, as shown in Figure 8.14(a). The difference is that optimizing $Q$ is guaranteed to lead to an improvement in the objective, even if it is not convex, whereas Newton’s method may overshoot or lead to a decrease in the objective, as shown in Figure 8.24, since it is a quadratic approximation and not a bound. \n8.7.2 The EM algorithm \nIn this section, we discuss the expectation maximization (EM) algorithm [DLR77; MK97], which is a bound optimization algorithm designed to compute the MLE or MAP parameter estimate for probability models that have missing data and/or hidden variables. We let ${ bf { y } } _ { n }$ be the visible data for example $n$ , and $z _ { n }$ be the hidden data. \nThe basic idea behind EM is to alternate between estimating the hidden variables (or missing values) during the $mathbf { E }$ step (expectation step), and then using the fully observed data to compute the MLE during the M step (maximization step). Of course, we need to iterate this process, since the expected values depend on the parameters, but the parameters depend on the expected values. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn Section 8.7.2.1, we show that EM is an MM algorithm, which implies that this iterative procedure will converge to a local maximum of the log likelihood. The speed of convergence depends on the amount of missing data, which affects the tightness of the bound [XJ96; MD97; SRG03; KKS20]. \n8.7.2.1 Lower bound \nThe goal of EM is to maximize the log likelihood of the observed data: \nwhere ${ bf { y } } _ { n }$ are the visible variables and $z _ { n }$ are the hidden variables. Unfortunately this is hard to optimize, since the log cannot be pushed inside the sum. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nEM gets around this problem as follows. First, consider a set of arbitrary distributions $q _ { n } ( z _ { n } )$ over each hidden variable $z _ { n }$ . The observed data log likelihood can be written as follows: \nUsing Jensen’s inequality (Equation (6.34)), we can push the log (which is a concave function) inside the expectation to get the following lower bound on the log likelihood: \nwhere $mathbb { H } ( q )$ is the entropy of probability distribution $q$ , and ${ mathrm { { L } } } ( theta , { q _ { n } } )$ is called the evidence lower bound or ELBO, since it is a lower bound on the log marginal likelihood, $log p ( { pmb y } _ { 1 : N } | pmb theta )$ , also called the evidence. Optimizing this bound is the basis of variational inference, which we discuss in Section 4.6.8.3. \n8.7.2.2 E step \nWe see that the lower bound is a sum of $N$ terms, each of which has the following form: \nbwehtewre $begin{array} { r } { D _ { mathbb { K L } } left( q parallel p right) triangleq sum _ { z } q ( z ) log frac { q ( z ) } { p ( z ) } } end{array}$ nisd . WKeulldbisacuks-sLteihbisleirndimvoeregednecteai(loirnKSeLctdiiovner6g.e2n,cbeuftorthsehokrety) $q$ $p$   \nproperty we need here is that $D _ { mathbb { K L } } left( q parallel p right) geq 0$ and $D _ { mathbb { K L } } left( q parallel p right) = 0$ iff $q = p$ . Hence we can maximize   \nthe lower bound $mathbb { L } ( pmb { theta } , { q _ { n } } )$ wrt $left{ q _ { n } right}$ by setting each one to $q _ { n } ^ { * } = p ( z _ { n } | pmb { y } _ { n } , pmb { theta } )$ . This is called the $mathbf { E }$   \nstep. This ensures the ELBO is a tight lower bound: \nTo see how this connects to bound optimization, let us define \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThen we have $Q ( pmb theta , pmb theta ^ { t } ) leq ell ( pmb theta )$ and $Q ( pmb theta ^ { t } , pmb theta ^ { t } ) = ell ( pmb theta ^ { t } )$ , as required. \nHowever, if we cannot compute the posteriors $p ( z _ { n } | pmb { y } _ { n } ; pmb { theta } ^ { t } )$ exactly, we can still use an approximate distribution $q ( z _ { n } | pmb { y } _ { n } ; pmb { theta } ^ { t } )$ ; this will yield a non-tight lower-bound on the log-likelihood. This generalized version of EM is known as variational EM [NH98]. See the sequel to this book, [Mur23], for details. \n8.7.2.3 M step \nIn the M step, we need to maximize $mathbb { E } ( pmb { theta } , { q _ { n } ^ { t } } )$ wrt $pmb theta$ , where the $q _ { n } ^ { t }$ are the distributions computed in the $mathrm { E }$ step at iteration $t$ . Since the entropy terms $mathbb { H } ( q _ { n } )$ are constant wrt $pmb theta$ , so we can drop them in the M step. We are left with \nThis is called the expected complete data log likelihood. If the joint probability is in the exponential family (Section 3.4), we can rewrite this as \nwhere $mathbb { E } left[ mathcal { T } ( pmb { y } _ { n } , pmb { z } _ { n } ) right]$ are called the expected sufficient statistics. \nIn the M step, we maximize the expected complete data log likelihood to ge \nIn the case of the exponential family, the maximization can be solved in closed-form by matching the moments of the expected sufficient statistics. \nWe see from the above that the E step does not in fact need to return the full set of posterior distributions ${ q ( z _ { n } ) }$ , but can instead just return the sum of the expected sufficient statistics, $begin{array} { r l } { sum _ { n } mathbb { E } _ { q ( pmb { z } _ { n } ) } left[ mathcal { T } left( pmb { y } _ { n } , pmb { z } _ { n } right) right] } & { { } } end{array}$ . This will become clearer in the examples below. \n8.7.3 Example: EM for a GMM \nIn this section, we show how to use the EM algorithm to compute MLE and MAP estimates of the parameters for a Gaussian mixture model (GMM). \n8.7.3.1 E step \nThe $mathrm { E }$ step simply computes the responsibility of cluster $k$ for generating data point $n$ , as estimated using the current parameter estimates ${ pmb theta } ^ { ( t ) }$ : \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Bound optimization *",
        "subsubsection": "The EM algorithm"
    },
    {
        "content": "Then we have $Q ( pmb theta , pmb theta ^ { t } ) leq ell ( pmb theta )$ and $Q ( pmb theta ^ { t } , pmb theta ^ { t } ) = ell ( pmb theta ^ { t } )$ , as required. \nHowever, if we cannot compute the posteriors $p ( z _ { n } | pmb { y } _ { n } ; pmb { theta } ^ { t } )$ exactly, we can still use an approximate distribution $q ( z _ { n } | pmb { y } _ { n } ; pmb { theta } ^ { t } )$ ; this will yield a non-tight lower-bound on the log-likelihood. This generalized version of EM is known as variational EM [NH98]. See the sequel to this book, [Mur23], for details. \n8.7.2.3 M step \nIn the M step, we need to maximize $mathbb { E } ( pmb { theta } , { q _ { n } ^ { t } } )$ wrt $pmb theta$ , where the $q _ { n } ^ { t }$ are the distributions computed in the $mathrm { E }$ step at iteration $t$ . Since the entropy terms $mathbb { H } ( q _ { n } )$ are constant wrt $pmb theta$ , so we can drop them in the M step. We are left with \nThis is called the expected complete data log likelihood. If the joint probability is in the exponential family (Section 3.4), we can rewrite this as \nwhere $mathbb { E } left[ mathcal { T } ( pmb { y } _ { n } , pmb { z } _ { n } ) right]$ are called the expected sufficient statistics. \nIn the M step, we maximize the expected complete data log likelihood to ge \nIn the case of the exponential family, the maximization can be solved in closed-form by matching the moments of the expected sufficient statistics. \nWe see from the above that the E step does not in fact need to return the full set of posterior distributions ${ q ( z _ { n } ) }$ , but can instead just return the sum of the expected sufficient statistics, $begin{array} { r l } { sum _ { n } mathbb { E } _ { q ( pmb { z } _ { n } ) } left[ mathcal { T } left( pmb { y } _ { n } , pmb { z } _ { n } right) right] } & { { } } end{array}$ . This will become clearer in the examples below. \n8.7.3 Example: EM for a GMM \nIn this section, we show how to use the EM algorithm to compute MLE and MAP estimates of the parameters for a Gaussian mixture model (GMM). \n8.7.3.1 E step \nThe $mathrm { E }$ step simply computes the responsibility of cluster $k$ for generating data point $n$ , as estimated using the current parameter estimates ${ pmb theta } ^ { ( t ) }$ : \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n8.7.3.2 M step \nThe M step maximizes the expected complete data log likelihood, given by \nwhere $z _ { n k } = mathbb { I } left( z _ { n } = k right)$ is a one-hot encoding of the categorical value $z _ { n }$ . This objective is just a weighted version of the standard problem of computing the MLEs of an MVN (see Section 4.2.6). One can show that the new parameter estimates are given by \njwuhsterteh $begin{array} { r } { boldsymbol { r } _ { k } ^ { ( t ) } triangleq sum _ { n } boldsymbol { r } _ { n k } ^ { ( t ) } } end{array}$ erisagteheofwaelilgphtoeindtsnuasmsibgenreodf tpo icnltus taesrs ,naenddtothcelucsotvear $k$ a.ncTehiesmpreoapnoorft colnuasltetro $k$ hies $k$   \nweighted empirical scatter matrix. \nThe M step for the mixture weights is simply a weighted form of the usual MLE: \n8.7.3.3 Example \nAn example of the algorithm in action is shown in Figure 8.25 where we fit some 2d data with a 2 component GMM. The data set, from [Bis06], is derived from measurements of the Old Faithful geyser in Yellowstone National Park. In particular, we plot the time to next eruption in minutes versus the duration of the eruption in minutes. The data was standardized, by removing the mean and dividing by the standard deviation, before processing; this often helps convergence. We start with $pmb { mu } _ { 1 } = ( - 1 , 1 )$ , $boldsymbol { Sigma } _ { 1 } = mathbf { I }$ , $mu _ { 2 } = ( 1 , - 1 )$ , $Sigma _ { 2 } = mathbf { I }$ . We then show the cluster assignments, and corresponding mixture components, at various iterations. \nFor more details on applying GMMs for clustering, see Section 21.4.1. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n8.7.3.4 MAP estimation \nComputing the MLE of a GMM often suffers from numerical problems and overfitting. To see why, suppose for simplicity that $boldsymbol { Sigma } _ { k } = sigma _ { k } ^ { 2 } mathbf { I }$ for all $k$ . It is possible to get an infinite likelihood by assigning one of the centers, say $pmb { mu } _ { k }$ , to a single data point, say ${ bf { y } } _ { n }$ , since then the likelihood of that data point is given by \nHence we can drive this term to infinity by letting $sigma _ { k } to 0$ , as shown in Figure 8.26(a). We call this the “collapsing variance problem”. \nAn easy solution to this is to perform MAP estimation. Fortunately, we can still use EM to find this MAP estimate. Our goal is now to maximize the expected complete data log-likelihood plus the log prior: \nNote that the $mathrm { E }$ step remains unchanged, but the M step needs to be modified, as we now explain. For the prior on the mixture weights, it is natural to use a Dirichlet prior (Section 4.6.3.2) $pi sim operatorname { D i r } ( alpha )$ , since this is conjugate to the categorical distribution. The MAP estimate is given by \nIf we use a uniform prior, $alpha _ { k } = 1$ , this reduces to the MLE. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFor the prior on the mixture components, let us consider a conjugate prior of the form \nThis is called the Normal-Inverse-Wishart distribution (see the sequel to this book, [Mur23], for details.) Suppose we set the hyper-parameters for $pmb { mu }$ to be $breve { kappa } = 0$ , so that the $pmb { mu } _ { k }$ are unregularized; thus the prior will only influence our estimate of $Sigma _ { k }$ . In this case, the MAP estimates are given by \nwhere $hat { pmb { mu } } _ { k }$ is the MLE for $pmb { mu } _ { k }$ from Equation (8.165), and $hat { Sigma } _ { k }$ is the MLE for $Sigma _ { k }$ from Equation (8.166). Now we discuss how to set the prior covariance, $breve { mathbf { S } }$ . One possibility (suggested in [FR07, p163]) is to use \nwhere $begin{array} { r } { s _ { d } ^ { 2 } = ( 1 / N ) sum _ { n = 1 } ^ { N } ( x _ { n d } - overline { { x } } _ { d } ) ^ { 2 } } end{array}$ is the pooled variance for dimension $d$ . The parameter $breve { nu }$ controls how strongly we believe this prior. The weakest prior we can use, while still being proper, is to set $scriptstyle { overrightarrow { nu } } = D + 2$ , so this is a common choice. \nWe now illustrate the benefits of using MAP estimation instead of ML estimation in the context of GMMs. We apply EM to some synthetic data with $N = 1 0 0$ samples in $D$ dimensions, using either ML or MAP estimation. We count the trial as a “failure” if there are numerical issues involving singular matrices. For each dimensionality, we conduct 5 random trials. The results are illustrated in Figure 8.26(b). We see that as soon as $D$ becomes even moderately large, ML estimation crashes and burns, whereas MAP with an appropriate prior estimation rarely encounters numerical problems. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nH 19.5 14. 9.5   \n25   \n20 −0.5   \n15 −5.5   \n10 −10.5 −15.5 −15.5 −10.5 −5.5 ${ { bf Pi } ^ { - 0 . 5 } } _ { { bf Pi } ^ { sharp } mathrm { bf _ { Pi } } }$ 4.5 9.5 14.5 19.5   \n−25 −20 −15 −10 10 (a) (b) \n8.7.3.5 Nonconvexity of the NLL \nThe likelihood for a mixture model is given by \nIn general, this will have multiple modes, and hence there will not be a unique global optimum. \nFigure 8.27 illustrates this for a mixture of 2 Gaussians in 1d. We see that there are two equally good global optima, corresponding to two different labelings of the clusters, one in which the left peak corresponds to $z = 1$ , and one in which the left peak corresponds to $z = 2$ . This is called the label switching problem; see Section 21.4.1.2 for more details. \nThe question of how many modes there are in the likelihood function is hard to answer. There are $K !$ possible labelings, but some of the peaks might get merged, depending on how far apart the $mu _ { k }$ are. Nevertheless, there can be an exponential number of modes. Consequently, finding any global optimum is NP-hard [Alo+09; Dri+04]. We will therefore have to be satisfied with finding a local optimum. To find a good local optimum, we can use Kmeans $^ { + + }$ (Section 21.3.4) to initialize EM. \n8.8 Blackbox and derivative free optimization \nIn some optimization problems, the objective function is a blackbox, meaning that its functional form is unknown. This means we cannot use gradient-based methods to optimize it. Instead, solving such problems require blackbox optimization (BBO) methods, also called derivative free optimization (DFO). \nIn ML, this kind of problem often arises when performing model selection. For example, suppose we have some hyper-parameters, $lambda in Lambda$ , which control the type or complexity of a model. We often define the objective function ${ mathcal { L } } ( lambda )$ to be the loss on a validation set (see Section 4.5.4). Since the validation loss depends on the optimal model parameters, which are computed using a complex \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license algorithm, this objective function is effectively a blackbox.4",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Bound optimization *",
        "subsubsection": "Example: EM for a GMM"
    },
    {
        "content": "H 19.5 14. 9.5   \n25   \n20 −0.5   \n15 −5.5   \n10 −10.5 −15.5 −15.5 −10.5 −5.5 ${ { bf Pi } ^ { - 0 . 5 } } _ { { bf Pi } ^ { sharp } mathrm { bf _ { Pi } } }$ 4.5 9.5 14.5 19.5   \n−25 −20 −15 −10 10 (a) (b) \n8.7.3.5 Nonconvexity of the NLL \nThe likelihood for a mixture model is given by \nIn general, this will have multiple modes, and hence there will not be a unique global optimum. \nFigure 8.27 illustrates this for a mixture of 2 Gaussians in 1d. We see that there are two equally good global optima, corresponding to two different labelings of the clusters, one in which the left peak corresponds to $z = 1$ , and one in which the left peak corresponds to $z = 2$ . This is called the label switching problem; see Section 21.4.1.2 for more details. \nThe question of how many modes there are in the likelihood function is hard to answer. There are $K !$ possible labelings, but some of the peaks might get merged, depending on how far apart the $mu _ { k }$ are. Nevertheless, there can be an exponential number of modes. Consequently, finding any global optimum is NP-hard [Alo+09; Dri+04]. We will therefore have to be satisfied with finding a local optimum. To find a good local optimum, we can use Kmeans $^ { + + }$ (Section 21.3.4) to initialize EM. \n8.8 Blackbox and derivative free optimization \nIn some optimization problems, the objective function is a blackbox, meaning that its functional form is unknown. This means we cannot use gradient-based methods to optimize it. Instead, solving such problems require blackbox optimization (BBO) methods, also called derivative free optimization (DFO). \nIn ML, this kind of problem often arises when performing model selection. For example, suppose we have some hyper-parameters, $lambda in Lambda$ , which control the type or complexity of a model. We often define the objective function ${ mathcal { L } } ( lambda )$ to be the loss on a validation set (see Section 4.5.4). Since the validation loss depends on the optimal model parameters, which are computed using a complex \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license algorithm, this objective function is effectively a blackbox.4 \n\nA simple approach to such problems is to use grid search, where we evaluate each point in the parameter space, and pick the one with the lowest loss. Unfortunately, this does not scale to high dimensions, because of the curse of dimensionality. In addition, even in low dimensions this can be expensive if evaluate the blackbox objective is expensive (e.g., if it first requires training the model before computing the validation loss). Various solutions to this problem have been proposed. See the sequel to this book, [Mur23], for details. \n8.9 Exercises \nExercise 8.1 [Subderivative of the hinge loss function $^ *$ ] Let $f ( x ) = ( 1 - x ) _ { + }$ be the hinge loss function, where $( z ) _ { + } = operatorname* { m a x } ( 0 , z )$ . What are $partial f ( 0 )$ , $partial f ( 1 )$ , and $partial f ( 2 )$ ? \nExercise 8.2 [EM for the Student distribution] \nDerive the EM equations for computing the MLE for a multivariate Student distribution. Consider the case where the dof parameter is known and unknown separately. Hint: write the Student distribution as a scale mixture of Gaussians.",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Blackbox and derivative free optimization",
        "subsubsection": "N/A"
    },
    {
        "content": "A simple approach to such problems is to use grid search, where we evaluate each point in the parameter space, and pick the one with the lowest loss. Unfortunately, this does not scale to high dimensions, because of the curse of dimensionality. In addition, even in low dimensions this can be expensive if evaluate the blackbox objective is expensive (e.g., if it first requires training the model before computing the validation loss). Various solutions to this problem have been proposed. See the sequel to this book, [Mur23], for details. \n8.9 Exercises \nExercise 8.1 [Subderivative of the hinge loss function $^ *$ ] Let $f ( x ) = ( 1 - x ) _ { + }$ be the hinge loss function, where $( z ) _ { + } = operatorname* { m a x } ( 0 , z )$ . What are $partial f ( 0 )$ , $partial f ( 1 )$ , and $partial f ( 2 )$ ? \nExercise 8.2 [EM for the Student distribution] \nDerive the EM equations for computing the MLE for a multivariate Student distribution. Consider the case where the dof parameter is known and unknown separately. Hint: write the Student distribution as a scale mixture of Gaussians. \nPart II \nLinear Models",
        "chapter": "I Foundations",
        "section": "Optimization",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "9 Linear Discriminant Analysis \n9.1 Introduction \nIn this chapter, we consider classification models of the following form: \nThe term $p ( y = c ; pmb theta )$ is the prior over class labels, and the term $p ( { pmb x } | y = c ; pmb theta )$ is called the class conditional density for class $c$ . \nThe overall model is called a generative classifier, since it specifies a way to generate the features $_ { x }$ for each class $c$ , by sampling from $p ( { pmb x } | y = c ; pmb theta )$ . By contrast, a discriminative classifier directly models the class posterior $p ( boldsymbol { y } | boldsymbol { x } ; boldsymbol { theta } )$ . We discuss the pros and cons of these two approaches to classification in Section 9.4. \nIf we choose the class conditional densities in a special way, we will see that the resulting posterior over classes is a linear function of $_ { x }$ , i.e., $log p ( boldsymbol { y } = boldsymbol { c } | boldsymbol { x } ; boldsymbol { theta } ) = boldsymbol { w } ^ { top } boldsymbol { x } + mathrm { c o n s t }$ , where $mathbf { boldsymbol { w } }$ is derived from $pmb theta$ . Thus the overall method is called linear discriminant analysis or LDA.1 \n9.2 Gaussian discriminant analysis \nIn this section, we consider a generative classifier where the class conditional densities are multivariate Gaussians: \nThe corresponding class posterior therefore has the form \nwhere $pi _ { c } = p ( y = c )$ is the prior probability of label $c$ . (Note that we can ignore the normalization constant in the denominator of the posterior, since it is independent of $c$ .) We call this model Gaussian discriminant analysis or GDA.",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "9.2.1 Quadratic decision boundaries \nFrom Equation (9.3), we see that the log posterior over class labels is given by \nThis is called the discriminant function. We see that the decision boundary between any two classes, say $c$ and $c ^ { prime }$ , will be a quadratic function of $_ { x }$ . Hence this is known as quadratic discriminant analysis (QDA). \nFor example, consider the 2d data from 3 different classes in Figure 9.1a. We fit full covariance Gaussian class-conditionals (using the method explained in Section 9.2.4), and plot the results in Figure 9.1b. We see that the features for the blue class are somewhat correlated, whereas the features for the green class are independent, and the features for the red class are independent and isotropic (spherical covariance). In Figure 9.2a, we see that the resulting decision boundaries are quadratic functions of $_ { x }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n9.2.2 Linear decision boundaries \nNow we consider a special case of Gaussian discriminant analysis in which the covariance matrices are tied or shared across classes, so $Sigma _ { c } = Sigma$ . If $pmb { Sigma }$ is independent of $c$ , we can simplify Equation (9.4) as follows: \nThe final term is independent of $c$ , and hence is an irrelevant additive constant that can be dropped. Hence we see that the discriminant function is a linear function of $_ { x }$ , so the decision boundaries will be linear. Hence this method is called linear discriminant analysis or LDA. See Figure 9.2b for an example. \n9.2.3 The connection between LDA and logistic regression \nIn this section, we derive an interesting connection between LDA and logistic regression, which we introduced in Section 2.5.3. From Equation (9.7) we can write \nwhere $pmb { w } _ { c } = [ gamma _ { c } , beta _ { c } ]$ . We see that Equation (9.8) has the same form as the multinomial logistic regression model. The key difference is that in LDA, we first fit the Gaussians (and class prior) to maximize the joint likelihood $p ( boldsymbol { x } , boldsymbol { y } | boldsymbol { theta } )$ , as discussed in Section 9.2.4, and then we derive $mathbf { boldsymbol { w } }$ from $pmb theta$ . By contrast, in logistic regression, we estimate $mathbf { boldsymbol { w } }$ directly to maximize the conditional likelihood $p ( boldsymbol { y } | boldsymbol { x } , boldsymbol { w } )$ . In general, these can give different results (see Exercise 10.3). \nTo gain further insight into Equation (9.8), let us consider the binary case. In this case, the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Gaussian discriminant analysis",
        "subsubsection": "Quadratic decision boundaries"
    },
    {
        "content": "9.2.2 Linear decision boundaries \nNow we consider a special case of Gaussian discriminant analysis in which the covariance matrices are tied or shared across classes, so $Sigma _ { c } = Sigma$ . If $pmb { Sigma }$ is independent of $c$ , we can simplify Equation (9.4) as follows: \nThe final term is independent of $c$ , and hence is an irrelevant additive constant that can be dropped. Hence we see that the discriminant function is a linear function of $_ { x }$ , so the decision boundaries will be linear. Hence this method is called linear discriminant analysis or LDA. See Figure 9.2b for an example. \n9.2.3 The connection between LDA and logistic regression \nIn this section, we derive an interesting connection between LDA and logistic regression, which we introduced in Section 2.5.3. From Equation (9.7) we can write \nwhere $pmb { w } _ { c } = [ gamma _ { c } , beta _ { c } ]$ . We see that Equation (9.8) has the same form as the multinomial logistic regression model. The key difference is that in LDA, we first fit the Gaussians (and class prior) to maximize the joint likelihood $p ( boldsymbol { x } , boldsymbol { y } | boldsymbol { theta } )$ , as discussed in Section 9.2.4, and then we derive $mathbf { boldsymbol { w } }$ from $pmb theta$ . By contrast, in logistic regression, we estimate $mathbf { boldsymbol { w } }$ directly to maximize the conditional likelihood $p ( boldsymbol { y } | boldsymbol { x } , boldsymbol { w } )$ . In general, these can give different results (see Exercise 10.3). \nTo gain further insight into Equation (9.8), let us consider the binary case. In this case, the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Gaussian discriminant analysis",
        "subsubsection": "Linear decision boundaries"
    },
    {
        "content": "9.2.2 Linear decision boundaries \nNow we consider a special case of Gaussian discriminant analysis in which the covariance matrices are tied or shared across classes, so $Sigma _ { c } = Sigma$ . If $pmb { Sigma }$ is independent of $c$ , we can simplify Equation (9.4) as follows: \nThe final term is independent of $c$ , and hence is an irrelevant additive constant that can be dropped. Hence we see that the discriminant function is a linear function of $_ { x }$ , so the decision boundaries will be linear. Hence this method is called linear discriminant analysis or LDA. See Figure 9.2b for an example. \n9.2.3 The connection between LDA and logistic regression \nIn this section, we derive an interesting connection between LDA and logistic regression, which we introduced in Section 2.5.3. From Equation (9.7) we can write \nwhere $pmb { w } _ { c } = [ gamma _ { c } , beta _ { c } ]$ . We see that Equation (9.8) has the same form as the multinomial logistic regression model. The key difference is that in LDA, we first fit the Gaussians (and class prior) to maximize the joint likelihood $p ( boldsymbol { x } , boldsymbol { y } | boldsymbol { theta } )$ , as discussed in Section 9.2.4, and then we derive $mathbf { boldsymbol { w } }$ from $pmb theta$ . By contrast, in logistic regression, we estimate $mathbf { boldsymbol { w } }$ directly to maximize the conditional likelihood $p ( boldsymbol { y } | boldsymbol { x } , boldsymbol { w } )$ . In general, these can give different results (see Exercise 10.3). \nTo gain further insight into Equation (9.8), let us consider the binary case. In this case, the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nposterior is given by \nwhere $sigma ( eta )$ refers to the sigmoid function. \nNow \nSo if we define \nthen we have $boldsymbol { w } ^ { intercal } boldsymbol { x } _ { 0 } = - ( gamma _ { 1 } - gamma _ { 0 } )$ , and hence \nThis has the same form as binary logistic regression. Hence the MAP decision rule is \nwhere $c = pmb { w } ^ { top } pmb { x } _ { 0 }$ . If $pi _ { 0 } = pi _ { 1 } = 0 . 5$ , then the threshold simplifies to ${ boldsymbol { c } } = { textstyle { frac { 1 } { 2 } } } { boldsymbol { w } } ^ { mathsf { T } } ( { pmb { mu } } _ { 1 } + { pmb { mu } } _ { 0 } )$ . \nTo interpret this equation geometrically, suppose $scriptstyle pmb { Sigma }  =  sigma ^ { 2 } mathbf { I }$ . In this case, ${ pmb w } = sigma ^ { - 2 } ( { pmb mu } _ { 1 } - { pmb mu } _ { 0 } )$ , which is parallel to a line joining the two centroids, $pmb { mu } _ { 0 }$ and $pmb { mu } _ { 1 }$ . So we can classify a point by projecting it onto this line, and then checking if the projection is closer to $pmb { mu } _ { 0 }$ or $pmb { mu } _ { 1 }$ , as illustrated in Figure 9.3. The question of how close it has to be depends on the prior over classes. If $pi _ { 1 } = pi _ { 0 }$ , then $begin{array} { r } { { pmb x } _ { 0 } = frac { 1 } { 2 } ( { pmb mu } _ { 1 } + { pmb mu } _ { 0 } ) } end{array}$ , which is halfway between the means. If we make $pi _ { 1 } > pi _ { 0 }$ , we have to be closer to $pmb { mu } _ { 0 }$ than halfway in order to pick class 0. And vice versa if $pi _ { 0 } > pi _ { 1 }$ . Thus we see that the class prior just changes the decision threshold, but not the overall shape of the decision boundary. (A similar argument applies in the multi-class case.) \n9.2.4 Model fitting \nWe now discuss how to fit a GDA model using maximum likelihood estimation. The likelihood function is as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Gaussian discriminant analysis",
        "subsubsection": "The connection between LDA and logistic regression"
    },
    {
        "content": "posterior is given by \nwhere $sigma ( eta )$ refers to the sigmoid function. \nNow \nSo if we define \nthen we have $boldsymbol { w } ^ { intercal } boldsymbol { x } _ { 0 } = - ( gamma _ { 1 } - gamma _ { 0 } )$ , and hence \nThis has the same form as binary logistic regression. Hence the MAP decision rule is \nwhere $c = pmb { w } ^ { top } pmb { x } _ { 0 }$ . If $pi _ { 0 } = pi _ { 1 } = 0 . 5$ , then the threshold simplifies to ${ boldsymbol { c } } = { textstyle { frac { 1 } { 2 } } } { boldsymbol { w } } ^ { mathsf { T } } ( { pmb { mu } } _ { 1 } + { pmb { mu } } _ { 0 } )$ . \nTo interpret this equation geometrically, suppose $scriptstyle pmb { Sigma }  =  sigma ^ { 2 } mathbf { I }$ . In this case, ${ pmb w } = sigma ^ { - 2 } ( { pmb mu } _ { 1 } - { pmb mu } _ { 0 } )$ , which is parallel to a line joining the two centroids, $pmb { mu } _ { 0 }$ and $pmb { mu } _ { 1 }$ . So we can classify a point by projecting it onto this line, and then checking if the projection is closer to $pmb { mu } _ { 0 }$ or $pmb { mu } _ { 1 }$ , as illustrated in Figure 9.3. The question of how close it has to be depends on the prior over classes. If $pi _ { 1 } = pi _ { 0 }$ , then $begin{array} { r } { { pmb x } _ { 0 } = frac { 1 } { 2 } ( { pmb mu } _ { 1 } + { pmb mu } _ { 0 } ) } end{array}$ , which is halfway between the means. If we make $pi _ { 1 } > pi _ { 0 }$ , we have to be closer to $pmb { mu } _ { 0 }$ than halfway in order to pick class 0. And vice versa if $pi _ { 0 } > pi _ { 1 }$ . Thus we see that the class prior just changes the decision threshold, but not the overall shape of the decision boundary. (A similar argument applies in the multi-class case.) \n9.2.4 Model fitting \nWe now discuss how to fit a GDA model using maximum likelihood estimation. The likelihood function is as follows \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHence the log-likelihood is given by \nThus we see that we can optimize $pi$ and the $( pmb { mu } _ { c } , pmb { Sigma } _ { c } )$ terms separately. \nFrom Section 4.2.4, we have that the MLE for the class prior is $begin{array} { r } { hat { pi } _ { c } = frac { N _ { c } } { N } } end{array}$ . Using the results from Section 4.2.6, we can derive the MLEs for the Gaussians as follows: \nUnfortunately the MLE for $hat { Sigma } _ { c }$ can easily overfit (i.e., the estimate may not be well-conditioned) if $N _ {  { D _ { c } } }$ is small compared to $D$ , the dimensionality of the input features. We discuss some solutions to this below. \n9.2.4.1 Tied covariances \nIf we force $Sigma _ { c } = Sigma$ to be tied, we will get linear decision boundaries, as we have seen. This also usually results in a more reliable parameter estimate, since we can pool all the samples across classes: \n9.2.4.2 Diagonal covariances \nIf we force $Sigma _ { c }$ to be diagonal, we reduce the number of parameters from $O ( C D ^ { 2 } )$ to $O ( C D )$ , which avoids the overfitting problem. However, this loses the ability to capture correlations between the features. (This is known as the naive Bayes assumption, which we discuss further in Section 9.3.) Despite this approximation, this approach scales well to high dimensions. \nWe can further restrict the model capacity by using a shared (tied) diagonal covariace matrix. This is called “diagonal LDA” [BL04]. \n9.2.4.3 MAP estimation \nForcing the covariance matrix to be diagonal is a rather strong assumption. An alternative approach is to perform MAP estimation of a (shared) full covariance Gaussian, rather than using the MLE. Based on the results of Section 4.5.2, we find that the MAP estimate is \nwhere $lambda$ controls the amount of regularization. This technique is known as regularized discriminant analysis or RDA [HTF09, p656]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n9.2.5 Nearest centroid classifier \nIf we assume a uniform prior over classes, we can compute the most probable class label as follows: \nThis is called the nearest centroid classifier, or nearest class mean classifier (NCM), since we are assigning $_ { x }$ to the class with the closest $pmb { mu } _ { c }$ , where distance is measured using (squared) Mahalanobis distance. \nWe can replace this with any other distance metric to get the decision rule \nWe discuss how to learn distance metrics in Section 16.2, but one simple approach is to use \nThe corresponding class posterior becomes \nWe can optimize W using gradient descent applied to the discriminative loss. This is called nearest class mean metric learning [Men+12]. The advantage of this technique is that it can be used for one-shot learning of new classes, since we just need to see a single labeled prototype $pmb { mu } _ { c }$ per class (assuming we have learned a good W already). \n9.2.6 Fisher’s linear discriminant analysis * \nDiscriminant analysis is a generative approach to classification, which requires fitting an MVN to the features. As we have discussed, this can be problematic in high dimensions. An alternative approach is to reduce the dimensionality of the features $pmb { x } in mathbb { R } ^ { D }$ and then fit an MVN to the resulting low-dimensional features $z in mathbb { R } ^ { K }$ . The simplest approach is to use a linear projection matrix, $z = mathbf { W } mathbf { x }$ , where $mathbf { W }$ is a $K times D$ matrix. One approach to finding W would be to use principal components analysis or PCA (Section 20.1). However, PCA is an unsupervised technique that does not take class labels into account. Thus the resulting low dimensional features are not necessarily optimal for classification, as illustrated in Figure 9.4. \nAn alternative approach is to use gradient based methods to optimize the log likelihood, derived from the class posterior in the low dimensional space, as we discussed in Section 9.2.5. \nA third approach (which relies on an eigendecomposition, rather than a gradient-based optimizer) is to find the matrix W such that the low-dimensional data can be classified as well as possible using a Gaussian class-conditional density model. The assumption of Gaussianity is reasonable since we are computing linear combinations of (potentially non-Gaussian) features. This approach is called Fisher’s linear discriminant analysis, or FLDA. \nFLDA is an interesting hybrid of discriminative and generative techniques. The drawback of this technique is that it is restricted to using $K leq C - 1$ dimensions, regardless of $D$ , for reasons that we will explain below. In the two-class case, this means we are seeking a single vector $mathbf { boldsymbol { w } }$ onto which we can project the data. Below we derive the optimal $mathbf { boldsymbol { w } }$ in the two-class case. We then generalize to the multi-class case, and finally we give a probabilistic interpretation of this technique. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Gaussian discriminant analysis",
        "subsubsection": "Model fitting"
    },
    {
        "content": "9.2.5 Nearest centroid classifier \nIf we assume a uniform prior over classes, we can compute the most probable class label as follows: \nThis is called the nearest centroid classifier, or nearest class mean classifier (NCM), since we are assigning $_ { x }$ to the class with the closest $pmb { mu } _ { c }$ , where distance is measured using (squared) Mahalanobis distance. \nWe can replace this with any other distance metric to get the decision rule \nWe discuss how to learn distance metrics in Section 16.2, but one simple approach is to use \nThe corresponding class posterior becomes \nWe can optimize W using gradient descent applied to the discriminative loss. This is called nearest class mean metric learning [Men+12]. The advantage of this technique is that it can be used for one-shot learning of new classes, since we just need to see a single labeled prototype $pmb { mu } _ { c }$ per class (assuming we have learned a good W already). \n9.2.6 Fisher’s linear discriminant analysis * \nDiscriminant analysis is a generative approach to classification, which requires fitting an MVN to the features. As we have discussed, this can be problematic in high dimensions. An alternative approach is to reduce the dimensionality of the features $pmb { x } in mathbb { R } ^ { D }$ and then fit an MVN to the resulting low-dimensional features $z in mathbb { R } ^ { K }$ . The simplest approach is to use a linear projection matrix, $z = mathbf { W } mathbf { x }$ , where $mathbf { W }$ is a $K times D$ matrix. One approach to finding W would be to use principal components analysis or PCA (Section 20.1). However, PCA is an unsupervised technique that does not take class labels into account. Thus the resulting low dimensional features are not necessarily optimal for classification, as illustrated in Figure 9.4. \nAn alternative approach is to use gradient based methods to optimize the log likelihood, derived from the class posterior in the low dimensional space, as we discussed in Section 9.2.5. \nA third approach (which relies on an eigendecomposition, rather than a gradient-based optimizer) is to find the matrix W such that the low-dimensional data can be classified as well as possible using a Gaussian class-conditional density model. The assumption of Gaussianity is reasonable since we are computing linear combinations of (potentially non-Gaussian) features. This approach is called Fisher’s linear discriminant analysis, or FLDA. \nFLDA is an interesting hybrid of discriminative and generative techniques. The drawback of this technique is that it is restricted to using $K leq C - 1$ dimensions, regardless of $D$ , for reasons that we will explain below. In the two-class case, this means we are seeking a single vector $mathbf { boldsymbol { w } }$ onto which we can project the data. Below we derive the optimal $mathbf { boldsymbol { w } }$ in the two-class case. We then generalize to the multi-class case, and finally we give a probabilistic interpretation of this technique. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Gaussian discriminant analysis",
        "subsubsection": "Nearest centroid classifier"
    },
    {
        "content": "9.2.5 Nearest centroid classifier \nIf we assume a uniform prior over classes, we can compute the most probable class label as follows: \nThis is called the nearest centroid classifier, or nearest class mean classifier (NCM), since we are assigning $_ { x }$ to the class with the closest $pmb { mu } _ { c }$ , where distance is measured using (squared) Mahalanobis distance. \nWe can replace this with any other distance metric to get the decision rule \nWe discuss how to learn distance metrics in Section 16.2, but one simple approach is to use \nThe corresponding class posterior becomes \nWe can optimize W using gradient descent applied to the discriminative loss. This is called nearest class mean metric learning [Men+12]. The advantage of this technique is that it can be used for one-shot learning of new classes, since we just need to see a single labeled prototype $pmb { mu } _ { c }$ per class (assuming we have learned a good W already). \n9.2.6 Fisher’s linear discriminant analysis * \nDiscriminant analysis is a generative approach to classification, which requires fitting an MVN to the features. As we have discussed, this can be problematic in high dimensions. An alternative approach is to reduce the dimensionality of the features $pmb { x } in mathbb { R } ^ { D }$ and then fit an MVN to the resulting low-dimensional features $z in mathbb { R } ^ { K }$ . The simplest approach is to use a linear projection matrix, $z = mathbf { W } mathbf { x }$ , where $mathbf { W }$ is a $K times D$ matrix. One approach to finding W would be to use principal components analysis or PCA (Section 20.1). However, PCA is an unsupervised technique that does not take class labels into account. Thus the resulting low dimensional features are not necessarily optimal for classification, as illustrated in Figure 9.4. \nAn alternative approach is to use gradient based methods to optimize the log likelihood, derived from the class posterior in the low dimensional space, as we discussed in Section 9.2.5. \nA third approach (which relies on an eigendecomposition, rather than a gradient-based optimizer) is to find the matrix W such that the low-dimensional data can be classified as well as possible using a Gaussian class-conditional density model. The assumption of Gaussianity is reasonable since we are computing linear combinations of (potentially non-Gaussian) features. This approach is called Fisher’s linear discriminant analysis, or FLDA. \nFLDA is an interesting hybrid of discriminative and generative techniques. The drawback of this technique is that it is restricted to using $K leq C - 1$ dimensions, regardless of $D$ , for reasons that we will explain below. In the two-class case, this means we are seeking a single vector $mathbf { boldsymbol { w } }$ onto which we can project the data. Below we derive the optimal $mathbf { boldsymbol { w } }$ in the two-class case. We then generalize to the multi-class case, and finally we give a probabilistic interpretation of this technique. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n9.2.6.1 Derivation of the optimal 1d projection \nWe now derive this optimal direction $mathbf { boldsymbol { w } }$ , for the two-class case, following the presentation of [Bis06, Sec 4.1.4]. Define the class-conditional means as \nLet $m _ { k } = pmb { w } ^ { vert } pmb { mu } _ { k }$ be the projection of each mean onto the line $mathbf { boldsymbol { w } }$ . Also, let $z _ { n } = w ^ { 1 } x _ { n }$ be the projection of the data onto the line. The variance of the projected points is proportional to \nThe goal is to find $mathbf { boldsymbol { w } }$ such that we maximize the distance between the means, $m _ { 2 } - m _ { 1 }$ , while also ensuring the projected clusters are “tight”, which we can do by minimizing their variance. This suggests the following objective: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can rewrite the right hand side of the above in terms of $mathbf { boldsymbol { w } }$ as follows \nwhere $mathbf { S } _ { B }$ is the between-class scatter matrix given by \nand $mathbf { s } _ { W }$ is the within-class scatter matrix, given by \nTo see this, note that \nand \nEquation (9.30) is a ratio of two scalars; we can take its derivative with respect to $mathbf { boldsymbol { w } }$ and equate to zero. One can show (Exercise 9.1) that $J ( w )$ is maximized when \nwhere \nEquation (9.36) is called a generalized eigenvalue problem. If $mathbf { s } _ { W }$ is invertible, we can convert it to a regular eigenvalue problem: \nHowever, in the two class case, there is a simpler solution. In particular, since \nthen, from Equation (9.38) we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nSince we only care about the directionality, and not the scale factor, we can just set \nThis is the optimal solution in the two-class case. If ${ bf S } _ { W } propto { bf I }$ , meaning the pooled covariance matrix is isotropic, then $mathbf { boldsymbol { w } }$ is proportional to the vector that joins the class means. This is an intuitively reasonable direction to project onto, as shown in Figure 9.3. \n9.2.6.2 Extension to higher dimensions and multiple classes \nWe can extend the above idea to multiple classes, and to higher dimensional subspaces, by finding a projection matrix W which maps from $D$ to $K$ . Let $z _ { n } = mathbf { W } pmb { x } _ { n }$ be the low dimensional projection of the $n$ ’th data point. Let $begin{array} { r } { pmb { m } _ { c } = frac { 1 } { N _ { c } } sum _ { n : y _ { n } = c } pmb { z } _ { n } } end{array}$ be the corresponding mean for the $c$ ’th class and $begin{array} { r } { m = { frac { 1 } { N } } sum _ { c = 1 } ^ { C } N _ { c } m _ { c } } end{array}$ be the overall mean, both in the low dimensional space. We define the following scatter matrices: \nFinally, we define the objective function as maximizing the following:2 \n2. An alternative criterion that is sometimes used [Fuk90] is $begin{array} { r } { J ( mathbf { W } ) = operatorname { t r } Big { tilde { mathbf { S } } _ { W } ^ { - 1 } tilde { mathbf { S } } _ { B } Big } = operatorname { t r } big { ( mathbf { W } mathbf { S } _ { W } mathbf { W } ^ { mathsf { T } } ) ^ { - 1 } ( mathbf { W } mathbf { S } _ { B } mathbf { W } ^ { mathsf { T } } ) big } . } end{array}$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $mathbf { s } _ { W }$ and $mathbf { S } _ { B }$ are defined in the original high dimensional space in the obvious way (namely using ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ instead of $z _ { n }$ , $pmb { mu } _ { c }$ instead of ${ pmb { m } } _ { c }$ , and $mu$ instead of $_ { mathbf { phi } ^ { prime } m }$ ). The solution can be shown [DHS01] to be $mathbf { W } = mathbf { S } _ { W } ^ { - frac { 1 } { 2 } } mathbf { U }$ , where $mathbf { U }$ are the $K$ leading eigenvectors of $mathbf { S } _ { W } ^ { - frac { 1 } { 2 } } mathbf { S } _ { B } mathbf { S } _ { W } ^ { - frac { 1 } { 2 } }$ , assuming $mathbf { s } _ { W }$ is non-singular. (If it is singular, we can first perform PCA on all the data.) \n\nFigure 9.5 gives an example of this method applied to some $D = 1 0$ dimensional speech data, representing $C = 1 1$ different vowel sounds. We project to $K = 2$ dimensions in order to visualize the data. We see that FLDA gives better class separation than PCA. \nNote that FLDA is restricted to finding at most a $K leq C - 1$ dimensional linear subspace, no matter how large $D$ , because the rank of the between class scatter matrix $mathbf { S } _ { B }$ is $C - 1$ . (The -1 term arises because of the $pmb { mu }$ term, which is a linear function of the $pmb { mu } _ { c }$ .) This is a rather severe restriction which limits the usefulness of FLDA. \n9.3 Naive Bayes classifiers \nIn this section, we discuss a simple generative approach to classification in which we assume the features are conditionally independent given the class label. This is called the naive Bayes assumption. The model is called “naive” since we do not expect the features to be independent, even conditional on the class label. However, even if the naive Bayes assumption is not true, it often results in classifiers that work well [DP97]. One reason for this is that the model is quite simple (it only has $O ( C D )$ parameters, for $C$ classes and $D$ features), and hence it is relatively immune to overfitting. \nMore precisely, the naive Bayes assumption corresponds to using a class conditional density of the following form: \nwhere $pmb { theta } _ { d c }$ are the parameters for the class conditional density for class $c$ and feature $d$ . Hence the posterior over class labels is given by \nwhere $pi _ { c }$ is the prior probability of class $c$ , and $pmb { theta } = ( pi , { pmb { theta } _ { d c } } ) )$ are all the parameters. This is known as a naive Bayes classifier or NBC. \n9.3.1 Example models \nWe still need to specify the form of the probability distributions in Equation (9.46). This depends on what type of feature $x _ { d }$ is. We give some examples below: \n• In the case of binary features, $x _ { d } in { 0 , 1 }$ , we can use the Bernoulli distribution: $p ( pmb { x } | y = c , pmb { theta } ) =$ $textstyle prod _ { d = 1 } ^ { D } operatorname { B e r } ( x _ { d } | theta _ { d c } )$ , where $theta _ { d c }$ is the probability that $x _ { d } = 1$ in class $c$ . This is sometimes called the multivariate Bernoulli naive Bayes model. For example, Figure 9.6 shows the estimated parameters for each class when we fit this model to a binarized version of MNIST. This approach does surprisingly well, and has a test set accuracy of 84.3%. (See Figure 9.7 for some sample predictions.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Gaussian discriminant analysis",
        "subsubsection": "Fisher's linear discriminant analysis *"
    },
    {
        "content": "Figure 9.5 gives an example of this method applied to some $D = 1 0$ dimensional speech data, representing $C = 1 1$ different vowel sounds. We project to $K = 2$ dimensions in order to visualize the data. We see that FLDA gives better class separation than PCA. \nNote that FLDA is restricted to finding at most a $K leq C - 1$ dimensional linear subspace, no matter how large $D$ , because the rank of the between class scatter matrix $mathbf { S } _ { B }$ is $C - 1$ . (The -1 term arises because of the $pmb { mu }$ term, which is a linear function of the $pmb { mu } _ { c }$ .) This is a rather severe restriction which limits the usefulness of FLDA. \n9.3 Naive Bayes classifiers \nIn this section, we discuss a simple generative approach to classification in which we assume the features are conditionally independent given the class label. This is called the naive Bayes assumption. The model is called “naive” since we do not expect the features to be independent, even conditional on the class label. However, even if the naive Bayes assumption is not true, it often results in classifiers that work well [DP97]. One reason for this is that the model is quite simple (it only has $O ( C D )$ parameters, for $C$ classes and $D$ features), and hence it is relatively immune to overfitting. \nMore precisely, the naive Bayes assumption corresponds to using a class conditional density of the following form: \nwhere $pmb { theta } _ { d c }$ are the parameters for the class conditional density for class $c$ and feature $d$ . Hence the posterior over class labels is given by \nwhere $pi _ { c }$ is the prior probability of class $c$ , and $pmb { theta } = ( pi , { pmb { theta } _ { d c } } ) )$ are all the parameters. This is known as a naive Bayes classifier or NBC. \n9.3.1 Example models \nWe still need to specify the form of the probability distributions in Equation (9.46). This depends on what type of feature $x _ { d }$ is. We give some examples below: \n• In the case of binary features, $x _ { d } in { 0 , 1 }$ , we can use the Bernoulli distribution: $p ( pmb { x } | y = c , pmb { theta } ) =$ $textstyle prod _ { d = 1 } ^ { D } operatorname { B e r } ( x _ { d } | theta _ { d c } )$ , where $theta _ { d c }$ is the probability that $x _ { d } = 1$ in class $c$ . This is sometimes called the multivariate Bernoulli naive Bayes model. For example, Figure 9.6 shows the estimated parameters for each class when we fit this model to a binarized version of MNIST. This approach does surprisingly well, and has a test set accuracy of 84.3%. (See Figure 9.7 for some sample predictions.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n0123456789 \nFigure 9.6: Visualization of the Bernoulli class conditional densities for a naive Bayes classifier fit to a binarized version of the MNIST dataset. Generated by naive_bayes_mnist_jax.ipynb. \n• In the case of categorical features, $x _ { d } in { 1 , ldots , K }$ , we can use the categorical distribution: $begin{array} { r } { p ( pmb { x } | y = c , pmb { theta } ) = prod _ { d = 1 } ^ { L } mathrm { C a t } ( x _ { d } | pmb { theta } _ { d c } ) } end{array}$ , where $theta _ { d c k }$ is the probability that $x _ { d } = k$ given that $y = c$ . \n• In the case of real-valued features, $x _ { d } in mathbb { R }$ , we can use the univariate Gaussian distribution: $begin{array} { r } { p ( { pmb x } | y = c , pmb theta ) = prod _ { d = 1 } ^ { L } mathcal { N } ( x _ { d } | mu _ { d c } , sigma _ { d c } ^ { 2 } ) } end{array}$ , where $mu _ { d c }$ is the mean of feature $d$ when the class label is $c$ , and $sigma _ { d c } ^ { 2 }$ is its variance. (This is equivalent to Gaussian discriminant analysis using diagonal covariance matrices.) \n9.3.2 Model fitting \nIn this section, we discuss how to fit a naive Bayes classifier using maximum likelihood estimation. We can write the likelihood as follows: \nso the log-likelihood is given by \nWe see that this decomposes into a term for $pi$ , and $ C D$ terms for each $pmb { theta } _ { d c }$ : \nwhere $mathcal { D } _ { y } = { y _ { n } : n = 1 : N }$ are all the labels, and $mathcal { D } _ { d c } = { x _ { n d } : y _ { n } = c }$ are all the values of feature $d$ for examples from class $c$ . Hence we can estimate these parameters separately. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Naive Bayes classifiers",
        "subsubsection": "Example models"
    },
    {
        "content": "0123456789 \nFigure 9.6: Visualization of the Bernoulli class conditional densities for a naive Bayes classifier fit to a binarized version of the MNIST dataset. Generated by naive_bayes_mnist_jax.ipynb. \n• In the case of categorical features, $x _ { d } in { 1 , ldots , K }$ , we can use the categorical distribution: $begin{array} { r } { p ( pmb { x } | y = c , pmb { theta } ) = prod _ { d = 1 } ^ { L } mathrm { C a t } ( x _ { d } | pmb { theta } _ { d c } ) } end{array}$ , where $theta _ { d c k }$ is the probability that $x _ { d } = k$ given that $y = c$ . \n• In the case of real-valued features, $x _ { d } in mathbb { R }$ , we can use the univariate Gaussian distribution: $begin{array} { r } { p ( { pmb x } | y = c , pmb theta ) = prod _ { d = 1 } ^ { L } mathcal { N } ( x _ { d } | mu _ { d c } , sigma _ { d c } ^ { 2 } ) } end{array}$ , where $mu _ { d c }$ is the mean of feature $d$ when the class label is $c$ , and $sigma _ { d c } ^ { 2 }$ is its variance. (This is equivalent to Gaussian discriminant analysis using diagonal covariance matrices.) \n9.3.2 Model fitting \nIn this section, we discuss how to fit a naive Bayes classifier using maximum likelihood estimation. We can write the likelihood as follows: \nso the log-likelihood is given by \nWe see that this decomposes into a term for $pi$ , and $ C D$ terms for each $pmb { theta } _ { d c }$ : \nwhere $mathcal { D } _ { y } = { y _ { n } : n = 1 : N }$ are all the labels, and $mathcal { D } _ { d c } = { x _ { n d } : y _ { n } = c }$ are all the values of feature $d$ for examples from class $c$ . Hence we can estimate these parameters separately. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIn Section 4.2.4, we show that the MLE for $pi$ is the vector of empirical counts, $begin{array} { r } { hat { pi } _ { c } = frac { N _ { c } } { N } } end{array}$ . The MLEs for $theta _ { d c }$ depend on the choice of the class conditional density for feature $d$ . We discuss some common choices below. \n• In the case of discrete features, we can use a categorical distribution. A straightforward extension of the results in Section 4.2.4 gives the following expression for the MLE: \nwhere $begin{array} { r } { N _ { d c k } = sum _ { n = 1 } ^ { N } mathbb { I } left( x _ { n d } = k , y _ { n } = c , right. } end{array}$ ) is the number of times that feature $d$ had value $k$ in examples of class $c$ . \n• In the case of binary features, the categorical distribution becomes the Bernoulli, and the MLE becomes \nwhich is the empirical fraction of times that feature $d$ is on in examples of class $c$ . \n• In the case of real-valued features, we can use a Gaussian distribution. A straightforward extension of the results in Section 4.2.5 gives the following expression for the MLE: \nThus we see that fitting a naive Bayes classifier is extremely simple and efficient. \n9.3.3 Bayesian naive Bayes \nIn this section, we extend our discussion of MLE estimation for naive Bayes classifiers from Section 9.3.2 to compute the posterior distribution over the parameters. For simplicity, let us assume we have categorical features, so $p ( x _ { d } | pmb { theta } _ { d c } ) = mathrm { C a t } ( x _ { d } | pmb { theta } _ { d c } )$ , where $theta _ { d c k } = p ( x _ { d } = k | y = c )$ . In Section 4.6.3.2, we show that the conjugate prior for the categorical likelihood is the Dirichlet distribution, $p ( pmb { theta } _ { d c } ) =$ $operatorname { D i r } ( pmb { theta } _ { d c } | beta _ { d c } )$ , where $beta _ { d c k }$ can be interpereted as a set of “pseudo counts”, corresponding to counts $N _ { d c k }$ that come from prior data. Similarly we use a Dirichlet prior for the label frequencies, $p ( { pmb pi } ) = operatorname { D i r } ( { pmb pi } | { pmb alpha } )$ . By using a conjugate prior, we can compute the posterior in closed form, as we explain in Section 4.6.3. In particular, we have \nwhere $widehat { alpha } _ { c } = breve { alpha } _ { c } + N _ { c }$ and $stackrel { triangledown } { beta } _ { d c k } = stackrel { triangledown } { beta } _ { d c k } + N _ { d c k }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Naive Bayes classifiers",
        "subsubsection": "Model fitting"
    },
    {
        "content": "In Section 4.2.4, we show that the MLE for $pi$ is the vector of empirical counts, $begin{array} { r } { hat { pi } _ { c } = frac { N _ { c } } { N } } end{array}$ . The MLEs for $theta _ { d c }$ depend on the choice of the class conditional density for feature $d$ . We discuss some common choices below. \n• In the case of discrete features, we can use a categorical distribution. A straightforward extension of the results in Section 4.2.4 gives the following expression for the MLE: \nwhere $begin{array} { r } { N _ { d c k } = sum _ { n = 1 } ^ { N } mathbb { I } left( x _ { n d } = k , y _ { n } = c , right. } end{array}$ ) is the number of times that feature $d$ had value $k$ in examples of class $c$ . \n• In the case of binary features, the categorical distribution becomes the Bernoulli, and the MLE becomes \nwhich is the empirical fraction of times that feature $d$ is on in examples of class $c$ . \n• In the case of real-valued features, we can use a Gaussian distribution. A straightforward extension of the results in Section 4.2.5 gives the following expression for the MLE: \nThus we see that fitting a naive Bayes classifier is extremely simple and efficient. \n9.3.3 Bayesian naive Bayes \nIn this section, we extend our discussion of MLE estimation for naive Bayes classifiers from Section 9.3.2 to compute the posterior distribution over the parameters. For simplicity, let us assume we have categorical features, so $p ( x _ { d } | pmb { theta } _ { d c } ) = mathrm { C a t } ( x _ { d } | pmb { theta } _ { d c } )$ , where $theta _ { d c k } = p ( x _ { d } = k | y = c )$ . In Section 4.6.3.2, we show that the conjugate prior for the categorical likelihood is the Dirichlet distribution, $p ( pmb { theta } _ { d c } ) =$ $operatorname { D i r } ( pmb { theta } _ { d c } | beta _ { d c } )$ , where $beta _ { d c k }$ can be interpereted as a set of “pseudo counts”, corresponding to counts $N _ { d c k }$ that come from prior data. Similarly we use a Dirichlet prior for the label frequencies, $p ( { pmb pi } ) = operatorname { D i r } ( { pmb pi } | { pmb alpha } )$ . By using a conjugate prior, we can compute the posterior in closed form, as we explain in Section 4.6.3. In particular, we have \nwhere $widehat { alpha } _ { c } = breve { alpha } _ { c } + N _ { c }$ and $stackrel { triangledown } { beta } _ { d c k } = stackrel { triangledown } { beta } _ { d c k } + N _ { d c k }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nUsing the results from Section 4.6.3.4, we can derive the posterior predictive distribution as follows. The prior over the label is given by $p ( y | mathcal { D } ) = operatorname { C a t } ( y | overline { { pi } } )$ , where $overline { { pi } } _ { c } = widehat { alpha } _ { c } mathrm { ~ / ~ } sum _ { c ^ { prime } } widehat { alpha } _ { c ^ { prime } }$ . For the features, we have $p ( x _ { d } = k | y = c , D ) = overline { { theta } } _ { d c k }$ , where \nis the posterior mean of the parameters. \nIf $breve { beta } _ { d c k } = 0$ , this reduces to the MLE in Equation (9.52). By contrast, if we set $breve { beta } _ { d c k } = 1$ , we add 1 to all the empirical counts before normalizing. This is called add-one smoothing or Laplace smoothing. For example, in the binary case, this gives \nOnce we have estimated the parameter posterior, we can compute the predicted distribution over the label as follows: \nThis gives us a fully Bayesian form of naive Bayes, in which we have integrated out all the parameters. (In this case, the predictive distribution can be obtained merely by plugging in the posterior mean parameters.) \n9.3.4 The connection between naive Bayes and logistic regression \nIn this section, we show that the class posterior $p ( boldsymbol { y } | boldsymbol { x } , boldsymbol { theta } )$ for a NBC model has the same form as multinomial logistic regression. For simplicity, we assume that the features are all discrete, and each has $K$ states, although the result holds for arbitrary feature distributions in the exponential family. Let $x _ { d k } = mathbb { I } left( x _ { d } = k right)$ , so ${ pmb x } _ { d }$ is a one-hot encoding of feature $d$ . Then the class conditional density can be written as follows: \nHence the posterior over classes is given by \nThis can be written as a softmax \nby suitably defining $beta _ { c }$ and $gamma _ { c }$ . This has exactly the same form as multinomial logistic regression in Section 2.5.3. The difference is that with naive Bayes we optimize the joint likelihood $begin{array} { r l } {  { prod _ { n } p ( y _ { n } , pmb { x } _ { n } | pmb { theta } ) } } end{array}$ , whereas with logistic regression, we optimize the conditional likelihood $begin{array} { r l } {  { prod _ { n } p ( y _ { n } | pmb { x } _ { n } , pmb theta ) } } end{array}$ . In general, these can give different results (see Exercise 10.3). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Naive Bayes classifiers",
        "subsubsection": "Bayesian naive Bayes"
    },
    {
        "content": "Using the results from Section 4.6.3.4, we can derive the posterior predictive distribution as follows. The prior over the label is given by $p ( y | mathcal { D } ) = operatorname { C a t } ( y | overline { { pi } } )$ , where $overline { { pi } } _ { c } = widehat { alpha } _ { c } mathrm { ~ / ~ } sum _ { c ^ { prime } } widehat { alpha } _ { c ^ { prime } }$ . For the features, we have $p ( x _ { d } = k | y = c , D ) = overline { { theta } } _ { d c k }$ , where \nis the posterior mean of the parameters. \nIf $breve { beta } _ { d c k } = 0$ , this reduces to the MLE in Equation (9.52). By contrast, if we set $breve { beta } _ { d c k } = 1$ , we add 1 to all the empirical counts before normalizing. This is called add-one smoothing or Laplace smoothing. For example, in the binary case, this gives \nOnce we have estimated the parameter posterior, we can compute the predicted distribution over the label as follows: \nThis gives us a fully Bayesian form of naive Bayes, in which we have integrated out all the parameters. (In this case, the predictive distribution can be obtained merely by plugging in the posterior mean parameters.) \n9.3.4 The connection between naive Bayes and logistic regression \nIn this section, we show that the class posterior $p ( boldsymbol { y } | boldsymbol { x } , boldsymbol { theta } )$ for a NBC model has the same form as multinomial logistic regression. For simplicity, we assume that the features are all discrete, and each has $K$ states, although the result holds for arbitrary feature distributions in the exponential family. Let $x _ { d k } = mathbb { I } left( x _ { d } = k right)$ , so ${ pmb x } _ { d }$ is a one-hot encoding of feature $d$ . Then the class conditional density can be written as follows: \nHence the posterior over classes is given by \nThis can be written as a softmax \nby suitably defining $beta _ { c }$ and $gamma _ { c }$ . This has exactly the same form as multinomial logistic regression in Section 2.5.3. The difference is that with naive Bayes we optimize the joint likelihood $begin{array} { r l } {  { prod _ { n } p ( y _ { n } , pmb { x } _ { n } | pmb { theta } ) } } end{array}$ , whereas with logistic regression, we optimize the conditional likelihood $begin{array} { r l } {  { prod _ { n } p ( y _ { n } | pmb { x } _ { n } , pmb theta ) } } end{array}$ . In general, these can give different results (see Exercise 10.3). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n9.4 Generative vs discriminative classifiers \nA model of the form $p ( { pmb x } , y ) = p ( y ) p ( { pmb x } | y )$ is called a generative classifier, since it can be used to generate examples $_ { x }$ from each class $y$ . By contrast, a model of the form $p ( boldsymbol { y } | boldsymbol { x } )$ is called a discriminative classifier, since it can only be used to discriminate between different classes. Below we discuss various pros and cons of the generative and discriminative approaches to classification. (See also [BT04; UB05; LBM06; BL07a; Rot+18].) \n9.4.1 Advantages of discriminative classifiers \nThe main advantages of discriminative classifiers are as follows: \n• Better predictive accuracy. Discriminative classifiers are often much more accurate than generative classifiers [NJ02]. The reason is that the conditional distribution $p ( boldsymbol { y } | boldsymbol { x } )$ is often much simpler (and therefore easier to learn) than the joint distribution $p ( boldsymbol { y } , pmb { x } )$ , as illustrated in Figure 9.8. In particular, discriminative models do not need to “waste effort” modeling the distribution of the input features. \n• Can handle feature preprocessing. A big advantage of discriminative methods is that they allow us to preprocess the input in arbitrary ways. For example, we can perform a polynomial expansion of the input features, and we can replace a string of words with embedding vectors (see Section 20.5). It is often hard to define a generative model on such pre-processed data, since the new features can be correlated in complex ways which are hard to model. \n• Well-calibrated probabilities. Some generative classifiers, such as naive Bayes (described in Section 9.3), make strong independence assumptions which are often not valid. This can result in very extreme posterior class probabilities (very near 0 or 1). Discriminative models, such as logistic regression, are often better calibrated in terms of their probability estimates, although they also sometimes need adjustment (see e.g., [NMC05]). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Naive Bayes classifiers",
        "subsubsection": "The connection between naive Bayes and logistic regression"
    },
    {
        "content": "9.4 Generative vs discriminative classifiers \nA model of the form $p ( { pmb x } , y ) = p ( y ) p ( { pmb x } | y )$ is called a generative classifier, since it can be used to generate examples $_ { x }$ from each class $y$ . By contrast, a model of the form $p ( boldsymbol { y } | boldsymbol { x } )$ is called a discriminative classifier, since it can only be used to discriminate between different classes. Below we discuss various pros and cons of the generative and discriminative approaches to classification. (See also [BT04; UB05; LBM06; BL07a; Rot+18].) \n9.4.1 Advantages of discriminative classifiers \nThe main advantages of discriminative classifiers are as follows: \n• Better predictive accuracy. Discriminative classifiers are often much more accurate than generative classifiers [NJ02]. The reason is that the conditional distribution $p ( boldsymbol { y } | boldsymbol { x } )$ is often much simpler (and therefore easier to learn) than the joint distribution $p ( boldsymbol { y } , pmb { x } )$ , as illustrated in Figure 9.8. In particular, discriminative models do not need to “waste effort” modeling the distribution of the input features. \n• Can handle feature preprocessing. A big advantage of discriminative methods is that they allow us to preprocess the input in arbitrary ways. For example, we can perform a polynomial expansion of the input features, and we can replace a string of words with embedding vectors (see Section 20.5). It is often hard to define a generative model on such pre-processed data, since the new features can be correlated in complex ways which are hard to model. \n• Well-calibrated probabilities. Some generative classifiers, such as naive Bayes (described in Section 9.3), make strong independence assumptions which are often not valid. This can result in very extreme posterior class probabilities (very near 0 or 1). Discriminative models, such as logistic regression, are often better calibrated in terms of their probability estimates, although they also sometimes need adjustment (see e.g., [NMC05]). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n9.4.2 Advantages of generative classifiers \nThe main advantages of generative classifiers are as follows: \n• Easy to fit. Generative classifiers are often very easy to fit. For example, in Section 9.3.2, we show how to fit a naive Bayes classifier by simple counting and averaging. By contrast, logistic regression requires solving a convex optimization problem (see Section 10.2.3 for the details), and neural nets require solving a non-convex optimization problem, both of which are much slower.   \n• Can easily handle missing input features. Sometimes some of the inputs (components of $_ { x }$ ) are not observed. In a generative classifier, there is a simple method for dealing with this, as we show in Section 1.5.5. However, in a discriminative classifier, there is no principled solution to this problem, since the model assumes that $_ x$ is always available to be conditioned on.   \n• Can fit classes separately. In a generative classifier, we estimate the parameters of each class conditional density independently (as we show in Section 9.3.2), so we do not have to retrain the model when we add more classes. In contrast, in discriminative models, all the parameters interact, so the whole model must be retrained if we add a new class.   \n• Can handle unlabeled training data. It is easy to use generative models for semi-supervised learning, in which we combine labeled data $mathcal { D } _ { x y } = { ( { pmb x } _ { n } , y _ { n } ) }$ and unlabeled data, ${ mathcal { D } } _ { x } = { { pmb x } _ { n } }$ . However, this is harder to do with discriminative models, since there is no uniquely optimal way to exploit $mathcal { D } _ { x }$ .   \n• May be more robust to spurious features. A discriminative model $p ( boldsymbol { y } | boldsymbol { x } )$ may pick up on features of the input $_ { x }$ that can discriminate different values of $y$ in the training set, but which are not robust and do not generalize beyond the training set. These are called spurious features (see e.g., [Arj21; Zho+21]). By contrast, a generative model $p ( { pmb x } | { pmb y } )$ may be better able to capture the causal mechanisms of the underlying data generating process; such causal models can be more robust to distribution shift (see e.g., [Sch19; LBS19; LN81]). \n9.4.3 Handling missing features \nSometimes we are missing parts of the input $_ { x }$ during training and/or testing. In a generative classifier, we can handle this situation by marginalizing out the missing values. (We assume that the missingness of a feature is not informative about its potential value.) By contrast, when using a discriminative model, there is no unique best way to handle missing inputs, as we discuss in Section 1.5.5. \nFor example, suppose we are missing the value of $x _ { 1 }$ . We just have to compute \nIn Gaussian discriminant analysis, we can marginalize out $x _ { 1 }$ using the equations from Section 3.2.3 \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Generative vs discriminative classifiers",
        "subsubsection": "Advantages of discriminative classifiers"
    },
    {
        "content": "9.4.2 Advantages of generative classifiers \nThe main advantages of generative classifiers are as follows: \n• Easy to fit. Generative classifiers are often very easy to fit. For example, in Section 9.3.2, we show how to fit a naive Bayes classifier by simple counting and averaging. By contrast, logistic regression requires solving a convex optimization problem (see Section 10.2.3 for the details), and neural nets require solving a non-convex optimization problem, both of which are much slower.   \n• Can easily handle missing input features. Sometimes some of the inputs (components of $_ { x }$ ) are not observed. In a generative classifier, there is a simple method for dealing with this, as we show in Section 1.5.5. However, in a discriminative classifier, there is no principled solution to this problem, since the model assumes that $_ x$ is always available to be conditioned on.   \n• Can fit classes separately. In a generative classifier, we estimate the parameters of each class conditional density independently (as we show in Section 9.3.2), so we do not have to retrain the model when we add more classes. In contrast, in discriminative models, all the parameters interact, so the whole model must be retrained if we add a new class.   \n• Can handle unlabeled training data. It is easy to use generative models for semi-supervised learning, in which we combine labeled data $mathcal { D } _ { x y } = { ( { pmb x } _ { n } , y _ { n } ) }$ and unlabeled data, ${ mathcal { D } } _ { x } = { { pmb x } _ { n } }$ . However, this is harder to do with discriminative models, since there is no uniquely optimal way to exploit $mathcal { D } _ { x }$ .   \n• May be more robust to spurious features. A discriminative model $p ( boldsymbol { y } | boldsymbol { x } )$ may pick up on features of the input $_ { x }$ that can discriminate different values of $y$ in the training set, but which are not robust and do not generalize beyond the training set. These are called spurious features (see e.g., [Arj21; Zho+21]). By contrast, a generative model $p ( { pmb x } | { pmb y } )$ may be better able to capture the causal mechanisms of the underlying data generating process; such causal models can be more robust to distribution shift (see e.g., [Sch19; LBS19; LN81]). \n9.4.3 Handling missing features \nSometimes we are missing parts of the input $_ { x }$ during training and/or testing. In a generative classifier, we can handle this situation by marginalizing out the missing values. (We assume that the missingness of a feature is not informative about its potential value.) By contrast, when using a discriminative model, there is no unique best way to handle missing inputs, as we discuss in Section 1.5.5. \nFor example, suppose we are missing the value of $x _ { 1 }$ . We just have to compute \nIn Gaussian discriminant analysis, we can marginalize out $x _ { 1 }$ using the equations from Section 3.2.3 \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Generative vs discriminative classifiers",
        "subsubsection": "Advantages of generative classifiers"
    },
    {
        "content": "9.4.2 Advantages of generative classifiers \nThe main advantages of generative classifiers are as follows: \n• Easy to fit. Generative classifiers are often very easy to fit. For example, in Section 9.3.2, we show how to fit a naive Bayes classifier by simple counting and averaging. By contrast, logistic regression requires solving a convex optimization problem (see Section 10.2.3 for the details), and neural nets require solving a non-convex optimization problem, both of which are much slower.   \n• Can easily handle missing input features. Sometimes some of the inputs (components of $_ { x }$ ) are not observed. In a generative classifier, there is a simple method for dealing with this, as we show in Section 1.5.5. However, in a discriminative classifier, there is no principled solution to this problem, since the model assumes that $_ x$ is always available to be conditioned on.   \n• Can fit classes separately. In a generative classifier, we estimate the parameters of each class conditional density independently (as we show in Section 9.3.2), so we do not have to retrain the model when we add more classes. In contrast, in discriminative models, all the parameters interact, so the whole model must be retrained if we add a new class.   \n• Can handle unlabeled training data. It is easy to use generative models for semi-supervised learning, in which we combine labeled data $mathcal { D } _ { x y } = { ( { pmb x } _ { n } , y _ { n } ) }$ and unlabeled data, ${ mathcal { D } } _ { x } = { { pmb x } _ { n } }$ . However, this is harder to do with discriminative models, since there is no uniquely optimal way to exploit $mathcal { D } _ { x }$ .   \n• May be more robust to spurious features. A discriminative model $p ( boldsymbol { y } | boldsymbol { x } )$ may pick up on features of the input $_ { x }$ that can discriminate different values of $y$ in the training set, but which are not robust and do not generalize beyond the training set. These are called spurious features (see e.g., [Arj21; Zho+21]). By contrast, a generative model $p ( { pmb x } | { pmb y } )$ may be better able to capture the causal mechanisms of the underlying data generating process; such causal models can be more robust to distribution shift (see e.g., [Sch19; LBS19; LN81]). \n9.4.3 Handling missing features \nSometimes we are missing parts of the input $_ { x }$ during training and/or testing. In a generative classifier, we can handle this situation by marginalizing out the missing values. (We assume that the missingness of a feature is not informative about its potential value.) By contrast, when using a discriminative model, there is no unique best way to handle missing inputs, as we discuss in Section 1.5.5. \nFor example, suppose we are missing the value of $x _ { 1 }$ . We just have to compute \nIn Gaussian discriminant analysis, we can marginalize out $x _ { 1 }$ using the equations from Section 3.2.3 \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf we make the naive Bayes assumption, things are even easier, since we can just ignore the likelihood term for $x _ { 1 }$ . This follows because \nwhere we exploited the fact that $begin{array} { r } { sum _ { x _ { 1 } } p ( x _ { 1 } | y = c , pmb { theta } _ { 1 c } ) = 1 } end{array}$ . \n9.5 Exercises \nExercise 9.1 [Derivation of Fisher’s linear discriminant] Swhoerwe i.mHuinmt:ofr $begin{array} { r } { J ( pmb { w } ) = frac { pmb { w } ^ { T } mathbf { S } _ { B } pmb { w } } { pmb { w } ^ { T } mathbf { S } _ { W } pmb { w } } } end{array}$ isvagtiivenobfya $mathbf { S } _ { B } pmb { w } = lambda mathbf { S } _ { W } pmb { w }$ calars is given by $begin{array} { r } { lambda = frac { { pmb w } ^ { T } { bf S } _ { B } { pmb w } } { { pmb w } ^ { T } { bf S } _ { W } { pmb w } } } end{array}$ $begin{array} { r } { frac { d } { d x } frac { f ( x ) } { g ( x ) } = frac { f ^ { prime } g - f g ^ { prime } } { g ^ { 2 } } } end{array}$ where $begin{array} { r } { f ^ { prime } = frac { d } { d x } f ( x ) } end{array}$ and $begin{array} { r } { g ^ { prime } = frac { d } { d x } g ( x ) } end{array}$ . Also, recall that $begin{array} { r } { frac { d } { d { bf x } } { bf x } ^ { T } { bf A } { bf x } = ( { bf A } + { bf A } ^ { T } ) { bf x } } end{array}$ .",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Generative vs discriminative classifiers",
        "subsubsection": "Handling missing features"
    },
    {
        "content": "If we make the naive Bayes assumption, things are even easier, since we can just ignore the likelihood term for $x _ { 1 }$ . This follows because \nwhere we exploited the fact that $begin{array} { r } { sum _ { x _ { 1 } } p ( x _ { 1 } | y = c , pmb { theta } _ { 1 c } ) = 1 } end{array}$ . \n9.5 Exercises \nExercise 9.1 [Derivation of Fisher’s linear discriminant] Swhoerwe i.mHuinmt:ofr $begin{array} { r } { J ( pmb { w } ) = frac { pmb { w } ^ { T } mathbf { S } _ { B } pmb { w } } { pmb { w } ^ { T } mathbf { S } _ { W } pmb { w } } } end{array}$ isvagtiivenobfya $mathbf { S } _ { B } pmb { w } = lambda mathbf { S } _ { W } pmb { w }$ calars is given by $begin{array} { r } { lambda = frac { { pmb w } ^ { T } { bf S } _ { B } { pmb w } } { { pmb w } ^ { T } { bf S } _ { W } { pmb w } } } end{array}$ $begin{array} { r } { frac { d } { d x } frac { f ( x ) } { g ( x ) } = frac { f ^ { prime } g - f g ^ { prime } } { g ^ { 2 } } } end{array}$ where $begin{array} { r } { f ^ { prime } = frac { d } { d x } f ( x ) } end{array}$ and $begin{array} { r } { g ^ { prime } = frac { d } { d x } g ( x ) } end{array}$ . Also, recall that $begin{array} { r } { frac { d } { d { bf x } } { bf x } ^ { T } { bf A } { bf x } = ( { bf A } + { bf A } ^ { T } ) { bf x } } end{array}$ . \n10 Logistic Regression \n10.1 Introduction \nLogistic regression is a widely used discriminative classification model $p ( boldsymbol { y } | boldsymbol { x } ; boldsymbol { theta } )$ , where $pmb { x } in mathbb { R } ^ { D }$ is a fixed-dimensional input vector, $y in { 1 , ldots , C }$ is the class label, and $pmb theta$ are the parameters. If $C = 2$ , this is known as binary logistic regression, and if $C > 2$ , it is known as multinomial logistic regression, or alternatively, multiclass logistic regression. We give the details below. \n10.2 Binary logistic regression \nBinary logistic regression corresponds to the following model \nwhere $sigma$ is the sigmoid function defined in Section 2.4.2, $mathbf { boldsymbol { w } }$ are the weights, $b$ is the bias, and $pmb theta = ( pmb w , b )$ are all the parameters. In other words, \nwhere $a = pmb { w } ^ { 1 } pmb { x } + b$ is the log-odds, $log ( p / 1 - p )$ , where $p = p ( y = 1 | boldsymbol { x } ; boldsymbol { theta } )$ , as explained in Section 2.4.2. (In ML, the quantity $a$ is usually called the logit or the pre-activation.) \nSometimes we choose to use the labels $tilde { y } in { - 1 , + 1 }$ instead of $y in { 0 , 1 }$ . We can compute the probability of these alternative labels using \nsince $sigma ( - a ) = 1 - sigma ( a )$ . This slightly more compact notation is widely used in the ML literature. \n10.2.1 Linear classifiers \nThe sigmoid gives the probability that the class label is $y = 1$ . If the loss for misclassifying each class is the same, then the optimal decision rule is to predict $y = 1$ iff class 1 is more likely than class $0$ , as we explained in Section 5.1.2.2. Thus",
        "chapter": "II Linear Models",
        "section": "Linear Discriminant Analysis",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "10 Logistic Regression \n10.1 Introduction \nLogistic regression is a widely used discriminative classification model $p ( boldsymbol { y } | boldsymbol { x } ; boldsymbol { theta } )$ , where $pmb { x } in mathbb { R } ^ { D }$ is a fixed-dimensional input vector, $y in { 1 , ldots , C }$ is the class label, and $pmb theta$ are the parameters. If $C = 2$ , this is known as binary logistic regression, and if $C > 2$ , it is known as multinomial logistic regression, or alternatively, multiclass logistic regression. We give the details below. \n10.2 Binary logistic regression \nBinary logistic regression corresponds to the following model \nwhere $sigma$ is the sigmoid function defined in Section 2.4.2, $mathbf { boldsymbol { w } }$ are the weights, $b$ is the bias, and $pmb theta = ( pmb w , b )$ are all the parameters. In other words, \nwhere $a = pmb { w } ^ { 1 } pmb { x } + b$ is the log-odds, $log ( p / 1 - p )$ , where $p = p ( y = 1 | boldsymbol { x } ; boldsymbol { theta } )$ , as explained in Section 2.4.2. (In ML, the quantity $a$ is usually called the logit or the pre-activation.) \nSometimes we choose to use the labels $tilde { y } in { - 1 , + 1 }$ instead of $y in { 0 , 1 }$ . We can compute the probability of these alternative labels using \nsince $sigma ( - a ) = 1 - sigma ( a )$ . This slightly more compact notation is widely used in the ML literature. \n10.2.1 Linear classifiers \nThe sigmoid gives the probability that the class label is $y = 1$ . If the loss for misclassifying each class is the same, then the optimal decision rule is to predict $y = 1$ iff class 1 is more likely than class $0$ , as we explained in Section 5.1.2.2. Thus",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "10 Logistic Regression \n10.1 Introduction \nLogistic regression is a widely used discriminative classification model $p ( boldsymbol { y } | boldsymbol { x } ; boldsymbol { theta } )$ , where $pmb { x } in mathbb { R } ^ { D }$ is a fixed-dimensional input vector, $y in { 1 , ldots , C }$ is the class label, and $pmb theta$ are the parameters. If $C = 2$ , this is known as binary logistic regression, and if $C > 2$ , it is known as multinomial logistic regression, or alternatively, multiclass logistic regression. We give the details below. \n10.2 Binary logistic regression \nBinary logistic regression corresponds to the following model \nwhere $sigma$ is the sigmoid function defined in Section 2.4.2, $mathbf { boldsymbol { w } }$ are the weights, $b$ is the bias, and $pmb theta = ( pmb w , b )$ are all the parameters. In other words, \nwhere $a = pmb { w } ^ { 1 } pmb { x } + b$ is the log-odds, $log ( p / 1 - p )$ , where $p = p ( y = 1 | boldsymbol { x } ; boldsymbol { theta } )$ , as explained in Section 2.4.2. (In ML, the quantity $a$ is usually called the logit or the pre-activation.) \nSometimes we choose to use the labels $tilde { y } in { - 1 , + 1 }$ instead of $y in { 0 , 1 }$ . We can compute the probability of these alternative labels using \nsince $sigma ( - a ) = 1 - sigma ( a )$ . This slightly more compact notation is widely used in the ML literature. \n10.2.1 Linear classifiers \nThe sigmoid gives the probability that the class label is $y = 1$ . If the loss for misclassifying each class is the same, then the optimal decision rule is to predict $y = 1$ iff class 1 is more likely than class $0$ , as we explained in Section 5.1.2.2. Thus \nwhere $boldsymbol { a } = boldsymbol { w } ^ { intercal } boldsymbol { x } + boldsymbol { b }$ . \nThus we can write the prediction function as follows: \nwhere $pmb { w } ^ { top } pmb { x } = langle pmb { w } , pmb { x } rangle$ is the inner product between the weight vector $mathbf { boldsymbol { w } }$ and the feature vector $_ { x }$ . This function defines a linear hyperplane, with normal vector ${ pmb w } in mathbb { R } ^ { D }$ and an offset $b in mathbb { R }$ from the origin. \nEquation (10.5) can be understood by looking at Figure 10.1a. Here we show a plane in a 3d feature space going through the point $scriptstyle { mathbf { x } } _ { 0 }$ with surface normal $mathbf { boldsymbol { w } }$ . Points on the surface satisfy ${ pmb w } ^ { mathrm { I } } ( { pmb x } - { pmb x } _ { 0 } ) = 0$ . If we define $b = - pmb { w } ^ { top } pmb { x } _ { 0 }$ , we can rewrite this as ${ pmb w } ^ { mathrm { 1 } } { pmb x } + b = 0$ . This plane separates 3d space into two half spaces. This linear plane is known as a decision boundary. If we can perfectly separate the training examples by such a linear boundary (without making any classification errors on the training set), we say the data is linearly separable. From Figure 10.1b, we see that the two-class, two-feature version of the iris dataset is not linearly separable. \nIn general, there will be uncertainty about the correct class label, so we need to predict a probability distribution over labels, and not just decide which side of the decision boundary we are on. In Figure 10.2, we plot $p ( y = 1 | x _ { 1 } , x _ { 2 } ; pmb { w } ) = sigma ( w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } )$ for different weight vectors $mathbf { boldsymbol { w } }$ . The vector $mathbf { boldsymbol { w } }$ defines the orientation of the decision boundary, and its magnitude, $| | boldsymbol { w } | | = sqrt { sum _ { d = 1 } ^ { D } w _ { d } ^ { 2 } }$ , controls the steepness of the sigmoid, and hence the confidence of the predictions. \n10.2.2 Nonlinear classifiers \nWe can often make a problem linearly separable by preprocessing the inputs in a suitable way. In particular, let $phi ( { pmb x } )$ be a transformed version of the input feature vector. For example, suppose we use $phi ( x _ { 1 } , x _ { 2 } ) = [ 1 , x _ { 1 } ^ { 2 } , x _ { 2 } ^ { 2 } ]$ , and we let ${ pmb w } = [ - R ^ { 2 } , 1 , 1 ]$ . Then ${ pmb w } ^ { 1 } phi ( { pmb x } ) = x _ { 1 } ^ { 2 } + x _ { 2 } ^ { 2 } - R ^ { 2 }$ , so the decision boundary (where $f ( { pmb x } ) = 0$ ) defines a circle with radius $R$ , as shown in Figure 10.3. The resulting \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 function $f$ is still linear in the parameters $mathbf { boldsymbol { w } }$ , which is important for simplifying the learning problem, as we will see in Section 10.2.3. However, we can gain even more power by learning the parameters of the feature extractor $phi ( { pmb x } )$ in addition to linear weights $mathbf { boldsymbol { w } }$ ; we discuss how to do this in Part III.",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Linear classifiers"
    },
    {
        "content": "where $boldsymbol { a } = boldsymbol { w } ^ { intercal } boldsymbol { x } + boldsymbol { b }$ . \nThus we can write the prediction function as follows: \nwhere $pmb { w } ^ { top } pmb { x } = langle pmb { w } , pmb { x } rangle$ is the inner product between the weight vector $mathbf { boldsymbol { w } }$ and the feature vector $_ { x }$ . This function defines a linear hyperplane, with normal vector ${ pmb w } in mathbb { R } ^ { D }$ and an offset $b in mathbb { R }$ from the origin. \nEquation (10.5) can be understood by looking at Figure 10.1a. Here we show a plane in a 3d feature space going through the point $scriptstyle { mathbf { x } } _ { 0 }$ with surface normal $mathbf { boldsymbol { w } }$ . Points on the surface satisfy ${ pmb w } ^ { mathrm { I } } ( { pmb x } - { pmb x } _ { 0 } ) = 0$ . If we define $b = - pmb { w } ^ { top } pmb { x } _ { 0 }$ , we can rewrite this as ${ pmb w } ^ { mathrm { 1 } } { pmb x } + b = 0$ . This plane separates 3d space into two half spaces. This linear plane is known as a decision boundary. If we can perfectly separate the training examples by such a linear boundary (without making any classification errors on the training set), we say the data is linearly separable. From Figure 10.1b, we see that the two-class, two-feature version of the iris dataset is not linearly separable. \nIn general, there will be uncertainty about the correct class label, so we need to predict a probability distribution over labels, and not just decide which side of the decision boundary we are on. In Figure 10.2, we plot $p ( y = 1 | x _ { 1 } , x _ { 2 } ; pmb { w } ) = sigma ( w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } )$ for different weight vectors $mathbf { boldsymbol { w } }$ . The vector $mathbf { boldsymbol { w } }$ defines the orientation of the decision boundary, and its magnitude, $| | boldsymbol { w } | | = sqrt { sum _ { d = 1 } ^ { D } w _ { d } ^ { 2 } }$ , controls the steepness of the sigmoid, and hence the confidence of the predictions. \n10.2.2 Nonlinear classifiers \nWe can often make a problem linearly separable by preprocessing the inputs in a suitable way. In particular, let $phi ( { pmb x } )$ be a transformed version of the input feature vector. For example, suppose we use $phi ( x _ { 1 } , x _ { 2 } ) = [ 1 , x _ { 1 } ^ { 2 } , x _ { 2 } ^ { 2 } ]$ , and we let ${ pmb w } = [ - R ^ { 2 } , 1 , 1 ]$ . Then ${ pmb w } ^ { 1 } phi ( { pmb x } ) = x _ { 1 } ^ { 2 } + x _ { 2 } ^ { 2 } - R ^ { 2 }$ , so the decision boundary (where $f ( { pmb x } ) = 0$ ) defines a circle with radius $R$ , as shown in Figure 10.3. The resulting \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 function $f$ is still linear in the parameters $mathbf { boldsymbol { w } }$ , which is important for simplifying the learning problem, as we will see in Section 10.2.3. However, we can gain even more power by learning the parameters of the feature extractor $phi ( { pmb x } )$ in addition to linear weights $mathbf { boldsymbol { w } }$ ; we discuss how to do this in Part III. \n\nIn Figure 10.3, we used a quadratic expansion of the features. We can also use a higher order polynomial, as in Section 1.2.2.2. In Figure 1.7, we show the effects of using polynomial expansion up to degree $K$ on a 2d logistic regression problem. As in Figure 1.7, we see that the model becomes more complex as the number of parameters increases, and eventually results in overfitting. We discuss ways to reduce overfitting in Section 10.2.7. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n10.2.3 Maximum likelihood estimation \nIn this section, we discuss how to estimate the parameters of a logistic regression model using maximum likelihood estimation. \n10.2.3.1 Objective function \nThe negative log likelihood (scaled by the dataset size $N$ ) is given by the following (we assume the bias term $b$ is absorbed into the weight vector $mathbf { boldsymbol { w } }$ ): \nwhere $mu _ { n } = sigma ( a _ { n } )$ is the probability of class $^ { 1 }$ , $a _ { n } = pmb { w } ^ { top } pmb { x } _ { n }$ is the logit, and $mathbb { H } _ { c e } ( y _ { n } , mu _ { n } )$ is the binary cross entropy defined by \nIf we use $tilde { y } _ { n } in { - 1 , + 1 }$ instead of $y _ { n } in { 0 , 1 }$ , then we can rewrite this as follows: \nHowever, in this book, we will mostly use the $y _ { n } in { 0 , 1 }$ notation, since it is easier to generalize to the multiclass case (Section 10.3), and makes the connection with cross-entropy easier to see. \n10.2.3.2 Optimizing the objective \nTo find the MLE, we must solve \nWe can use any gradient-based optimization algorithm to solve this, such as those we discuss in Chapter 8. We give a specific example in Section 10.2.4. But first we must derive the gradient, as we explain below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Nonlinear classifiers"
    },
    {
        "content": "10.2.3 Maximum likelihood estimation \nIn this section, we discuss how to estimate the parameters of a logistic regression model using maximum likelihood estimation. \n10.2.3.1 Objective function \nThe negative log likelihood (scaled by the dataset size $N$ ) is given by the following (we assume the bias term $b$ is absorbed into the weight vector $mathbf { boldsymbol { w } }$ ): \nwhere $mu _ { n } = sigma ( a _ { n } )$ is the probability of class $^ { 1 }$ , $a _ { n } = pmb { w } ^ { top } pmb { x } _ { n }$ is the logit, and $mathbb { H } _ { c e } ( y _ { n } , mu _ { n } )$ is the binary cross entropy defined by \nIf we use $tilde { y } _ { n } in { - 1 , + 1 }$ instead of $y _ { n } in { 0 , 1 }$ , then we can rewrite this as follows: \nHowever, in this book, we will mostly use the $y _ { n } in { 0 , 1 }$ notation, since it is easier to generalize to the multiclass case (Section 10.3), and makes the connection with cross-entropy easier to see. \n10.2.3.2 Optimizing the objective \nTo find the MLE, we must solve \nWe can use any gradient-based optimization algorithm to solve this, such as those we discuss in Chapter 8. We give a specific example in Section 10.2.4. But first we must derive the gradient, as we explain below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n10.2.3.3 Deriving the gradient \nAlthough we can use automatic differentiation methods (Section 13.3) to compute the gradient of the NLL, it is also easy to do explicitly, as we show below. Fortunately the resulting equations will turn out to have a simple and intuitive interpretation, which can be used to derive other methods, as we will see. \nTo start, note that \nwhere $a _ { n } = pmb { w } ^ { 1 } pmb { x } _ { n }$ and $mu _ { n } = sigma ( a _ { n } )$ . Hence by the chain rule (and the rules of vector calculus, discussed in Section 7.8) we have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe gradient for the bias term can be derived in the same way, by using the input $x _ { n 0 } = 1$ in the above equation. However, we will ignore the bias term for simplicity. Hence \nSimilarly, \nThus the gradient vector of the NLL is given by \nIf we interpret $e _ { n } = mu _ { n } - y _ { n }$ as an error signal, we can see that the gradient weights each input ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ by its error, and then averages the result. Note that we can rewrite the gradient in matrix form as follows: \n10.2.3.4 Deriving the Hessian \nGradient-based optimizers will find a stationary point where $mathbf { nabla } _ { mathbf { boldsymbol { g } } ( mathbf { boldsymbol { w } } ) } = mathbf { 0 }$ . This could either be a global optimum or a local optimum. To be sure the stationary point is the global optimum, we must show that the objective is convex, for reasons we explain in Section 8.1.1.1. Intuitvely this means that the NLL has a bowl shape, with a unique lowest point, which is indeed the case, as illustrated in Figure 10.5b. \nMore formally, we must prove that the Hessian is positive semi-definite, which we now do. (See Chapter 7 for relevant background information on linear algebra.) One can show that the Hessian is given by \nwhere \nWe see that $mathbf { H }$ is positive definite, since for any nonzero vector $mathbf { nabla } _ { mathbf { v } }$ , we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis follows since $mu _ { n } > 0$ for all $n$ , because of the use of the sigmoid function. Consequently the NLL is strictly convex. However, in practice, values of $mu _ { n }$ which are close to 0 or 1 might cause the Hessian to be close to singular. We can avoid this by using $ell _ { 2 }$ regularization, as we discuss in Section 10.2.7. \n10.2.4 Stochastic gradient descent \nOur goal is to solve the following optimization problem \nwhere $mathcal { L } ( w )$ is the loss function, in this case the negative log likelihood: \nwhere $mu _ { n } = sigma ( a _ { n } )$ is the probability of class 1, and $a _ { n } = pmb { w } ^ { vert } pmb { x } _ { n }$ is the log odds. \nThere are many algorithms we could use to solve Equation (10.26), as we discuss in Chapter 8. Perhaps the simplest is to use stochastic gradient descent (Section 8.4). If we use a minibatch of size $^ { 1 }$ , then we get the following simple update equation: \nwhere we replaced the average over all $N$ examples in the gradient of Equation (10.21) with a single stochastically chosen sample $n$ . (The index $n$ changes with $t$ .) \nSince we know the objective is convex (see Section 10.2.3.4), then one can show that this procedure will converge to the global optimum, provided we decay the learning rate at the appropriate rate (see Section 8.4.3). We can improve the convergence speed using variance reduction techniques such as SAGA (Section 8.4.5.2). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Maximum likelihood estimation"
    },
    {
        "content": "This follows since $mu _ { n } > 0$ for all $n$ , because of the use of the sigmoid function. Consequently the NLL is strictly convex. However, in practice, values of $mu _ { n }$ which are close to 0 or 1 might cause the Hessian to be close to singular. We can avoid this by using $ell _ { 2 }$ regularization, as we discuss in Section 10.2.7. \n10.2.4 Stochastic gradient descent \nOur goal is to solve the following optimization problem \nwhere $mathcal { L } ( w )$ is the loss function, in this case the negative log likelihood: \nwhere $mu _ { n } = sigma ( a _ { n } )$ is the probability of class 1, and $a _ { n } = pmb { w } ^ { vert } pmb { x } _ { n }$ is the log odds. \nThere are many algorithms we could use to solve Equation (10.26), as we discuss in Chapter 8. Perhaps the simplest is to use stochastic gradient descent (Section 8.4). If we use a minibatch of size $^ { 1 }$ , then we get the following simple update equation: \nwhere we replaced the average over all $N$ examples in the gradient of Equation (10.21) with a single stochastically chosen sample $n$ . (The index $n$ changes with $t$ .) \nSince we know the objective is convex (see Section 10.2.3.4), then one can show that this procedure will converge to the global optimum, provided we decay the learning rate at the appropriate rate (see Section 8.4.3). We can improve the convergence speed using variance reduction techniques such as SAGA (Section 8.4.5.2). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n10.2.5 Perceptron algorithm \nA perceptron, first introduced in [Ros58], is a deterministic binary classifier of the following form: \nThis can be seen to be a limiting case of a binary logistic regression classifier, in which the sigmoid function $sigma ( a )$ is replaced by the Heaviside step function $H ( a ) triangleq mathbb { I } ( a > 0 )$ . See Figure 2.10 for a comparison of these two functions. \nSince the Heaviside function is not differentiable, we cannot use gradient-based optimization methods to fit this model. However, Rosenblatt proposed the perceptron learning algorithm instead. The basic idea is to start with random weights, and then iteratively update them whenever the model makes a prediction mistake. More precisely, we update the weights using \nwhere $( { pmb x } _ { n } , y _ { n } )$ is the labeled example sampled at iteration $t$ , and $eta _ { t }$ is the learning rate or step size. (We can set the step size to 1, since the magnitude of the weights does not affect the decision boundary.) See perceptron_demo_2d.ipynb for a simple implementation of this algorithm. \nThe perceptron update rule in Equation (10.30) has an intuitive interpretation: if the prediction is correct, no change is made, otherwise we move the weights in a direction so as to make the correct answer more likely. More precisely, if $y _ { n } = 1$ and ${ hat { y } } _ { n } = 0$ , we have $pmb { w } _ { t + 1 } = pmb { w } _ { t } + pmb { x } _ { n }$ , and if $y _ { n } = 0$ and ${ hat { y } } _ { n } = 1$ , we have ${ pmb w } _ { t + 1 } = { pmb w } _ { t } - { pmb x } _ { n }$ . \nBy comparing Equation (10.30) to Equation (10.28), we see that the perceptron update rule is equivalent to the SGD update rule for binary logistic regression using the approximation where we replace the soft probabilities $mu _ { n } = p ( y _ { n } = 1 | x _ { n } )$ with hard labels ${ hat { y } } _ { n } = f ( mathbf { x } _ { n } )$ . The advantage of the perceptron method is that we don’t need to compute probabilities, which can be useful when the label space is very large. The disadvantage is that the method will only converge when the data is linearly separable [Nov62], whereas SGD for minimizing the NLL for logistic regression will always converge to the globally optimal MLE, even if the data is not linearly separable. \nIn Section 13.2, we will generalize perceptrons to nonlinear functions, thus significantly enhancing their usefulness. \n10.2.6 Iteratively reweighted least squares \nGradient descent is a first order optimization method, which means it only uses first order gradients to navigate through the loss landscape. This can be slow, especially when some directions of space point steeply downhill, whereas other have a shallower gradient, as is the case in Figure 10.5a. In such problems, it can be much faster to use a second order optimization method, that takes the curvature of the space into account. \nWe discuss such methods in more detail in Section 8.3. Here we just consider a simple second order method that works well for logistic regression. We focus on the full batch setting (so we assume $N$ is small), since it is harder to make second order methods work in the stochastic setting (see e.g., [Byr+16; Liu+18b] for some methods). \nThe classic second-order method is Newton’s method. This consists of updates of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Stochastic gradient descent"
    },
    {
        "content": "10.2.5 Perceptron algorithm \nA perceptron, first introduced in [Ros58], is a deterministic binary classifier of the following form: \nThis can be seen to be a limiting case of a binary logistic regression classifier, in which the sigmoid function $sigma ( a )$ is replaced by the Heaviside step function $H ( a ) triangleq mathbb { I } ( a > 0 )$ . See Figure 2.10 for a comparison of these two functions. \nSince the Heaviside function is not differentiable, we cannot use gradient-based optimization methods to fit this model. However, Rosenblatt proposed the perceptron learning algorithm instead. The basic idea is to start with random weights, and then iteratively update them whenever the model makes a prediction mistake. More precisely, we update the weights using \nwhere $( { pmb x } _ { n } , y _ { n } )$ is the labeled example sampled at iteration $t$ , and $eta _ { t }$ is the learning rate or step size. (We can set the step size to 1, since the magnitude of the weights does not affect the decision boundary.) See perceptron_demo_2d.ipynb for a simple implementation of this algorithm. \nThe perceptron update rule in Equation (10.30) has an intuitive interpretation: if the prediction is correct, no change is made, otherwise we move the weights in a direction so as to make the correct answer more likely. More precisely, if $y _ { n } = 1$ and ${ hat { y } } _ { n } = 0$ , we have $pmb { w } _ { t + 1 } = pmb { w } _ { t } + pmb { x } _ { n }$ , and if $y _ { n } = 0$ and ${ hat { y } } _ { n } = 1$ , we have ${ pmb w } _ { t + 1 } = { pmb w } _ { t } - { pmb x } _ { n }$ . \nBy comparing Equation (10.30) to Equation (10.28), we see that the perceptron update rule is equivalent to the SGD update rule for binary logistic regression using the approximation where we replace the soft probabilities $mu _ { n } = p ( y _ { n } = 1 | x _ { n } )$ with hard labels ${ hat { y } } _ { n } = f ( mathbf { x } _ { n } )$ . The advantage of the perceptron method is that we don’t need to compute probabilities, which can be useful when the label space is very large. The disadvantage is that the method will only converge when the data is linearly separable [Nov62], whereas SGD for minimizing the NLL for logistic regression will always converge to the globally optimal MLE, even if the data is not linearly separable. \nIn Section 13.2, we will generalize perceptrons to nonlinear functions, thus significantly enhancing their usefulness. \n10.2.6 Iteratively reweighted least squares \nGradient descent is a first order optimization method, which means it only uses first order gradients to navigate through the loss landscape. This can be slow, especially when some directions of space point steeply downhill, whereas other have a shallower gradient, as is the case in Figure 10.5a. In such problems, it can be much faster to use a second order optimization method, that takes the curvature of the space into account. \nWe discuss such methods in more detail in Section 8.3. Here we just consider a simple second order method that works well for logistic regression. We focus on the full batch setting (so we assume $N$ is small), since it is harder to make second order methods work in the stochastic setting (see e.g., [Byr+16; Liu+18b] for some methods). \nThe classic second-order method is Newton’s method. This consists of updates of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Perceptron algorithm"
    },
    {
        "content": "10.2.5 Perceptron algorithm \nA perceptron, first introduced in [Ros58], is a deterministic binary classifier of the following form: \nThis can be seen to be a limiting case of a binary logistic regression classifier, in which the sigmoid function $sigma ( a )$ is replaced by the Heaviside step function $H ( a ) triangleq mathbb { I } ( a > 0 )$ . See Figure 2.10 for a comparison of these two functions. \nSince the Heaviside function is not differentiable, we cannot use gradient-based optimization methods to fit this model. However, Rosenblatt proposed the perceptron learning algorithm instead. The basic idea is to start with random weights, and then iteratively update them whenever the model makes a prediction mistake. More precisely, we update the weights using \nwhere $( { pmb x } _ { n } , y _ { n } )$ is the labeled example sampled at iteration $t$ , and $eta _ { t }$ is the learning rate or step size. (We can set the step size to 1, since the magnitude of the weights does not affect the decision boundary.) See perceptron_demo_2d.ipynb for a simple implementation of this algorithm. \nThe perceptron update rule in Equation (10.30) has an intuitive interpretation: if the prediction is correct, no change is made, otherwise we move the weights in a direction so as to make the correct answer more likely. More precisely, if $y _ { n } = 1$ and ${ hat { y } } _ { n } = 0$ , we have $pmb { w } _ { t + 1 } = pmb { w } _ { t } + pmb { x } _ { n }$ , and if $y _ { n } = 0$ and ${ hat { y } } _ { n } = 1$ , we have ${ pmb w } _ { t + 1 } = { pmb w } _ { t } - { pmb x } _ { n }$ . \nBy comparing Equation (10.30) to Equation (10.28), we see that the perceptron update rule is equivalent to the SGD update rule for binary logistic regression using the approximation where we replace the soft probabilities $mu _ { n } = p ( y _ { n } = 1 | x _ { n } )$ with hard labels ${ hat { y } } _ { n } = f ( mathbf { x } _ { n } )$ . The advantage of the perceptron method is that we don’t need to compute probabilities, which can be useful when the label space is very large. The disadvantage is that the method will only converge when the data is linearly separable [Nov62], whereas SGD for minimizing the NLL for logistic regression will always converge to the globally optimal MLE, even if the data is not linearly separable. \nIn Section 13.2, we will generalize perceptrons to nonlinear functions, thus significantly enhancing their usefulness. \n10.2.6 Iteratively reweighted least squares \nGradient descent is a first order optimization method, which means it only uses first order gradients to navigate through the loss landscape. This can be slow, especially when some directions of space point steeply downhill, whereas other have a shallower gradient, as is the case in Figure 10.5a. In such problems, it can be much faster to use a second order optimization method, that takes the curvature of the space into account. \nWe discuss such methods in more detail in Section 8.3. Here we just consider a simple second order method that works well for logistic regression. We focus on the full batch setting (so we assume $N$ is small), since it is harder to make second order methods work in the stochastic setting (see e.g., [Byr+16; Liu+18b] for some methods). \nThe classic second-order method is Newton’s method. This consists of updates of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere \nis assumed to be positive-definite to ensure the update is well-defined. If the Hessian is exact, we can set the step size to $eta _ { t } = 1$ . \nWe now apply this method to logistic regression. Recall from Section 10.2.3.3 that the gradient and Hessian are given by \nHence the Newton update has the form \nwhere we have defined the working response as \nand $mathbf { S } _ { t } = mathrm { d i a g } ( mu _ { t , n } ( 1 - mu _ { t , n } ) )$ . Since $mathbf { S } _ { t }$ is a diagonal matrix, we can rewrite the targets in component form as follows: \nEquation (10.40) is an example of a weighted least squares problem (Section 11.2.2.4), which is a minimizer of \nThe overall method is therefore known as the iteratively reweighted least squares (IRLS) algorithm, since at each iteration we solve a weighted least squares problem, where the weight matrix $mathbf { S } _ { t }$ changes at each iteration. See Algorithm 2 for some pseudocode. \nNote that Fisher scoring is the same as IRLS except we replace the Hessian of the actual log-likelihood with its expectation, i.e., we use the Fisher information matrix (Section 4.7.2) instead of $mathbf { H }$ . Since the Fisher information matrix is independent of the data, it can be precomputed, unlike the Hessian, which must be reevaluated at every iteration. This can be faster for problems with many parameters. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n10.2.7 MAP estimation \nIn Figure 10.4, we saw how logistic regression can overfit when there are too many parameters compared to training examples. This is a consequence of the ability of maximum likelihood to find weights that force the decision boundary to “wiggle” in just the right way so as to curve around the examples. To get this behavior, the weights often need to be set to large values. For example, in Figure 10.4, when we use degree $K = 1$ , we find that the MLE for the two input weights (ignoring the bias) is \nWhen we use degree $K = 2$ , we get \nwˆ = [2.27510513, 0.05970325, 11.84198867, 15.40355969, 2.51242311] \nAnd when $K = 4$ , we get \nOne way to reduce such overfitting is to prevent the weights from becoming so large. We can do this by using a zero-mean Gaussian prior, $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , C mathbf { I } )$ , and then using MAP estimation, as we discussed in Section 4.5.3. The new training objective becomes \nlwahrgereer $begin{array} { r } { lvert | boldsymbol { mathbf { w } } rvert | _ { 2 } ^ { 2 } = sum _ { d = 1 } ^ { D } w _ { d } ^ { 2 } } end{array}$ eamnodr $lambda = 1 / C$ . mTehtiesrsisacrae lpeedn $ell _ { 2 }$ redgfuolrarbieziantg “olnarogre w(ediegvhiat idnegcfaryo.mTthe $lambda$ zero-mean prior), and thus the less flexible the model. See Figure 10.6 for an illustration. \nWe can compute the MAP estimate by slightly modifying the input to the above gradient-based optimization algorithms. The gradient and Hessian of the penalized negative log likelihood have the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Iteratively reweighted least squares"
    },
    {
        "content": "10.2.7 MAP estimation \nIn Figure 10.4, we saw how logistic regression can overfit when there are too many parameters compared to training examples. This is a consequence of the ability of maximum likelihood to find weights that force the decision boundary to “wiggle” in just the right way so as to curve around the examples. To get this behavior, the weights often need to be set to large values. For example, in Figure 10.4, when we use degree $K = 1$ , we find that the MLE for the two input weights (ignoring the bias) is \nWhen we use degree $K = 2$ , we get \nwˆ = [2.27510513, 0.05970325, 11.84198867, 15.40355969, 2.51242311] \nAnd when $K = 4$ , we get \nOne way to reduce such overfitting is to prevent the weights from becoming so large. We can do this by using a zero-mean Gaussian prior, $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , C mathbf { I } )$ , and then using MAP estimation, as we discussed in Section 4.5.3. The new training objective becomes \nlwahrgereer $begin{array} { r } { lvert | boldsymbol { mathbf { w } } rvert | _ { 2 } ^ { 2 } = sum _ { d = 1 } ^ { D } w _ { d } ^ { 2 } } end{array}$ eamnodr $lambda = 1 / C$ . mTehtiesrsisacrae lpeedn $ell _ { 2 }$ redgfuolrarbieziantg “olnarogre w(ediegvhiat idnegcfaryo.mTthe $lambda$ zero-mean prior), and thus the less flexible the model. See Figure 10.6 for an illustration. \nWe can compute the MAP estimate by slightly modifying the input to the above gradient-based optimization algorithms. The gradient and Hessian of the penalized negative log likelihood have the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nfollowing forms: \nwhere $mathbf g ( pmb { w } )$ is the gradient and $mathbf { H } ( w )$ is the Hessian of the unpenalized NLL. For an interesting exercise related to $ell _ { 2 }$ regularized logistic regression, see Exercise 10.2. \n10.2.8 Standardization \nIn Section 10.2.7, we use an isotropic prior $mathcal { N } ( pmb { w } | mathbf { 0 } , lambda ^ { - 1 } mathbf { I } )$ to prevent overfitting. This implicitly encodes the assumption that we expect all weights to be similar in magnitude, which in turn encodes the assumption we expect all input features to be similar in magnitude. However, in many datasets, input features are on different scales. In such cases, it is common to standardize the data, to ensure \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license each feature has mean 0 and variance 1. We can do this by subtracting the mean and dividing by the standard deviation of each feature, as follows:",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "MAP estimation"
    },
    {
        "content": "following forms: \nwhere $mathbf g ( pmb { w } )$ is the gradient and $mathbf { H } ( w )$ is the Hessian of the unpenalized NLL. For an interesting exercise related to $ell _ { 2 }$ regularized logistic regression, see Exercise 10.2. \n10.2.8 Standardization \nIn Section 10.2.7, we use an isotropic prior $mathcal { N } ( pmb { w } | mathbf { 0 } , lambda ^ { - 1 } mathbf { I } )$ to prevent overfitting. This implicitly encodes the assumption that we expect all weights to be similar in magnitude, which in turn encodes the assumption we expect all input features to be similar in magnitude. However, in many datasets, input features are on different scales. In such cases, it is common to standardize the data, to ensure \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license each feature has mean 0 and variance 1. We can do this by subtracting the mean and dividing by the standard deviation of each feature, as follows: \n\nAn alternative is to use min-max scaling, in which we rescale the inputs so they lie in the interval $[ 0 , 1 ]$ . Both methods ensure the features are comparable in magnitude, which can help with model fitting and inference, even if we don’t use MAP estimation. (See Section 11.7.5 for a discussion of this point.) \n10.3 Multinomial logistic regression \nMultinomial logistic regression is a discriminative classification model of the following form: \nwhere $pmb { x } in mathbb { R } ^ { D }$ is the input vector, $y in { 1 , ldots , C }$ is the class label, softmax() is the softmax function (Section 2.5.2), $mathbf { W }$ is a $C times D$ weight matrix, $^ { b }$ is $C$ -dimensional bias vector, $pmb theta = ( mathbf W , pmb b )$ are all the parameters. We will henceforth ignore the bias term $^ { b }$ ; we assume we prepend each $_ { x }$ with a $^ { 1 }$ , and add $^ { b }$ to the first column of $mathbf { W }$ . Thus $mathbf { theta } theta = mathbf { W }$ . \nIf we let $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ be the $C$ -dimensional vector of logits, then we can rewrite the above as follows: \nBecause of the normalization condition cC=1 p(yn = c|xn; θ) = 1, we can set wC = 0. (For example, in binary logistic regression, where $C = 2$ , we only learn a single weight vector.) Therefore the parameters $pmb theta$ correspond to a weight matrix $mathbf { W }$ of size $( C - 1 ) times D$ , where $pmb { x } _ { n } in mathbb { R } ^ { D }$ . \nNote that this model assumes the labels are mutually exclusive, i.e., there is only one true label. For some applications (e.g., image tagging), we want to predict one or more labels for an input; in this case, the output space is the set of subsets of ${ 1 , ldots , C }$ . This is called multi-label classification, as opposed to multi-class classification. This can be viewed as a bit vector, $mathcal { Y } = { 0 , 1 } ^ { C }$ , where the $c$ ’th output is set to 1 if the $c$ ’th tag is present. We can tackle this using a modified version of binary logistic regression with multiple outputs: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Binary logistic regression",
        "subsubsection": "Standardization"
    },
    {
        "content": "10.3.1 Linear and nonlinear classifiers \nLogistic regression computes linear decision boundaries in the input space, as shown in Figure 10.7(a) for the case where $pmb { x } in mathbb { R } ^ { 2 }$ and we have $C = 3$ classes. However, we can always transform the inputs in some way to create nonlinear boundaries. For example, suppose we replace ${ pmb x } = ( x _ { 1 } , x _ { 2 } )$ by \nThis lets us create quadratic decision boundaries, as illustrated in Figure 10.7(b). \n10.3.2 Maximum likelihood estimation \nIn this section, we discuss how to compute the maximum likelihood estimate (MLE) by minimizing the negative log likelihood (NLL). \n10.3.2.1 Objective \nThe NLL is given by \nwhere $mu _ { n c } = p ( y _ { n c } = 1 | pmb { x } _ { n } , pmb { theta } ) = mathrm { s o f t m a x } ( f ( pmb { x } _ { n } ; pmb { theta } ) ) _ { c }$ , ${ bf { nabla } } mathbf { mathbf { { y } } } _ { n }$ is the one-hot encoding of the label (so $y _ { n c } = mathbb { I } left( y _ { n } = c right) )$ ), and $mathbb { H } _ { c e } ( pmb { y } _ { n } , pmb { mu } _ { n } )$ is the cross-entropy: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Linear and nonlinear classifiers"
    },
    {
        "content": "10.3.1 Linear and nonlinear classifiers \nLogistic regression computes linear decision boundaries in the input space, as shown in Figure 10.7(a) for the case where $pmb { x } in mathbb { R } ^ { 2 }$ and we have $C = 3$ classes. However, we can always transform the inputs in some way to create nonlinear boundaries. For example, suppose we replace ${ pmb x } = ( x _ { 1 } , x _ { 2 } )$ by \nThis lets us create quadratic decision boundaries, as illustrated in Figure 10.7(b). \n10.3.2 Maximum likelihood estimation \nIn this section, we discuss how to compute the maximum likelihood estimate (MLE) by minimizing the negative log likelihood (NLL). \n10.3.2.1 Objective \nThe NLL is given by \nwhere $mu _ { n c } = p ( y _ { n c } = 1 | pmb { x } _ { n } , pmb { theta } ) = mathrm { s o f t m a x } ( f ( pmb { x } _ { n } ; pmb { theta } ) ) _ { c }$ , ${ bf { nabla } } mathbf { mathbf { { y } } } _ { n }$ is the one-hot encoding of the label (so $y _ { n c } = mathbb { I } left( y _ { n } = c right) )$ ), and $mathbb { H } _ { c e } ( pmb { y } _ { n } , pmb { mu } _ { n } )$ is the cross-entropy: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n10.3.2.2 Optimizing the objective \nTo find the optimum, we need to solve $nabla _ { mathbf { boldsymbol { w } } } mathrm { N L L } ( mathbf { boldsymbol { w } } ) = mathbf { 0 }$ , where $mathbf { boldsymbol { w } }$ is a vectorized version of the weight matrix W, and where we are ignoring the bias term for notational simplicity. We can find such a stationary point using any gradient-based optimizer; we give some examples below. But first we derive the gradient and Hessian, and then prove that the objective is convex. \n10.3.2.3 Deriving the gradient \nTo derive the gradient of the NLL, we need to use the Jacobian of the softmax function, which is as follows (see Exercise 10.1 for the proof): \nwhere $delta _ { c j } = mathbb { I } left( c = j right)$ . For example, if we have 3 classes, the Jacobian matrix is given by \nIn matrix form, this can be written as \nwhere $odot$ is elementwise product, $mu mathbf { 1 } ^ { mathsf { I } }$ copies $pmb { mu }$ across each column, and $mathbf { 1 } mu ^ { prime }$ copies $pmb { mu }$ across each row. \nWe now derive the gradient of the NLL for a single example, indexed by $n$ . To do this, we flatten the $D times C$ weight matrix into a vector $mathbf { boldsymbol { w } }$ of size $boldsymbol C boldsymbol D$ (or $( C - 1 ) D$ if we freeze one of the classes to have zero weight) by concatenating the rows, and then transposing into a column vector. We use ${ pmb w } _ { j }$ to denote the vector of weights associated with class $j$ . The gradient wrt this vector is giving by the following (where we use the Kronecker delta notation, $delta _ { j c }$ , which equals $^ { 1 }$ if $j = c$ and $0$ otherwise): \nWe can repeat this computation for each class, to get the full gradient vector. The gradient of the overall NLL is obtained by summing over examples, to give the $D times C$ matrix \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis has the same form as in the binary logistic regression case, namely an error term times the input. \n10.3.2.4 Deriving the Hessian \nExercise 10.1 asks you to show that the Hessian of the NLL for multinomial logistic regression is given by \nwhere $mathbf { A } otimes mathbf { B }$ is the Kronecker product (Section 7.2.5). In other words, the block $c , c ^ { prime }$ submatrix is given by \nFor example, if we have 3 features and 2 classes, this becomes \nwhere ${ bf X } _ { n } = { pmb x } _ { n } { pmb x } _ { n } ^ { sf I }$ . Exercise 10.1 also asks you to show that this is a positive definite matrix, so the objective is convex. \n10.3.3 Gradient-based optimization \nIt is straightforward to use the gradient in Section 10.3.2.3 to derive the SGD algorithm. Similarly, we can use the Hessian in Section 10.3.2.4 to derive a second-order optimization method. However, computing the Hessian can be expensive, so it is common to approximate it using quasi-Newton methods, such as limited memory BFGS. (BFGS stands for Broyden, Fletcher, Goldfarb and Shanno.) See Section 8.3.2 for details. Another approach, which is similar to IRLS, is described in Section 10.3.4. All of these methods rely on computing the gradient of the log-likelihood, which in turn requires computing normalized probabilities, which can be computed from the logits vector $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ using \nwhere lse is the log-sum-exp function defined in Section 2.5.4. For this reason, many software libraries define a version of the cross-entropy loss that takes unnormalized logits as input. \n10.3.4 Bound optimization \nIn this section, we consider an approach for fitting logistic regression using a class of algorithms known as bound optimization, which we describe in Section 8.7. The basic idea is to iteratively \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Maximum likelihood estimation"
    },
    {
        "content": "This has the same form as in the binary logistic regression case, namely an error term times the input. \n10.3.2.4 Deriving the Hessian \nExercise 10.1 asks you to show that the Hessian of the NLL for multinomial logistic regression is given by \nwhere $mathbf { A } otimes mathbf { B }$ is the Kronecker product (Section 7.2.5). In other words, the block $c , c ^ { prime }$ submatrix is given by \nFor example, if we have 3 features and 2 classes, this becomes \nwhere ${ bf X } _ { n } = { pmb x } _ { n } { pmb x } _ { n } ^ { sf I }$ . Exercise 10.1 also asks you to show that this is a positive definite matrix, so the objective is convex. \n10.3.3 Gradient-based optimization \nIt is straightforward to use the gradient in Section 10.3.2.3 to derive the SGD algorithm. Similarly, we can use the Hessian in Section 10.3.2.4 to derive a second-order optimization method. However, computing the Hessian can be expensive, so it is common to approximate it using quasi-Newton methods, such as limited memory BFGS. (BFGS stands for Broyden, Fletcher, Goldfarb and Shanno.) See Section 8.3.2 for details. Another approach, which is similar to IRLS, is described in Section 10.3.4. All of these methods rely on computing the gradient of the log-likelihood, which in turn requires computing normalized probabilities, which can be computed from the logits vector $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ using \nwhere lse is the log-sum-exp function defined in Section 2.5.4. For this reason, many software libraries define a version of the cross-entropy loss that takes unnormalized logits as input. \n10.3.4 Bound optimization \nIn this section, we consider an approach for fitting logistic regression using a class of algorithms known as bound optimization, which we describe in Section 8.7. The basic idea is to iteratively \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Gradient-based optimization"
    },
    {
        "content": "This has the same form as in the binary logistic regression case, namely an error term times the input. \n10.3.2.4 Deriving the Hessian \nExercise 10.1 asks you to show that the Hessian of the NLL for multinomial logistic regression is given by \nwhere $mathbf { A } otimes mathbf { B }$ is the Kronecker product (Section 7.2.5). In other words, the block $c , c ^ { prime }$ submatrix is given by \nFor example, if we have 3 features and 2 classes, this becomes \nwhere ${ bf X } _ { n } = { pmb x } _ { n } { pmb x } _ { n } ^ { sf I }$ . Exercise 10.1 also asks you to show that this is a positive definite matrix, so the objective is convex. \n10.3.3 Gradient-based optimization \nIt is straightforward to use the gradient in Section 10.3.2.3 to derive the SGD algorithm. Similarly, we can use the Hessian in Section 10.3.2.4 to derive a second-order optimization method. However, computing the Hessian can be expensive, so it is common to approximate it using quasi-Newton methods, such as limited memory BFGS. (BFGS stands for Broyden, Fletcher, Goldfarb and Shanno.) See Section 8.3.2 for details. Another approach, which is similar to IRLS, is described in Section 10.3.4. All of these methods rely on computing the gradient of the log-likelihood, which in turn requires computing normalized probabilities, which can be computed from the logits vector $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ using \nwhere lse is the log-sum-exp function defined in Section 2.5.4. For this reason, many software libraries define a version of the cross-entropy loss that takes unnormalized logits as input. \n10.3.4 Bound optimization \nIn this section, we consider an approach for fitting logistic regression using a class of algorithms known as bound optimization, which we describe in Section 8.7. The basic idea is to iteratively \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nconstruct a lower bound on the function you want to maximize, and then to update the bound, so it “pushes up” on the true function. Optimizing the bound is often easier than updating the function directly. \nIf $ell ( pmb theta )$ is a concave function we want to maximize, then one way to obtain a valid lower bound is to use a bound on its Hessian, i.e., to find a negative definite matrix $mathbf { B }$ such that $mathbf { H } ( pmb { theta } ) succ mathbf { B }$ . In this case, one can show that \nwhere $pmb { g } ( pmb { theta } ^ { t } ) = nabla ell ( pmb { theta } ^ { t } )$ . Defining $Q ( pmb theta , pmb theta ^ { t } )$ as the right-hand-side of Equation (10.74), the update becomes \nThis is similar to a Newton update, except we use $mathbf { B }$ , which is a fixed matrix, rather than $mathbf { H } ( pmb theta ^ { t } )$ , which changes at each iteration. This can give us some of the advantages of second order methods at lower computational cost. \nLet us now apply this to logistic regression, following [Kri+05], Let $pmb { mu } _ { n } ( pmb { w } ) = [ p ( y _ { n } = 1 | pmb { x } _ { n } , pmb { w } ) , dots , p ( y _ { n } =$ $C | mathbf { boldsymbol { x } } _ { n } , mathbf { boldsymbol { w } } ) ]$ and $pmb { y } _ { n } = mathbb { I } left( y _ { n } = 1 right) , ldots , mathbb { I } left( y _ { n } = C right) mathrm { l }$ . We want to maximize the log-likelihood, which is as follows: \nThe gradient is given by the following (see Section 10.3.2.3 for details of the derivation): \nwhere $otimes$ denotes Kronecker product (which, in this case, is just outer product of the two vectors). The Hessian is given by the following (see Section 10.3.2.4 for details of the derivation): \nWe can construct a lower bound on the Hessian, as shown in [Boh92]: \nwhere $mathbf { I }$ is a $C$ -dimensional identity matrix, and $mathbf { 1 }$ is a $C$ -dimensional vector of all 1s.1 In the binary case, this becomes \nThis follows since $mu _ { n } leq 0 . 5$ so $- ( mu _ { n } - mu _ { n } ^ { 2 } ) ge - 0 . 2 5$ . \nWe can use this lower bound to construct an MM algorithm to find the MLE. The update becomes \nThis iteration can be faster than IRLS (Section 10.2.6) since we can precompute $mathbf { B } ^ { - 1 }$ in time independent of $N$ , rather than having to invert the Hessian at each iteration. For example, let us consider the binary case, so $pmb { g } ^ { t } = nabla ell ( pmb { w } ^ { t } ) = mathbf { X } ^ { 1 } left( pmb { y } - pmb { mu } ^ { t } right)$ , where $pmb { mu } ^ { t } = [ p _ { n } ( pmb { w } ^ { t } ) , ( 1 - p _ { n } ( pmb { w } ^ { t } ) ) ] _ { n = 1 } ^ { N }$ . The update becomes \nCompare this to Equation (10.37), which has the following form: \nwhere $mathbf { S } ^ { t } = mathrm { d i a g } ( { pmb { mu } } ^ { t } odot ( 1 - { pmb { mu } } ^ { t } ) )$ . We see that Equation (10.82) is faster to compute, since we can precompute the constant matrix $( mathbf { X } ^ { mathsf { I } } mathbf { X } ) ^ { - 1 }$ . \n10.3.5 MAP estimation \nIn Section 10.2.7 we discussed the benefits of $ell _ { 2 }$ regularization for binary logistic regression. These benefits hold also in the multi-class case. However, there is also an additional, and surprising, benefit to do with identifiability of the parameters, as pointed out in [HTF09, Ex.18.3]. (We say that the parameters are identifiable if there is a unique value that maximizes the likelihood; equivalently, we require that the NLL be strictly convex.) \nTo see why identifiability is an issue, recall that multiclass logistic regression has the form \nwhere $mathbf { W }$ is a $C times D$ weight matrix. We can arbitrarily define ${ pmb w } _ { c } = { bf 0 }$ for one of the classes, say $c = C$ , since $begin{array} { r } { p ( y = C | pmb { x } , mathbf { W } ) = 1 - sum _ { c = 1 } ^ { C - 1 } p ( y = c | pmb { x } , pmb { w } ) } end{array}$ . In this case, the model has the form \nIf we don’t “clamp” one of the vectors to some constant value, the parameters will be unidentifiable. However, suppose we don’t clamp ${ pmb w } _ { c } = { pmb 0 }$ , so we are using Equation 10.84, but we add $ell _ { 2 }$ regularization by optimizing \nwhere we have absorbed the $1 / N$ term into $lambda$ . At the optimum we have $textstyle sum _ { c = 1 } ^ { C } hat { w } _ { c j } = 0$ for $j = 1 : D$ , so the weights automatically satisfy a sum-to-zero constraint, thus making them uniquely identifiable. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Bound optimization"
    },
    {
        "content": "This follows since $mu _ { n } leq 0 . 5$ so $- ( mu _ { n } - mu _ { n } ^ { 2 } ) ge - 0 . 2 5$ . \nWe can use this lower bound to construct an MM algorithm to find the MLE. The update becomes \nThis iteration can be faster than IRLS (Section 10.2.6) since we can precompute $mathbf { B } ^ { - 1 }$ in time independent of $N$ , rather than having to invert the Hessian at each iteration. For example, let us consider the binary case, so $pmb { g } ^ { t } = nabla ell ( pmb { w } ^ { t } ) = mathbf { X } ^ { 1 } left( pmb { y } - pmb { mu } ^ { t } right)$ , where $pmb { mu } ^ { t } = [ p _ { n } ( pmb { w } ^ { t } ) , ( 1 - p _ { n } ( pmb { w } ^ { t } ) ) ] _ { n = 1 } ^ { N }$ . The update becomes \nCompare this to Equation (10.37), which has the following form: \nwhere $mathbf { S } ^ { t } = mathrm { d i a g } ( { pmb { mu } } ^ { t } odot ( 1 - { pmb { mu } } ^ { t } ) )$ . We see that Equation (10.82) is faster to compute, since we can precompute the constant matrix $( mathbf { X } ^ { mathsf { I } } mathbf { X } ) ^ { - 1 }$ . \n10.3.5 MAP estimation \nIn Section 10.2.7 we discussed the benefits of $ell _ { 2 }$ regularization for binary logistic regression. These benefits hold also in the multi-class case. However, there is also an additional, and surprising, benefit to do with identifiability of the parameters, as pointed out in [HTF09, Ex.18.3]. (We say that the parameters are identifiable if there is a unique value that maximizes the likelihood; equivalently, we require that the NLL be strictly convex.) \nTo see why identifiability is an issue, recall that multiclass logistic regression has the form \nwhere $mathbf { W }$ is a $C times D$ weight matrix. We can arbitrarily define ${ pmb w } _ { c } = { bf 0 }$ for one of the classes, say $c = C$ , since $begin{array} { r } { p ( y = C | pmb { x } , mathbf { W } ) = 1 - sum _ { c = 1 } ^ { C - 1 } p ( y = c | pmb { x } , pmb { w } ) } end{array}$ . In this case, the model has the form \nIf we don’t “clamp” one of the vectors to some constant value, the parameters will be unidentifiable. However, suppose we don’t clamp ${ pmb w } _ { c } = { pmb 0 }$ , so we are using Equation 10.84, but we add $ell _ { 2 }$ regularization by optimizing \nwhere we have absorbed the $1 / N$ term into $lambda$ . At the optimum we have $textstyle sum _ { c = 1 } ^ { C } hat { w } _ { c j } = 0$ for $j = 1 : D$ , so the weights automatically satisfy a sum-to-zero constraint, thus making them uniquely identifiable. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTo see why, note that at the optimum we have \nHence for any feature dimension $j$ we have \nThus if $lambda > 0$ we have $begin{array} { r } { sum _ { c } hat { w } _ { c j } = 0 } end{array}$ , so the weights will sum to zero across classes for each feature dimension. \n10.3.6 Maximum entropy classifiers \nRecall that the multinomial logistic regression model can be written as \nwhere $begin{array} { r } { Z ( pmb { w } , pmb { x } ) = sum _ { c } exp ( pmb { w } _ { c } ^ { 1 } pmb { x } ) } end{array}$ is the partition function (normalization constant). This uses the same features, but a different weight vector, for every class. There is a slight extension of this model that allows us to use features that are class-dependent. This model can be written as \nwhere $phi ( { pmb x } , c )$ is the feature vector for class $c$ . This is called a maximum entropy classifer, or maxent classifier for short. (The origin of this term is explained in Section 3.4.4.) \nMaxent classifiers include multinomial logistic regression as a special case. To see this let $mathbf { nabla } w =$ $[ { pmb w } _ { 1 } , dots , { pmb w } _ { C } ]$ , and define the feature vector as follows: \nwhere $_ { x }$ is embedded in the $c$ ’th block, and the remaining blocks are zero. In this case, ${ pmb w } ^ { scriptscriptstyle 1 } phi ( { pmb x } , c ) =$ ${ pmb w } _ { c } ^ { 1 } { pmb x }$ , so we recover multinomial logistic regression. \nMaxent classifiers are very widely used in the field of natural language processing. For example, consider the problem of semantic role labeling, where we classify a word $_ { x }$ into a semantic role $y$ , such as person, place or thing. We might define (binary) features such as the following: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "MAP estimation"
    },
    {
        "content": "To see why, note that at the optimum we have \nHence for any feature dimension $j$ we have \nThus if $lambda > 0$ we have $begin{array} { r } { sum _ { c } hat { w } _ { c j } = 0 } end{array}$ , so the weights will sum to zero across classes for each feature dimension. \n10.3.6 Maximum entropy classifiers \nRecall that the multinomial logistic regression model can be written as \nwhere $begin{array} { r } { Z ( pmb { w } , pmb { x } ) = sum _ { c } exp ( pmb { w } _ { c } ^ { 1 } pmb { x } ) } end{array}$ is the partition function (normalization constant). This uses the same features, but a different weight vector, for every class. There is a slight extension of this model that allows us to use features that are class-dependent. This model can be written as \nwhere $phi ( { pmb x } , c )$ is the feature vector for class $c$ . This is called a maximum entropy classifer, or maxent classifier for short. (The origin of this term is explained in Section 3.4.4.) \nMaxent classifiers include multinomial logistic regression as a special case. To see this let $mathbf { nabla } w =$ $[ { pmb w } _ { 1 } , dots , { pmb w } _ { C } ]$ , and define the feature vector as follows: \nwhere $_ { x }$ is embedded in the $c$ ’th block, and the remaining blocks are zero. In this case, ${ pmb w } ^ { scriptscriptstyle 1 } phi ( { pmb x } , c ) =$ ${ pmb w } _ { c } ^ { 1 } { pmb x }$ , so we recover multinomial logistic regression. \nMaxent classifiers are very widely used in the field of natural language processing. For example, consider the problem of semantic role labeling, where we classify a word $_ { x }$ into a semantic role $y$ , such as person, place or thing. We might define (binary) features such as the following: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe see that the features we use depend on the label. \nThere are two main ways of creating these features. The first is to manually specify many possibly useful features using various templates, and then use a feature selection algorithm, such as the group lasso method of Section 11.4.7. The second is to incrementally add features to the model, using a heuristic feature generation method. \n10.3.7 Hierarchical classification \nSometimes the set of possible labels can be structured into a hierarchy or taxonomy. For example, we might want to predict what kind of an animal is in an image: it could be a dog or a cat; if it is a dog, it could be a golden retriever or a German shepherd, etc. Intuitively, it makes sense to try to predict the most precise label for which we are confident [Den+12], that is, the system should “hedge its bets”. \nOne simple way to achieve this, proposed in [RF17], is as follows. First, create a model with a binary output label for every possible node in the tree. Before training the model, we will use label smearing, so that a label is propagated to all of its parents (hypernyms). For example, if an image is labeled “golden retriever”, we will also label it “dog”. If we train a multi-label classifier (which produces a vector $p ( pmb { y } | pmb { x } )$ of binary labels) on such smeared data, it will perform hierarchical classification, predicting a set of labels at different levels of abstraction. \nHowever, this method could predict “golden retriever”, “cat” and “bird” all with probability 1.0, since the model does not capture the fact that some labels are mutually exclusive. To prevent this, we can add a mutual exclusion constraint between all label nodes which are siblings, as shown in Figure 10.8. For example, this model enforces that $p ( mathrm { m a m m a l } | pmb { x } ) + p ( mathrm { b i r d } | pmb { x } ) = 1$ , since these two labels are children of the root node. We can further partition the mammal probability into dogs and cats, so we have $p ( deg | pmb { x } ) + p ( mathrm { c a t } | pmb { x } ) = p ( mathrm { m a m m a l } | pmb { x } )$ . \n[Den+14; Din+15] generalize the above method by using a conditional graphical model where the graph structure can be more complex than a tree. In addition, they allow for soft constraints between labels, in addition to hard constraints. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Maximum entropy classifiers"
    },
    {
        "content": "We see that the features we use depend on the label. \nThere are two main ways of creating these features. The first is to manually specify many possibly useful features using various templates, and then use a feature selection algorithm, such as the group lasso method of Section 11.4.7. The second is to incrementally add features to the model, using a heuristic feature generation method. \n10.3.7 Hierarchical classification \nSometimes the set of possible labels can be structured into a hierarchy or taxonomy. For example, we might want to predict what kind of an animal is in an image: it could be a dog or a cat; if it is a dog, it could be a golden retriever or a German shepherd, etc. Intuitively, it makes sense to try to predict the most precise label for which we are confident [Den+12], that is, the system should “hedge its bets”. \nOne simple way to achieve this, proposed in [RF17], is as follows. First, create a model with a binary output label for every possible node in the tree. Before training the model, we will use label smearing, so that a label is propagated to all of its parents (hypernyms). For example, if an image is labeled “golden retriever”, we will also label it “dog”. If we train a multi-label classifier (which produces a vector $p ( pmb { y } | pmb { x } )$ of binary labels) on such smeared data, it will perform hierarchical classification, predicting a set of labels at different levels of abstraction. \nHowever, this method could predict “golden retriever”, “cat” and “bird” all with probability 1.0, since the model does not capture the fact that some labels are mutually exclusive. To prevent this, we can add a mutual exclusion constraint between all label nodes which are siblings, as shown in Figure 10.8. For example, this model enforces that $p ( mathrm { m a m m a l } | pmb { x } ) + p ( mathrm { b i r d } | pmb { x } ) = 1$ , since these two labels are children of the root node. We can further partition the mammal probability into dogs and cats, so we have $p ( deg | pmb { x } ) + p ( mathrm { c a t } | pmb { x } ) = p ( mathrm { m a m m a l } | pmb { x } )$ . \n[Den+14; Din+15] generalize the above method by using a conditional graphical model where the graph structure can be more complex than a tree. In addition, they allow for soft constraints between labels, in addition to hard constraints. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n10.3.8 Handling large numbers of classes \nIn this section, we discuss some issues that arise when there are a large number of potential labels, e.g., if the labels correspond to words from a language. \n10.3.8.1 Hierarchical softmax \nIn regular softmax classifiers, computing the normalization constant, which is needed to compute the gradient of the log likelihood, takes $O ( C )$ time, which can become the bottleneck if $C$ is large. However, if we structure the labels as a tree, we can compute the probability of any label in $O ( log C )$ time, by multiplying the probabilities of each edge on the path from the root to the leaf. For example, consider the tree in Figure 10.9. We have \nThus we replace the “flat” output softmax with a tree-structured sequence of binary classifiers. This is called hierarchical softmax [Goo01; MB05]. \nA good way to structure such a tree is to use Huffman encoding, where the most frequent labels are placed near the top of the tree, as suggested in [Mik+13a]. (For a different appproach, based on clustering the most common labels together, see [Gra+17]. And for yet another approach, based on sampling labels, see [Tit16].) \n10.3.8.2 Class imbalance and the long tail \nAnother issue that often arises when there are a large number of classes is that for most classes, we may have very few examples. More precisely, if $N _ { c }$ is the number of examples of class $c$ , then the empirical distribution $p ( N _ { 1 } , ldots , N _ { C } )$ may have a long tail. The result is an extreme form of class imbalance (see e.g., [ASR15]). Since the rare classes will have a smaller effect on the overall loss than the common classes, the model may “focus its attention” on the common classes. \nOne method that can help is to set the bias terms $^ { b }$ such that softmax $(  { b } ) _ { c } = N _ { c } / N$ ; such a model will match the empirical label prior even when using weights of $mathbf { nabla } { boldsymbol { w } } = mathbf { 0 }$ . As the weights are adjusted, the model can learn input-dependent deviations from this prior. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Hierarchical classification"
    },
    {
        "content": "10.3.8 Handling large numbers of classes \nIn this section, we discuss some issues that arise when there are a large number of potential labels, e.g., if the labels correspond to words from a language. \n10.3.8.1 Hierarchical softmax \nIn regular softmax classifiers, computing the normalization constant, which is needed to compute the gradient of the log likelihood, takes $O ( C )$ time, which can become the bottleneck if $C$ is large. However, if we structure the labels as a tree, we can compute the probability of any label in $O ( log C )$ time, by multiplying the probabilities of each edge on the path from the root to the leaf. For example, consider the tree in Figure 10.9. We have \nThus we replace the “flat” output softmax with a tree-structured sequence of binary classifiers. This is called hierarchical softmax [Goo01; MB05]. \nA good way to structure such a tree is to use Huffman encoding, where the most frequent labels are placed near the top of the tree, as suggested in [Mik+13a]. (For a different appproach, based on clustering the most common labels together, see [Gra+17]. And for yet another approach, based on sampling labels, see [Tit16].) \n10.3.8.2 Class imbalance and the long tail \nAnother issue that often arises when there are a large number of classes is that for most classes, we may have very few examples. More precisely, if $N _ { c }$ is the number of examples of class $c$ , then the empirical distribution $p ( N _ { 1 } , ldots , N _ { C } )$ may have a long tail. The result is an extreme form of class imbalance (see e.g., [ASR15]). Since the rare classes will have a smaller effect on the overall loss than the common classes, the model may “focus its attention” on the common classes. \nOne method that can help is to set the bias terms $^ { b }$ such that softmax $(  { b } ) _ { c } = N _ { c } / N$ ; such a model will match the empirical label prior even when using weights of $mathbf { nabla } { boldsymbol { w } } = mathbf { 0 }$ . As the weights are adjusted, the model can learn input-dependent deviations from this prior. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nAnother common approach is to resample the data to make it more balanced, before (or during) training. In particular, suppose we sample a datapoint from class $c$ with probability \nIf we set $q = 1$ , we recover standard instance-balanced sampling, where $p _ { c } propto N _ { c }$ ; the common classes will be sampled more than rare classes. If we set $q = 0$ , we recover class-balanced sampling, where $p _ { c } = 1 / C$ ; this can be thought of as first sampling a class uniformly at random, and then sampling an instance of this class. Finally, we can consider other options, such as $q = 0 . 5$ , which is known as square-root sampling [Mah+18]. \nYet another method that is simple and can easily handle the long tail is to use the nearest class mean classifier. This has the form \nwhere $begin{array} { r } { pmb { mu } _ { c } = frac { 1 } { N _ { c } } sum _ { n : y _ { n } = c } pmb { x } _ { n } } end{array}$ is the mean of the features belonging to class $c$ . This induces a softmax posterior, as we discussed in Section 9.2.5. We can get much better results if we first use a neural network (see Part III) to learn good features, by training a DNN classifier with cross-entropy loss on the original unbalanced data. We then replace $_ { x }$ with $phi ( { pmb x } )$ in Equation (10.98). This simple approach can give very good performance on long-tailed distributions [Kan+20]. \n10.4 Robust logistic regression * \nSometimes we have outliers in our data, which are often due to labeling errors, also called label noise. To prevent the model from being adversely affected by such contamination, we will use robust logistic regression. In this section, we discuss some approaches to this problem. (Note that the methods can also be applied to DNNs. For a more thorough survey of label noise, and how it impacts deep learning, see [Han+20].) \n10.4.1 Mixture model for the likelihood \nOne of the simplest ways to define a robust logistic regression model is to modify the likelihood so that it predicts that each output label $y$ is generated uniformly at random with probability $pi$ , and otherwise is generated using the usual conditional model. In the binary case, this becomes \nThis approach, of using a mixture model for the observation model to make it robust, can be applied to many different models (e.g., DNNs). \nWe can fit this model using standard methods, such as SGD or Bayesian inference methods such as MCMC. For example, let us create a “contaminated” version of the 1d, two-class Iris dataset that we discussed in Section 4.6.7.2. We will add 6 examples of class 1 (Versicolor) with abnormally low sepal length. In Figure 10.10a, we show the results of fitting a standard (Bayesian) logistic regression model to this dataset. In Figure 10.10b, we show the results of fitting the above robust model. In the latter case, we see that the decision boundary is similar to the one we inferred from non-contaminated data, as shown in Figure 4.20b. We also see that the posterior uncertainty about the decision boundary’s location is smaller than when using a non-robust model. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Multinomial logistic regression",
        "subsubsection": "Handling large numbers of classes"
    },
    {
        "content": "Another common approach is to resample the data to make it more balanced, before (or during) training. In particular, suppose we sample a datapoint from class $c$ with probability \nIf we set $q = 1$ , we recover standard instance-balanced sampling, where $p _ { c } propto N _ { c }$ ; the common classes will be sampled more than rare classes. If we set $q = 0$ , we recover class-balanced sampling, where $p _ { c } = 1 / C$ ; this can be thought of as first sampling a class uniformly at random, and then sampling an instance of this class. Finally, we can consider other options, such as $q = 0 . 5$ , which is known as square-root sampling [Mah+18]. \nYet another method that is simple and can easily handle the long tail is to use the nearest class mean classifier. This has the form \nwhere $begin{array} { r } { pmb { mu } _ { c } = frac { 1 } { N _ { c } } sum _ { n : y _ { n } = c } pmb { x } _ { n } } end{array}$ is the mean of the features belonging to class $c$ . This induces a softmax posterior, as we discussed in Section 9.2.5. We can get much better results if we first use a neural network (see Part III) to learn good features, by training a DNN classifier with cross-entropy loss on the original unbalanced data. We then replace $_ { x }$ with $phi ( { pmb x } )$ in Equation (10.98). This simple approach can give very good performance on long-tailed distributions [Kan+20]. \n10.4 Robust logistic regression * \nSometimes we have outliers in our data, which are often due to labeling errors, also called label noise. To prevent the model from being adversely affected by such contamination, we will use robust logistic regression. In this section, we discuss some approaches to this problem. (Note that the methods can also be applied to DNNs. For a more thorough survey of label noise, and how it impacts deep learning, see [Han+20].) \n10.4.1 Mixture model for the likelihood \nOne of the simplest ways to define a robust logistic regression model is to modify the likelihood so that it predicts that each output label $y$ is generated uniformly at random with probability $pi$ , and otherwise is generated using the usual conditional model. In the binary case, this becomes \nThis approach, of using a mixture model for the observation model to make it robust, can be applied to many different models (e.g., DNNs). \nWe can fit this model using standard methods, such as SGD or Bayesian inference methods such as MCMC. For example, let us create a “contaminated” version of the 1d, two-class Iris dataset that we discussed in Section 4.6.7.2. We will add 6 examples of class 1 (Versicolor) with abnormally low sepal length. In Figure 10.10a, we show the results of fitting a standard (Bayesian) logistic regression model to this dataset. In Figure 10.10b, we show the results of fitting the above robust model. In the latter case, we see that the decision boundary is similar to the one we inferred from non-contaminated data, as shown in Figure 4.20b. We also see that the posterior uncertainty about the decision boundary’s location is smaller than when using a non-robust model. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n10.4.2 Bi-tempered loss \nIn this section, we present an approach to robust logistic regression proposed in [Ami+19]. \nThe first observation is that examples that are far from the decision boundary, but mislabeled, will have undue adverse affect on the model if the loss function is convex [LS10]. This can be overcome by replacing the usual cross entropy loss with a “tempered” version, that uses a temperature parameter $0 leq t _ { 1 } < 1$ to ensure the loss from outliers is bounded. In particular, consider the standard relative entropy loss function: \nwhere $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ is the true label distribution (often one-hot) and $hat { pmb y }$ is the predicted distribution. We define the tempered cross entropy loss as follows: \nwhich simplifes to the following when the true distribution $pmb { y }$ is one-hot, with all its mass on class $c$ : \nHere $log _ { t }$ is tempered version of the log function: \nThis is mononotically increasing and concave, and reduces to the standard (natural) logarithm when $t = 1$ . (Similarly, tempered cross entropy reduces to standard cross entropy when $t = 1$ .) However, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the tempered log function is bounded from below by $- 1 / ( 1 - t )$ for $0 leq t < 1$ , and hence the cross entropy loss is bounded from above (see Figure 10.11).",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Robust logistic regression *",
        "subsubsection": "Mixture model for the likelihood"
    },
    {
        "content": "10.4.2 Bi-tempered loss \nIn this section, we present an approach to robust logistic regression proposed in [Ami+19]. \nThe first observation is that examples that are far from the decision boundary, but mislabeled, will have undue adverse affect on the model if the loss function is convex [LS10]. This can be overcome by replacing the usual cross entropy loss with a “tempered” version, that uses a temperature parameter $0 leq t _ { 1 } < 1$ to ensure the loss from outliers is bounded. In particular, consider the standard relative entropy loss function: \nwhere $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ is the true label distribution (often one-hot) and $hat { pmb y }$ is the predicted distribution. We define the tempered cross entropy loss as follows: \nwhich simplifes to the following when the true distribution $pmb { y }$ is one-hot, with all its mass on class $c$ : \nHere $log _ { t }$ is tempered version of the log function: \nThis is mononotically increasing and concave, and reduces to the standard (natural) logarithm when $t = 1$ . (Similarly, tempered cross entropy reduces to standard cross entropy when $t = 1$ .) However, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the tempered log function is bounded from below by $- 1 / ( 1 - t )$ for $0 leq t < 1$ , and hence the cross entropy loss is bounded from above (see Figure 10.11). \n\nThe second observation is that examples that are near the decision boundary, but mislabeled, need to use a transfer function (that maps from activations $mathbb { R } ^ { C }$ to probabilities $[ 0 , 1 ] ^ { C }$ ) that has heavier tails than the softmax, which is based on the exponential, so it can “look past” the neighborhood of the immediate examples. In particular, the standard softmax is defined by \nwhere $textbf { em a }$ is the logits vector. We can make a heavy tailed version by using the tempered softmax, which uses a temperature parameter $t _ { 2 } > 1 > t _ { 1 }$ as follows: \nwhere \nis a tempered version of the exponential function. (This reduces to the standard exponental function as $t to 1$ .) In Figure $1 0 . 1 1 ( mathrm { r i g h t } )$ , we show that the tempered softmax (in the two-class case) has heavier tails, as desired. \nAll that remains is a way to compute $lambda _ { t _ { 2 } } ( pmb { a } )$ . This must satisfy the following fixed point equation \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n1 Input: logits   \n2 $mu : = operatorname* { m a x } ( pmb { a } )$   \n3 $pmb { a } : = pmb { a } - mu$   \n4 while $textbf { em a }$   \n5   \n6   \n7 Return − logt Z(1a) + µ \nWe can solve for $lambda$ using binary search, or by using the iterative procedure in Algorithm 3. \nCombining the tempered softmax with the tempered cross entropy results in a method called bi-tempered logistic regression. In Figure 10.12, we show an example of this in 2d. The top row is standard logistic regression, the bottom row is bi-tempered. The first column is clean data. The second column has label noise near the boundary. The robust version uses $t _ { 1 } = 1$ (standard cross entropy) but $t _ { 2 } = 4$ (tempered softmax with heavy tails). The third column has label noise far from the boundary. The robust version uses $t _ { 1 } = 0 . 2$ (tempered cross entropy with bounded loss) but $t _ { 2 } = 1$ (standard softmax). The fourth column has both kinds of noise; in this case, the robust version uses $t _ { 1 } = 0 . 2$ and $t _ { 2 } = 4$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n10.5 Bayesian logistic regression * \nSo far we have focused on point estimates of the parameters, either the MLE or the MAP estimate. However, in some cases we want to compute the posterior, $p ( pmb { w } | mathcal { D } )$ , in order to capture our uncertainty. This can be particularly useful in settings where we have little data, and where choosing the wrong decision may be costly. \nUnlike with linear regression, it is not possible to compute the posterior exactly for a logistic regression model. A wide range of approximate algorithms can be used,. In this section, we use one of the simplest, known as the Laplace approximation (Section 4.6.8.2). See the sequel to this book, [Mur23] for more advanced approximations. \n10.5.1 Laplace approximation \nAs we discuss in Section 4.6.8.2, the Laplace approximation approximates the posterior using a Gaussian. The mean of the Gaussian is equal to the MAP estimate $hat { pmb w }$ , and the covariance is equal to the inverse Hessian $mathbf { H }$ computed at the MAP estimate, i.e., $p ( pmb { w } | mathcal { D } ) approx mathcal { N } ( pmb { w } | hat { pmb { w } } , mathbf { H } ^ { - 1 } )$ , We can find the mode using a standard optimization method (see Section 10.2.7), and then we can use the results from Section 10.2.3.4 to compute the Hessian at the mode. \nAs an example, consider the data illustrated in Figure 10.13(a). There are many parameter settings that correspond to lines that perfectly separate the training data; we show 4 example lines. The likelihood surface is shown in Figure 10.13(b). The diagonal line connects the origin to the point in the grid with maximum likelihood, ${ hat { w } } _ { mathrm { m l e } } = ( 8 . 0 , 3 . 4 )$ . (The unconstrained MLE has $| | pmb { w } | | = infty$ , as we discussed in Section 10.2.7; this point can be obtained by following the diagonal line infinitely far to the right.) \nFor each decision boundary in Figure 10.13(a), we plot the corresponding parameter vector in Figure 10.13(b). These parameters values are $pmb { w } _ { 1 } = ( 3 , 1 )$ , $pmb { w } _ { 2 } = ( 4 , 2 )$ , $pmb { w } _ { 3 } = ( 5 , 3 )$ , and $pmb { w } _ { 4 } = ( 7 , 3 )$ . These points all approximately satisfy ${ pmb w } _ { i } ( 1 ) / { pmb w } _ { i } ( 2 ) approx hat { pmb w } _ { mathrm { m l e } } ( 1 ) / hat { pmb w } _ { mathrm { m l e } } ( 2 )$ , and hence are close to the orientation of the maximum likelihood decision boundary. The points are ordered by increasing weight norm (3.16, 4.47, 5.83, and 7.62). \nTo ensure a unique solution, we use a (spherical) Gaussian prior centered at the origin, $mathcal { N } ( boldsymbol { mathbf { mathit { w } } } | mathbf { mathbf { 0 } } , sigma ^ { mathrm { 2 } } mathbf { I } )$ . The value of $sigma ^ { 2 }$ controls the strength of the prior. If we set $sigma ^ { 2 } = 0$ , we force the MAP estimate to be $mathbf { nabla } { boldsymbol { w } } = mathbf { 0 }$ ; this will result in maximally uncertain predictions, since all points $_ { x }$ will produce a predictive distribution of the form $p ( y = 1 | pmb { x } ) = 0 . 5$ . If we set $sigma ^ { 2 } = infty$ , the prior becomes uninformative, and MAP estimate becomes the MLE, resulting in minimally uncertain predictions. (In particular, all positively labeled points will have $p ( y = 1 | pmb { x } ) = 1 . 0$ , and all negatively labeled points will have $p ( y = 1 | pmb { x } ) = 0 . 0$ , since the data is separable.) As a compromise (to make a nice illustration), we pick the value $sigma ^ { 2 } = 1 0 0$ . \nMultiplying this prior by the likelihood results in the unnormalized posterior shown in Figure 10.13(c). The MAP estimate is shown by the blue dot. The Laplace approximation to this posterior is shown in Figure 10.13(d). We see that it gets the mode correct (by construction), but the shape of the posterior is somewhat distorted. (The southwest-northeast orientation captures uncertainty about the magnitude of $mathbf { boldsymbol { w } }$ , and the southeast-northwest orientation captures uncertainty about the orientation of the decision boundary.) \nIn Figure 10.14, we show contours of the posterior predictive distribution. Figure 10.14(a) shows the plugin approximation using the MAP estimate. We see that there is no uncertainty about the decision \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license boundary, even though we are generating probabilistic predictions over the labels. Figure 10.14(b) shows what happens when we plug in samples from the Gaussian posterior. Now we see that there is considerable uncertainty about the orientation of the “best” decision boundary. Figure 10.14(c) shows the average of these samples. By averaging over multiple predictions, we see that the uncertainty in the decision boundary “splays out” as we move further from the training data. Figure 10.14(d) shows that the probit approximation gives very similar results to the Monte Carlo approximation.",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Robust logistic regression *",
        "subsubsection": "Bi-tempered loss"
    },
    {
        "content": "10.5 Bayesian logistic regression * \nSo far we have focused on point estimates of the parameters, either the MLE or the MAP estimate. However, in some cases we want to compute the posterior, $p ( pmb { w } | mathcal { D } )$ , in order to capture our uncertainty. This can be particularly useful in settings where we have little data, and where choosing the wrong decision may be costly. \nUnlike with linear regression, it is not possible to compute the posterior exactly for a logistic regression model. A wide range of approximate algorithms can be used,. In this section, we use one of the simplest, known as the Laplace approximation (Section 4.6.8.2). See the sequel to this book, [Mur23] for more advanced approximations. \n10.5.1 Laplace approximation \nAs we discuss in Section 4.6.8.2, the Laplace approximation approximates the posterior using a Gaussian. The mean of the Gaussian is equal to the MAP estimate $hat { pmb w }$ , and the covariance is equal to the inverse Hessian $mathbf { H }$ computed at the MAP estimate, i.e., $p ( pmb { w } | mathcal { D } ) approx mathcal { N } ( pmb { w } | hat { pmb { w } } , mathbf { H } ^ { - 1 } )$ , We can find the mode using a standard optimization method (see Section 10.2.7), and then we can use the results from Section 10.2.3.4 to compute the Hessian at the mode. \nAs an example, consider the data illustrated in Figure 10.13(a). There are many parameter settings that correspond to lines that perfectly separate the training data; we show 4 example lines. The likelihood surface is shown in Figure 10.13(b). The diagonal line connects the origin to the point in the grid with maximum likelihood, ${ hat { w } } _ { mathrm { m l e } } = ( 8 . 0 , 3 . 4 )$ . (The unconstrained MLE has $| | pmb { w } | | = infty$ , as we discussed in Section 10.2.7; this point can be obtained by following the diagonal line infinitely far to the right.) \nFor each decision boundary in Figure 10.13(a), we plot the corresponding parameter vector in Figure 10.13(b). These parameters values are $pmb { w } _ { 1 } = ( 3 , 1 )$ , $pmb { w } _ { 2 } = ( 4 , 2 )$ , $pmb { w } _ { 3 } = ( 5 , 3 )$ , and $pmb { w } _ { 4 } = ( 7 , 3 )$ . These points all approximately satisfy ${ pmb w } _ { i } ( 1 ) / { pmb w } _ { i } ( 2 ) approx hat { pmb w } _ { mathrm { m l e } } ( 1 ) / hat { pmb w } _ { mathrm { m l e } } ( 2 )$ , and hence are close to the orientation of the maximum likelihood decision boundary. The points are ordered by increasing weight norm (3.16, 4.47, 5.83, and 7.62). \nTo ensure a unique solution, we use a (spherical) Gaussian prior centered at the origin, $mathcal { N } ( boldsymbol { mathbf { mathit { w } } } | mathbf { mathbf { 0 } } , sigma ^ { mathrm { 2 } } mathbf { I } )$ . The value of $sigma ^ { 2 }$ controls the strength of the prior. If we set $sigma ^ { 2 } = 0$ , we force the MAP estimate to be $mathbf { nabla } { boldsymbol { w } } = mathbf { 0 }$ ; this will result in maximally uncertain predictions, since all points $_ { x }$ will produce a predictive distribution of the form $p ( y = 1 | pmb { x } ) = 0 . 5$ . If we set $sigma ^ { 2 } = infty$ , the prior becomes uninformative, and MAP estimate becomes the MLE, resulting in minimally uncertain predictions. (In particular, all positively labeled points will have $p ( y = 1 | pmb { x } ) = 1 . 0$ , and all negatively labeled points will have $p ( y = 1 | pmb { x } ) = 0 . 0$ , since the data is separable.) As a compromise (to make a nice illustration), we pick the value $sigma ^ { 2 } = 1 0 0$ . \nMultiplying this prior by the likelihood results in the unnormalized posterior shown in Figure 10.13(c). The MAP estimate is shown by the blue dot. The Laplace approximation to this posterior is shown in Figure 10.13(d). We see that it gets the mode correct (by construction), but the shape of the posterior is somewhat distorted. (The southwest-northeast orientation captures uncertainty about the magnitude of $mathbf { boldsymbol { w } }$ , and the southeast-northwest orientation captures uncertainty about the orientation of the decision boundary.) \nIn Figure 10.14, we show contours of the posterior predictive distribution. Figure 10.14(a) shows the plugin approximation using the MAP estimate. We see that there is no uncertainty about the decision \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license boundary, even though we are generating probabilistic predictions over the labels. Figure 10.14(b) shows what happens when we plug in samples from the Gaussian posterior. Now we see that there is considerable uncertainty about the orientation of the “best” decision boundary. Figure 10.14(c) shows the average of these samples. By averaging over multiple predictions, we see that the uncertainty in the decision boundary “splays out” as we move further from the training data. Figure 10.14(d) shows that the probit approximation gives very similar results to the Monte Carlo approximation. \n\n10.5.2 Approximating the posterior predictive \nThe posterior $p ( pmb { w } | mathcal { D } )$ tells us everything we know about the parameters of the model given the data. However, in machine learning applications, the main task of interest is usually to predict an output $y$ \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Bayesian logistic regression *",
        "subsubsection": "Laplace approximation"
    },
    {
        "content": "10.5.2 Approximating the posterior predictive \nThe posterior $p ( pmb { w } | mathcal { D } )$ tells us everything we know about the parameters of the model given the data. However, in machine learning applications, the main task of interest is usually to predict an output $y$ \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \ngiven an input $_ { x }$ , rather than to try to understand the parameters of our model. Thus we need to compute the posterior predictive distribution \nAs we discussed in Section 4.6.7.1, a simple approach to this is to first compute a point estimate $hat { pmb w }$ of the parameters, such as the MLE or MAP estimate, and then to ignore all posterior uncertainty, by assuming $p ( pmb { w } | mathcal { D } ) = delta ( pmb { w } - hat { pmb { w } } )$ . In this case, the above integral reduces to the following plugin approximation: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHowever, if we want to compute uncertainty in our predictions, we should use a non-degenerate posterior. It is common to use a Gaussian posterior, as we will see. But we still need to approximate the integral in Equation (10.108). We discuss some approaches to this below. \n10.5.2.1 Monte Carlo approximation \nThe simplest approach is to use a Monte Carlo approximation to the integral. This means we draw $S$ samples from the posterior, $mathbf { boldsymbol { w } } _ { s } sim p ( mathbf { boldsymbol { w } } | mathcal { D } )$ . and then compute \n10.5.2.2 Probit approximation \nAlthough the Monte Carlo approximation is simple, it can be slow, since we need to draw $S$ samples at test time for each input $_ { x }$ . Fortunately, if $p ( pmb { w } | mathcal { D } ) = mathcal { N } ( pmb { w } | pmb { mu } , pmb { Sigma } )$ , there is a simple yet accurate deterministic approximation, first suggested in [SL90]. To explain this approximation, we follow the presentation of [Bis06, p219]. The key observation is that the sigmoid function $sigma ( a )$ is similar in shape to the Gaussian cdf (see Section 2.6.1) $Phi ( a )$ . In particular we have $sigma ( a ) approx Phi ( lambda a )$ , where $lambda ^ { 2 } = pi / 8$ ensures the two functions have the same slope at the origin. This is useful since we can integrate a Gaussian cdf wrt a Gaussian pdf exactly: \nwhere we have defined \nThus if we define $a = pmb { x } ^ { top } pmb { w }$ , we have \nwhere we used Equation (2.165) in the last line. Since $Phi$ is the inverse of the probit function, we will call this the probit approximation. \nUsing Equation (10.113) results in predictions that are less extreme (in terms of their confidence) than the plug-in estimate. To see this, note that $0 < kappa ( v ) < 1$ and hence $kappa ( v ) m < m$ , so $sigma ( kappa ( v ) m )$ is closer to 0.5 than $sigma ( m )$ is. However, the decision boundary itself will not be affected. To see this, note that the decision boundary is the set of points $_ { x }$ for which $p ( y = 1 | x , D ) = 0 . 5$ . This implies $kappa ( v ) m = 0$ , which implies $m = overline { { w } } ^ { 1 } x = 0$ ; but this is the same as the decision boundary from the plugin estimate. Thus “being Bayesian” doesn’t change the misclassification rate (in this case), but it does change the confidence estimates of the model, which can be important, as we illustrate in Section 10.5.1. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn the multiclass case we can use the generalized probit approximation [Gib97]: \nwhere $kappa$ is defined in Equation (10.112). Unlike the binary case, taking into account posterior covariance gives different predictions than the plug-in approach (see Exercise 3.10.3 of [RW06]). \nFor further approximations of Gaussian integrals combined with sigmoid and softmax functions, see [Dau17]. \n10.6 Exercises \nExercise 10.1 [Gradient and Hessian of log-likelihood for multinomial logistic regression] \na. Let $mu _ { i k } = mathrm { s o f t m a x } ( pmb { eta } _ { i } ) _ { k }$ , where $pmb { eta } _ { i } = pmb { w } ^ { T } pmb { x } _ { i }$ . Show that the Jacobian of the softmax is \nwhere $delta _ { k j } = I ( k = j )$ . \nb. Hence show that the gradient of the NLL is given by \nHint: use the chain rule and the fact that $textstyle sum _ { c } y _ { i c } = 1$ . \nc. Show that the block submatrix of the Hessian for classes $c$ and $c ^ { prime }$ is given by \nHence show that the Hessian of the NLL is positive definite. \nExercise 10.2 [Regularizing separate terms in 2d logistic regression $^ *$ ] \n(Source: Jaakkola.) \na. Consider the data in Figure 10.15a, where we fit the model $p ( y = 1 | pmb { x } , pmb { w } ) = sigma big ( w _ { 0 } + w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } big )$ . Suppose we fit the model by maximum likelihood, i.e., we minimize \nwhere $ell ( { boldsymbol { mathbf { mathit { w } } } } , { mathcal { D } } _ { mathrm { t r a i n } } )$ is the log likelihood on the training set. Sketch a possible decision boundary corresponding to $hat { textbf { textit { w } } }$ . (Copy the figure first (a rough sketch is enough), and then superimpose your answer on your copy, since you will need multiple versions of this figure). Is your answer (decision boundary) unique? How many classification errors does your method make on the training set? \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license b. Now suppose we regularize only the $w _ { 0 }$ parameter, i.e., we minimize",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Bayesian logistic regression *",
        "subsubsection": "Approximating the posterior predictive"
    },
    {
        "content": "In the multiclass case we can use the generalized probit approximation [Gib97]: \nwhere $kappa$ is defined in Equation (10.112). Unlike the binary case, taking into account posterior covariance gives different predictions than the plug-in approach (see Exercise 3.10.3 of [RW06]). \nFor further approximations of Gaussian integrals combined with sigmoid and softmax functions, see [Dau17]. \n10.6 Exercises \nExercise 10.1 [Gradient and Hessian of log-likelihood for multinomial logistic regression] \na. Let $mu _ { i k } = mathrm { s o f t m a x } ( pmb { eta } _ { i } ) _ { k }$ , where $pmb { eta } _ { i } = pmb { w } ^ { T } pmb { x } _ { i }$ . Show that the Jacobian of the softmax is \nwhere $delta _ { k j } = I ( k = j )$ . \nb. Hence show that the gradient of the NLL is given by \nHint: use the chain rule and the fact that $textstyle sum _ { c } y _ { i c } = 1$ . \nc. Show that the block submatrix of the Hessian for classes $c$ and $c ^ { prime }$ is given by \nHence show that the Hessian of the NLL is positive definite. \nExercise 10.2 [Regularizing separate terms in 2d logistic regression $^ *$ ] \n(Source: Jaakkola.) \na. Consider the data in Figure 10.15a, where we fit the model $p ( y = 1 | pmb { x } , pmb { w } ) = sigma big ( w _ { 0 } + w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } big )$ . Suppose we fit the model by maximum likelihood, i.e., we minimize \nwhere $ell ( { boldsymbol { mathbf { mathit { w } } } } , { mathcal { D } } _ { mathrm { t r a i n } } )$ is the log likelihood on the training set. Sketch a possible decision boundary corresponding to $hat { textbf { textit { w } } }$ . (Copy the figure first (a rough sketch is enough), and then superimpose your answer on your copy, since you will need multiple versions of this figure). Is your answer (decision boundary) unique? How many classification errors does your method make on the training set? \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license b. Now suppose we regularize only the $w _ { 0 }$ parameter, i.e., we minimize \n\nSuppose $lambda$ is a very large number, so we regularize $w _ { 0 }$ all the way to 0, but all other parameters are unregularized. Sketch a possible decision boundary. How many classification errors does your method make on the training set? Hint: consider the behavior of simple linear regression, $w _ { 0 } + w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 }$ when $x _ { 1 } = x _ { 2 } = 0$ . \nc. Now suppose we heavily regularize only the $w _ { 1 }$ parameter, i.e., we minimize \nSketch a possible decision boundary. How many classification errors does your method make on the training set? \nd. Now suppose we heavily regularize only the $w _ { 2 }$ parameter. Sketch a possible decision boundary. How many classification errors does your method make on the training set? \nExercise 10.3 [Logistic regression vs LDA/QDA *] (Source: Jaakkola.) Suppose we train the following binary classifiers via maximum likelihood. \n\na. GaussI: A generative classifier, where the class-conditional densities are Gaussian, with both covariance matrices set to I (identity matrix), i.e., $p ( pmb { x } | y = c ) = mathcal { N } ( pmb { x } | pmb { mu } _ { c } , mathbf { I } )$ . We assume $p ( y )$ is uniform.   \nb. GaussX: as for GaussI, but the covariance matrices are unconstrained, i.e., $p ( pmb { x } | y = c ) = mathcal { N } ( pmb { x } | pmb { mu } _ { c } , pmb { Sigma } _ { c } )$ .   \nc. LinLog: A logistic regression model with linear features.   \nd. QuadLog: A logistic regression model, using linear and quadratic features (i.e., polynomial basis function expansion of degree 2). \nAfter training we compute the performance of each model $M$ on the training set as follows: \n(Note that this is the conditional log-likelihood $p ( boldsymbol { y } | boldsymbol { x } , hat { pmb { theta } } )$ and not the joint log-likelihood $p ( y , x | hat { pmb theta } )$ .) We now want to compare the performance of each model. We will write $L ( M ) leq L ( M ^ { prime } )$ if model $M$ must have lower \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 (or equal) log likelihood (on the training set) than $M ^ { prime }$ , for any training set (in other words, $M$ is worse than $M ^ { prime }$ , at least as far as training set logprob is concerned). For each of the following model pairs, state whether $L ( M ) leq L ( M ^ { prime } ) .$ , $L ( M ) ge L ( M ^ { prime } )$ , or whether no such statement can be made (i.e., $M$ might sometimes be better than $M ^ { prime }$ and sometimes worse); also, for each question, briefly (1-2 sentences) explain why. \n\na. GaussI, LinLog.   \nb. GaussX, QuadLog.   \nc. LinLog, QuadLog.   \nd. GaussI, QuadLog.   \ne. Now suppose we measure performance in terms of the average misclassification rate on the training set: \nIs it true in general that $L ( M ) > L ( M ^ { prime } )$ implies that $R ( M ) < R ( M ^ { prime } )$ ? Explain why or why not. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n11 Linear Regression \n11.1 Introduction \nIn this chapter, we discuss linear regression, which is a very widely used method for predicting a real-valued output (also called the dependent variable or target) $y in mathbb { R }$ , given a vector of real-valued inputs (also called independent variables, explanatory variables, or covariates) $pmb { x } in mathbb { R } ^ { D }$ . The key property of the model is that the expected value of the output is assumed to be a linear function of the input, $mathbb { E } left[ y | x right] = w ^ { top } x$ , which makes the model easy to interpret, and easy to fit to data. We discuss nonlinear extensions later in this book. \n11.2 Least squares linear regression \nIn this section, we discuss the most common form of linear regression model. \n11.2.1 Terminology \nThe term “linear regression” usually refers to a model of the following form: \nwhere $pmb theta = ( w _ { 0 } , pmb w , sigma ^ { 2 } )$ are all the parameters of the model. (In statistics, the parameters $w _ { 0 }$ and $mathbf { boldsymbol { w } }$ are usually denoted by $beta _ { 0 }$ and $beta$ .) \nThe vector of parameters ${ pmb w } _ { 1 : D }$ are known as the weights or regression coefficients. Each coefficient $w _ { d }$ specifies the change in the output we expect if we change the corresponding input feature $x _ { d }$ by one unit. For example, suppose $x _ { 1 }$ is the age of a person, $x _ { 2 }$ is their education level (represented as a continuous number), and $y$ is their income. Thus $w _ { 1 }$ corresponds to the increase in income we expect as someone becomes one year older (and hence get more experience), and $w _ { 2 }$ corresponds to the increase in income we expect as someone’s education level increases by one level. The term $w _ { 0 }$ is the offset or bias term, and specifies the output value if all the inputs are 0. This captures the unconditional mean of the response, $boldsymbol { w } _ { 0 } = mathbb { E } left[ boldsymbol { y } right]$ , and acts as a baseline. We will usually assume that $_ { x }$ is written as $[ 1 , x _ { 1 } , ldots , x _ { D } ]$ , so we can absorb the offset term $w _ { 0 }$ into the weight vector $mathbf { boldsymbol { w } }$ . \nIf the input is one-dimensional (so $D = 1$ ), the model has the form $f ( pmb { x } ; pmb { w } ) = a pmb { x } + b$ , where $b = w _ { 0 }$ is the intercept, and $a = w _ { 1 }$ is the slope. This is called simple linear regression. If the input is multi-dimensional, $pmb { x } in mathbb { R } ^ { D }$ where $D > 1$ , the method is called multiple linear regression. If the output is also multi-dimensional, $pmb { y } in mathbb { R } ^ { J }$ , where $J > 1$ , it is called multivariate linear regression,",
        "chapter": "II Linear Models",
        "section": "Logistic Regression",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "11 Linear Regression \n11.1 Introduction \nIn this chapter, we discuss linear regression, which is a very widely used method for predicting a real-valued output (also called the dependent variable or target) $y in mathbb { R }$ , given a vector of real-valued inputs (also called independent variables, explanatory variables, or covariates) $pmb { x } in mathbb { R } ^ { D }$ . The key property of the model is that the expected value of the output is assumed to be a linear function of the input, $mathbb { E } left[ y | x right] = w ^ { top } x$ , which makes the model easy to interpret, and easy to fit to data. We discuss nonlinear extensions later in this book. \n11.2 Least squares linear regression \nIn this section, we discuss the most common form of linear regression model. \n11.2.1 Terminology \nThe term “linear regression” usually refers to a model of the following form: \nwhere $pmb theta = ( w _ { 0 } , pmb w , sigma ^ { 2 } )$ are all the parameters of the model. (In statistics, the parameters $w _ { 0 }$ and $mathbf { boldsymbol { w } }$ are usually denoted by $beta _ { 0 }$ and $beta$ .) \nThe vector of parameters ${ pmb w } _ { 1 : D }$ are known as the weights or regression coefficients. Each coefficient $w _ { d }$ specifies the change in the output we expect if we change the corresponding input feature $x _ { d }$ by one unit. For example, suppose $x _ { 1 }$ is the age of a person, $x _ { 2 }$ is their education level (represented as a continuous number), and $y$ is their income. Thus $w _ { 1 }$ corresponds to the increase in income we expect as someone becomes one year older (and hence get more experience), and $w _ { 2 }$ corresponds to the increase in income we expect as someone’s education level increases by one level. The term $w _ { 0 }$ is the offset or bias term, and specifies the output value if all the inputs are 0. This captures the unconditional mean of the response, $boldsymbol { w } _ { 0 } = mathbb { E } left[ boldsymbol { y } right]$ , and acts as a baseline. We will usually assume that $_ { x }$ is written as $[ 1 , x _ { 1 } , ldots , x _ { D } ]$ , so we can absorb the offset term $w _ { 0 }$ into the weight vector $mathbf { boldsymbol { w } }$ . \nIf the input is one-dimensional (so $D = 1$ ), the model has the form $f ( pmb { x } ; pmb { w } ) = a pmb { x } + b$ , where $b = w _ { 0 }$ is the intercept, and $a = w _ { 1 }$ is the slope. This is called simple linear regression. If the input is multi-dimensional, $pmb { x } in mathbb { R } ^ { D }$ where $D > 1$ , the method is called multiple linear regression. If the output is also multi-dimensional, $pmb { y } in mathbb { R } ^ { J }$ , where $J > 1$ , it is called multivariate linear regression,",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "11 Linear Regression \n11.1 Introduction \nIn this chapter, we discuss linear regression, which is a very widely used method for predicting a real-valued output (also called the dependent variable or target) $y in mathbb { R }$ , given a vector of real-valued inputs (also called independent variables, explanatory variables, or covariates) $pmb { x } in mathbb { R } ^ { D }$ . The key property of the model is that the expected value of the output is assumed to be a linear function of the input, $mathbb { E } left[ y | x right] = w ^ { top } x$ , which makes the model easy to interpret, and easy to fit to data. We discuss nonlinear extensions later in this book. \n11.2 Least squares linear regression \nIn this section, we discuss the most common form of linear regression model. \n11.2.1 Terminology \nThe term “linear regression” usually refers to a model of the following form: \nwhere $pmb theta = ( w _ { 0 } , pmb w , sigma ^ { 2 } )$ are all the parameters of the model. (In statistics, the parameters $w _ { 0 }$ and $mathbf { boldsymbol { w } }$ are usually denoted by $beta _ { 0 }$ and $beta$ .) \nThe vector of parameters ${ pmb w } _ { 1 : D }$ are known as the weights or regression coefficients. Each coefficient $w _ { d }$ specifies the change in the output we expect if we change the corresponding input feature $x _ { d }$ by one unit. For example, suppose $x _ { 1 }$ is the age of a person, $x _ { 2 }$ is their education level (represented as a continuous number), and $y$ is their income. Thus $w _ { 1 }$ corresponds to the increase in income we expect as someone becomes one year older (and hence get more experience), and $w _ { 2 }$ corresponds to the increase in income we expect as someone’s education level increases by one level. The term $w _ { 0 }$ is the offset or bias term, and specifies the output value if all the inputs are 0. This captures the unconditional mean of the response, $boldsymbol { w } _ { 0 } = mathbb { E } left[ boldsymbol { y } right]$ , and acts as a baseline. We will usually assume that $_ { x }$ is written as $[ 1 , x _ { 1 } , ldots , x _ { D } ]$ , so we can absorb the offset term $w _ { 0 }$ into the weight vector $mathbf { boldsymbol { w } }$ . \nIf the input is one-dimensional (so $D = 1$ ), the model has the form $f ( pmb { x } ; pmb { w } ) = a pmb { x } + b$ , where $b = w _ { 0 }$ is the intercept, and $a = w _ { 1 }$ is the slope. This is called simple linear regression. If the input is multi-dimensional, $pmb { x } in mathbb { R } ^ { D }$ where $D > 1$ , the method is called multiple linear regression. If the output is also multi-dimensional, $pmb { y } in mathbb { R } ^ { J }$ , where $J > 1$ , it is called multivariate linear regression, \n\nSee Exercise 11.1 for a simple numerical example. \nIn general, a straight line will not provide a good fit to most data sets. However, we can always apply a nonlinear transformation to the input features, by replacing $_ { x }$ with $phi ( { pmb x } )$ to get \nAs long as the parameters of the feature extractor $phi$ are fixed, the model remains linear in the parameters, even if it is not linear in the inputs. (We discuss ways to learn the feature extractor, and the final linear mapping, in Part III.) \nAs a simple example of a nonlinear transformation, consider the case of polynomial regression, which we introduced in Section 1.2.2.2. If the input is 1d, and we use a polynomial expansion of degree $d$ , we get $phi ( x ) = [ 1 , x , x ^ { 2 } , . . . , x ^ { d } ]$ . See Figure 11.1 for an example. (See also Section 11.5 where we discuss splines.) \n11.2.2 Least squares estimation \nTo fit a linear regression model to data, we will minimize the negative log likelihood on the training set. The objective function is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Least squares linear regression",
        "subsubsection": "Terminology"
    },
    {
        "content": "See Exercise 11.1 for a simple numerical example. \nIn general, a straight line will not provide a good fit to most data sets. However, we can always apply a nonlinear transformation to the input features, by replacing $_ { x }$ with $phi ( { pmb x } )$ to get \nAs long as the parameters of the feature extractor $phi$ are fixed, the model remains linear in the parameters, even if it is not linear in the inputs. (We discuss ways to learn the feature extractor, and the final linear mapping, in Part III.) \nAs a simple example of a nonlinear transformation, consider the case of polynomial regression, which we introduced in Section 1.2.2.2. If the input is 1d, and we use a polynomial expansion of degree $d$ , we get $phi ( x ) = [ 1 , x , x ^ { 2 } , . . . , x ^ { d } ]$ . See Figure 11.1 for an example. (See also Section 11.5 where we discuss splines.) \n11.2.2 Least squares estimation \nTo fit a linear regression model to data, we will minimize the negative log likelihood on the training set. The objective function is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere we have defined the predicted response ${ hat { y } } _ { n } triangleq w ^ { prime } x _ { n }$ . The MLE is the point where $nabla _ { pmb { w } , sigma } mathrm { N L L } ( pmb { w } , sigma ^ { 2 } ) =$ 0. We can first optimize wrt $mathbf { boldsymbol { w } }$ , and then solve for the optimal $sigma$ . \nIn this section, we just focus on estimating the weights $mathbf { boldsymbol { w } }$ . In this case, the NLL is equal (up to irrelevant constants) to the residual sum of squares, which is given by \nWe discuss how to optimize this below. \n11.2.2.1 Ordinary least squares \nFrom Equation (7.264) we can show that the gradient is given by \nSetting the gradient to zero and solving gives \nThese are known as the normal equations, since, at the optimal solution, $mathbf { nabla } y mathrm { ~ - ~ } mathbf { X } w$ is normal (orthogonal) to the range of $mathbf { X }$ , as we explain in Section 11.2.2.2. The corresponding solution $hat { textbf { textit { w } } }$ is the ordinary least squares (OLS) solution, which is given by \nThe quantity $mathbf { X } ^ { dagger } = ( mathbf { X } ^ { mathsf { T } } mathbf { X } ) ^ { - 1 } mathbf { X } ^ { mathsf { T } }$ is the (left) pseudo inverse of the (non-square) matrix $mathbf { X }$ (see Section 7.5.3 for more details). \nWe can check that the solution is unique by showing that the Hessian is positive definite. In this case, the Hessian is given by \nIf $mathbf { X }$ is full rank (so the columns of $mathbf { X }$ are linearly independent), then $mathbf { H }$ is positive definite, since for any $v > 0$ , we have \nHence in the full rank case, the least squares objective has a unique global minimum. See Figure 11.2 for an illustration. \n11.2.2.2 Geometric interpretation of least squares \nThe normal equations have an elegant geometrical interpretation, deriving from Section 7.7, as we now explain. We will assume $N > D$ , so there are more observations than unknowns. (This is known \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nas an overdetermined system.) We seek a vector $hat { pmb y } in mathbb { R } ^ { N }$ that lies in the linear subspace spanned by $mathbf { X }$ and is as close as possible to $pmb { y }$ , i.e., we want to find \nwhere $pmb { x } _ { : , d }$ is the $d$ ’th column of $mathbf { X }$ . Since ${ hat { pmb y } } in operatorname { s p a n } ( mathbf { X } )$ , there exists some weight vector $mathbf { boldsymbol { w } }$ such that \nTo minimize the norm of the residual, $mathbf y - hat { mathbf y }$ , we want the residual vector to be orthogonal to every column of $mathbf { X }$ . Hence \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHence our projected value of $textbf {  { y } }$ is given by \nThis corresponds to an orthogonal projection of $pmb { y }$ onto the column space of $mathbf { X }$ . For example, consider the case where we have $N = 3$ training examples, each of dimensionality $D = 2$ . The training data defines a 2d linear subspace, defined by the 2 columns of $mathbf { X }$ , each of which is a point in 3d. We project $textbf {  { y } }$ , which is also a point in 3d, onto this 2d subspace, as shown in Figure 11.3. \nThe projection matrix \nis sometimes called the hat matrix, since $hat { pmb { y } } = mathrm { P r o j } ( mathbf { X } ) pmb { y }$ . In the special case that $mathbf { X } = { boldsymbol { x } }$ is a column vector, the orthogonal projection of $_ y$ onto the line $_ { x }$ becomes \n11.2.2.3 Algorithmic issues \nRecall that the OLS solution is \nHowever, even if it is theoretically possible to compute the pseudo-inverse by inverting $mathbf { X } ^ { mathsf { I } } mathbf { X }$ , we should not do so for numerical reasons, since $mathbf { X } ^ { mathsf { I } } mathbf { X }$ may be ill conditioned or singular. \nA better (and more general) approach is to compute the pseudo-inverse using the SVD. Indeed, if you look at the source code for the function sklearn.linear_model.fit, you will see that it uses the scipy.linalg.lstsq function, which in turns calls DGELSD, which is an SVD-based solver implemented by the LAPACK library, written in Fortran.1 \nHowever, if $mathbf { X }$ is tall and skinny (i.e., $N gg D$ ), it can be quicker to use QR decomposition (Section 7.6.2). To do this, let $mathbf { X } = mathbf { Q } mathbf { R }$ , where $mathbf { Q } ^ { 1 } mathbf { Q } = mathbf { I }$ . In Section 7.7, we show that OLS is equivalent to solving the system of linear equations $mathbf { X } w = y$ in a way that minimizes $| | mathbf { X } pmb { w } - pmb { y } | | _ { 2 } ^ { 2 }$ . (If $N = D$ and $mathbf { X }$ is full rank, the equations have a unique solution, and the error will be $0$ .) Using QR decomposition, we can rewrite this system of equations as follows: \nSince $mathbf { R }$ is upper triangular, we can solve this last set of equations using backsubstitution, thus avoiding matrix inversion. See linsys_solve_demo.ipynb for a demo. \nAn alternative to the use of direct methods based on matrix decomposition (such as SVD and QR) is to use iterative solvers, such as the conjugate gradient method (which assumes $mathbf { X }$ is symmetric positive definite), and the GMRES (generalized minimal residual method), that works for general $mathbf { X }$ . (In SciPy, this is implemented by sparse.linalg.gmres.) These methods just require the ability to perform matrix-vector multiplications (i.e., an implementation of a linear operator), and thus are well-suited to problems where $mathbf { X }$ is sparse or structured. For details, see e.g., [TB97]. \n\nA final important issue is that it is usually essential to standardize the input features before fitting the model, to ensure that they are zero mean and unit variance. We can do this using Equation (10.51). \n11.2.2.4 Weighted least squares \nIn some cases, we want to associate a weight with each example. For example, in heteroskedastic regression, the variance depends on the input, so the model has the form \nThus \nwhere $pmb { Lambda } = mathrm { d i a g } ( 1 / sigma ^ { 2 } ( pmb { x } _ { n } ) )$ . This is known as weighted linear regression. One can show that the MLE is given by \nThis is known as the weighted least squares estimate. \n11.2.3 Other approaches to computing the MLE \nIn this section, we discuss other approaches for computing the MLE. \n11.2.3.1 Solving for offset and slope separately \nTypically we use a model of the form $p ( y | mathbf { x } , pmb { theta } ) = mathcal { N } ( y | w _ { 0 } + w ^ { textnormal { l } } mathbf { x } , sigma ^ { 2 } )$ , where $w _ { 0 }$ is an offset or “bias” term. We can compute $( w _ { 0 } , pmb { w } )$ at the same time by adding a column of 1s to $mathbf { X }$ , and the computing the MLE as above. Alternatively, we can solve for $mathbf { boldsymbol { w } }$ and $w _ { 0 }$ separately. (This will be useful later.) In particular, one can show that \nwhere $mathbf { X } _ { c }$ is the centered input matrix containing $pmb { x } _ { n } ^ { c } = pmb { x } _ { n } - overline { { pmb { x } } }$ along its rows, and ${ pmb y } _ { c } = { pmb y } - { pmb y }$ is the centered output vector. Thus we can first compute $hat { pmb w }$ on centered data, and then estimate $w _ { 0 }$ using $overline { { y } } - overline { { x } } ^ { top } hat { w }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Least squares linear regression",
        "subsubsection": "Least squares estimation"
    },
    {
        "content": "A final important issue is that it is usually essential to standardize the input features before fitting the model, to ensure that they are zero mean and unit variance. We can do this using Equation (10.51). \n11.2.2.4 Weighted least squares \nIn some cases, we want to associate a weight with each example. For example, in heteroskedastic regression, the variance depends on the input, so the model has the form \nThus \nwhere $pmb { Lambda } = mathrm { d i a g } ( 1 / sigma ^ { 2 } ( pmb { x } _ { n } ) )$ . This is known as weighted linear regression. One can show that the MLE is given by \nThis is known as the weighted least squares estimate. \n11.2.3 Other approaches to computing the MLE \nIn this section, we discuss other approaches for computing the MLE. \n11.2.3.1 Solving for offset and slope separately \nTypically we use a model of the form $p ( y | mathbf { x } , pmb { theta } ) = mathcal { N } ( y | w _ { 0 } + w ^ { textnormal { l } } mathbf { x } , sigma ^ { 2 } )$ , where $w _ { 0 }$ is an offset or “bias” term. We can compute $( w _ { 0 } , pmb { w } )$ at the same time by adding a column of 1s to $mathbf { X }$ , and the computing the MLE as above. Alternatively, we can solve for $mathbf { boldsymbol { w } }$ and $w _ { 0 }$ separately. (This will be useful later.) In particular, one can show that \nwhere $mathbf { X } _ { c }$ is the centered input matrix containing $pmb { x } _ { n } ^ { c } = pmb { x } _ { n } - overline { { pmb { x } } }$ along its rows, and ${ pmb y } _ { c } = { pmb y } - { pmb y }$ is the centered output vector. Thus we can first compute $hat { pmb w }$ on centered data, and then estimate $w _ { 0 }$ using $overline { { y } } - overline { { x } } ^ { top } hat { w }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n11.2.3.2 Simple linear regression (1d inputs) \nIn the case of 1d (scalar) inputs, the results from Section 11.2.3.1 reduce to the following simple form, which may be familiar from basic statistics classes: \nwhere $C _ { x y } = mathrm { C o v } left[ X , Y right]$ and $C _ { x x } = operatorname { C o v } left[ X , X right] = mathbb { V } left[ X right]$ . We will use this result below. \n11.2.3.3 Partial regression \nFrom Equation (11.27), we can compute the regression coefficient of $Y$ on $X$ as follows: \nThis is the slope of the linear prediction for $Y$ given $X$ . \nNow consider the case where we have 2 inputs, so $begin{array} { r } { Y = w _ { 0 } + w _ { 1 } X _ { 1 } + w _ { 2 } X _ { 2 } + epsilon } end{array}$ , where $mathbb { E } left[ boldsymbol { epsilon } right] = 0$ . One can show that the optimal regression coefficient for $w _ { 1 }$ is given by $R _ { Y X _ { 1 } cdot X _ { 2 } }$ , which is the partial regression coefficient of $Y$ on $X _ { 1 }$ , keeping $X _ { 2 }$ constant: \nNote that this quantity is invariant to the specific value of $X _ { 2 }$ we condition on. \nWe can derive $w _ { 2 }$ in a similar manner. Indeed, we can extend this to multiple input variables. In each case, we find the optimal coefficients are equal to the partial regression coefficients. This means that we can interpret the $j$ ’th coefficient $hat { w } _ { j }$ as the change in output $y$ we expect per unit change in input $x _ { j }$ , keeping all the other inputs constant. \n11.2.3.4 Recursively computing the MLE \nOLS is a batch method for computing the MLE. In some applications, the data arrives in a continual stream, so we want to compute the estimate online, or recursively, as we discussed in Section 4.4.2. In this section, we show how to do this for the case of simple (1d) linear regession. \nRecall from Section 11.2.3.2 that the batch MLE for simple linear regression is given by \nwhere $C _ { x y } = mathrm { C o v } left[ X , Y right]$ and $C _ { x x } = operatorname { C o v } left[ X , X right] = mathbb { V } left[ X right]$ . \nWe now discuss how to compute these results in a recursive fashion. To do this, let us define the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nfollowing sufficient statistics: \nWe can update the means online using \nTo update the covariance terms, let us first rewrite Cx(ny as follows: \nHence \nand so \nWe can derive the update for $C _ { x x } ^ { ( n + 1 ) }$ in a similar manner. \nSee Figure 11.4 for a simple illustration of these equations in action for a 1d regression model. \nTo extend the above analysis to $D$ -dimensional inputs, the easiest approach is to use SGD. The esulting algorithm is called the least mean squares algorithm; see Section 8.4.2 for details. \n11.2.3.5 Deriving the MLE from a generative perspective \nLinear regression is a discriminative model of the form $p ( boldsymbol { y } | boldsymbol { x } )$ . However, we can also use generative models for regression, by analogy to how we use generative models for classification in Chapter 9, The goal is to compute the conditional expectation \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nSuppose we fit $p ( { pmb x } , { boldsymbol y } )$ using an MVN. The MLEs for the parameters of the joint distribution are the empiricial means and covariances (see Section 4.2.6 for a proof of this result): \nHence from Equation (3.28), we have \nWe can rewrite this as $mathbb { E } left[ y | pmb { x } right] = w _ { 0 } + pmb { w } ^ { 1 } pmb { x }$ by defining \nThis matches the MLEs for the discriminative model as we showed in Section 11.2.3.1. Thus we see that fitting the joint model, and then conditioning it, yields the same result as fitting the conditional model. However, this is only true for Gaussian models (see Section 9.4 for further discussion of this point). \n11.2.3.6 Deriving the MLE for $sigma ^ { 2 }$ \nAfter estimating $hat { pmb w } _ { mathrm { m l e } }$ using one of the above methods, we can estimate the noise variance. It is easy to show that the MLE is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis is just the MSE of the residuals, which is an intuitive result. \n11.2.4 Measuring goodness of fit \nIn this section, we discuss some simple ways to assess how well a regression model fits the data (which is known as goodness of fit). \n11.2.4.1 Residual plots \nFor 1d inputs, we can check the reasonableness of the model by plotting the residuals, $boldsymbol { r } _ { n } = boldsymbol { y } _ { n } - boldsymbol { hat { y } } _ { n }$ , vs the input $x _ { n }$ . This is called a residual plot. The model assumes that the residuals have a ${ mathcal { N } } ( 0 , sigma ^ { 2 } )$ distribution, so the residual plot should be a cloud of points more or less equally above and below the horizontal line at 0, without any obvious trends. \nAs an example, in Figure 11.5(a), we plot the residuals for the linear model in Figure 1.7a(a). We see that there is some curved structure to the residuals, indicating a lack of fit. In Figure 11.5(b), we plot the residuals for the quadratic model in Figure 1.7a(b). We see a much better fit. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Least squares linear regression",
        "subsubsection": "Other approaches to computing the MLE"
    },
    {
        "content": "This is just the MSE of the residuals, which is an intuitive result. \n11.2.4 Measuring goodness of fit \nIn this section, we discuss some simple ways to assess how well a regression model fits the data (which is known as goodness of fit). \n11.2.4.1 Residual plots \nFor 1d inputs, we can check the reasonableness of the model by plotting the residuals, $boldsymbol { r } _ { n } = boldsymbol { y } _ { n } - boldsymbol { hat { y } } _ { n }$ , vs the input $x _ { n }$ . This is called a residual plot. The model assumes that the residuals have a ${ mathcal { N } } ( 0 , sigma ^ { 2 } )$ distribution, so the residual plot should be a cloud of points more or less equally above and below the horizontal line at 0, without any obvious trends. \nAs an example, in Figure 11.5(a), we plot the residuals for the linear model in Figure 1.7a(a). We see that there is some curved structure to the residuals, indicating a lack of fit. In Figure 11.5(b), we plot the residuals for the quadratic model in Figure 1.7a(b). We see a much better fit. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo extend this approach to multi-dimensional inputs, we can plot predictions ${ hat { y } } _ { n }$ vs the true output $y _ { n }$ , rather than plotting vs $x _ { n }$ . A good model will have points that lie on a diagonal line. See Figure 11.6 for some examples. \n11.2.4.2 Prediction accuracy and $R ^ { 2 }$ \nWe can assess the fit quantitatively by computing the RSS (residual sum of squares) on the dataset: $begin{array} { r } { operatorname { R S S } ( pmb { w } ) = sum _ { n = 1 } ^ { N } ( y _ { n } - pmb { w } ^ { top } pmb { x } _ { n } ) ^ { 2 } } end{array}$ . A model with lower RSS fits the data better. Another measure that is used is root mean squared error or RMSE: \nA more interpretable measure can be computed using the coefficient of determination, denoted by $R ^ { 2 }$ : \nwhere $begin{array} { r } { overline { { y } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } y _ { n } } end{array}$ is the empirical mean of the response, $begin{array} { r } { mathrm { R S S } = sum _ { n = 1 } ^ { N } ( y _ { n } - hat { y } _ { n } ) ^ { 2 } } end{array}$ is the residual sum of squares, and $begin{array} { r } { mathrm { T S S } = sum _ { n = 1 } ^ { N } ( y _ { n } - overline { { y } } ) ^ { 2 } } end{array}$ is the total sum of squares. Thus we see that $R ^ { 2 }$ measures the variance in the predictions relative to a simple constant prediction of ${ hat { y } } _ { n } = { overline { { y } } }$ . One can show that $0 leq R ^ { 2 } leq 1$ , where larger values imply a greater reduction in variance (better fit). This is illustrated in Figure 11.6. \n11.3 Ridge regression \nMaximum likelihood estimation can result in overfitting, as we discussed in Section 1.2.2.2. A simple solution to this is to use MAP estimation with a zero-mean Gaussian prior on the weights, $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , lambda ^ { - 1 } mathbf { I } )$ , as we discused in Section 4.5.3. This is called ridge regression. \nIn more detail, we compute the MAP estimate as follows: \nwhere λ ≜ σ2 is proportional to the strength of the prior, and \nis the $ell _ { 2 }$ norm of the vector $mathbf { boldsymbol { w } }$ . Thus we are penalizing weights that become too large in magnitude.   \nIn general, this technique is called $ell _ { 2 }$ regularization or weight decay, and is very widely used.   \nSee Figure 4.5 for an illustration. \nNote that we do not penalize the offset term $w _ { 0 }$ , since that only affects the global mean of the output, and does not contribute to overfitting. See Exercise 11.2. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Least squares linear regression",
        "subsubsection": "Measuring goodness of fit"
    },
    {
        "content": "11.3.1 Computing the MAP estimate \nIn this section, we discuss algorithms for computing the MAP estimate. The MAP estimate corresponds to minimizing the following penalized objective: \nwhere $lambda = sigma ^ { 2 } / tau ^ { 2 }$ is the strength of the regularizer. The derivative is given by \nand hence \n11.3.1.1 Solving using QR \nNaively computing the primal estimate ${ pmb w } = ( { pmb X } ^ { 1 } { pmb X } + lambda { bf I } ) ^ { - 1 } { pmb X } ^ { 1 } { pmb y }$ using matrix inversion is a bad idea, since it can be slow and numerically unstable. In this section, we describe a way to convert the problem to a standard least squares problem, to which we can apply QR decomposition, as discussed in Section 11.2.2.3. \nWe assume the prior has the form $p ( pmb { w } ) = mathcal { N } ( mathbf { 0 } , pmb { Lambda } ^ { - 1 } )$ , where $pmb { Lambda }$ is the precision matrix. In the case of ridge regression, $mathbf { Delta } Lambda = ( 1 / tau ^ { 2 } ) mathbf { I }$ . We can emulate this prior by adding “virtual data” to the training set to get \nwhere $mathbf { Lambda } mathbf { Lambda } = sqrt { mathbf { A } } sqrt { mathbf { A } } ^ { top }$ is a Cholesky decomposition of $pmb { Lambda }$ . We see that $tilde { mathbf { X } }$ is $( N _ { D } + D ) times D$ , where the extra rows represent pseudo-data from the prior. \nWe now show that the RSS on this expanded data is equivalent to penalized RSS on the original data: \nHence the MAP estimate is given by \nwhich can be solved using standard OLS methods. In particular, we can compute the QR decomposition of $ddot { bf X }$ , and then proceed as in Section 11.2.2.3. This takes $O ( ( N + D ) D ^ { 2 } )$ time. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n11.3.1.2 Solving using SVD \nIn this section, we assume $D > N$ , which is the usual case when using ridge regression. In this case, it is faster to use SVD than QR. To see how this works, let $mathbf { X } = mathbf { U S V } ^ { mid }$ be the SVD of $mathbf { X }$ , where $mathbf { V } ^ { mathsf { I } } mathbf { V } = mathbf { I } _ { N }$ , $mathbf { U U } ^ { mathsf { I } } = mathbf { U } ^ { mathsf { I } } mathbf { U } = mathbf { I } _ { N }$ , and $mathbf { s }$ is a diagonal $N times N$ matrix. Now let $mathbf { R } = mathbf { U } mathbf { S }$ be an $N _ { mathcal { D } } times N _ { mathcal { D } }$ matrix. One can show (see Exercise 18.4 of [HTF09]) that \nIn other words, we can replace the $D$ -dimensional vectors ${ bf { x } } _ { i }$ with the $N _ { mathcal { D } }$ -dimensional vectors $boldsymbol { r } _ { i }$ and perform our penalized fit as before. The overall time is now $O ( D N _ { D } { } ^ { 2 } )$ operations, which is less than $O ( D ^ { 3 } )$ if $D > N _ { mathcal { D } }$ . \n11.3.2 Connection between ridge regression and PCA \nIn this section, we discuss an interesting connection between ridge regression and PCA (which we describe in Section 20.1), in order to gain further insight into why ridge regression works well. Our discussion is based on [HTF09, p66]. \nLet $mathbf { X } = mathbf { U S V } ^ { mathsf { T } }$ be the SVD of $mathbf { X }$ , where $mathbf { V } ^ { mathsf { T } } mathbf { V } = mathbf { I } _ { N }$ , $mathbf { U U } ^ { parallel } = mathbf { U } ^ { parallel } mathbf { U } = mathbf { I } _ { N }$ , and $mathbf { s }$ is a diagonal $N times N$ matrix. Using Equation (11.65) we can see that the ridge predictions on the training set are given by \nwhere \nand σj are the singular values of $mathbf { X }$ . Hence \nIn contrast, the least squares prediction is \nIf $sigma _ { j } ^ { 2 }$ is small compared to $lambda$ , then direction $boldsymbol { mathbf { mathit { u } } } _ { j }$ will not have much effect on the prediction. In view of this, we define the effective number of degrees of freedom of the model as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Ridge regression",
        "subsubsection": "Computing the MAP estimate"
    },
    {
        "content": "11.3.1.2 Solving using SVD \nIn this section, we assume $D > N$ , which is the usual case when using ridge regression. In this case, it is faster to use SVD than QR. To see how this works, let $mathbf { X } = mathbf { U S V } ^ { mid }$ be the SVD of $mathbf { X }$ , where $mathbf { V } ^ { mathsf { I } } mathbf { V } = mathbf { I } _ { N }$ , $mathbf { U U } ^ { mathsf { I } } = mathbf { U } ^ { mathsf { I } } mathbf { U } = mathbf { I } _ { N }$ , and $mathbf { s }$ is a diagonal $N times N$ matrix. Now let $mathbf { R } = mathbf { U } mathbf { S }$ be an $N _ { mathcal { D } } times N _ { mathcal { D } }$ matrix. One can show (see Exercise 18.4 of [HTF09]) that \nIn other words, we can replace the $D$ -dimensional vectors ${ bf { x } } _ { i }$ with the $N _ { mathcal { D } }$ -dimensional vectors $boldsymbol { r } _ { i }$ and perform our penalized fit as before. The overall time is now $O ( D N _ { D } { } ^ { 2 } )$ operations, which is less than $O ( D ^ { 3 } )$ if $D > N _ { mathcal { D } }$ . \n11.3.2 Connection between ridge regression and PCA \nIn this section, we discuss an interesting connection between ridge regression and PCA (which we describe in Section 20.1), in order to gain further insight into why ridge regression works well. Our discussion is based on [HTF09, p66]. \nLet $mathbf { X } = mathbf { U S V } ^ { mathsf { T } }$ be the SVD of $mathbf { X }$ , where $mathbf { V } ^ { mathsf { T } } mathbf { V } = mathbf { I } _ { N }$ , $mathbf { U U } ^ { parallel } = mathbf { U } ^ { parallel } mathbf { U } = mathbf { I } _ { N }$ , and $mathbf { s }$ is a diagonal $N times N$ matrix. Using Equation (11.65) we can see that the ridge predictions on the training set are given by \nwhere \nand σj are the singular values of $mathbf { X }$ . Hence \nIn contrast, the least squares prediction is \nIf $sigma _ { j } ^ { 2 }$ is small compared to $lambda$ , then direction $boldsymbol { mathbf { mathit { u } } } _ { j }$ will not have much effect on the prediction. In view of this, we define the effective number of degrees of freedom of the model as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWhen $lambda = 0$ , $operatorname* { d o f } ( lambda ) = D$ , and as $lambda to infty$ , $operatorname* { d o f } ( lambda )  0$ . \nLet us try to understand why this behavior is desirable. In Section 11.7, we show that Cov $[ { pmb w } | { pmb { mathcal D } } ] propto$ $( mathbf { X } ^ { mathsf { I } } mathbf { X } ) ^ { - 1 }$ , if we use a uniform prior for $mathbf { boldsymbol { w } }$ . Thus the directions in which we are most uncertain about $mathbf { boldsymbol { w } }$ are determined by the eigenvectors of $( mathbf { X } ^ { mathsf { I } } mathbf { X } ) ^ { - 1 }$ with the largest eigenvalues, as shown in Figure 7.6; these correspond to the eigenvectors of $mathbf { X } ^ { mathsf { I } } mathbf { X }$ with the smallest eigenvalues. In Section 7.5.2, we show that the squared singular values $sigma _ { j } ^ { 2 }$ are equal to the eigenvalues of $mathbf { X } ^ { mathsf { I } } mathbf { X }$ . Hence small singular values $sigma _ { j }$ correspond to directions with high posterior variance. It is these directions which ridge shrinks the most. \nThis process is illustrated in Figure 11.7. The horizontal $w _ { 1 }$ parameter is not-well determined by the data (has high posterior variance), but the vertical $w _ { 2 }$ parameter is well-determined. Hence $w _ { mathrm { m a p } } ( 2 )$ is close to $w _ { mathrm { m l e } } ( 2 )$ , but $w _ { mathrm { m a p } } ( 1 )$ is shifted strongly towards the prior mean, which is 0. In this way, ill-determined parameters are reduced in size towards 0. This is called shrinkage. \nThere is a related, but different, technique called principal components regression, which is a supervised version of PCA, which we explain in Section 20.1. The idea is this: first use PCA to reduce the dimensionality to $K$ dimensions, and then use these low dimensional features as input to regression. However, this technique does not work as well as ridge regression in terms of predictive accuracy [HTF01, p70]. The reason is that in PC regression, only the first $K$ (derived) dimensions are retained, and the remaining $D - K$ dimensions are entirely ignored. By contrast, ridge regression uses a “soft” weighting of all the dimensions. \n11.3.3 Choosing the strength of the regularizer \nTo find the optimal value of $lambda$ , we can try a finite number of distinct values, and use cross validation to estimate their expected loss, as discussed in Section 4.5.5.2. See Figure 4.5d for an example. \nThis approach can be quite expensive if we have many values to choose from. Fortunately, we can often warm start the optimization procedure, using the value of $hat { pmb w } ( lambda _ { k } )$ as an initializer for $hat { pmb { w } } ( lambda _ { k + 1 } )$ , where $lambda _ { k + 1 } < lambda _ { k }$ ; in other words, we start with a highly constrained model (strong regularizer), and then gradually relax the constraints (decrease the amount of regularization). The set of parameters $hat { pmb { w } } _ { k }$ that we sweep out in this way is known as the regularization path. See Figure 11.10(a) for an example. \nWe can also use an empirical Bayes approach to choose $lambda$ . In particular, we choose the hyperpa\nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 rameter by computing $hat { lambda } = operatorname { a r g m a x } _ { lambda } log p ( mathcal { D } | lambda )$ , where $p ( mathcal { D } | lambda )$ is the marginal likelihood or evidence. Figure 4.7b shows that this gives essentially the same result as the CV estimate. However, the Bayesian approach has several advantages: computing $p ( mathcal { D } | lambda )$ can be done by fitting a single model, whereas CV has to fit the same model $K$ times; and $p ( mathcal { D } | lambda )$ is a smooth function of $lambda$ , so we can use gradient-based optimization instead of discrete search.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Ridge regression",
        "subsubsection": "Connection between ridge regression and PCA"
    },
    {
        "content": "When $lambda = 0$ , $operatorname* { d o f } ( lambda ) = D$ , and as $lambda to infty$ , $operatorname* { d o f } ( lambda )  0$ . \nLet us try to understand why this behavior is desirable. In Section 11.7, we show that Cov $[ { pmb w } | { pmb { mathcal D } } ] propto$ $( mathbf { X } ^ { mathsf { I } } mathbf { X } ) ^ { - 1 }$ , if we use a uniform prior for $mathbf { boldsymbol { w } }$ . Thus the directions in which we are most uncertain about $mathbf { boldsymbol { w } }$ are determined by the eigenvectors of $( mathbf { X } ^ { mathsf { I } } mathbf { X } ) ^ { - 1 }$ with the largest eigenvalues, as shown in Figure 7.6; these correspond to the eigenvectors of $mathbf { X } ^ { mathsf { I } } mathbf { X }$ with the smallest eigenvalues. In Section 7.5.2, we show that the squared singular values $sigma _ { j } ^ { 2 }$ are equal to the eigenvalues of $mathbf { X } ^ { mathsf { I } } mathbf { X }$ . Hence small singular values $sigma _ { j }$ correspond to directions with high posterior variance. It is these directions which ridge shrinks the most. \nThis process is illustrated in Figure 11.7. The horizontal $w _ { 1 }$ parameter is not-well determined by the data (has high posterior variance), but the vertical $w _ { 2 }$ parameter is well-determined. Hence $w _ { mathrm { m a p } } ( 2 )$ is close to $w _ { mathrm { m l e } } ( 2 )$ , but $w _ { mathrm { m a p } } ( 1 )$ is shifted strongly towards the prior mean, which is 0. In this way, ill-determined parameters are reduced in size towards 0. This is called shrinkage. \nThere is a related, but different, technique called principal components regression, which is a supervised version of PCA, which we explain in Section 20.1. The idea is this: first use PCA to reduce the dimensionality to $K$ dimensions, and then use these low dimensional features as input to regression. However, this technique does not work as well as ridge regression in terms of predictive accuracy [HTF01, p70]. The reason is that in PC regression, only the first $K$ (derived) dimensions are retained, and the remaining $D - K$ dimensions are entirely ignored. By contrast, ridge regression uses a “soft” weighting of all the dimensions. \n11.3.3 Choosing the strength of the regularizer \nTo find the optimal value of $lambda$ , we can try a finite number of distinct values, and use cross validation to estimate their expected loss, as discussed in Section 4.5.5.2. See Figure 4.5d for an example. \nThis approach can be quite expensive if we have many values to choose from. Fortunately, we can often warm start the optimization procedure, using the value of $hat { pmb w } ( lambda _ { k } )$ as an initializer for $hat { pmb { w } } ( lambda _ { k + 1 } )$ , where $lambda _ { k + 1 } < lambda _ { k }$ ; in other words, we start with a highly constrained model (strong regularizer), and then gradually relax the constraints (decrease the amount of regularization). The set of parameters $hat { pmb { w } } _ { k }$ that we sweep out in this way is known as the regularization path. See Figure 11.10(a) for an example. \nWe can also use an empirical Bayes approach to choose $lambda$ . In particular, we choose the hyperpa\nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 rameter by computing $hat { lambda } = operatorname { a r g m a x } _ { lambda } log p ( mathcal { D } | lambda )$ , where $p ( mathcal { D } | lambda )$ is the marginal likelihood or evidence. Figure 4.7b shows that this gives essentially the same result as the CV estimate. However, the Bayesian approach has several advantages: computing $p ( mathcal { D } | lambda )$ can be done by fitting a single model, whereas CV has to fit the same model $K$ times; and $p ( mathcal { D } | lambda )$ is a smooth function of $lambda$ , so we can use gradient-based optimization instead of discrete search. \n\n11.4 Lasso regression \nIn Section 11.3, we assumed a Gaussian prior for the regression coefficients when fitting linear regression models. This is often a good choice, since it encourages the parameters to be small, and hence prevents overfitting. However, sometimes we want the parameters to not just be small, but to be exactly zero, i.e., we want $hat { pmb { w } }$ to be sparse, so that we minimize the L0-norm: \nThis is useful because it can be used to perform feature selection. To see this, note that the prediction has the form $begin{array} { r } { f ( pmb { x } ; pmb { w } ) = sum _ { d = 1 } ^ { D } w _ { d } x _ { d } } end{array}$ , so if any $w _ { d } = 0$ , we ignore the corresponding feature $x _ { d }$ . (The same idea can be applied to nonlinear models, such as DNNs, by encouraging the first layer weights to be sparse.) \n11.4.1 MAP estimation with a Laplace prior ( $ell _ { 1 }$ regularization) \nThere are many ways to compute such sparse estimates (see e.g., [Bha+19]). In this section we focus on MAP estimation using the Laplace distribution (which we discussed in Section 11.6.1) as the prior: \nwhere $lambda$ is the sparsity parameter, and \nHere $mu$ is a location parameter and $b > 0$ is a scale parameter. Figure 2.15 shows that Laplace $( w | 0 , b )$ puts more density on 0 than $mathcal { N } ( w | 0 , sigma ^ { 2 } )$ , even when we fix the variance to be the same. \nTo perform MAP estimation of a linear regression model with this prior, we just have to minimize the following objective: \nwhere $begin{array} { r } { lvert lvert pmb { w } rvert rvert _ { 1 } triangleq sum _ { d = 1 } ^ { D } lvert w _ { d } rvert } end{array}$ is the $ell _ { 1 }$ norm of $mathbf { boldsymbol { w } }$ . This method is called lasso, which stands for “least absolute shrinkage and selection operator” [Tib96]. (We explain the reason for this name below.) More generally, MAP estimation with a Laplace prior is called $ell _ { 1 }$ -regularization. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Ridge regression",
        "subsubsection": "Choosing the strength of the regularizer"
    },
    {
        "content": "11.4 Lasso regression \nIn Section 11.3, we assumed a Gaussian prior for the regression coefficients when fitting linear regression models. This is often a good choice, since it encourages the parameters to be small, and hence prevents overfitting. However, sometimes we want the parameters to not just be small, but to be exactly zero, i.e., we want $hat { pmb { w } }$ to be sparse, so that we minimize the L0-norm: \nThis is useful because it can be used to perform feature selection. To see this, note that the prediction has the form $begin{array} { r } { f ( pmb { x } ; pmb { w } ) = sum _ { d = 1 } ^ { D } w _ { d } x _ { d } } end{array}$ , so if any $w _ { d } = 0$ , we ignore the corresponding feature $x _ { d }$ . (The same idea can be applied to nonlinear models, such as DNNs, by encouraging the first layer weights to be sparse.) \n11.4.1 MAP estimation with a Laplace prior ( $ell _ { 1 }$ regularization) \nThere are many ways to compute such sparse estimates (see e.g., [Bha+19]). In this section we focus on MAP estimation using the Laplace distribution (which we discussed in Section 11.6.1) as the prior: \nwhere $lambda$ is the sparsity parameter, and \nHere $mu$ is a location parameter and $b > 0$ is a scale parameter. Figure 2.15 shows that Laplace $( w | 0 , b )$ puts more density on 0 than $mathcal { N } ( w | 0 , sigma ^ { 2 } )$ , even when we fix the variance to be the same. \nTo perform MAP estimation of a linear regression model with this prior, we just have to minimize the following objective: \nwhere $begin{array} { r } { lvert lvert pmb { w } rvert rvert _ { 1 } triangleq sum _ { d = 1 } ^ { D } lvert w _ { d } rvert } end{array}$ is the $ell _ { 1 }$ norm of $mathbf { boldsymbol { w } }$ . This method is called lasso, which stands for “least absolute shrinkage and selection operator” [Tib96]. (We explain the reason for this name below.) More generally, MAP estimation with a Laplace prior is called $ell _ { 1 }$ -regularization. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nNote also that we could use other norms for the weight vector. In general, the $q$ -norm is defined as follows: \nFor $q < 1$ , we can get even sparser solutions. In the limit where $q = 0$ , we get the $ell _ { 0 }$ -norm: \nHowever, one can show that for any $q < 1$ , the problem becomes non-convex (see e.g., [HTW15]).   \nThus $ell _ { 1 }$ -norm is the tightest convex relaxation of the $ell _ { 0 }$ -norm. \n11.4.2 Why does $ell _ { 1 }$ regularization yield sparse solutions? \nWe now explain why $ell _ { 1 }$ regularization results in sparse solutions, whereas $ell _ { 2 }$ regularization does not.   \nWe focus on the case of linear regression, although similar arguments hold for other models. \nThe lasso objective is the following non-smooth objective (see Section 8.1.4 for a discussion of smoothness): \nThis is the Lagrangian for the following quadratic program (see Section 8.5.4): \nwhere $B$ is an upper bound on the $ell _ { 1 }$ -norm of the weights: a small (tight) bound $B$ corresponds to a large penalty $lambda$ , and vice versa. \nSimilarly, we can write the ridge regression objective min $mathbf { boldsymbol { w } }$ $mathrm { N L L } ( pmb { w } ) + lambda | | pmb { w } | | _ { 2 } ^ { 2 }$ in bound constrained form: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "MAP estimation with a Laplace prior (1 regularization)"
    },
    {
        "content": "Note also that we could use other norms for the weight vector. In general, the $q$ -norm is defined as follows: \nFor $q < 1$ , we can get even sparser solutions. In the limit where $q = 0$ , we get the $ell _ { 0 }$ -norm: \nHowever, one can show that for any $q < 1$ , the problem becomes non-convex (see e.g., [HTW15]).   \nThus $ell _ { 1 }$ -norm is the tightest convex relaxation of the $ell _ { 0 }$ -norm. \n11.4.2 Why does $ell _ { 1 }$ regularization yield sparse solutions? \nWe now explain why $ell _ { 1 }$ regularization results in sparse solutions, whereas $ell _ { 2 }$ regularization does not.   \nWe focus on the case of linear regression, although similar arguments hold for other models. \nThe lasso objective is the following non-smooth objective (see Section 8.1.4 for a discussion of smoothness): \nThis is the Lagrangian for the following quadratic program (see Section 8.5.4): \nwhere $B$ is an upper bound on the $ell _ { 1 }$ -norm of the weights: a small (tight) bound $B$ corresponds to a large penalty $lambda$ , and vice versa. \nSimilarly, we can write the ridge regression objective min $mathbf { boldsymbol { w } }$ $mathrm { N L L } ( pmb { w } ) + lambda | | pmb { w } | | _ { 2 } ^ { 2 }$ in bound constrained form: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn Figure 11.8, we plot the contours of the NLL objective function, as well as the contours of the $ell _ { 2 }$ and $ell _ { 1 }$ constraint surfaces. From the theory of constrained optimization (Section 8.5) we know that the optimal solution occurs at the point where the lowest level set of the objective function intersects the constraint surface (assuming the constraint is active). It should be geometrically clear that as we relax the constraint $B$ , we “grow” the $ell _ { 1 }$ “ball” until it meets the objective; the corners of the ball are more likely to intersect the ellipse than one of the sides, especially in high dimensions, because the corners “stick out” more. The corners correspond to sparse solutions, which lie on the coordinate axes. By contrast, when we grow the $ell _ { 2 }$ ball, it can intersect the objective at any point; there are no “corners”, so there is no preference for sparsity. \n11.4.3 Hard vs soft thresholding \nThe lasso objective has the form $mathcal { L } ( pmb { w } ) = mathrm { N L L } ( pmb { w } ) + lambda | | pmb { w } | | _ { 1 }$ . One can show (Exercise 11.3) that the gradient for the smooth NLL part is given by \nwhere ${ pmb w } _ { - d }$ is $mathbf { boldsymbol { w } }$ without component $d$ , and similarly ${ boldsymbol { mathbf { mathit { x } } } } _ { n , - d }$ is feature vector ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ without component $d$ . We see that $c _ { d }$ is proportional to the correlation between $d { mathrm { ~ } }$ ’th column of features, $pmb { x } _ { : , d }$ , and the residual error obtained by predicting using all the other features, $pmb { r } _ { - d } = pmb { y } - pmb { X }$ :, ${ } _ { , - d } pmb { w } _ { - d }$ . Hence the magnitude of $c _ { d }$ is an indication of how relevant feature $d$ is for predicting $pmb { y }$ , relative to the other features and the current parameters. Setting the gradient to 0 gives the optimal update for $w _ { d }$ , keeping all other weights fixed: \nThe corresponding new prediction for $pmb { r } _ { - d }$ becomes $hat { pmb { r } } _ { - d } = w _ { d } pmb { x } _ { : , d }$ , which is the orthogonal projection of the residual onto the column vector $pmb { x } _ { : , d }$ , consistent with Equation (11.15). \nNow we add in the $ell _ { 1 }$ term. Unfortunately, the $| | pmb { w } | | _ { 1 }$ term is not differentiable whenever $w _ { d } = 0$ Fortunately, we can still compute a subgradient at this point. Using Equation (8.14) we find that \nDepending on the value of $c _ { d }$ , the solution to $partial _ { w _ { d } } mathcal { L } ( pmb { w } ) = 0$ can occur at 3 different values of $w _ { d }$ , as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Why does 1 regularization yield sparse solutions?"
    },
    {
        "content": "In Figure 11.8, we plot the contours of the NLL objective function, as well as the contours of the $ell _ { 2 }$ and $ell _ { 1 }$ constraint surfaces. From the theory of constrained optimization (Section 8.5) we know that the optimal solution occurs at the point where the lowest level set of the objective function intersects the constraint surface (assuming the constraint is active). It should be geometrically clear that as we relax the constraint $B$ , we “grow” the $ell _ { 1 }$ “ball” until it meets the objective; the corners of the ball are more likely to intersect the ellipse than one of the sides, especially in high dimensions, because the corners “stick out” more. The corners correspond to sparse solutions, which lie on the coordinate axes. By contrast, when we grow the $ell _ { 2 }$ ball, it can intersect the objective at any point; there are no “corners”, so there is no preference for sparsity. \n11.4.3 Hard vs soft thresholding \nThe lasso objective has the form $mathcal { L } ( pmb { w } ) = mathrm { N L L } ( pmb { w } ) + lambda | | pmb { w } | | _ { 1 }$ . One can show (Exercise 11.3) that the gradient for the smooth NLL part is given by \nwhere ${ pmb w } _ { - d }$ is $mathbf { boldsymbol { w } }$ without component $d$ , and similarly ${ boldsymbol { mathbf { mathit { x } } } } _ { n , - d }$ is feature vector ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ without component $d$ . We see that $c _ { d }$ is proportional to the correlation between $d { mathrm { ~ } }$ ’th column of features, $pmb { x } _ { : , d }$ , and the residual error obtained by predicting using all the other features, $pmb { r } _ { - d } = pmb { y } - pmb { X }$ :, ${ } _ { , - d } pmb { w } _ { - d }$ . Hence the magnitude of $c _ { d }$ is an indication of how relevant feature $d$ is for predicting $pmb { y }$ , relative to the other features and the current parameters. Setting the gradient to 0 gives the optimal update for $w _ { d }$ , keeping all other weights fixed: \nThe corresponding new prediction for $pmb { r } _ { - d }$ becomes $hat { pmb { r } } _ { - d } = w _ { d } pmb { x } _ { : , d }$ , which is the orthogonal projection of the residual onto the column vector $pmb { x } _ { : , d }$ , consistent with Equation (11.15). \nNow we add in the $ell _ { 1 }$ term. Unfortunately, the $| | pmb { w } | | _ { 1 }$ term is not differentiable whenever $w _ { d } = 0$ Fortunately, we can still compute a subgradient at this point. Using Equation (8.14) we find that \nDepending on the value of $c _ { d }$ , the solution to $partial _ { w _ { d } } mathcal { L } ( pmb { w } ) = 0$ can occur at 3 different values of $w _ { d }$ , as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n1. If $c _ { d } < - lambda$ , so the feature is strongly negatively correlated with the residual, then the subgradient is zero at $begin{array} { r } { hat { w } _ { d } = frac { c _ { d } + lambda } { a _ { d } } < 0 } end{array}$ .   \n2. If $c _ { d } in [ - lambda , lambda ]$ , so the feature is only weakly correlated with the residual, then the subgradient is zero at $hat { w } _ { d } = 0$ .   \n3. If $c _ { d } > lambda$ , so the feature is strongly positively correlated with the residual, then the subgradient is zero at $begin{array} { r } { hat { w } _ { d } = frac { c _ { d } - lambda } { a _ { d } } > 0 } end{array}$ . \nIn summary, we have \nWe can write this as follows: \nwhere \nand $x _ { + } = operatorname* { m a x } ( x , 0 )$ is the positive part of $x$ . This is called soft thresholding (see also Section 8.6.2). This is illustrated in Figure 11.9(a), where we plot $hat { w } _ { d }$ vs $c _ { d }$ . The dotted black line is the line ${ w _ { d } } = { c _ { d } } / { a _ { d } }$ corresponding to the least squares fit. The solid red line, which represents the regularized estimate $hat { w } _ { d }$ , shifts the dotted line down (or up) by $lambda$ , except when $- lambda leq c _ { d } leq lambda$ , in which case it sets $w _ { d } = 0$ . \nBy contrast, in Figure 11.9(b), we illustrate hard thresholding. This sets values of $w _ { d }$ to $0$ if $- lambda leq c _ { d } leq lambda$ , but it does not shrink the values of $w _ { d }$ outside of this interval. The slope of the soft \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 thresholding line does not coincide with the diagonal, which means that even large coefficients are shrunk towards zero. This is why lasso stands for “least absolute selection and shrinkage operator”. Consequently, lasso is a biased estimator (see Section 4.7.6.1). \n\nA simple solution to the biased estimate problem, known as debiasing, is to use a two-stage estimation process: we first estimate the support of the weight vector (i.e., identify which elements are non-zero) using lasso; we then re-estimate the chosen coefficients using least squares. For an example of this in action, see Figure 11.13. \n11.4.4 Regularization path \nIf $lambda = 0$ , we get the OLS solution. which will be dense. As we increase $lambda$ , the solution vector $hat { pmb { w } } ( lambda )$ will tend to get sparser. If $lambda$ is bigger than some critical value, we get $hat { mathbf { Omega } } hat { mathbf { Omega } } vec { mathbf { Omega } } hat { mathbf { Omega } } vec { mathbf { Omega } } hat { mathbf { Omega } } vec { mathbf { Omega } } hat { mathbf { Omega } } mathrm { ~ Omega ~ } hat { mathbf { Omega } } left. hat { mathbf { Omega } } right.$ . This critical value is obtained when the gradient of the NLL cancels out with the gradient of the penalty: \nAlternatively, we can work with the bound $B$ on the $ell _ { 1 }$ norm. When $B = 0$ , we get $hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } mathrm { ~ Omega ~ } hat { mathbf { Omega } } hat { mathbf { Omega } } mathrm { ~ Omega ~ } hat { mathbf { Omega } } mathrm { ~ Omega ~ }$ . As we increase $B$ , the solution becomes denser. The largest value of $B$ for which any component is zero is given by $B _ { mathrm { m a x } } = | | hat { pmb w } _ { mathrm { m l e } } | | _ { 1 }$ . \nAs we increase $lambda$ , the solution vector $hat { textbf { textit { w } } }$ gets sparser, although not necessarily monotonically. We can plot the values $hat { w } _ { d }$ vs $lambda$ (or vs the bound $B$ ) for each feature $d$ ; this is known as the regularization path. This is illustrated in Figure 11.10(b), where we apply lasso to the prostate cancer regression dataset from [HTF09]. (We treat features gleason and svi as numeric, not categorical.) On the left, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Hard vs soft thresholding"
    },
    {
        "content": "A simple solution to the biased estimate problem, known as debiasing, is to use a two-stage estimation process: we first estimate the support of the weight vector (i.e., identify which elements are non-zero) using lasso; we then re-estimate the chosen coefficients using least squares. For an example of this in action, see Figure 11.13. \n11.4.4 Regularization path \nIf $lambda = 0$ , we get the OLS solution. which will be dense. As we increase $lambda$ , the solution vector $hat { pmb { w } } ( lambda )$ will tend to get sparser. If $lambda$ is bigger than some critical value, we get $hat { mathbf { Omega } } hat { mathbf { Omega } } vec { mathbf { Omega } } hat { mathbf { Omega } } vec { mathbf { Omega } } hat { mathbf { Omega } } vec { mathbf { Omega } } hat { mathbf { Omega } } mathrm { ~ Omega ~ } hat { mathbf { Omega } } left. hat { mathbf { Omega } } right.$ . This critical value is obtained when the gradient of the NLL cancels out with the gradient of the penalty: \nAlternatively, we can work with the bound $B$ on the $ell _ { 1 }$ norm. When $B = 0$ , we get $hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } hat { mathbf { Omega } } mathrm { ~ Omega ~ } hat { mathbf { Omega } } hat { mathbf { Omega } } mathrm { ~ Omega ~ } hat { mathbf { Omega } } mathrm { ~ Omega ~ }$ . As we increase $B$ , the solution becomes denser. The largest value of $B$ for which any component is zero is given by $B _ { mathrm { m a x } } = | | hat { pmb w } _ { mathrm { m l e } } | | _ { 1 }$ . \nAs we increase $lambda$ , the solution vector $hat { textbf { textit { w } } }$ gets sparser, although not necessarily monotonically. We can plot the values $hat { w } _ { d }$ vs $lambda$ (or vs the bound $B$ ) for each feature $d$ ; this is known as the regularization path. This is illustrated in Figure 11.10(b), where we apply lasso to the prostate cancer regression dataset from [HTF09]. (We treat features gleason and svi as numeric, not categorical.) On the left, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTable 11.1: Values of the coefficients for linear regression model fit to prostate cancer dataset as we vary the strength of the $ell _ { 1 }$ regularizer. These numbers are plotted in Figure 11.10(b). \nwhen $B = 0$ , all the coefficients are zero. As we increase $B$ , the coefficients gradually “turn on”.2 The analogous result for ridge regression is shown in Figure 11.10(a). For ridge, we see all coefficients are non-zero (assuming $lambda > 0$ ), so the solution is not sparse. \nRemarkably, it can be shown that the lasso solution path is a piecewise linear function of $lambda$ [Efr+04; GL15]. That is, there are a set of critical values of $lambda$ where the active set of non-zero coefficients changes. For values of $lambda$ between these critical values, each non-zero coefficient increases or decreases in a linear fashion. This is illustrated in Figure 11.10(b). Furthermore, one can solve for these critical values analytically [Efr+04]. In Table 11.1. we display the actual coefficient values at each of these critical steps along the regularization path (the last line is the least squares solution). \nBy changing $lambda$ from $lambda _ { mathrm { m a x } }$ to 0, we can go from a solution in which all the weights are zero to a solution in which all weights are non-zero. Unfortunately, not all subset sizes are achievable using lasso. In particular, one can show that, if $D > N _ { mathcal { D } }$ , the optimal solution can have at most $N _ { mathcal { D } }$ variables in it, before reaching the complete set corresponding to the OLS solution of minimal $ell _ { 1 }$ norm. In Section 11.4.8, we will see that by using an $ell _ { 2 }$ regularizer as well as an $ell _ { 1 }$ regularizer (a method known as the elastic net), we can achieve sparse solutions which contain more variables than training cases. This lets us explore model sizes between $N _ { mathcal { D } }$ and $D$ . \n11.4.5 Comparison of least squares, lasso, ridge and subset selection \nIn this section, we compare least squares, lasso, ridge and subset selection. For simplicity, we assume all the features of $mathbf { X }$ are orthonormal, so $mathbf { X } ^ { mid } mathbf { X } = mathbf { I }$ . In this case, the NLL is given by \nso we see this factorizes into a sum of terms, one per dimension. Hence we can write down the MAP and ML estimates analytically for each $w _ { d }$ separately, as given below. \n• MLE From Equation (11.85), the OLS solution is given by \nwhere $pmb { x } _ { : d }$ is the $d ^ { prime }$ th column of $mathbf { X }$ .",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Regularization path"
    },
    {
        "content": "Table 11.1: Values of the coefficients for linear regression model fit to prostate cancer dataset as we vary the strength of the $ell _ { 1 }$ regularizer. These numbers are plotted in Figure 11.10(b). \nwhen $B = 0$ , all the coefficients are zero. As we increase $B$ , the coefficients gradually “turn on”.2 The analogous result for ridge regression is shown in Figure 11.10(a). For ridge, we see all coefficients are non-zero (assuming $lambda > 0$ ), so the solution is not sparse. \nRemarkably, it can be shown that the lasso solution path is a piecewise linear function of $lambda$ [Efr+04; GL15]. That is, there are a set of critical values of $lambda$ where the active set of non-zero coefficients changes. For values of $lambda$ between these critical values, each non-zero coefficient increases or decreases in a linear fashion. This is illustrated in Figure 11.10(b). Furthermore, one can solve for these critical values analytically [Efr+04]. In Table 11.1. we display the actual coefficient values at each of these critical steps along the regularization path (the last line is the least squares solution). \nBy changing $lambda$ from $lambda _ { mathrm { m a x } }$ to 0, we can go from a solution in which all the weights are zero to a solution in which all weights are non-zero. Unfortunately, not all subset sizes are achievable using lasso. In particular, one can show that, if $D > N _ { mathcal { D } }$ , the optimal solution can have at most $N _ { mathcal { D } }$ variables in it, before reaching the complete set corresponding to the OLS solution of minimal $ell _ { 1 }$ norm. In Section 11.4.8, we will see that by using an $ell _ { 2 }$ regularizer as well as an $ell _ { 1 }$ regularizer (a method known as the elastic net), we can achieve sparse solutions which contain more variables than training cases. This lets us explore model sizes between $N _ { mathcal { D } }$ and $D$ . \n11.4.5 Comparison of least squares, lasso, ridge and subset selection \nIn this section, we compare least squares, lasso, ridge and subset selection. For simplicity, we assume all the features of $mathbf { X }$ are orthonormal, so $mathbf { X } ^ { mid } mathbf { X } = mathbf { I }$ . In this case, the NLL is given by \nso we see this factorizes into a sum of terms, one per dimension. Hence we can write down the MAP and ML estimates analytically for each $w _ { d }$ separately, as given below. \n• MLE From Equation (11.85), the OLS solution is given by \nwhere $pmb { x } _ { : d }$ is the $d ^ { prime }$ th column of $mathbf { X }$ . \nFigure 11.11: Results of different methods on the prostate cancer data, which has 8 features and 67 training cases. Methods are: $O L S =$ ordinary least squares, Subset = best subset regression, Ridge, Lasso. Rows represent the coefficients; we see that subset regression and lasso give sparse solutions. Bottom row is the mean squared error on the test set (30 cases). Adapted from Table 3.3. of [HTF09]. Generated by prostate_comparison.ipynb. \n• Ridge One can show that the ridge estimate is given by \n• Lasso From Equation (11.88), and using the fact that $hat { w } _ { d } ^ { mathrm { m l e } } = c _ { d } / a _ { d }$ , we have \nThis corresponds to soft thresholding, shown in Figure 11.9(a). \n• Subset selection If we pick the best $K$ features using subset selection, the parameter estimate is as follows \nwhere rank refers to the location in the sorted list of weight magnitudes. This corresponds to hard thresholding, shown in Figure 11.9(b). \nWe now experimentally compare the prediction performance of these methods on the prostate cancer regression dataset from [HTF09]. (We treat features gleason and svi as numeric, not categorical.) Figure 11.11 shows the estimated coefficients at the value of $lambda$ (or $K$ ) chosen by cross-validation; we see that the subset method is the sparsest, then lasso. In terms of predictive performance, all methods are very similar, as can be seen from Figure 11.12. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n11.4.6 Variable selection consistency \nIt is common to use $ell _ { 1 }$ regularization to estimate the set of relevant variables, a process known as variable selection. A method that can recover the true set of relevant variables (i.e., the support of $boldsymbol { w } ^ { * }$ ) in the $N  infty$ limit is called model selection consistent. (This is a theoretical notion that assumes the data comes from the model.) \nLet us give an example. We first generate a sparse signal $boldsymbol { w } ^ { * }$ of size $D = 4 0 9 6$ , consisting of 160 randomly placed $pm 1$ spikes. Next we generate a random design matrix $mathbf { X }$ of size $N times D$ , where $N = 1 0 2 4$ . Finally we generate a noisy observation $pmb { y } = mathbf { X } pmb { w } ^ { ast } + pmb { epsilon }$ , where $epsilon _ { n } sim mathcal { N } ( 0 , 0 . 0 1 ^ { 2 } )$ . We then estimate $mathbf { boldsymbol { w } }$ from $pmb { y }$ and $mathbf { X }$ . The original $boldsymbol { w } ^ { * }$ is shown in the first row of Figure 11.13. The second row is the $ell _ { 1 }$ estimate $hat { pmb { w } } _ { L 1 }$ using $lambda = 0 . 1 lambda _ { operatorname* { m a x } }$ . We see that this has “spikes” in the right places, so it has correctly identified the relevant variables. However, although we see that $hat { pmb { w } } _ { L 1 }$ has correctly identified the non-zero components, but they are too small, due to shrinkage. In the third row, we show the results of using the debiasing technique discussed in Section 11.4.3. This shows that we can recover the original weight vector. By contrast, the final row shows the OLS estimate, which is dense. Furthermore, it is visually clear that there is no single threshold value we can apply to $hat { pmb w } _ { mathrm { m l e } }$ to recover the correct sparse weight vector. \nTo use lasso to perform variable selection, we have to pick $lambda$ . It is common to use cross validation to pick the optimal value on the regularization path. However, it is important to note that cross validation is picking a value of $lambda$ that results in good predictive accuracy. This is not usually the same value as the one that is likely to recover the “true” model. To see why, recall that $ell _ { 1 }$ regularization performs selection and shrinkage, that is, the chosen coefficients are brought closer to 0. In order to prevent relevant coefficients from being shrunk in this way, cross validation will tend to pick a value of $lambda$ that is not too large. Of course, this will result in a less sparse model which contains irrelevant variables (false positives). Indeed, it was proved in [MB06] that the prediction-optimal value of $lambda$ does not result in model selection consistency. However, various extensions to the basic method have been devised that are model selection consistent (see e.g., [BG11; HTW15]). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Comparison of least squares, lasso, ridge and subset selection"
    },
    {
        "content": "11.4.6 Variable selection consistency \nIt is common to use $ell _ { 1 }$ regularization to estimate the set of relevant variables, a process known as variable selection. A method that can recover the true set of relevant variables (i.e., the support of $boldsymbol { w } ^ { * }$ ) in the $N  infty$ limit is called model selection consistent. (This is a theoretical notion that assumes the data comes from the model.) \nLet us give an example. We first generate a sparse signal $boldsymbol { w } ^ { * }$ of size $D = 4 0 9 6$ , consisting of 160 randomly placed $pm 1$ spikes. Next we generate a random design matrix $mathbf { X }$ of size $N times D$ , where $N = 1 0 2 4$ . Finally we generate a noisy observation $pmb { y } = mathbf { X } pmb { w } ^ { ast } + pmb { epsilon }$ , where $epsilon _ { n } sim mathcal { N } ( 0 , 0 . 0 1 ^ { 2 } )$ . We then estimate $mathbf { boldsymbol { w } }$ from $pmb { y }$ and $mathbf { X }$ . The original $boldsymbol { w } ^ { * }$ is shown in the first row of Figure 11.13. The second row is the $ell _ { 1 }$ estimate $hat { pmb { w } } _ { L 1 }$ using $lambda = 0 . 1 lambda _ { operatorname* { m a x } }$ . We see that this has “spikes” in the right places, so it has correctly identified the relevant variables. However, although we see that $hat { pmb { w } } _ { L 1 }$ has correctly identified the non-zero components, but they are too small, due to shrinkage. In the third row, we show the results of using the debiasing technique discussed in Section 11.4.3. This shows that we can recover the original weight vector. By contrast, the final row shows the OLS estimate, which is dense. Furthermore, it is visually clear that there is no single threshold value we can apply to $hat { pmb w } _ { mathrm { m l e } }$ to recover the correct sparse weight vector. \nTo use lasso to perform variable selection, we have to pick $lambda$ . It is common to use cross validation to pick the optimal value on the regularization path. However, it is important to note that cross validation is picking a value of $lambda$ that results in good predictive accuracy. This is not usually the same value as the one that is likely to recover the “true” model. To see why, recall that $ell _ { 1 }$ regularization performs selection and shrinkage, that is, the chosen coefficients are brought closer to 0. In order to prevent relevant coefficients from being shrunk in this way, cross validation will tend to pick a value of $lambda$ that is not too large. Of course, this will result in a less sparse model which contains irrelevant variables (false positives). Indeed, it was proved in [MB06] that the prediction-optimal value of $lambda$ does not result in model selection consistency. However, various extensions to the basic method have been devised that are model selection consistent (see e.g., [BG11; HTW15]). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nOriginal $mathrm { ( D = 4 0 9 6 }$ , number of nonzeros $= 1 6 0$ ) 1 □ □□ 1 0 1000 2000 3000 4000 L1 reconstruction $mathrm { ~ K 0 } = 1 0 2 4$ , lambda = 0.0516, MSE = 0.0027) L 0 1000 2000 3000 4000 Debiased (MSE = 3.26e−005) □ 中 1 0 1000 2000 3000 4000 Minimum norm solution $( mathbf { M S E } = 0 . 0 2 9 2 )$   \n0.5 0   \n−0.5 0 1000 2000 3000 4000 \n11.4.7 Group lasso \nIn standard $ell _ { 1 }$ regularization, we assume that there is a 1:1 correspondence between parameters and variables, so that if $hat { w } _ { d } = 0$ , we interpret this to mean that variable $d$ is excluded. But in more complex models, there may be many parameters associated with a given variable. In particular, each variable $d$ may have a vector of weights ${ pmb w } _ { d }$ associated with it, so the overall weight vector has block structure, ${ pmb w } = [ { pmb w } _ { 1 } , { pmb w } _ { 2 } , dots , { pmb w } _ { D } ]$ . If we want to exclude variable $d$ , we have to force the whole subvector ${ pmb w } _ { d }$ to go to zero. This is called group sparsity. \n11.4.7.1 Applications \nHere are some examples where group sparsity is useful: \n• Linear regression with categorical inputs: If the $d$ ’th variable is categorical with $K$ possible levels, then it will be represented as a one-hot vector of length $K$ (Section 1.5.3.1), so to exclude variable $d$ , we have to set the whole vector of incoming weights to $0$ .   \n• Multinomial logistic regression: The $d$ ’th variable will be associated with $C$ different weights, one per class (Section 10.3), so to exclude variable $d$ , we have to set the whole vector of outgoing weights to 0.   \n• Neural networks: the $k$ ’th neuron will have multiple inputs, so if we want to “turn the neuron off”, we have to set all the incoming weights to zero. This allows us to use group sparsity to learn neural network structure (for details, see e.g., [GEH19]). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license • Multi-task learning: each input feature is associated with $C$ different weights, one per output task. If we want to use a feature for all of the tasks or none of the tasks, we should select weights at the group level [OTJ07].",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Variable selection consistency"
    },
    {
        "content": "Original $mathrm { ( D = 4 0 9 6 }$ , number of nonzeros $= 1 6 0$ ) 1 □ □□ 1 0 1000 2000 3000 4000 L1 reconstruction $mathrm { ~ K 0 } = 1 0 2 4$ , lambda = 0.0516, MSE = 0.0027) L 0 1000 2000 3000 4000 Debiased (MSE = 3.26e−005) □ 中 1 0 1000 2000 3000 4000 Minimum norm solution $( mathbf { M S E } = 0 . 0 2 9 2 )$   \n0.5 0   \n−0.5 0 1000 2000 3000 4000 \n11.4.7 Group lasso \nIn standard $ell _ { 1 }$ regularization, we assume that there is a 1:1 correspondence between parameters and variables, so that if $hat { w } _ { d } = 0$ , we interpret this to mean that variable $d$ is excluded. But in more complex models, there may be many parameters associated with a given variable. In particular, each variable $d$ may have a vector of weights ${ pmb w } _ { d }$ associated with it, so the overall weight vector has block structure, ${ pmb w } = [ { pmb w } _ { 1 } , { pmb w } _ { 2 } , dots , { pmb w } _ { D } ]$ . If we want to exclude variable $d$ , we have to force the whole subvector ${ pmb w } _ { d }$ to go to zero. This is called group sparsity. \n11.4.7.1 Applications \nHere are some examples where group sparsity is useful: \n• Linear regression with categorical inputs: If the $d$ ’th variable is categorical with $K$ possible levels, then it will be represented as a one-hot vector of length $K$ (Section 1.5.3.1), so to exclude variable $d$ , we have to set the whole vector of incoming weights to $0$ .   \n• Multinomial logistic regression: The $d$ ’th variable will be associated with $C$ different weights, one per class (Section 10.3), so to exclude variable $d$ , we have to set the whole vector of outgoing weights to 0.   \n• Neural networks: the $k$ ’th neuron will have multiple inputs, so if we want to “turn the neuron off”, we have to set all the incoming weights to zero. This allows us to use group sparsity to learn neural network structure (for details, see e.g., [GEH19]). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license • Multi-task learning: each input feature is associated with $C$ different weights, one per output task. If we want to use a feature for all of the tasks or none of the tasks, we should select weights at the group level [OTJ07]. \n\n11.4.7.2 Penalizing the two-norm \nTo encourage group sparsity, we partition the parameter vector into $G$ groups, $pmb { w } = [ pmb { w } _ { 1 } , dots , pmb { w } _ { G } ]$ . Then we minimize the following objective \nwhere $begin{array} { r } { vert vert pmb { w } _ { g } vert vert _ { 2 } = sqrt { sum _ { d in g } w _ { d } ^ { 2 } } } end{array}$ is the 2-norm of the group weight vector. If the NLL is least squares, this method is called group lasso [YL06; Kyu+10]. \nNote that if we had used the sum of the squared 2-norms in Equation (11.97), then the model would become equivalent to ridge regression, since \nBy using the square root, we are penalizing the radius of a ball containing the group’s weight vector: the only way for the radius to be small is if all elements are small. \nAnother way to see why the square root version enforces sparsity at the group level is to consider the gradient of the objective. Suppose there is only one group of two variables, so the penalty has the form $sqrt { w _ { 1 } ^ { 2 } + w _ { 2 } ^ { 2 } }$ . The derivative wrt $w _ { 1 }$ is \nIf $w _ { 2 }$ is close to zero, then the derivative approaches 1, and $w _ { 1 }$ is driven to zero as well, with force proportional to $lambda$ . If, however, $w _ { 2 }$ is large, the derivative approaches $0$ , and $w _ { 1 }$ is free to stay large as well. So all the coefficients in the group will have similar size. \n11.4.7.3 Penalizing the infinity norm \nA variant of this technique replaces the 2-norm with the infinity-norm [TVW05; ZRY05]: \nIt is clear that this will also result in group sparsity, since if the largest element in the group is forced to zero, all the smaller ones will be as well. \n11.4.7.4 Example \nAn illustration of these techniques is shown in Figure 11.14 and Figure 11.15. We have a true signal $mathbf { boldsymbol { w } }$ of size $D = 2 ^ { 1 2 } = 4 0 9 6$ , divided into 64 groups each of size 64. We randomly choose 8 groups \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 of $mathbf { boldsymbol { w } }$ and assign them non-zero values. In Figure 11.14 the values are drawn from a $mathcal { N } ( 0 , 1 )$ ; in Figure 11.15, the values are all set to 1. We then sample a random design matrix $mathbf { X }$ of size $N times D$ , where $N = 2 ^ { 1 0 } = 1 0 2 4$ . Finally, we generate $pmb { y } = mathbf { X } pmb { w } + pmb { epsilon }$ , where $epsilon sim mathcal { N } ( mathbf { 0 } , 1 0 ^ { - 4 } mathbf { I } _ { N } )$ . Given this data, we estimate the support of $mathbf { boldsymbol { w } }$ using $ell _ { 1 }$ or group $ell _ { 1 }$ , and then estimate the non-zero values using least squares (debiased estimate). \n\nWe see from the figures that group lasso does a much better job than vanilla lasso, since it respects the known group structure. We also see that the $ell _ { infty }$ norm has a tendency to make all the elements within a block to have similar magnitude. This is appropriate in the second example, but not the first. (The value of $lambda$ was the same in all examples, and was chosen by hand.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n11.4.8 Elastic net (ridge and lasso combined) \nIn group lasso, we need to specify the group structure ahead of time. For some problems, we don’t know the group structure, and yet we would still like highly correlated coefficients to be treated as an implicit group. One way to achieve this effect, proposed in [ZH05], is to use the elastic net, which is a hybrid between lasso and ridge regression.3 This corresponds to minimizing the following objective: \nThis penalty function is strictly convex (assuming $lambda _ { 2 } > 0$ ) so there is a unique global minimum, even if $mathbf { X }$ is not full rank. It can be shown [ZH05] that any strictly convex penalty on $pmb { w }$ will exhibit a grouping effect, which means that the regression coefficients of highly correlated variables tend to be equal. In particular, if two features are identically equal, so ${ bf { X } } _ { : j } = { bf { X } } _ { : k }$ , one can show that their estimates are also equal, $hat { w } _ { j } = hat { w } _ { k }$ . By contrast, with lasso, we may have that $hat { w } _ { j } = 0$ and $hat { w } _ { k } neq 0$ or vice versa, resulting in less stable estimates.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Group lasso"
    },
    {
        "content": "11.4.8 Elastic net (ridge and lasso combined) \nIn group lasso, we need to specify the group structure ahead of time. For some problems, we don’t know the group structure, and yet we would still like highly correlated coefficients to be treated as an implicit group. One way to achieve this effect, proposed in [ZH05], is to use the elastic net, which is a hybrid between lasso and ridge regression.3 This corresponds to minimizing the following objective: \nThis penalty function is strictly convex (assuming $lambda _ { 2 } > 0$ ) so there is a unique global minimum, even if $mathbf { X }$ is not full rank. It can be shown [ZH05] that any strictly convex penalty on $pmb { w }$ will exhibit a grouping effect, which means that the regression coefficients of highly correlated variables tend to be equal. In particular, if two features are identically equal, so ${ bf { X } } _ { : j } = { bf { X } } _ { : k }$ , one can show that their estimates are also equal, $hat { w } _ { j } = hat { w } _ { k }$ . By contrast, with lasso, we may have that $hat { w } _ { j } = 0$ and $hat { w } _ { k } neq 0$ or vice versa, resulting in less stable estimates. \n\nIn addition to its soft grouping behavior, elastic net has other advantages. In particular, if $D > N _ { mathcal { D } }$ , the maximum number of non-zero elements that can be selected (excluding the MLE, which has $D$ non-zero elements) is $N _ { mathcal { D } }$ . By contrast, elastic net can select more than $N _ { mathcal { D } }$ non-zero variables on its path to the dense estimate, thus exploring more possible subsets of variables. \n11.4.9 Optimization algorithms \nA large variety of algorithms have been proposed to solve the lasso problem, and other $ell _ { 1 }$ -regularized convex objectives. In this section, we briefly mention some of the most popular methods. \n11.4.9.1 Coordinate descent \nSometimes it is hard to optimize all the variables simultaneously, but it easy to optimize them one by one. In particular, we can solve for the $j$ ’th coefficient with all the others held fixed as follows: \nwhere $e _ { j }$ is the $j$ ’th unit vector. This is called coordinate descent. We can either cycle through the coordinates in a deterministic fashion, or we can sample them at random, or we can choose to update the coordinate for which the gradient is steepest. \nThis method is particularly appealing if each one-dimensional optimization problem can be solved analytically, as is the case for lasso (see Equation (11.87)). This is known as the shooting algorithm [Fu98; WL08]. (The term “shooting” is a reference to cowboy theme inspired by the term “lasso”.) See Algorithm 4 for details. \nThis coordinate descent method has been generalized to the GLM case in [FHT10], and is the basis of the popular glmnet software library. \n11.4.9.2 Projected gradient descent \nIn this section, we convert the non-differentiable $ell _ { 1 }$ penalty into a smooth regularizer. To do this, we first use the split variable trick to define $pmb { w } = pmb { w } ^ { + } - pmb { w } ^ { - }$ , where ${ pmb w } ^ { + } = operatorname* { m a x } { { pmb w } , 0 }$ and \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Elastic net (ridge and lasso combined)"
    },
    {
        "content": "In addition to its soft grouping behavior, elastic net has other advantages. In particular, if $D > N _ { mathcal { D } }$ , the maximum number of non-zero elements that can be selected (excluding the MLE, which has $D$ non-zero elements) is $N _ { mathcal { D } }$ . By contrast, elastic net can select more than $N _ { mathcal { D } }$ non-zero variables on its path to the dense estimate, thus exploring more possible subsets of variables. \n11.4.9 Optimization algorithms \nA large variety of algorithms have been proposed to solve the lasso problem, and other $ell _ { 1 }$ -regularized convex objectives. In this section, we briefly mention some of the most popular methods. \n11.4.9.1 Coordinate descent \nSometimes it is hard to optimize all the variables simultaneously, but it easy to optimize them one by one. In particular, we can solve for the $j$ ’th coefficient with all the others held fixed as follows: \nwhere $e _ { j }$ is the $j$ ’th unit vector. This is called coordinate descent. We can either cycle through the coordinates in a deterministic fashion, or we can sample them at random, or we can choose to update the coordinate for which the gradient is steepest. \nThis method is particularly appealing if each one-dimensional optimization problem can be solved analytically, as is the case for lasso (see Equation (11.87)). This is known as the shooting algorithm [Fu98; WL08]. (The term “shooting” is a reference to cowboy theme inspired by the term “lasso”.) See Algorithm 4 for details. \nThis coordinate descent method has been generalized to the GLM case in [FHT10], and is the basis of the popular glmnet software library. \n11.4.9.2 Projected gradient descent \nIn this section, we convert the non-differentiable $ell _ { 1 }$ penalty into a smooth regularizer. To do this, we first use the split variable trick to define $pmb { w } = pmb { w } ^ { + } - pmb { w } ^ { - }$ , where ${ pmb w } ^ { + } = operatorname* { m a x } { { pmb w } , 0 }$ and \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n${ pmb w } ^ { - } = - operatorname* { m i n } { { pmb w } , 0 }$ . Now we can replace $| | pmb { w } | | _ { 1 }$ with $textstyle sum _ { d } ( w _ { d } ^ { + } + w _ { d } ^ { - } )$ . We also have to replace $mathrm { N L L } ( w )$ with $mathrm { N L L } ( w ^ { + } + w ^ { - } )$ . Thus we get the following smooth, but constrained, optimization problem: \nIn this case of a Gaussian likelihood, the NLL becomes a least squares loss, and the objective becomes a quadratic program (Section 8.5.4). One way to solve such problems is to use projected gradient descent (Section 8.6.1). Specifically, we can enforce the constraint by projecting onto the positive orthant, which we can do using $w _ { d } : = operatorname* { m a x } ( w _ { d } , 0 )$ ; this operation is denoted by $P _ { + }$ . Thus the projected gradient update takes the following form: \nwhere $e$ is the unit vector of all ones. \n11.4.9.3 Proximal gradient descent \nIn Section 8.6, we introduced proximal gradient descent, which can be used to optimize smooth functions with non-smooth penalties, such as $ell _ { 1 }$ . In Section 8.6.2, we showed that the proximal operator for the $ell _ { 1 }$ penalty corresponds to soft thresholding. Thus the proximal gradient descent update can be written as \nwhere the soft thresholding operator (Equation (8.134)) is applied elementwise. This is called the iterative soft thresholding algorithm or ISTA [DDDM04; Don95]. If we combine this with Nesterov acceleration, we get the method known as “fast ISTA” or FISTA [BT09], which is widely used to fit sparse linear models. \n11.4.9.4 LARS \nIn this section, we discuss methods that can generate a set of solutions for different values of $lambda$ , starting with the empty set, i.e., they compute the full regularization path (Section 11.4.4). These algorithms exploit the fact that one can quickly compute $hat { pmb w } ( lambda _ { k } )$ from $hat { w } ( lambda _ { k - 1 } )$ if $lambda _ { k } approx lambda _ { k - 1 }$ ; this is known as warm starting. In fact, even if we only want the solution for a single value of $lambda$ , call it $lambda _ { * }$ , it can sometimes be computationally more efficient to compute a set of solutions, from $lambda _ { mathrm { m a x } }$ down to $lambda _ { * }$ , using warm-starting; this is called a continuation method or homotopy method. This is often much faster than directly “cold-starting” at $lambda _ { * }$ ; this is particularly true if $lambda _ { * }$ is small. \nThe LARS algorithm [Efr+04], which stands for “least angle regression and shrinkage”, is an example of a homotopy method for the lasso problem. This can compute $hat { pmb { w } } ( lambda )$ for all possible values of $lambda$ in an efficient manner. (A similar algorithm was independently invented in [OPT00b; OPT00a]). LARS works as follows. It starts with a large value of $lambda$ , such that only the variable that is most correlated with the response vector $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ is chosen. Then $lambda$ is decreased until a second variable is found which has the same correlation (in terms of magnitude) with the current residual as the first variable, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where the residual at step $k$ on the path is defined as $pmb { r } _ { k } = pmb { y } - pmb { X }$ :, ${ F _ { k } } ^ { w _ { k } }$ , where $F _ { k }$ is the current active set (cf., Equation (11.83)). Remarkably, one can solve for this new value of $lambda$ analytically, by using a geometric argument (hence the term “least angle”). This allows the algorithm to quickly “jump” to the next point on the regularization path where the active set changes. This repeats until all the variables are added. \n\nIt is necessary to allow variables to be removed from the current active set, even as we increase $lambda$ , if we want the sequence of solutions to correspond to the regularization path of lasso. If we disallow variable removal, we get a slightly different algorithm called least angle regression or LAR. LAR is very similar to greedy forward selection, and a method known as least squares boosting (see e.g., [HTW15]). \n11.5 Regression splines * \nWe have seen how we can use polynomial basis functions to create nonlinear mappings from input to output, even though the model remains linear in the parameters. One problem with polynomials is that they are a global approximation to the function. We can achieve more flexibility by using a series of local approximations. To do this, we just need to define a set of basis functions that have local support. The notion of “locality” is hard to define in high-dimensional input spaces, so in this section, we restrict ourselves to 1d inputs. We can then approximate the function using \nwhere $B _ { i }$ is the $textit { textbf {  i } }$ ’th basis function. \nA common way to define such basis functions is to use $mathbf { B }$ -splines. (“B” stands for “basis”, and the term “spline” refers to a flexible piece of material used by artists to draw curves.) We discuss this in more detail in Section 11.5.1. \n11.5.1 B-spline basis functions \nA spline is a piecewise polynomial of degree $D$ , where the locations of the pieces are defined by a set of knots, $t _ { 1 } < cdots < t _ { m }$ . More precisely, the polynomial is defined on each of the intervals $( - infty , t _ { 1 } )$ , $[ t _ { 1 } , t _ { 2 } ]$ , , $[ t _ { m } , infty )$ . The function is continuous and has continuous derivatives of orders $1 , ldots , D - 1$ at its knot points. It is common to use cubic splines, in which $D = 3$ . This ensures the function is continuous, and has continuous first and second derivatives at each knot. \nWe will skip the details on how B-splines are computed, since it is not relevant to our purposes. Suffice it to say that we can call the patsy.bs function to convert the $N times 1$ data matrix $mathbf { X }$ into an $N times ( K + D + 1 )$ design matrix $mathbf { B }$ , where $K$ is the number of knots and $D$ is the degree. (Alternatively, you can specify the desired number of basis functions, and let patsy work out the number and locations of the knots.) \nFigure 11.16 illustrates this approach, where we use B-splines of degree 0, 1 and 3, with 3 knots. By taking a weighted combination of these basis functions, we can get increasingly smooth functions, as shown in the bottom row. \nWe see from Figure 11.16 that each individual basis function has local support. At any given input point $x$ , only $D + 1$ basis functions will be “active”. This is more obvious if we plot the design matrix \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license $mathbf { B }$ itself. Let us first consider the piecewise constant spline, shown in Figure 11.17(a). The first B-spline (column 1) is 1 for the first 5 observations, and otherwise 0. The second B-spline (column $0$ ) is 0 for the first 5 observations, 1 for the second 5, and then 0 again. And so on. Now consider the linear spline, shown in Figure 11.17(b). The first B-spline (column 0) goes from 1 to 0, the next three splines go from 0 to 1 and back to 0; and the last spline (column 4) goes from 0 to 1; this reflects the triangular shapes shown in the top middle panel of Figure 11.16. Finally consider the cubic spline, shown in Figure 11.17(c). Here the pattern of activations is smoother, and the resulting model fits will be smoother too.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Lasso regression",
        "subsubsection": "Optimization algorithms"
    },
    {
        "content": "It is necessary to allow variables to be removed from the current active set, even as we increase $lambda$ , if we want the sequence of solutions to correspond to the regularization path of lasso. If we disallow variable removal, we get a slightly different algorithm called least angle regression or LAR. LAR is very similar to greedy forward selection, and a method known as least squares boosting (see e.g., [HTW15]). \n11.5 Regression splines * \nWe have seen how we can use polynomial basis functions to create nonlinear mappings from input to output, even though the model remains linear in the parameters. One problem with polynomials is that they are a global approximation to the function. We can achieve more flexibility by using a series of local approximations. To do this, we just need to define a set of basis functions that have local support. The notion of “locality” is hard to define in high-dimensional input spaces, so in this section, we restrict ourselves to 1d inputs. We can then approximate the function using \nwhere $B _ { i }$ is the $textit { textbf {  i } }$ ’th basis function. \nA common way to define such basis functions is to use $mathbf { B }$ -splines. (“B” stands for “basis”, and the term “spline” refers to a flexible piece of material used by artists to draw curves.) We discuss this in more detail in Section 11.5.1. \n11.5.1 B-spline basis functions \nA spline is a piecewise polynomial of degree $D$ , where the locations of the pieces are defined by a set of knots, $t _ { 1 } < cdots < t _ { m }$ . More precisely, the polynomial is defined on each of the intervals $( - infty , t _ { 1 } )$ , $[ t _ { 1 } , t _ { 2 } ]$ , , $[ t _ { m } , infty )$ . The function is continuous and has continuous derivatives of orders $1 , ldots , D - 1$ at its knot points. It is common to use cubic splines, in which $D = 3$ . This ensures the function is continuous, and has continuous first and second derivatives at each knot. \nWe will skip the details on how B-splines are computed, since it is not relevant to our purposes. Suffice it to say that we can call the patsy.bs function to convert the $N times 1$ data matrix $mathbf { X }$ into an $N times ( K + D + 1 )$ design matrix $mathbf { B }$ , where $K$ is the number of knots and $D$ is the degree. (Alternatively, you can specify the desired number of basis functions, and let patsy work out the number and locations of the knots.) \nFigure 11.16 illustrates this approach, where we use B-splines of degree 0, 1 and 3, with 3 knots. By taking a weighted combination of these basis functions, we can get increasingly smooth functions, as shown in the bottom row. \nWe see from Figure 11.16 that each individual basis function has local support. At any given input point $x$ , only $D + 1$ basis functions will be “active”. This is more obvious if we plot the design matrix \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license $mathbf { B }$ itself. Let us first consider the piecewise constant spline, shown in Figure 11.17(a). The first B-spline (column 1) is 1 for the first 5 observations, and otherwise 0. The second B-spline (column $0$ ) is 0 for the first 5 observations, 1 for the second 5, and then 0 again. And so on. Now consider the linear spline, shown in Figure 11.17(b). The first B-spline (column 0) goes from 1 to 0, the next three splines go from 0 to 1 and back to 0; and the last spline (column 4) goes from 0 to 1; this reflects the triangular shapes shown in the top middle panel of Figure 11.16. Finally consider the cubic spline, shown in Figure 11.17(c). Here the pattern of activations is smoother, and the resulting model fits will be smoother too. \n\nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n11.5.2 Fitting a linear model using a spline basis \nOnce we have computed the design matrix $mathbf { B }$ , we can use it to fit a linear model using least squares or ridge regression. (It is usually best to use some regularization.) As an example, we consider a dataset from [McE20, Sec 4.5], which records the the first day of the year, and the corresponding temperature, that marks the start of the cherry blossom season in Japan. (We use this dataset since it has interesting semi-periodic structure.) We fit the data using a cubic spline. We pick 15 knots, spaced according to quantiles of the data. The results are shown in Figure 11.18. We see that the fit is reasonable. Using more knots would improve the quality of the fit, but would eventually result in overfitting. We can select the number of knots using a model selection method, such as grid search plus cross validation. \n11.5.3 Smoothing splines \nSmoothing splines are related to regression splines, but use $N$ knots, where $N$ is the number of datapoints. That is, they are non-parametric models, since the number of parameters grows with the size of the data, rather than being fixed a priori. To avoid overfitting, smoothing splines rely on $ell _ { 2 }$ regularization. This technique is closely related to Gaussian process regression, which we discuss in Section 17.2. \n11.5.4 Generalized additive models \nA generalized additive model or GAM extends spline regression to the case of multidimensional inputs [HT90]. It does this by ignoring interactions between the inputs, and assuming the function has the following additive form: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where each $f _ { d }$ is a regression or smoothing spline. This model can be fit using backfitting, which iteratively fits each $f _ { d }$ to the partial residuals generated by the other terms. We can extend GAMs beyond the regression case (e.g., to classification) by using a link function, as in generalized linear models (Chapter 12).",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Regression splines *",
        "subsubsection": "B-spline basis functions"
    },
    {
        "content": "11.5.2 Fitting a linear model using a spline basis \nOnce we have computed the design matrix $mathbf { B }$ , we can use it to fit a linear model using least squares or ridge regression. (It is usually best to use some regularization.) As an example, we consider a dataset from [McE20, Sec 4.5], which records the the first day of the year, and the corresponding temperature, that marks the start of the cherry blossom season in Japan. (We use this dataset since it has interesting semi-periodic structure.) We fit the data using a cubic spline. We pick 15 knots, spaced according to quantiles of the data. The results are shown in Figure 11.18. We see that the fit is reasonable. Using more knots would improve the quality of the fit, but would eventually result in overfitting. We can select the number of knots using a model selection method, such as grid search plus cross validation. \n11.5.3 Smoothing splines \nSmoothing splines are related to regression splines, but use $N$ knots, where $N$ is the number of datapoints. That is, they are non-parametric models, since the number of parameters grows with the size of the data, rather than being fixed a priori. To avoid overfitting, smoothing splines rely on $ell _ { 2 }$ regularization. This technique is closely related to Gaussian process regression, which we discuss in Section 17.2. \n11.5.4 Generalized additive models \nA generalized additive model or GAM extends spline regression to the case of multidimensional inputs [HT90]. It does this by ignoring interactions between the inputs, and assuming the function has the following additive form: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where each $f _ { d }$ is a regression or smoothing spline. This model can be fit using backfitting, which iteratively fits each $f _ { d }$ to the partial residuals generated by the other terms. We can extend GAMs beyond the regression case (e.g., to classification) by using a link function, as in generalized linear models (Chapter 12).",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Regression splines *",
        "subsubsection": "Fitting a linear model using a spline basis"
    },
    {
        "content": "11.5.2 Fitting a linear model using a spline basis \nOnce we have computed the design matrix $mathbf { B }$ , we can use it to fit a linear model using least squares or ridge regression. (It is usually best to use some regularization.) As an example, we consider a dataset from [McE20, Sec 4.5], which records the the first day of the year, and the corresponding temperature, that marks the start of the cherry blossom season in Japan. (We use this dataset since it has interesting semi-periodic structure.) We fit the data using a cubic spline. We pick 15 knots, spaced according to quantiles of the data. The results are shown in Figure 11.18. We see that the fit is reasonable. Using more knots would improve the quality of the fit, but would eventually result in overfitting. We can select the number of knots using a model selection method, such as grid search plus cross validation. \n11.5.3 Smoothing splines \nSmoothing splines are related to regression splines, but use $N$ knots, where $N$ is the number of datapoints. That is, they are non-parametric models, since the number of parameters grows with the size of the data, rather than being fixed a priori. To avoid overfitting, smoothing splines rely on $ell _ { 2 }$ regularization. This technique is closely related to Gaussian process regression, which we discuss in Section 17.2. \n11.5.4 Generalized additive models \nA generalized additive model or GAM extends spline regression to the case of multidimensional inputs [HT90]. It does this by ignoring interactions between the inputs, and assuming the function has the following additive form: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where each $f _ { d }$ is a regression or smoothing spline. This model can be fit using backfitting, which iteratively fits each $f _ { d }$ to the partial residuals generated by the other terms. We can extend GAMs beyond the regression case (e.g., to classification) by using a link function, as in generalized linear models (Chapter 12).",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Regression splines *",
        "subsubsection": "Smoothing splines"
    },
    {
        "content": "11.5.2 Fitting a linear model using a spline basis \nOnce we have computed the design matrix $mathbf { B }$ , we can use it to fit a linear model using least squares or ridge regression. (It is usually best to use some regularization.) As an example, we consider a dataset from [McE20, Sec 4.5], which records the the first day of the year, and the corresponding temperature, that marks the start of the cherry blossom season in Japan. (We use this dataset since it has interesting semi-periodic structure.) We fit the data using a cubic spline. We pick 15 knots, spaced according to quantiles of the data. The results are shown in Figure 11.18. We see that the fit is reasonable. Using more knots would improve the quality of the fit, but would eventually result in overfitting. We can select the number of knots using a model selection method, such as grid search plus cross validation. \n11.5.3 Smoothing splines \nSmoothing splines are related to regression splines, but use $N$ knots, where $N$ is the number of datapoints. That is, they are non-parametric models, since the number of parameters grows with the size of the data, rather than being fixed a priori. To avoid overfitting, smoothing splines rely on $ell _ { 2 }$ regularization. This technique is closely related to Gaussian process regression, which we discuss in Section 17.2. \n11.5.4 Generalized additive models \nA generalized additive model or GAM extends spline regression to the case of multidimensional inputs [HT90]. It does this by ignoring interactions between the inputs, and assuming the function has the following additive form: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where each $f _ { d }$ is a regression or smoothing spline. This model can be fit using backfitting, which iteratively fits each $f _ { d }$ to the partial residuals generated by the other terms. We can extend GAMs beyond the regression case (e.g., to classification) by using a link function, as in generalized linear models (Chapter 12). \n\n11.6 Robust linear regression * \nIt is very common to model the noise in regression models using a Gaussian distribution with zero mean and constant variance, $r _ { n } sim mathcal { N } ( 0 , sigma ^ { 2 } )$ , where $r _ { n } = y _ { n } - w ^ { mathrm { { I } } } x _ { n }$ . In this case, maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if we have outliers in our data, this can result in a poor fit, as illustrated in Figure 11.19(a). (The outliers are the points on the bottom of the figure.) This is because squared error penalizes deviations quadratically, so points far from the line have more effect on the fit than points near to the line. \nOne way to achieve robustness to outliers is to replace the Gaussian distribution for the response variable with a distribution that has heavy tails. Such a distribution will assign higher likelihood to outliers, without having to perturb the straight line to “explain” them. We discuss several possible alternative probability distributions for the response variable below; see Table 11.2 for a summary. \n11.6.1 Laplace likelihood \nIn Section 2.7.3, we noted that the Laplace distribution is also robust to outliers. If we use this as our observation model for regression, we get the following likelihood: \nThe robustness arises from the use of $| y - w ^ { boldsymbol { mathsf { I } } } boldsymbol { x } |$ instead of $( y - w ^ { mathsf { I } } x ) ^ { 2 }$ . Figure 11.19(a) gives an example of the method in action. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Regression splines *",
        "subsubsection": "Generalized additive models"
    },
    {
        "content": "11.6 Robust linear regression * \nIt is very common to model the noise in regression models using a Gaussian distribution with zero mean and constant variance, $r _ { n } sim mathcal { N } ( 0 , sigma ^ { 2 } )$ , where $r _ { n } = y _ { n } - w ^ { mathrm { { I } } } x _ { n }$ . In this case, maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if we have outliers in our data, this can result in a poor fit, as illustrated in Figure 11.19(a). (The outliers are the points on the bottom of the figure.) This is because squared error penalizes deviations quadratically, so points far from the line have more effect on the fit than points near to the line. \nOne way to achieve robustness to outliers is to replace the Gaussian distribution for the response variable with a distribution that has heavy tails. Such a distribution will assign higher likelihood to outliers, without having to perturb the straight line to “explain” them. We discuss several possible alternative probability distributions for the response variable below; see Table 11.2 for a summary. \n11.6.1 Laplace likelihood \nIn Section 2.7.3, we noted that the Laplace distribution is also robust to outliers. If we use this as our observation model for regression, we get the following likelihood: \nThe robustness arises from the use of $| y - w ^ { boldsymbol { mathsf { I } } } boldsymbol { x } |$ instead of $( y - w ^ { mathsf { I } } x ) ^ { 2 }$ . Figure 11.19(a) gives an example of the method in action. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTable 11.2: Summary of various likelihoods, priors and posteriors used for linear regression. The likelihood refers to the distributional form of $p ( boldsymbol { y } | boldsymbol { mathbf { x } } , boldsymbol { w } , sigma ^ { 2 } )$ , and the prior refers to the distributional form of $p ( pmb { w } )$ . The posterior refers to the distributional form of $p ( pmb { w } | mathcal { D } )$ . “Point” stands for the degenerate distribution $delta ( boldsymbol { w } - hat { boldsymbol { w } } )$ , where wˆ is the MAP estimate. MLE is equivalent to using a point posterior and a uniform prior. \n11.6.1.1 Computing the MLE using linear programming \nWe can compute the MLE for this model using linear programming. As we explain in Section 8.5.3, this is a way to solve a constrained optimization problems of the form \nwhere $pmb { v } in mathbb { R } ^ { n }$ is the set of $n$ unknown parameters, $boldsymbol { c } ^ { flat } boldsymbol { v }$ is the linear objective function we want to minimize, and $mathbf {  { a } } _ { i } ^ { top } mathbf {  { v } } le b _ { i }$ is a set of $m$ linear constraints we must satisfy. To apply this to our problem, let us define $pmb { v } = ( w _ { 1 } , dots , w _ { D } , e _ { 1 } , dots , e _ { N } ) in mathbb { R } ^ { D + N }$ , where $e _ { i } = | y _ { i } - { hat { y } } _ { i } |$ is the residual error for example $i$ . We want to minimize the sum of the residuals, so we define $pmb { c } = ( 0 , cdots , 0 , 1 , cdots , 1 ) in mathbb { R } ^ { D + N }$ , where the first $D$ elements are 0, and the last $N$ elements are 1. \nWe need to enforce the constraint that $e _ { i } = | hat { y } _ { i } - y _ { i } |$ . In fact it is sufficient to enforce the constraint that $| w ^ { mathsf { I } } x _ { i } - y _ { i } | leq e _ { i }$ , since minimizing the sum of the $e _ { i }$ ’s will “push down” on this constraint and make it tight. Since $| a | leq b implies - b leq a leq b$ , we can encode $| boldsymbol { w } ^ { intercal } boldsymbol { x } _ { i } - y _ { i } | leq e _ { i }$ as two linear constraints: \nWe can write Equation (11.110) as \nwhere the first $D$ entries are filled with ${ bf { x } } _ { i }$ , and the $- 1$ is in the $( D + i )$ ’th entry of the vector. Similarly we can write Equation (11.111) as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can write these constraints in the form $mathbf { A } v leq b$ by defining $mathbf { A } in mathbb { R } ^ { 2 N times ( N + D ) }$ as follows: \nand defining $pmb { b } in mathbb { R } ^ { 2 N }$ as \n11.6.2 Student- $scriptstyle t$ likelihood \nIn Section 2.7.1, we discussed the robustness properties of the Student distribution. To use this in a regression context, we can just make the mean be a linear function of the inputs, as proposed in [Zel76]: \nWe can fit this model using SGD or EM (see [Mur23] for details). \n11.6.3 Huber loss \nAn alternative to minimizing the NLL using a Laplace or Student likelihood is to use the Huber loss, which is defined as follows: \nThis is equivalent to $ell _ { 2 }$ for errors that are smaller than $delta$ , and is equivalent to $ell _ { 1 }$ for larger errors.   \nSee Figure 5.3 for a plot. \nThe advantage of this loss function is that it is everywhere differentiable. Consequently optimizing the Huber loss is much faster than using the Laplace likelihood, since we can use standard smooth optimization methods (such as SGD) instead of linear programming. Figure 11.19 gives an illustration of the Huber loss function in action. The results are qualitatively similiar to the Laplace and Student methods. \nThe parameter $delta$ , which controls the degree of robustness, is usually set by hand, or by crossvalidation. However, [Bar19] shows how to approximate the Huber loss such that we can optimize $delta$ by gradient methods. \n11.6.4 RANSAC \nIn the computer vision community, a common approach to robust regression is to use RANSAC, which stands for “random sample consensus” [FB81]. This works as follows: we sample a small initial set of points, fit the model to them, identify outliers wrt this model (based on large residuals), remove \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the outliers, and then refit the model to the inliers. We repeat this for many random initial sets and pick the best model.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Robust linear regression *",
        "subsubsection": "Laplace likelihood"
    },
    {
        "content": "We can write these constraints in the form $mathbf { A } v leq b$ by defining $mathbf { A } in mathbb { R } ^ { 2 N times ( N + D ) }$ as follows: \nand defining $pmb { b } in mathbb { R } ^ { 2 N }$ as \n11.6.2 Student- $scriptstyle t$ likelihood \nIn Section 2.7.1, we discussed the robustness properties of the Student distribution. To use this in a regression context, we can just make the mean be a linear function of the inputs, as proposed in [Zel76]: \nWe can fit this model using SGD or EM (see [Mur23] for details). \n11.6.3 Huber loss \nAn alternative to minimizing the NLL using a Laplace or Student likelihood is to use the Huber loss, which is defined as follows: \nThis is equivalent to $ell _ { 2 }$ for errors that are smaller than $delta$ , and is equivalent to $ell _ { 1 }$ for larger errors.   \nSee Figure 5.3 for a plot. \nThe advantage of this loss function is that it is everywhere differentiable. Consequently optimizing the Huber loss is much faster than using the Laplace likelihood, since we can use standard smooth optimization methods (such as SGD) instead of linear programming. Figure 11.19 gives an illustration of the Huber loss function in action. The results are qualitatively similiar to the Laplace and Student methods. \nThe parameter $delta$ , which controls the degree of robustness, is usually set by hand, or by crossvalidation. However, [Bar19] shows how to approximate the Huber loss such that we can optimize $delta$ by gradient methods. \n11.6.4 RANSAC \nIn the computer vision community, a common approach to robust regression is to use RANSAC, which stands for “random sample consensus” [FB81]. This works as follows: we sample a small initial set of points, fit the model to them, identify outliers wrt this model (based on large residuals), remove \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the outliers, and then refit the model to the inliers. We repeat this for many random initial sets and pick the best model.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Robust linear regression *",
        "subsubsection": "Student-t likelihood"
    },
    {
        "content": "We can write these constraints in the form $mathbf { A } v leq b$ by defining $mathbf { A } in mathbb { R } ^ { 2 N times ( N + D ) }$ as follows: \nand defining $pmb { b } in mathbb { R } ^ { 2 N }$ as \n11.6.2 Student- $scriptstyle t$ likelihood \nIn Section 2.7.1, we discussed the robustness properties of the Student distribution. To use this in a regression context, we can just make the mean be a linear function of the inputs, as proposed in [Zel76]: \nWe can fit this model using SGD or EM (see [Mur23] for details). \n11.6.3 Huber loss \nAn alternative to minimizing the NLL using a Laplace or Student likelihood is to use the Huber loss, which is defined as follows: \nThis is equivalent to $ell _ { 2 }$ for errors that are smaller than $delta$ , and is equivalent to $ell _ { 1 }$ for larger errors.   \nSee Figure 5.3 for a plot. \nThe advantage of this loss function is that it is everywhere differentiable. Consequently optimizing the Huber loss is much faster than using the Laplace likelihood, since we can use standard smooth optimization methods (such as SGD) instead of linear programming. Figure 11.19 gives an illustration of the Huber loss function in action. The results are qualitatively similiar to the Laplace and Student methods. \nThe parameter $delta$ , which controls the degree of robustness, is usually set by hand, or by crossvalidation. However, [Bar19] shows how to approximate the Huber loss such that we can optimize $delta$ by gradient methods. \n11.6.4 RANSAC \nIn the computer vision community, a common approach to robust regression is to use RANSAC, which stands for “random sample consensus” [FB81]. This works as follows: we sample a small initial set of points, fit the model to them, identify outliers wrt this model (based on large residuals), remove \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the outliers, and then refit the model to the inliers. We repeat this for many random initial sets and pick the best model.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Robust linear regression *",
        "subsubsection": "Huber loss"
    },
    {
        "content": "We can write these constraints in the form $mathbf { A } v leq b$ by defining $mathbf { A } in mathbb { R } ^ { 2 N times ( N + D ) }$ as follows: \nand defining $pmb { b } in mathbb { R } ^ { 2 N }$ as \n11.6.2 Student- $scriptstyle t$ likelihood \nIn Section 2.7.1, we discussed the robustness properties of the Student distribution. To use this in a regression context, we can just make the mean be a linear function of the inputs, as proposed in [Zel76]: \nWe can fit this model using SGD or EM (see [Mur23] for details). \n11.6.3 Huber loss \nAn alternative to minimizing the NLL using a Laplace or Student likelihood is to use the Huber loss, which is defined as follows: \nThis is equivalent to $ell _ { 2 }$ for errors that are smaller than $delta$ , and is equivalent to $ell _ { 1 }$ for larger errors.   \nSee Figure 5.3 for a plot. \nThe advantage of this loss function is that it is everywhere differentiable. Consequently optimizing the Huber loss is much faster than using the Laplace likelihood, since we can use standard smooth optimization methods (such as SGD) instead of linear programming. Figure 11.19 gives an illustration of the Huber loss function in action. The results are qualitatively similiar to the Laplace and Student methods. \nThe parameter $delta$ , which controls the degree of robustness, is usually set by hand, or by crossvalidation. However, [Bar19] shows how to approximate the Huber loss such that we can optimize $delta$ by gradient methods. \n11.6.4 RANSAC \nIn the computer vision community, a common approach to robust regression is to use RANSAC, which stands for “random sample consensus” [FB81]. This works as follows: we sample a small initial set of points, fit the model to them, identify outliers wrt this model (based on large residuals), remove \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the outliers, and then refit the model to the inliers. We repeat this for many random initial sets and pick the best model. \n\nA deterministic alternative to RANSAC is the following iterative scheme: intially we assume that all datapoints are inliers, and we fit the model to compute $hat { pmb { w } } _ { 0 }$ ; then, for each iteration $t$ , we identify the outlier points as those with large residual under the model $hat { mathbf { Omega } } hat { mathbf { Omega } } ^ { hat { mathbf { Omega } } } hat { mathbf { Omega } } ^ { hat { mathbf { Omega } } }$ , remove them, and refit the model to the remaining points to get $hat { pmb { w } } _ { t + 1 }$ . Even though this hard thresholding scheme makes the problem nonconvex, this simple scheme can be proved to rapidly converge to the optimal estimate under some reasonable assumptions [Muk+19; Sug+19]. \n11.7 Bayesian linear regression * \nWe have seen how to compute the MLE and MAP estimate for linear regression models under various priors. In this section, we discuss how to compute the posterior over the parameters, $p ( pmb { theta } | mathcal { D } )$ . For simplicity, we assume the variance is known, so we just want to compute $p ( { pmb w } | mathcal { D } , sigma ^ { 2 } )$ . See the sequel to this book, [Mur23], for the general case. \n11.7.1 Priors \nFor simplicity, we will use a Gaussian prior: \nThis is a small generalization of the prior that we use in ridge regression (Section 11.3). See the sequel to this book, [Mur23], for a discussion of other priors. \n11.7.2 Posteriors \nWe can rewrite the likelihood in terms of an MVN as follows: \nwhere ${ mathbf { I } } _ { N }$ is the $N times N$ identity matrix. We can then use Bayes rule for Gaussians (Equation (3.37)) to derive the posterior, which is as follows: \nwhere $dot { pmb w }$ is the posterior mean, and $hat { Sigma }$ is the posterior covariance. \nIf $stackrel { triangledown } { mathbf { nabla } } psi = { bf 0 }$ and $breve { Sigma } = tau ^ { 2 } mathbf { I }$ , then the posterior mean becomes ${ widehat { pmb w } } = { frac { 1 } { sigma ^ { 2 } } }  { widehat { pmb x } } ^ { top } { pmb y }$ . If we define $begin{array} { r } { lambda = frac { sigma ^ { 2 } } { tau ^ { 2 } } } end{array}$ , we recover the ridge regression estimate, ${ pmb w } = ( lambda { bf I } + { bf X } ^ { mathsf { I } } { bf X } ) ^ { - 1 } { bf X } ^ { mathsf { I } } { pmb y }$ , which matches Equation (11.57). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Robust linear regression *",
        "subsubsection": "RANSAC"
    },
    {
        "content": "A deterministic alternative to RANSAC is the following iterative scheme: intially we assume that all datapoints are inliers, and we fit the model to compute $hat { pmb { w } } _ { 0 }$ ; then, for each iteration $t$ , we identify the outlier points as those with large residual under the model $hat { mathbf { Omega } } hat { mathbf { Omega } } ^ { hat { mathbf { Omega } } } hat { mathbf { Omega } } ^ { hat { mathbf { Omega } } }$ , remove them, and refit the model to the remaining points to get $hat { pmb { w } } _ { t + 1 }$ . Even though this hard thresholding scheme makes the problem nonconvex, this simple scheme can be proved to rapidly converge to the optimal estimate under some reasonable assumptions [Muk+19; Sug+19]. \n11.7 Bayesian linear regression * \nWe have seen how to compute the MLE and MAP estimate for linear regression models under various priors. In this section, we discuss how to compute the posterior over the parameters, $p ( pmb { theta } | mathcal { D } )$ . For simplicity, we assume the variance is known, so we just want to compute $p ( { pmb w } | mathcal { D } , sigma ^ { 2 } )$ . See the sequel to this book, [Mur23], for the general case. \n11.7.1 Priors \nFor simplicity, we will use a Gaussian prior: \nThis is a small generalization of the prior that we use in ridge regression (Section 11.3). See the sequel to this book, [Mur23], for a discussion of other priors. \n11.7.2 Posteriors \nWe can rewrite the likelihood in terms of an MVN as follows: \nwhere ${ mathbf { I } } _ { N }$ is the $N times N$ identity matrix. We can then use Bayes rule for Gaussians (Equation (3.37)) to derive the posterior, which is as follows: \nwhere $dot { pmb w }$ is the posterior mean, and $hat { Sigma }$ is the posterior covariance. \nIf $stackrel { triangledown } { mathbf { nabla } } psi = { bf 0 }$ and $breve { Sigma } = tau ^ { 2 } mathbf { I }$ , then the posterior mean becomes ${ widehat { pmb w } } = { frac { 1 } { sigma ^ { 2 } } }  { widehat { pmb x } } ^ { top } { pmb y }$ . If we define $begin{array} { r } { lambda = frac { sigma ^ { 2 } } { tau ^ { 2 } } } end{array}$ , we recover the ridge regression estimate, ${ pmb w } = ( lambda { bf I } + { bf X } ^ { mathsf { I } } { bf X } ) ^ { - 1 } { bf X } ^ { mathsf { I } } { pmb y }$ , which matches Equation (11.57). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "Priors"
    },
    {
        "content": "A deterministic alternative to RANSAC is the following iterative scheme: intially we assume that all datapoints are inliers, and we fit the model to compute $hat { pmb { w } } _ { 0 }$ ; then, for each iteration $t$ , we identify the outlier points as those with large residual under the model $hat { mathbf { Omega } } hat { mathbf { Omega } } ^ { hat { mathbf { Omega } } } hat { mathbf { Omega } } ^ { hat { mathbf { Omega } } }$ , remove them, and refit the model to the remaining points to get $hat { pmb { w } } _ { t + 1 }$ . Even though this hard thresholding scheme makes the problem nonconvex, this simple scheme can be proved to rapidly converge to the optimal estimate under some reasonable assumptions [Muk+19; Sug+19]. \n11.7 Bayesian linear regression * \nWe have seen how to compute the MLE and MAP estimate for linear regression models under various priors. In this section, we discuss how to compute the posterior over the parameters, $p ( pmb { theta } | mathcal { D } )$ . For simplicity, we assume the variance is known, so we just want to compute $p ( { pmb w } | mathcal { D } , sigma ^ { 2 } )$ . See the sequel to this book, [Mur23], for the general case. \n11.7.1 Priors \nFor simplicity, we will use a Gaussian prior: \nThis is a small generalization of the prior that we use in ridge regression (Section 11.3). See the sequel to this book, [Mur23], for a discussion of other priors. \n11.7.2 Posteriors \nWe can rewrite the likelihood in terms of an MVN as follows: \nwhere ${ mathbf { I } } _ { N }$ is the $N times N$ identity matrix. We can then use Bayes rule for Gaussians (Equation (3.37)) to derive the posterior, which is as follows: \nwhere $dot { pmb w }$ is the posterior mean, and $hat { Sigma }$ is the posterior covariance. \nIf $stackrel { triangledown } { mathbf { nabla } } psi = { bf 0 }$ and $breve { Sigma } = tau ^ { 2 } mathbf { I }$ , then the posterior mean becomes ${ widehat { pmb w } } = { frac { 1 } { sigma ^ { 2 } } }  { widehat { pmb x } } ^ { top } { pmb y }$ . If we define $begin{array} { r } { lambda = frac { sigma ^ { 2 } } { tau ^ { 2 } } } end{array}$ , we recover the ridge regression estimate, ${ pmb w } = ( lambda { bf I } + { bf X } ^ { mathsf { I } } { bf X } ) ^ { - 1 } { bf X } ^ { mathsf { I } } { pmb y }$ , which matches Equation (11.57). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n11.7.3 Example \nSuppose we have a 1d regression model of the form $f ( x ; pmb { w } ) = { w } _ { 0 } + { w } _ { 1 } x _ { 1 }$ , where the true parameters are $w _ { 0 } = - 0 . 3$ and $w _ { 1 } = 0 . 5$ . We now perform inference $p ( pmb { w } | mathcal { D } )$ and visualize the 2d prior and posterior as the size of the training set $N$ increases. \nIn particular, in Figure 11.20 (which inspired the front cover of this book), we plot the likelihood, the posterior, and an approximation to the posterior predictive distribution.4 Each row plots these distributions as we increase the amount of training data, $N$ . We now explain each row: \n• In the first row, $N = 0$ , so the posterior is the same as the prior. In this case, our predictions are “all over the place”, since our prior is essentially uniform.   \n• In the second row, $N = 1$ , so we have seen one data point (the blue circle in the plot in the third column). Our posterior becomes constrained by the corresponding likelihood, and our predictions pass close to the observed data. However, we see that the posterior has a ridge-like shape, reflecting the fact that there are many possible solutions, with different slopes/intercepts. This makes sense since we cannot uniquely infer two parameters ( $w _ { 0 }$ and $w _ { 1 }$ ) from one observation.   \n• In the third row, $N = 2$ . In this case, the posterior becomes much narrower since we have two constraints from the likelihood. Our predictions about the future are all now closer to the training data.   \n• In the fourth (last) row, $N = 1 0 0$ . Now the posterior is essentially a delta function, centered on the true value of ${ pmb w } _ { ast } = ( - 0 . 3 , 0 . 5 )$ , indicated by a white cross in the plots in the first and second columns. The variation in our predictions is due to the inherent Gaussian noise with magnitude $sigma ^ { 2 }$ . \nThis example illustrates that, as the amount of data increases, the posterior mean estimate, $scriptstyle { widehat { pmb { mu } } } = mathbb { E } left[ pmb { w } | mathcal { D } right]$ , converges to the true value $^ { w _ { * } }$ that generated the data. We thus say that the Bayesian estimate is a consistent estimator (see Section 5.3.2 for more details). We also see that our posterior uncertainty decreases over time. This is what we mean when we say we are “learning” about the parameters as we see more data. \n11.7.4 Computing the posterior predictive \nWe have discussed how to compute our uncertainty about the parameters of the model, $p ( pmb { w } | mathcal { D } )$ . But what about the uncertainty associated with our predictions about future outputs? Using Equation (3.38), we can show that the posterior predictive distribution at a test point $_ { x }$ is also Gaussian:",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "Posteriors"
    },
    {
        "content": "11.7.3 Example \nSuppose we have a 1d regression model of the form $f ( x ; pmb { w } ) = { w } _ { 0 } + { w } _ { 1 } x _ { 1 }$ , where the true parameters are $w _ { 0 } = - 0 . 3$ and $w _ { 1 } = 0 . 5$ . We now perform inference $p ( pmb { w } | mathcal { D } )$ and visualize the 2d prior and posterior as the size of the training set $N$ increases. \nIn particular, in Figure 11.20 (which inspired the front cover of this book), we plot the likelihood, the posterior, and an approximation to the posterior predictive distribution.4 Each row plots these distributions as we increase the amount of training data, $N$ . We now explain each row: \n• In the first row, $N = 0$ , so the posterior is the same as the prior. In this case, our predictions are “all over the place”, since our prior is essentially uniform.   \n• In the second row, $N = 1$ , so we have seen one data point (the blue circle in the plot in the third column). Our posterior becomes constrained by the corresponding likelihood, and our predictions pass close to the observed data. However, we see that the posterior has a ridge-like shape, reflecting the fact that there are many possible solutions, with different slopes/intercepts. This makes sense since we cannot uniquely infer two parameters ( $w _ { 0 }$ and $w _ { 1 }$ ) from one observation.   \n• In the third row, $N = 2$ . In this case, the posterior becomes much narrower since we have two constraints from the likelihood. Our predictions about the future are all now closer to the training data.   \n• In the fourth (last) row, $N = 1 0 0$ . Now the posterior is essentially a delta function, centered on the true value of ${ pmb w } _ { ast } = ( - 0 . 3 , 0 . 5 )$ , indicated by a white cross in the plots in the first and second columns. The variation in our predictions is due to the inherent Gaussian noise with magnitude $sigma ^ { 2 }$ . \nThis example illustrates that, as the amount of data increases, the posterior mean estimate, $scriptstyle { widehat { pmb { mu } } } = mathbb { E } left[ pmb { w } | mathcal { D } right]$ , converges to the true value $^ { w _ { * } }$ that generated the data. We thus say that the Bayesian estimate is a consistent estimator (see Section 5.3.2 for more details). We also see that our posterior uncertainty decreases over time. This is what we mean when we say we are “learning” about the parameters as we see more data. \n11.7.4 Computing the posterior predictive \nWe have discussed how to compute our uncertainty about the parameters of the model, $p ( pmb { w } | mathcal { D } )$ . But what about the uncertainty associated with our predictions about future outputs? Using Equation (3.38), we can show that the posterior predictive distribution at a test point $_ { x }$ is also Gaussian:",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "Example"
    },
    {
        "content": "11.7.3 Example \nSuppose we have a 1d regression model of the form $f ( x ; pmb { w } ) = { w } _ { 0 } + { w } _ { 1 } x _ { 1 }$ , where the true parameters are $w _ { 0 } = - 0 . 3$ and $w _ { 1 } = 0 . 5$ . We now perform inference $p ( pmb { w } | mathcal { D } )$ and visualize the 2d prior and posterior as the size of the training set $N$ increases. \nIn particular, in Figure 11.20 (which inspired the front cover of this book), we plot the likelihood, the posterior, and an approximation to the posterior predictive distribution.4 Each row plots these distributions as we increase the amount of training data, $N$ . We now explain each row: \n• In the first row, $N = 0$ , so the posterior is the same as the prior. In this case, our predictions are “all over the place”, since our prior is essentially uniform.   \n• In the second row, $N = 1$ , so we have seen one data point (the blue circle in the plot in the third column). Our posterior becomes constrained by the corresponding likelihood, and our predictions pass close to the observed data. However, we see that the posterior has a ridge-like shape, reflecting the fact that there are many possible solutions, with different slopes/intercepts. This makes sense since we cannot uniquely infer two parameters ( $w _ { 0 }$ and $w _ { 1 }$ ) from one observation.   \n• In the third row, $N = 2$ . In this case, the posterior becomes much narrower since we have two constraints from the likelihood. Our predictions about the future are all now closer to the training data.   \n• In the fourth (last) row, $N = 1 0 0$ . Now the posterior is essentially a delta function, centered on the true value of ${ pmb w } _ { ast } = ( - 0 . 3 , 0 . 5 )$ , indicated by a white cross in the plots in the first and second columns. The variation in our predictions is due to the inherent Gaussian noise with magnitude $sigma ^ { 2 }$ . \nThis example illustrates that, as the amount of data increases, the posterior mean estimate, $scriptstyle { widehat { pmb { mu } } } = mathbb { E } left[ pmb { w } | mathcal { D } right]$ , converges to the true value $^ { w _ { * } }$ that generated the data. We thus say that the Bayesian estimate is a consistent estimator (see Section 5.3.2 for more details). We also see that our posterior uncertainty decreases over time. This is what we mean when we say we are “learning” about the parameters as we see more data. \n11.7.4 Computing the posterior predictive \nWe have discussed how to compute our uncertainty about the parameters of the model, $p ( pmb { w } | mathcal { D } )$ . But what about the uncertainty associated with our predictions about future outputs? Using Equation (3.38), we can show that the posterior predictive distribution at a test point $_ { x }$ is also Gaussian: \nwhere ${ widehat { sigma } } ^ { 2 }  ( { pmb x } ) triangleq sigma ^ { 2 } + { pmb x } ^ { top }  { widehat { Sigma } }  { pmb x }$ is the variance of the posterior predictive distribution at point $_ x$ after seeing the $N$ training examples. The predicted variance depends on two terms: the variance of the observation noise, $sigma ^ { 2 }$ , and the variance in the parameters, $hat { Sigma }$ . The latter translates into variance about observations in a way which depends on how close $_ { x }$ is to the training data $mathcal { D }$ . This is illustrated in Figure 11.21(b), where we see that the error bars get larger as we move away from the training points, representing increased uncertainty. This can be important for certain applications, such as active learning, where we choose where to collect training data (see Section 19.4). \nIn some cases, it is computationally intractable to compute the parameter posterior, $p ( pmb { w } | mathcal { D } )$ . In such cases, we may choose to use a point estimate, $hat { pmb { w } }$ , and then to use the plugin approximation. This gives \nWe see that the posterior predictive variance is constant, and independent of the data, as illustrated in Figure 11.21(a). If we sample a parameter from this posterior, we will always recover a single function, as shown in Figure 11.21(c). By contrast, if we sample from the true posterior, $mathbf { w } _ { s } sim p ( mathbf { w } | mathcal { D } , sigma ^ { 2 } )$ , we will get a range of different functions, as shown in Figure 11.21(d), which more accurately reflects our uncertainty. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n11.7.5 The advantage of centering \nThe astute reader might notice that the shape of the 2d posterior in Figure 11.20 is an elongated ellipse (which eventually collapses to a point as $N  infty$ ). This implies that there is a lot of posterior correlation between the two parameters, which can cause computational difficulties. \nTo understand why this happens, note that each data point induces a likelihood function corresponding to a line which goes through that data point. When we look at all the data together, we see that predictions with maximum likelihood must correspond to lines that go through the mean of the data, $( { overline { { x } } } , { overline { { y } } } )$ . There are many such lines, but if we increase the slope, we must decrease the intercept. Thus we can think of the set of high probability lines as spinning around the data mean, like a wheel of fortune.5 This correlation between $w _ { 0 }$ and $w _ { 1 }$ is why the posterior has the form of a diagonal line. (The Gaussian prior converts this into an elongated ellipse, but the posterior correlation still persists until the sample size causes the posterior to shrink to a point.) \nIt can be hard to compute such elongated posteriors. One simple solution is to center the input data, i.e., by using $x _ { n } ^ { prime } = x _ { n } - { overline { { x } } }$ . Now the lines can pivot around the origin, reducing the posterior correlation between $w _ { 0 }$ and $w _ { 1 }$ . See Figure 11.22 for an illustration. (We may also choose to divide each $x _ { n }$ by the standard deviation of that feature, as discussed in Section 10.2.8.)",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "Computing the posterior predictive"
    },
    {
        "content": "11.7.5 The advantage of centering \nThe astute reader might notice that the shape of the 2d posterior in Figure 11.20 is an elongated ellipse (which eventually collapses to a point as $N  infty$ ). This implies that there is a lot of posterior correlation between the two parameters, which can cause computational difficulties. \nTo understand why this happens, note that each data point induces a likelihood function corresponding to a line which goes through that data point. When we look at all the data together, we see that predictions with maximum likelihood must correspond to lines that go through the mean of the data, $( { overline { { x } } } , { overline { { y } } } )$ . There are many such lines, but if we increase the slope, we must decrease the intercept. Thus we can think of the set of high probability lines as spinning around the data mean, like a wheel of fortune.5 This correlation between $w _ { 0 }$ and $w _ { 1 }$ is why the posterior has the form of a diagonal line. (The Gaussian prior converts this into an elongated ellipse, but the posterior correlation still persists until the sample size causes the posterior to shrink to a point.) \nIt can be hard to compute such elongated posteriors. One simple solution is to center the input data, i.e., by using $x _ { n } ^ { prime } = x _ { n } - { overline { { x } } }$ . Now the lines can pivot around the origin, reducing the posterior correlation between $w _ { 0 }$ and $w _ { 1 }$ . See Figure 11.22 for an illustration. (We may also choose to divide each $x _ { n }$ by the standard deviation of that feature, as discussed in Section 10.2.8.) \n\nNote that we can convert the posterior derived from fitting to the centered data back to the original coordinates by noting that \nThus the parameters on the uncentered data are $w _ { 0 } = w _ { 0 } ^ { prime } - w _ { 1 } ^ { prime } overline { { x } }$ and $w _ { 1 } = w _ { 1 } ^ { prime }$ . \n11.7.6 Dealing with multicollinearity \nIn many datasets, the input variables can be highly correlated with each other. Including all of them does not generally harm predictive accuracy (provided you use a suitable prior or regularizer to prevent overfitting). However, it can make interpretation of the coefficients more difficult. \nTo illustrate this, we use a toy example from [McE20, Sec 6.1]. Suppose we have a dataset of $N$ people in which we record their heights $h _ { i }$ , as well as the length of their left legs $boldsymbol { l } _ { i }$ and right legs $r _ { i }$ . Suppose $h _ { i } sim mathcal { N } ( 1 0 , 2 )$ , so the average height is $overline { { h } } = 1 0$ (in unspecified units). Suppose the length of the legs is some fraction $rho _ { i } sim mathrm { U n i f } ( 0 . 4 , 0 . 5 )$ of the height, plus a bit of Gaussian noise, specifically $l _ { i } sim mathcal { N } ( rho _ { i } h _ { i } , 0 . 0 2 )$ and $r _ { i } sim mathcal { N } ( rho _ { i } h _ { i } , 0 . 0 2 )$ . \nNow suppose we want to predict the height of a person given measurement of their leg lengths. (I did mention this is a toy example!) Since both left and right legs are noisy measurements of the unknown quantity, it is useful to use both of them. So we use linear regression to fit $p ( h | l , r ) = mathcal { N } ( h | alpha + beta _ { l } l + beta _ { r } r , sigma ^ { 2 } )$ . We use vague priors, $alpha , beta _ { l } , beta _ { r } sim mathcal { N } ( 0 , 1 0 0 )$ , and $sigma sim mathrm { E x p o n } ( 1 )$ . \nSince the average leg length is $bar { l } = 0 . 4 5 bar { h } = 4 . 5$ , we might expect each $beta$ coefficient to be around $overline { { h } } / overline { { l } } = 1 0 / 4 . 5 = 2 . 2$ . However, the posterior marginals shown in Figure 11.23 tell a different story: we see that the posterior mean of $beta _ { l }$ is near 2.6, but $beta _ { r }$ is near -0.6. Thus it seems like the right leg feature is not needed. This is because the regression coefficient for feature $j$ encodes the value of knowing $x _ { j }$ given that all the other features $mathbf { delta } _ { mathbf { x } _ { - } j }$ are already known, as we discussed in Section 11.2.2.1. If we already know the left leg, the marginal value of also knowing the right leg is small. However, if we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license rerun this example with slightly different data, we may reach the opposite conclusion, and favor the right leg over the left.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "The advantage of centering"
    },
    {
        "content": "Note that we can convert the posterior derived from fitting to the centered data back to the original coordinates by noting that \nThus the parameters on the uncentered data are $w _ { 0 } = w _ { 0 } ^ { prime } - w _ { 1 } ^ { prime } overline { { x } }$ and $w _ { 1 } = w _ { 1 } ^ { prime }$ . \n11.7.6 Dealing with multicollinearity \nIn many datasets, the input variables can be highly correlated with each other. Including all of them does not generally harm predictive accuracy (provided you use a suitable prior or regularizer to prevent overfitting). However, it can make interpretation of the coefficients more difficult. \nTo illustrate this, we use a toy example from [McE20, Sec 6.1]. Suppose we have a dataset of $N$ people in which we record their heights $h _ { i }$ , as well as the length of their left legs $boldsymbol { l } _ { i }$ and right legs $r _ { i }$ . Suppose $h _ { i } sim mathcal { N } ( 1 0 , 2 )$ , so the average height is $overline { { h } } = 1 0$ (in unspecified units). Suppose the length of the legs is some fraction $rho _ { i } sim mathrm { U n i f } ( 0 . 4 , 0 . 5 )$ of the height, plus a bit of Gaussian noise, specifically $l _ { i } sim mathcal { N } ( rho _ { i } h _ { i } , 0 . 0 2 )$ and $r _ { i } sim mathcal { N } ( rho _ { i } h _ { i } , 0 . 0 2 )$ . \nNow suppose we want to predict the height of a person given measurement of their leg lengths. (I did mention this is a toy example!) Since both left and right legs are noisy measurements of the unknown quantity, it is useful to use both of them. So we use linear regression to fit $p ( h | l , r ) = mathcal { N } ( h | alpha + beta _ { l } l + beta _ { r } r , sigma ^ { 2 } )$ . We use vague priors, $alpha , beta _ { l } , beta _ { r } sim mathcal { N } ( 0 , 1 0 0 )$ , and $sigma sim mathrm { E x p o n } ( 1 )$ . \nSince the average leg length is $bar { l } = 0 . 4 5 bar { h } = 4 . 5$ , we might expect each $beta$ coefficient to be around $overline { { h } } / overline { { l } } = 1 0 / 4 . 5 = 2 . 2$ . However, the posterior marginals shown in Figure 11.23 tell a different story: we see that the posterior mean of $beta _ { l }$ is near 2.6, but $beta _ { r }$ is near -0.6. Thus it seems like the right leg feature is not needed. This is because the regression coefficient for feature $j$ encodes the value of knowing $x _ { j }$ given that all the other features $mathbf { delta } _ { mathbf { x } _ { - } j }$ are already known, as we discussed in Section 11.2.2.1. If we already know the left leg, the marginal value of also knowing the right leg is small. However, if we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license rerun this example with slightly different data, we may reach the opposite conclusion, and favor the right leg over the left. \n\nWe can gain more insight by looking at the joint distribution $p ( beta _ { l } , beta _ { r } | mathcal { D } )$ , shown in Figure 11.24a. We see that the parameters are very highly correlated, so if $beta _ { r }$ is large, then $beta _ { l }$ is small, and vice versa. The marginal distribution for each parameter does not capture this. However, it does show that there is a lot of uncertainty about each parameter, showing that they are non-identifiable. However, their sum is well-determined, as can be seen from Figure 11.24b, where we plot $p ( beta _ { l } + beta _ { r } | mathcal { D } )$ ; this is centered on 2.2, as we might expect. \nThis example goes to show that we must be careful trying to interpret the significance of individua coefficient estimates in a model, since they do not mean much in isolation. \n11.7.7 Automatic relevancy determination (ARD) * \nConsider a linear regression model with known observation noise but unknown regression weights, $mathcal { N } ( pmb { y } | mathbf { X } pmb { w } , sigma ^ { 2 } mathbf { I } )$ . Suppose we use a Gaussian prior for the weights, $w _ { j } sim mathcal { N } ( 0 , 1 / alpha _ { j } )$ , where $alpha _ { j }$ is the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "Dealing with multicollinearity"
    },
    {
        "content": "We can gain more insight by looking at the joint distribution $p ( beta _ { l } , beta _ { r } | mathcal { D } )$ , shown in Figure 11.24a. We see that the parameters are very highly correlated, so if $beta _ { r }$ is large, then $beta _ { l }$ is small, and vice versa. The marginal distribution for each parameter does not capture this. However, it does show that there is a lot of uncertainty about each parameter, showing that they are non-identifiable. However, their sum is well-determined, as can be seen from Figure 11.24b, where we plot $p ( beta _ { l } + beta _ { r } | mathcal { D } )$ ; this is centered on 2.2, as we might expect. \nThis example goes to show that we must be careful trying to interpret the significance of individua coefficient estimates in a model, since they do not mean much in isolation. \n11.7.7 Automatic relevancy determination (ARD) * \nConsider a linear regression model with known observation noise but unknown regression weights, $mathcal { N } ( pmb { y } | mathbf { X } pmb { w } , sigma ^ { 2 } mathbf { I } )$ . Suppose we use a Gaussian prior for the weights, $w _ { j } sim mathcal { N } ( 0 , 1 / alpha _ { j } )$ , where $alpha _ { j }$ is the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nprecision of the $j$ ’th parameter. Now suppose we estimate the prior precisions as follows: \nwhere \nis the marginal likelihood. This is an example of empirical Bayes, since we are estimating the prior from data. We can view this as a computational shortcut to a fully Bayesian approach. However, there are additional advantages. In particular, suppose, after estimating $pmb { alpha }$ , we compute the MAP estimate \nThis results in a sparse estimate for $hat { pmb { w } }$ , which is perhaps surprising given that the Gaussian prior for $mathbf { boldsymbol { w } }$ is not sparsity promoting. The reasons for this are explained in the sequel to this book. \nThis technique is known as sparse Bayesian learning [Tip01] or automatic relevancy determination (ARD) [Mac95; Nea96]. It was originally developed for neural networks (where sparsity is applied to the first layer weights), but here we apply it to linear models. See also Section 17.4.1, where we apply it kernelized linear models. \n11.8 Exercises \nExercise 11.1 [Multi-output linear regression *] \n(Source: Jaakkola.) \nConsider a linear regression model with a 2 dimensional response vector $pmb { y } _ { i } in mathbb { R } ^ { 2 }$ . Suppose we have some binary input data, $x _ { i } in { 0 , 1 }$ . The training data is as follows: \nLet us embed each $x _ { i }$ into 2d using the following basis function: \nThe model becomes \nwhere $mathbf { W }$ is a $2 times 2$ matrix. Compute the MLE for $mathbf { W }$ from the above data. \nExercise 11.2 [Centering and ridge regression] Assume that $overline { { boldsymbol { x } } } = 0$ , so the input data has been centered. Show that the optimizer of \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Bayesian linear regression *",
        "subsubsection": "Automatic relevancy determination (ARD) *"
    },
    {
        "content": "precision of the $j$ ’th parameter. Now suppose we estimate the prior precisions as follows: \nwhere \nis the marginal likelihood. This is an example of empirical Bayes, since we are estimating the prior from data. We can view this as a computational shortcut to a fully Bayesian approach. However, there are additional advantages. In particular, suppose, after estimating $pmb { alpha }$ , we compute the MAP estimate \nThis results in a sparse estimate for $hat { pmb { w } }$ , which is perhaps surprising given that the Gaussian prior for $mathbf { boldsymbol { w } }$ is not sparsity promoting. The reasons for this are explained in the sequel to this book. \nThis technique is known as sparse Bayesian learning [Tip01] or automatic relevancy determination (ARD) [Mac95; Nea96]. It was originally developed for neural networks (where sparsity is applied to the first layer weights), but here we apply it to linear models. See also Section 17.4.1, where we apply it kernelized linear models. \n11.8 Exercises \nExercise 11.1 [Multi-output linear regression *] \n(Source: Jaakkola.) \nConsider a linear regression model with a 2 dimensional response vector $pmb { y } _ { i } in mathbb { R } ^ { 2 }$ . Suppose we have some binary input data, $x _ { i } in { 0 , 1 }$ . The training data is as follows: \nLet us embed each $x _ { i }$ into 2d using the following basis function: \nThe model becomes \nwhere $mathbf { W }$ is a $2 times 2$ matrix. Compute the MLE for $mathbf { W }$ from the above data. \nExercise 11.2 [Centering and ridge regression] Assume that $overline { { boldsymbol { x } } } = 0$ , so the input data has been centered. Show that the optimizer of \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nis \nExercise 11.3 [Partial derivative of the RSS *] \nLet $R S S ( pmb { w } ) = | | mathbf { X } pmb { w } - pmb { y } | | _ { 2 } ^ { 2 }$ be the residual sum of squares. \na. Show that \nwhere ${ pmb w } _ { - k } = { pmb w }$ without component $k$ , ${ bf { x } } _ { i , - k }$ is ${ pmb x } _ { i }$ without component $k$ , and $pmb { r } _ { k } = pmb { y } - pmb { w } _ { - k } ^ { T } pmb { x } _ { : , - k }$ is the residual due to using all the features except feature $k$ . Hint: Partition the weights into those involving $k$ and those not involving $k$ . \nb. Show that if $begin{array} { r } { frac { partial } { partial w _ { k } } R S S ( { pmb w } ) = 0 } end{array}$ , then \nHence when we sequentially add features, the optimal weight for feature $k$ is computed by computing orthogonally projecting $scriptstyle { pmb { x } } _ { : , k }$ onto the current residual. \nExercise 11.4 [Reducing elastic net to lasso] \nDefine \nand \nwhere $c = ( 1 + lambda _ { 2 } ) ^ { - frac { 1 } { 2 } }$ and \nShow \ni.e. \nand hence that one can solve an elastic net problem using a lasso solver on modified data. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nExercise 11.5 [Shrinkage in linear regression $^ * ]$ \n(Source: Jaakkola.) Consider performing linear regression with an orthonormal design matrix, so $| | pmb { x } _ { : , k } | | _ { 2 } ^ { 2 } = 1$ for each column (feature) $k$ , and $pmb { x } _ { : , k } ^ { T } pmb { x } _ { : , j } = 0$ , so we can estimate each parameter $w _ { k }$ separately. \nFigure 10.15b plots $hat { w } _ { k }$ vs $c _ { k } = 2 y ^ { T } x _ { : , k }$ , the correlation of feature $k$ with the response, for 3 different estimation methods: ordinary least squares (OLS), ridge regression with parameter $lambda _ { 2 }$ , and lasso with parameter $lambda _ { 1 }$ . \na. Unfortunately we forgot to label the plots. Which method does the solid (1), dotted (2) and dashed (3) line correspond to?   \nb. What is the value of $lambda _ { 1 }$ ?   \nc. What is the value of $lambda _ { 2 }$ ? \nExercise 11.6 [EM for mixture of linear regression experts] Derive the EM equations for fitting a mixture of linear regression experts. \n12 Generalized Linear Models * \n12.1 Introduction \nIn Chapter 10, we discussed logistic regression, which, in the binary case, corresponds to the model $p ( y | mathbf x , pmb w ) = mathrm { B e r } ( y | sigma ( pmb w ^ { top } pmb x ) )$ . In Chapter 11, we discussed linear regression, which corresponds to the model $p ( y | mathbf { x } , pmb { w } ) = mathcal { N } ( y | pmb { w } ^ { prime } mathbf { x } , sigma ^ { 2 } )$ . These are obviously very similar to each other. In particular, the mean of the output, $mathbb { E } left[ boldsymbol { y } | boldsymbol { x } , boldsymbol { w } right]$ , is a linear function of the inputs $_ { x }$ in both cases. \nIt turns out that there is a broad family of models with this property, known as generalized linear models or GLMs [MN89]. \nA GLM is a conditional version of an exponential family distribution (Section 3.4), in which the natural parameters are a linear function of the input. More precisely, the model has the following form: \nwhere $eta _ { n } triangleq w ^ { mathsf { I } } pmb { x } _ { n }$ is the (input dependent) natural parameter, $A ( eta _ { n } )$ is the log normalizer, $boldsymbol { mathcal { T } } ( boldsymbol { y } ) = boldsymbol { y }$ is the sufficient statistic, and $sigma ^ { 2 }$ is the dispersion term.1 \nWe will denote the mapping from the linear inputs to the mean of the output using $mu _ { n } = ell ^ { - 1 } ( eta _ { n } )$ where the function $ell$ is known as the link function, and $ell ^ { - 1 }$ is known as the mean function. \nBased on the results in Section 3.4.3, we can show that the mean and variance of the response variable are as follows: \n12.2 Examples \nIn this section, we give some examples of widely used GLMs.",
        "chapter": "II Linear Models",
        "section": "Linear Regression",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "12 Generalized Linear Models * \n12.1 Introduction \nIn Chapter 10, we discussed logistic regression, which, in the binary case, corresponds to the model $p ( y | mathbf x , pmb w ) = mathrm { B e r } ( y | sigma ( pmb w ^ { top } pmb x ) )$ . In Chapter 11, we discussed linear regression, which corresponds to the model $p ( y | mathbf { x } , pmb { w } ) = mathcal { N } ( y | pmb { w } ^ { prime } mathbf { x } , sigma ^ { 2 } )$ . These are obviously very similar to each other. In particular, the mean of the output, $mathbb { E } left[ boldsymbol { y } | boldsymbol { x } , boldsymbol { w } right]$ , is a linear function of the inputs $_ { x }$ in both cases. \nIt turns out that there is a broad family of models with this property, known as generalized linear models or GLMs [MN89]. \nA GLM is a conditional version of an exponential family distribution (Section 3.4), in which the natural parameters are a linear function of the input. More precisely, the model has the following form: \nwhere $eta _ { n } triangleq w ^ { mathsf { I } } pmb { x } _ { n }$ is the (input dependent) natural parameter, $A ( eta _ { n } )$ is the log normalizer, $boldsymbol { mathcal { T } } ( boldsymbol { y } ) = boldsymbol { y }$ is the sufficient statistic, and $sigma ^ { 2 }$ is the dispersion term.1 \nWe will denote the mapping from the linear inputs to the mean of the output using $mu _ { n } = ell ^ { - 1 } ( eta _ { n } )$ where the function $ell$ is known as the link function, and $ell ^ { - 1 }$ is known as the mean function. \nBased on the results in Section 3.4.3, we can show that the mean and variance of the response variable are as follows: \n12.2 Examples \nIn this section, we give some examples of widely used GLMs.",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "12.2.1 Linear regression \nRecall that linear regression has the form \nHence \nwhere $eta _ { n } = { pmb w } ^ { 1 } { pmb x } _ { n }$ . We can write this in GLM form as follows: \nWe see that $A ( eta _ { n } ) = eta _ { n } ^ { 2 } / 2$ and hence \n12.2.2 Binomial regression \nIf the response variable is the number of successes in $N _ { n }$ trials, $y _ { n } in { 0 , ldots , N _ { n } }$ , we can use binomial regression, which is defined by \nWe see that binary logistic regression is the special case when $N _ { n } = 1$ . The log pdf is given by \nwhere $mu _ { n } = sigma ( eta _ { n } )$ . To rewrite this in GLM form, let us define \nHence we can write binomial regression in GLM form as follows \nwhere $h ( y _ { n } ) = log binom { N _ { n } } { y _ { n } }$ and \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "Examples",
        "subsubsection": "Linear regression"
    },
    {
        "content": "12.2.1 Linear regression \nRecall that linear regression has the form \nHence \nwhere $eta _ { n } = { pmb w } ^ { 1 } { pmb x } _ { n }$ . We can write this in GLM form as follows: \nWe see that $A ( eta _ { n } ) = eta _ { n } ^ { 2 } / 2$ and hence \n12.2.2 Binomial regression \nIf the response variable is the number of successes in $N _ { n }$ trials, $y _ { n } in { 0 , ldots , N _ { n } }$ , we can use binomial regression, which is defined by \nWe see that binary logistic regression is the special case when $N _ { n } = 1$ . The log pdf is given by \nwhere $mu _ { n } = sigma ( eta _ { n } )$ . To rewrite this in GLM form, let us define \nHence we can write binomial regression in GLM form as follows \nwhere $h ( y _ { n } ) = log binom { N _ { n } } { y _ { n } }$ and \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHence \nand \n12.2.3 Poisson regression \nIf the response variable is an integer count, $y _ { n } in { 0 , 1 , . . . }$ , we can use Poisson regression, which is defined by \nwhere \nis the Poisson distribution. Poisson regression is widely used in bio-statistical applications, where $y _ { n }$ might represent the number of diseases of a given person or place, or the number of reads at a genomic location in a high-throughput sequencing context (see e.g., [Kua+09]). \nThe log pdf is given by \nwhere $mu _ { n } = exp ( w ^ { boldsymbol { mathsf { I } } } mathbf { x } _ { n } )$ . Hence in GLM form we have \nwhere $eta _ { n } = log ( mu _ { n } ) = w ^ { mathsf { I } } x _ { n }$ , $A ( eta _ { n } ) = mu _ { n } = e ^ { eta _ { n } }$ , and $h ( y _ { n } ) = - log ( y _ { n } ! )$ . Hence \nand \n12.3 GLMs with non-canonical link functions \nWe have seen how the mean parameters of the output distribution are given by $mu = ell ^ { - 1 } ( eta )$ , where the function $ell$ is the link function. There are several choices for this function, as we now discuss. \nThe canonical link function $ell$ satisfies the property that $theta = ell ( mu )$ , where $theta$ are the canonical (natural) parameters. Hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "Examples",
        "subsubsection": "Binomial regression"
    },
    {
        "content": "Hence \nand \n12.2.3 Poisson regression \nIf the response variable is an integer count, $y _ { n } in { 0 , 1 , . . . }$ , we can use Poisson regression, which is defined by \nwhere \nis the Poisson distribution. Poisson regression is widely used in bio-statistical applications, where $y _ { n }$ might represent the number of diseases of a given person or place, or the number of reads at a genomic location in a high-throughput sequencing context (see e.g., [Kua+09]). \nThe log pdf is given by \nwhere $mu _ { n } = exp ( w ^ { boldsymbol { mathsf { I } } } mathbf { x } _ { n } )$ . Hence in GLM form we have \nwhere $eta _ { n } = log ( mu _ { n } ) = w ^ { mathsf { I } } x _ { n }$ , $A ( eta _ { n } ) = mu _ { n } = e ^ { eta _ { n } }$ , and $h ( y _ { n } ) = - log ( y _ { n } ! )$ . Hence \nand \n12.3 GLMs with non-canonical link functions \nWe have seen how the mean parameters of the output distribution are given by $mu = ell ^ { - 1 } ( eta )$ , where the function $ell$ is the link function. There are several choices for this function, as we now discuss. \nThe canonical link function $ell$ satisfies the property that $theta = ell ( mu )$ , where $theta$ are the canonical (natural) parameters. Hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "Examples",
        "subsubsection": "Poisson regression"
    },
    {
        "content": "Hence \nand \n12.2.3 Poisson regression \nIf the response variable is an integer count, $y _ { n } in { 0 , 1 , . . . }$ , we can use Poisson regression, which is defined by \nwhere \nis the Poisson distribution. Poisson regression is widely used in bio-statistical applications, where $y _ { n }$ might represent the number of diseases of a given person or place, or the number of reads at a genomic location in a high-throughput sequencing context (see e.g., [Kua+09]). \nThe log pdf is given by \nwhere $mu _ { n } = exp ( w ^ { boldsymbol { mathsf { I } } } mathbf { x } _ { n } )$ . Hence in GLM form we have \nwhere $eta _ { n } = log ( mu _ { n } ) = w ^ { mathsf { I } } x _ { n }$ , $A ( eta _ { n } ) = mu _ { n } = e ^ { eta _ { n } }$ , and $h ( y _ { n } ) = - log ( y _ { n } ! )$ . Hence \nand \n12.3 GLMs with non-canonical link functions \nWe have seen how the mean parameters of the output distribution are given by $mu = ell ^ { - 1 } ( eta )$ , where the function $ell$ is the link function. There are several choices for this function, as we now discuss. \nThe canonical link function $ell$ satisfies the property that $theta = ell ( mu )$ , where $theta$ are the canonical (natural) parameters. Hence \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis is what we have assumed so far. For example, for the Bernoulli distribution, the canonical parameter is the log-odds $theta = log ( mu / ( 1 - mu ) )$ , which is given by the logit transform \nThe inverse of this is the sigmoid or logistic function $mu = sigma ( theta ) = 1 / ( 1 + e ^ { - theta } )$ . \nHowever, we are free to use other kinds of link function. For example, the probit link function has the form \nAnother link function that is sometimes used for binary responses is the complementary log-log function \nThis is used in applications where we either observe 0 events (denoted by $y = 0$ ) or one or more (denoted by $y = 1$ ), where events are assumed to be governed by a Poisson distribution with rate $lambda$ . Let $E$ be the number of events. The Poisson assumption means $p ( E = 0 ) = exp ( - lambda )$ and hence \nThus $lambda = - log ( 1 - mu )$ . When $lambda$ is a function of covariates, we need to ensure it is positive, so we use $lambda = e ^ { eta }$ , and hence \n12.4 Maximum likelihood estimation \nGLMs can be fit using similar methods to those that we used to fit logistic regression. In particular, the negative log-likelihood has the following form (ignoring constant terms): \nwhere \nwhere $eta _ { n } = { pmb w } ^ { vert } { pmb x } _ { n }$ . For notational simplicity, we will assume $sigma ^ { 2 } = 1$ . \nWe can compute the gradient for a single term as follows: \nwhere $mu _ { n } = f ( { pmb w } ^ { 1 } { pmb x } )$ , and $f$ is the inverse link function that maps from canonical parameters to mean parameters. For example, in the case of logistic regression, $f ( eta _ { n } ) = sigma ( eta _ { n } )$ , so we recover \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "GLMs with non-canonical link functions",
        "subsubsection": "N/A"
    },
    {
        "content": "This is what we have assumed so far. For example, for the Bernoulli distribution, the canonical parameter is the log-odds $theta = log ( mu / ( 1 - mu ) )$ , which is given by the logit transform \nThe inverse of this is the sigmoid or logistic function $mu = sigma ( theta ) = 1 / ( 1 + e ^ { - theta } )$ . \nHowever, we are free to use other kinds of link function. For example, the probit link function has the form \nAnother link function that is sometimes used for binary responses is the complementary log-log function \nThis is used in applications where we either observe 0 events (denoted by $y = 0$ ) or one or more (denoted by $y = 1$ ), where events are assumed to be governed by a Poisson distribution with rate $lambda$ . Let $E$ be the number of events. The Poisson assumption means $p ( E = 0 ) = exp ( - lambda )$ and hence \nThus $lambda = - log ( 1 - mu )$ . When $lambda$ is a function of covariates, we need to ensure it is positive, so we use $lambda = e ^ { eta }$ , and hence \n12.4 Maximum likelihood estimation \nGLMs can be fit using similar methods to those that we used to fit logistic regression. In particular, the negative log-likelihood has the following form (ignoring constant terms): \nwhere \nwhere $eta _ { n } = { pmb w } ^ { vert } { pmb x } _ { n }$ . For notational simplicity, we will assume $sigma ^ { 2 } = 1$ . \nWe can compute the gradient for a single term as follows: \nwhere $mu _ { n } = f ( { pmb w } ^ { 1 } { pmb x } )$ , and $f$ is the inverse link function that maps from canonical parameters to mean parameters. For example, in the case of logistic regression, $f ( eta _ { n } ) = sigma ( eta _ { n } )$ , so we recover \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nEquation (10.21). This gradient expression can be used inside SGD, or some other gradient method, in the obvious way. \nThe Hessian is given by \nwhere \nHence \nFor example, in the case of logistic regression, $f ( eta _ { n } ) = sigma ( eta _ { n } )$ , and $f ^ { prime } ( eta _ { n } ) = sigma ( eta _ { n } ) ( 1 - sigma ( eta _ { n } ) )$ , so we recover Equation (10.23). In general, we see that the Hessian is positive definite, since $f ^ { prime } ( eta _ { n } ) > 0$ ; hence the negative log likelihood is convex, so the MLE for a GLM is unique (assuming $f ( eta _ { n } ) > 0$ for all $n$ ). \nBased on the above results, we can fit GLMs using gradient based solvers in a manner that is very similar to how we fit logistic regression models. \n12.5 Worked example: predicting insurance claims \nIn this section, we give an example of predicting insurance claims using linear and Poisson regression.2. The goal is to predict the expected number of insurance claims per year following car accidents. The dataset consists of 678k examples with 9 features, such as driver age, vehicle age, vehicle power,",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "Maximum likelihood estimation",
        "subsubsection": "N/A"
    },
    {
        "content": "Equation (10.21). This gradient expression can be used inside SGD, or some other gradient method, in the obvious way. \nThe Hessian is given by \nwhere \nHence \nFor example, in the case of logistic regression, $f ( eta _ { n } ) = sigma ( eta _ { n } )$ , and $f ^ { prime } ( eta _ { n } ) = sigma ( eta _ { n } ) ( 1 - sigma ( eta _ { n } ) )$ , so we recover Equation (10.23). In general, we see that the Hessian is positive definite, since $f ^ { prime } ( eta _ { n } ) > 0$ ; hence the negative log likelihood is convex, so the MLE for a GLM is unique (assuming $f ( eta _ { n } ) > 0$ for all $n$ ). \nBased on the above results, we can fit GLMs using gradient based solvers in a manner that is very similar to how we fit logistic regression models. \n12.5 Worked example: predicting insurance claims \nIn this section, we give an example of predicting insurance claims using linear and Poisson regression.2. The goal is to predict the expected number of insurance claims per year following car accidents. The dataset consists of 678k examples with 9 features, such as driver age, vehicle age, vehicle power, \nTable 12.1: Performance metrics on the test set. MSE = mean squared error. MAE = mean absolute error.   \nDeviance = Poisson deviance. \netc. The target is the frequency of claims, which is the number of claims per policy divided by the exposure (i.e., the duration of the policy in years). \nWe plot the test set in Figure 12.1(a). We see that for 94% of the policies, no claims are made, so the data has lots of 0s, as is typical for count and rate data. The average frequency of claims is $1 0 %$ . This can be converted into a dummy model, which always predicts this constant. This results in the predictions shown in Figure 12.1(b). The goal is to do better than this. \nA simple approach is to use linear regression, combined with some simple feature engineering (binning the continuous values, and one-hot encoding the categoricals). (We use a small amount of $ell _ { 2 }$ regularization, so technically this is ridge regression.) This gives the results shown in Figure 12.1(c). This is better than the baseline, but still not very good. In particular, it can predict negative outcomes, and fails to capture the long tail. \nWe can do better using Poisson regression, using the same features but a log link function. The results are shown in Figure 12.1(d). We see that predictions are much better. \nAn interesting question is how to quantify performance in this kind of problem. If we use mean squared error, or mean absolute error, we may conclude from Table 12.1 that ridge regression is better than Poisson regression, but this is clearly not true, as shown in Figure 12.1. Instead it is more common to measure performance using the deviance, which is defined as \nwhere $mu _ { i }$ is the predicted parameters for the $i$ ’th example (based on the input features ${ boldsymbol { mathbf { mathit { x } } } } _ { i }$ and the training set $mathcal { D }$ ), and $mu _ { i } ^ { * }$ is the optimal parameter estimated by fitting the model just to the true output $y _ { i }$ . (This is the so-called saturated model, that perfectly fits the test set.) In the case of Poisson regression, we have $mu _ { i } ^ { * } = y _ { i }$ . Hence \nBy this metric, the Poisson model is clearly better (see last column of Table 12.1). \nWe can also compute a calibration plot, which plots the actual frequency vs the predicted frequency. To compute this, we bin the predictions into intervals, and then count the empirical frequency of claims for all examples whose predicted frequency falls into that bin. The results are shown in Figure 12.2. We see that the constant baseline is well calibrated, but of course it is not very accurate. The ridge model is miscalibrated in the low frequency regime. In particular, it \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 underestimates the total number of claims in the test set to be 10,693, whereas the truth is 11,935. The Poisson model is better calibrated (i.e., when it predicts examples will have a high claim rate, they do in fact have a high claim rate), and it predicts the total number of claims to be 11,930. \n\nPart III \nDeep Neural Networks",
        "chapter": "II Linear Models",
        "section": "Generalized Linear Models *",
        "subsection": "Worked example: predicting insurance claims",
        "subsubsection": "N/A"
    },
    {
        "content": "13 Neural Networks for Tabular Data \n13.1 Introduction \nIn Part II, we discussed linear models for regression and classification. In particular, in Chapter 10, we discussed logistic regression, which, in the binary case, corresponds to the model $p ( y | mathbf { boldsymbol { x } } , mathbf { boldsymbol { w } } ) = mathrm { B e r } ( y | sigma ( mathbf { boldsymbol { w } } ^ { mathrm { { I } } } mathbf { boldsymbol { x } } ) )$ , and in the multiclass case corresponds to the model $p ( y | mathbf { x } , mathbf { W } ) =$ $mathrm { C a t } ( y | mathrm { s o f t m a x } ( mathbf { W } x ) )$ . In Chapter 11, we discussed linear regression, which corresponds to the model $p ( y | mathbf { x } , pmb { w } ) = mathcal { N } ( y | pmb { w } ^ { mathrm { { I } } } mathbf { x } , sigma ^ { 2 } )$ . And in Chapter 12, we discussed generalized linear models, which generalizes these models to other kinds of output distributions, such as Poisson. However, all these models make the strong assumption that the input-output mapping is linear. \nA simple way of increasing the flexibility of such models is to perform a feature transformation, by replacing $_ { x }$ with $phi ( { pmb x } )$ . For example, we can use a polynomial transform, which in 1d is given by $phi ( x ) = [ 1 , x , x ^ { 2 } , x ^ { 3 } , . . . ]$ , as we discussed in Section 1.2.2.2. This is sometimes called basis function expansion. The model now becomes \nThis is still linear in the parameters $pmb theta = ( mathbf W , pmb b )$ , which makes model fitting easy (since the negative log-likelihood is convex). However, having to specify the feature transformation by hand is very limiting. \nA natural extension is to endow the feature extractor with its own parameters, $pmb { theta } _ { 2 }$ , to get \nwhere $pmb { theta } = ( theta _ { 1 } , theta _ { 2 } )$ and $pmb { theta } _ { 1 } = ( mathbf { W } , pmb { b } )$ . We can obviously repeat this process recursively, to create more and more complex functions. If we compose $L$ functions, we get \nwhere $f _ { ell } ( pmb { x } ) = f ( pmb { x } ; pmb { theta } _ { ell } )$ is the function at layer $ell$ . This is the key idea behind deep neural networks or DNNs. \nThe term “DNN” actually encompasses a larger family of models, in which we compose differentiable functions into any kind of DAG (directed acyclic graph), mapping input to output. Equation (13.3) is the simplest example where the DAG is a chain. This is known as a feedforward neural network (FFNN) or multilayer perceptron (MLP). \nAn MLP assumes that the input is a fixed-dimensional vector, say $pmb { x } in mathbb { R } ^ { D }$ . It is common to call such data “structured data” or “tabular data”, since the data is often stored in an $N times D$ design matrix, where each column (feature) has a specific meaning, such as height, weight, age, etc. In later chapters, we discuss other kinds of DNNs that are more suited to “unstructured data” such as images and text, where the input data is variable sized, and each individual element (e.g., pixel or word) is often meaningless on its own.1 In particular, in Chapter 14, we discuss convolutional neural networks (CNN), which are designed to work with images; in Chapter 15, we discuss recurrent neural networks (RNN) and transformers, which are designed to work with sequences; and in Chapter 23, we discuss graph neural networks (GNN), which are designed to work with graphs. \n\nAlthough DNNs can work well, there are often a lot of engineering details that need to be addressed to get good performance. Some of these details are discussed in the supplementary material to this book, available at probml.ai. There are also various other books that cover this topic in more depth (e.g., [Zha+20; Cho21; Gér19; GBC16]), as well as a multitude of online courses. For a more theoretical treatment, see e.g., [Ber+21; Cal20; Aro+21; RY21]. \n13.2 Multilayer perceptrons (MLPs) \nIn Section 10.2.5, we explained that a perceptron is a deterministic version of logistic regression. Specifically, it is a mapping of the following form: \nwhere $H ( a )$ is the heaviside step function, also known as a linear threshold function. Since the decision boundaries represented by perceptrons are linear, they are very limited in what they can represent. In 1969, Marvin Minsky and Seymour Papert published a famous book called Perceptrons [MP69] in which they gave numerous examples of pattern recognition problems which perceptrons cannot solve. We give a specific example below, before discussing how to solve the problem. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "13.2.1 The XOR problem \nOne of the most famous examples from the Perceptrons book is the XOR problem. Here the goal is to learn a function that computes the exclusive OR of its two binary inputs. The truth table for this function is given in Table 13.1. We visualize this function in Figure 13.1a. It is clear that the data is not linearly separable, so a perceptron cannot represent this mapping. \nHowever, we can overcome this problem by stacking multiple perceptrons on top of each other. This is called a multilayer perceptron (MLP). For example, to solve the XOR problem, we can use the MLP shown in Figure 13.1b. This consists of 3 perceptrons, denoted $h _ { 1 }$ , $h _ { 2 }$ and $y$ . The nodes marked $x$ are inputs, and the nodes marked 1 are constant terms. The nodes $h _ { 1 }$ and $h _ { 2 }$ are called hidden units, since their values are not observed in the training data. \nThe first hidden unit computes $h _ { 1 } = x _ { 1 } wedge x _ { 2 }$ by using appropriately set weights. (Here $wedge$ is the AND operation.) In particular, it has inputs from $x _ { 1 }$ and $x _ { 2 }$ , both weighted by 1.0, but has a bias term of -1.5 (this is implemented by a “wire” with weight -1.5 coming from a dummy node whose value is fixed to 1). Thus $h _ { 1 }$ will fire iff $x _ { 1 }$ and $x _ { 2 }$ are both on, since then \nSimilarly, the second hidden unit computes $h _ { 2 } = x _ { 1 } vee x _ { 2 }$ , where ∨ is the OR operation, and the third computes the output $y = overline { { h _ { 1 } } } wedge h _ { 2 }$ , where $overline { { h } } = neg h$ is the NOT (logical negation) operation. Thus $y$ computes \nThis is equivalent to the XOR function. \nBy generalizing this example, we can show that an MLP can represent any logical function. However, we obviously want to avoid having to specify the weights and biases by hand. In the rest of this chapter, we discuss ways to learn these parameters from data. \n13.2.2 Differentiable MLPs \nThe MLP we discussed in Section 13.2.1 was defined as a stack of perceptrons, each of which involved the non-differentiable Heaviside function. This makes such models difficult to train, which is why they were never widely used. However, suppose we replace the Heaviside function $H : mathbb { R }  { 0 , 1 }$ with a differentiable activation function $varphi : mathbb { R }  mathbb { R }$ . More precisely, we define the hidden units $z _ { l }$ at each layer $it { l }$ to be a linear transformation of the hidden units at the previous layer passed elementwise through this activation function: \nor, in scalar form, \nThe quantity that is passed to the activation function is called the pre-activations: \nso $z _ { l } = varphi _ { l } ( a _ { l } )$ . \nIf we now compose $L$ of these functions together, as in Equation (13.3), then we can compute the gradient of the output wrt the parameters in each layer using the chain rule, also known as backpropagation, as we explain in Section 13.3. (This is true for any kind of differentiable activation function, although some kinds work better than others, as we discuss in Section 13.2.3.) We can then pass the gradient to an optimizer, and thus minimize some training objective, as we discuss in Section 13.4. For this reason, the term “MLP” almost always refers to this differentiable form of the model, rather than the historical version with non-differentiable linear threshold units. \n13.2.3 Activation functions \nWe are free to use any kind of differentiable activation function we like at each layer. However, if we use a linear activation function, $varphi _ { ell } ( a ) = c _ { ell } a$ , then the whole model reduces to a regular linear model. To see this, note that Equation (13.3) becomes \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "The XOR problem"
    },
    {
        "content": "Similarly, the second hidden unit computes $h _ { 2 } = x _ { 1 } vee x _ { 2 }$ , where ∨ is the OR operation, and the third computes the output $y = overline { { h _ { 1 } } } wedge h _ { 2 }$ , where $overline { { h } } = neg h$ is the NOT (logical negation) operation. Thus $y$ computes \nThis is equivalent to the XOR function. \nBy generalizing this example, we can show that an MLP can represent any logical function. However, we obviously want to avoid having to specify the weights and biases by hand. In the rest of this chapter, we discuss ways to learn these parameters from data. \n13.2.2 Differentiable MLPs \nThe MLP we discussed in Section 13.2.1 was defined as a stack of perceptrons, each of which involved the non-differentiable Heaviside function. This makes such models difficult to train, which is why they were never widely used. However, suppose we replace the Heaviside function $H : mathbb { R }  { 0 , 1 }$ with a differentiable activation function $varphi : mathbb { R }  mathbb { R }$ . More precisely, we define the hidden units $z _ { l }$ at each layer $it { l }$ to be a linear transformation of the hidden units at the previous layer passed elementwise through this activation function: \nor, in scalar form, \nThe quantity that is passed to the activation function is called the pre-activations: \nso $z _ { l } = varphi _ { l } ( a _ { l } )$ . \nIf we now compose $L$ of these functions together, as in Equation (13.3), then we can compute the gradient of the output wrt the parameters in each layer using the chain rule, also known as backpropagation, as we explain in Section 13.3. (This is true for any kind of differentiable activation function, although some kinds work better than others, as we discuss in Section 13.2.3.) We can then pass the gradient to an optimizer, and thus minimize some training objective, as we discuss in Section 13.4. For this reason, the term “MLP” almost always refers to this differentiable form of the model, rather than the historical version with non-differentiable linear threshold units. \n13.2.3 Activation functions \nWe are free to use any kind of differentiable activation function we like at each layer. However, if we use a linear activation function, $varphi _ { ell } ( a ) = c _ { ell } a$ , then the whole model reduces to a regular linear model. To see this, note that Equation (13.3) becomes \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "Differentiable MLPs"
    },
    {
        "content": "Similarly, the second hidden unit computes $h _ { 2 } = x _ { 1 } vee x _ { 2 }$ , where ∨ is the OR operation, and the third computes the output $y = overline { { h _ { 1 } } } wedge h _ { 2 }$ , where $overline { { h } } = neg h$ is the NOT (logical negation) operation. Thus $y$ computes \nThis is equivalent to the XOR function. \nBy generalizing this example, we can show that an MLP can represent any logical function. However, we obviously want to avoid having to specify the weights and biases by hand. In the rest of this chapter, we discuss ways to learn these parameters from data. \n13.2.2 Differentiable MLPs \nThe MLP we discussed in Section 13.2.1 was defined as a stack of perceptrons, each of which involved the non-differentiable Heaviside function. This makes such models difficult to train, which is why they were never widely used. However, suppose we replace the Heaviside function $H : mathbb { R }  { 0 , 1 }$ with a differentiable activation function $varphi : mathbb { R }  mathbb { R }$ . More precisely, we define the hidden units $z _ { l }$ at each layer $it { l }$ to be a linear transformation of the hidden units at the previous layer passed elementwise through this activation function: \nor, in scalar form, \nThe quantity that is passed to the activation function is called the pre-activations: \nso $z _ { l } = varphi _ { l } ( a _ { l } )$ . \nIf we now compose $L$ of these functions together, as in Equation (13.3), then we can compute the gradient of the output wrt the parameters in each layer using the chain rule, also known as backpropagation, as we explain in Section 13.3. (This is true for any kind of differentiable activation function, although some kinds work better than others, as we discuss in Section 13.2.3.) We can then pass the gradient to an optimizer, and thus minimize some training objective, as we discuss in Section 13.4. For this reason, the term “MLP” almost always refers to this differentiable form of the model, rather than the historical version with non-differentiable linear threshold units. \n13.2.3 Activation functions \nWe are free to use any kind of differentiable activation function we like at each layer. However, if we use a linear activation function, $varphi _ { ell } ( a ) = c _ { ell } a$ , then the whole model reduces to a regular linear model. To see this, note that Equation (13.3) becomes \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere we dropped the bias terms for notational simplicity. For this reason, it is important to use nonlinear activation functions. \nIn the early days of neural networks, a common choice was to use a sigmoid (logistic) function, which can be seen as a smooth approximation to the Heaviside function used in a perceptron: \nHowever, as shown in Figure 13.2a, the sigmoid function saturates at 1 for large positive inputs, and at 0 for large negative inputs. Another common choice is the tanh function, which has a similar shape, but saturates at -1 and $+ 1$ . See Figure 13.2b. \nIn the saturated regimes, the gradient of the output wrt the input will be close to zero, so any gradient signal from higher layers will not be able to propagate back to earlier layers. This is called the vanishing gradient problem, and it makes it hard to train the model using gradient descent (see Section 13.4.2 for details). One of the keys to being able to train very deep models is to use non-saturating activation functions. Several different functions have been proposed. The most common is rectified linear unit or ReLU, proposed in [GBB11; KSH12]. This is defined as \nThe ReLU function simply “turns off” negative inputs, and passes positive inputs unchanged: see Figure 13.2b for a plot, and Section 13.4.3 for more details. \n13.2.4 Example models \nMLPs can be used to perform classification and regression for many kinds of data. We give some examples below. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "Activation functions"
    },
    {
        "content": "where we dropped the bias terms for notational simplicity. For this reason, it is important to use nonlinear activation functions. \nIn the early days of neural networks, a common choice was to use a sigmoid (logistic) function, which can be seen as a smooth approximation to the Heaviside function used in a perceptron: \nHowever, as shown in Figure 13.2a, the sigmoid function saturates at 1 for large positive inputs, and at 0 for large negative inputs. Another common choice is the tanh function, which has a similar shape, but saturates at -1 and $+ 1$ . See Figure 13.2b. \nIn the saturated regimes, the gradient of the output wrt the input will be close to zero, so any gradient signal from higher layers will not be able to propagate back to earlier layers. This is called the vanishing gradient problem, and it makes it hard to train the model using gradient descent (see Section 13.4.2 for details). One of the keys to being able to train very deep models is to use non-saturating activation functions. Several different functions have been proposed. The most common is rectified linear unit or ReLU, proposed in [GBB11; KSH12]. This is defined as \nThe ReLU function simply “turns off” negative inputs, and passes positive inputs unchanged: see Figure 13.2b for a plot, and Section 13.4.3 for more details. \n13.2.4 Example models \nMLPs can be used to perform classification and regression for many kinds of data. We give some examples below. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n13.2.4.1 MLP for classifying 2d data into 2 categories \nFigure 13.3 gives an illustration of an MLP with two hidden layers applied to a 2d input vector, corresponding to points in the plane, coming from two concentric circles. This model has the following form: \nHere $a _ { 3 }$ is the final logit score, which is converted to a probability via the sigmoid (logistic) function. The value $a _ { 3 }$ is computed by taking a linear combination of the 2 hidden units in layer 2, using $a _ { 3 } = { pmb w } _ { 3 } ^ { 1 } z _ { 2 } + b _ { 3 }$ . In turn, layer 2 is computed by taking a nonlinear combination of the 4 hidden units in layer 1, using $z _ { mathrm { 2 } } = varphi ( mathbf { W } _ { mathrm { 2 } } z _ { mathrm { 1 } } + b _ { mathrm { 2 } } )$ . Finally, layer 1 is computed by taking a nonlinear combination of the 2 input units, using $z _ { 1 } = varphi ( mathbf { W } _ { 1 } pmb { x } + b _ { 1 } )$ . By adjusting the parameters, $pmb { theta } = ( mathbf { W } _ { 1 } , b _ { 1 } , mathbf { W } _ { 2 } , b _ { 2 } , pmb { w } _ { 3 } , b _ { 3 } )$ , to minimize the negative log likelihood, we can fit the training data very well, despite the highly nonlinear nature of the decision boundary. (You can find an interactive version of this figure at http://playground.tensorflow.org.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nModel: \"sequential\" \nLayer (type) Output Shape Param #   \nflatten (Flatten) (None, 784) 0   \ndense (Dense) (None, 128) 100480   \ndense_1 (Dense) (None, 128) 16512   \ndense_2 (Dense) (None, 10) 1290 \nTable 13.2: Structure of the MLP used for MNIST classification. Note that 1 $0 0 , 4 8 0 = ( 7 8 4 + 1 ) times 1 2 8$ , and $1 6 , 5 1 2 = ( 1 2 8 + 1 ) times 1 2 8$ . mlp_mnist_tf.ipynb. \n13.2.4.2 MLP for image classification \nTo apply an MLP to image classification, we need to “flatten” the 2d input into 1d vector. We can then use a feedforward architecture similar to the one described in Section 13.2.4.1. For example, consider building an MLP to classifiy MNIST digits (Section 3.5.2). These are $2 8 times 2 8 = 7 8 4 -$ dimensional. If we use 2 hidden layers with 128 units each, followed by a final 10 way softmax layer, we get the model shown in Table 13.2. \nWe show some predictions from this model in Figure 13.4. We train it for just two “epochs” (passes over the dataset), but already the model is doing quite well, with a test set accuracy of $9 7 . 1 %$ . Furthermore, the errors seem sensible, e.g., 9 is mistaken as a 3. Training for more epochs can further improve test accuracy. \nIn Chapter 14 we discuss a different kind of model, called a convolutional neural network, which is better suited to images. This gets even better performance and uses fewer parameters, by exploiting prior knowledge about the spatial structure of images. By contrast, with an MLP, we can randomly shuffle (permute) the pixels without affecting the output (assuming we use the same random permutation for all inputs). \n13.2.4.3 MLP for text classification \nTo apply MLPs to text classification, we need to convert the variable-length sequence of words $pmb { v } _ { 1 } , ldots , pmb { v } _ { T }$ (where each ${ mathbf { } } _ { { mathbf { } } } mathbf { Delta } mathbf { v } _ { t }$ is a one-hot vector of length $V$ , where $V$ is the vocabulary size) into a fixed dimensional vector $_ { x }$ . The easiest way to do this is as follows. First we treat the input as an unordered bag of words (Section 1.5.4.1), $left{ pmb { v } _ { t } right}$ . The first layer of the model is a $E times V$ embedding matrix $mathbf { W } _ { 1 }$ , which converts each sparse $V$ -dimensional vector to a dense $E$ -dimensional embedding, $e _ { t } = mathbf { W } _ { 1 } mathbf { v } _ { t }$ (see Section 20.5 for more details on word embeddings). Next we convert this set of $T$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n$E$ -dimensional embeddings into a fixed-sized vector using global average pooling, $textstyle { overline { { e } } } = { frac { 1 } { T } } sum _ { t = 1 } ^ { T } e _ { t }$ . This can then be passed as input to an MLP. For example, if we use a single hidden layer, and a logistic output (for binary classification), we get \nIf we use a vocabulary size of $V = 1 0 , 0 0 0$ , an embedding size of $E = 1 6$ , and a hidden layer of size 16, we get the model shown in Table 13.3. If we apply this to the IMDB movie review sentiment classification dataset discussed in Section 1.5.2.1, we get 86% on the validation set. \nWe see from Table 13.3 that the model has a lot of parameters, which can result in overfitting, since the IMDB training set only has 25k examples. However, we also see that most of the parameters are in the embedding matrix, so instead of learning these in a supervised way, we can perform unsupervised pre-training of word embedding models, as we discuss in Section 20.5. If the embedding matrix $mathbf { W } _ { 1 }$ is fixed, we just have to fine-tune the parameters in layers 2 and 3 for this specific labeled task, which requires much less data. (See also Chapter 19, where we discuss general techniques for training with limited labeled data.) \n13.2.4.4 MLP for heteroskedastic regression \nWe can also use MLPs for regression. Figure 13.5 shows how we can make a model for heteroskedastic nonlinear regression. (The term “heteroskedastic” just means that the predicted output variance is input-dependent, as discussed in Section 2.6.3.) This function has two outputs which compute $f _ { mu } ( pmb { x } ) = mathbb { E } left[ y | pmb { x } , pmb { theta } right]$ and $f _ { sigma } ( pmb { x } ) = sqrt { mathbb { V } left[ y | pmb { x } , pmb { theta } right] }$ . We can share most of the layers (and hence parameters) between these two functions by using a common “backbone” and two output “heads”, as shown in \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nModel: \"sequential\" \nLayer (type) Output Shape Param #   \nembedding (Embedding) (None, None, 16) 160000   \nglobal_average_pooling1d (Gl (None, 16) 0   \ndense (Dense) (None, 16) 272   \ndense_1 (Dense) (None, 1) 17   \nTotal params: 160,289   \nTrainable params: 160,289   \nNon-trainable params: 0 \n\nTable 13.3: Structure of the MLP used for IMDB review classification. We use a vocabulary size of $V = 1 0 , 0 0 0$ , an embedding size of $E = 1 6$ , and a hidden layer of size 16. The embedding matrix $mathbf { W } _ { 1 }$ has size $1 0 , 0 0 0 times 1 6$ , the hidden layer (labeled “dense”) has a weight matrix $mathbf { W } _ { 2 }$ of size $1 6 times 1 6$ and bias $scriptstyle b _ { 2 }$ of size 16 (note that $1 6 times 1 6 + 1 6 = 2 7 2$ ), and the final layer (labeled “dense_1”) has a weight vector ${ pmb w } _ { 3 }$ of size 16 and a bias $b _ { 3 }$ of size 1. The global average pooling layer has no free parameters. mlp_imdb_tf.ipynb. \nFigure 13.5. For the $mu$ head, we use a linear activation, $varphi ( a ) = a$ . For the $sigma$ head, we use a softplus activation, $varphi ( a ) = sigma _ { + } ( a ) = log ( 1 + e ^ { a } )$ . If we use linear heads and a nonlinear backbone, the overall model is given by \nFigure 13.6 shows the advantage of this kind of model on a dataset where the mean grows linearly over time, with seasonal oscillations, and the variance increases quadratically. (This is a simple example of a stochastic volatility model; it can be used to model financial data, as well as the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license global temperature of the earth, which (due to climate change) is increasing in mean and in variance.) We see that a regression model where the output variance $sigma ^ { 2 }$ is treated as a fixed (input-independent) parameter will sometimes be underconfident, since it needs to adjust to the overall noise level, and cannot adapt to the noise level at each point in input space. \n\n13.2.5 The importance of depth \nOne can show that an MLP with one hidden layer is a universal function approximator, meaning it can model any suitably smooth function, given enough hidden units, to any desired level of accuracy [HSW89; Cyb89; Hor91]. Intuitively, the reason for this is that each hidden unit can specify a half plane, and a sufficiently large combination of these can “carve up” any region of space, to which we can associate any response (this is easiest to see when using piecewise linear activation functions, as shown in Figure 13.7). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "Example models"
    },
    {
        "content": "13.2.5 The importance of depth \nOne can show that an MLP with one hidden layer is a universal function approximator, meaning it can model any suitably smooth function, given enough hidden units, to any desired level of accuracy [HSW89; Cyb89; Hor91]. Intuitively, the reason for this is that each hidden unit can specify a half plane, and a sufficiently large combination of these can “carve up” any region of space, to which we can associate any response (this is easiest to see when using piecewise linear activation functions, as shown in Figure 13.7). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nHowever, various arguments, both experimental and theoretical (e.g., [Has87; Mon+14; Rag+17; Pog+17]), have shown that deep networks work better than shallow ones. The reason is that later layers can leverage the features that are learned by earlier layers; that is, the function is defined in a compositional or hierarchical way. For example, suppose we want to classify DNA strings, and the positive class is associated with the regular expression *AA??CGCG??AA*. Although we could fit this with a single hidden layer model, intuitively it will be easier to learn if the model first learns to detect the AA and CG “motifs” using the hidden units in layer 1, and then uses these features to define a simple linear classifier in layer 2, analogously to how we solved the XOR problem in Section 13.2.1. \n13.2.6 The “deep learning revolution” \nAlthough the ideas behind DNNs date back several decades, it was not until the 2010s that they started to become very widely used. The first area to adopt these methods was the field of automatic speech recognition (ASR), based on breakthrough results in [Dah+11]. This approach rapidly became the standard paradigm, and was widely adopted in academia and industry [Hin+12]. \nHowever, the moment that got the most attention was when [KSH12] showed that deep CNNs could significantly improve performance on the challenging ImageNet image classification benchmark, reducing the error rate from 26% to $1 6 %$ in a single year (see Figure 1.14b); this was a huge jump compared to the previous rate of progress of about 2% reduction per year. \nThe “explosion” in the usage of DNNs has several contributing factors. One is the availability of cheap GPUs (graphics processing units); these were originally developed to speed up image rendering for video games, but they can also massively reduce the time it takes to fit large CNNs, which involve similar kinds of matrix-vector computations. Another is the growth in large labeled datasets, which enables us to fit complex function approximators with many parameters without overfitting. (For example, ImageNet has 1.3M labeled images, and is used to fit models that have millions of parameters.) Indeed, if deep learning systems are viewed as “rockets”, then large datasets have been called the fuel.2 \nMotivated by the outstanding empirical success of DNNs, various companies started to become interested in this technology. This had led to the development of high quality open-source software libraries, such as Tensorflow (made by Google), PyTorch (made by Facebook), and MXNet (made by Amazon). These libraries support automatic differentiation (see Section 13.3) and scalable gradient-based optimization (see Section 8.4) of complex differentiable functions. We will use some of these libraries in various places throughout the book to implement a variety of models, not just DNNs.3 \nMore details on the history of the “deep learning revolution” can be found in e.g., [Sej18; Met21]. \n13.2.7 Connections with biology \nIn this section, we discuss the connections between the kinds of neural networks we have discussed above, known as artificial neural networks or ANNs, and real neural networks. The details on how real biological brains work are quite complex (see e.g., [Kan+12]), but we can give a simple “cartoon”.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "The importance of depth"
    },
    {
        "content": "However, various arguments, both experimental and theoretical (e.g., [Has87; Mon+14; Rag+17; Pog+17]), have shown that deep networks work better than shallow ones. The reason is that later layers can leverage the features that are learned by earlier layers; that is, the function is defined in a compositional or hierarchical way. For example, suppose we want to classify DNA strings, and the positive class is associated with the regular expression *AA??CGCG??AA*. Although we could fit this with a single hidden layer model, intuitively it will be easier to learn if the model first learns to detect the AA and CG “motifs” using the hidden units in layer 1, and then uses these features to define a simple linear classifier in layer 2, analogously to how we solved the XOR problem in Section 13.2.1. \n13.2.6 The “deep learning revolution” \nAlthough the ideas behind DNNs date back several decades, it was not until the 2010s that they started to become very widely used. The first area to adopt these methods was the field of automatic speech recognition (ASR), based on breakthrough results in [Dah+11]. This approach rapidly became the standard paradigm, and was widely adopted in academia and industry [Hin+12]. \nHowever, the moment that got the most attention was when [KSH12] showed that deep CNNs could significantly improve performance on the challenging ImageNet image classification benchmark, reducing the error rate from 26% to $1 6 %$ in a single year (see Figure 1.14b); this was a huge jump compared to the previous rate of progress of about 2% reduction per year. \nThe “explosion” in the usage of DNNs has several contributing factors. One is the availability of cheap GPUs (graphics processing units); these were originally developed to speed up image rendering for video games, but they can also massively reduce the time it takes to fit large CNNs, which involve similar kinds of matrix-vector computations. Another is the growth in large labeled datasets, which enables us to fit complex function approximators with many parameters without overfitting. (For example, ImageNet has 1.3M labeled images, and is used to fit models that have millions of parameters.) Indeed, if deep learning systems are viewed as “rockets”, then large datasets have been called the fuel.2 \nMotivated by the outstanding empirical success of DNNs, various companies started to become interested in this technology. This had led to the development of high quality open-source software libraries, such as Tensorflow (made by Google), PyTorch (made by Facebook), and MXNet (made by Amazon). These libraries support automatic differentiation (see Section 13.3) and scalable gradient-based optimization (see Section 8.4) of complex differentiable functions. We will use some of these libraries in various places throughout the book to implement a variety of models, not just DNNs.3 \nMore details on the history of the “deep learning revolution” can be found in e.g., [Sej18; Met21]. \n13.2.7 Connections with biology \nIn this section, we discuss the connections between the kinds of neural networks we have discussed above, known as artificial neural networks or ANNs, and real neural networks. The details on how real biological brains work are quite complex (see e.g., [Kan+12]), but we can give a simple “cartoon”.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "The ``deep learning revolution''"
    },
    {
        "content": "However, various arguments, both experimental and theoretical (e.g., [Has87; Mon+14; Rag+17; Pog+17]), have shown that deep networks work better than shallow ones. The reason is that later layers can leverage the features that are learned by earlier layers; that is, the function is defined in a compositional or hierarchical way. For example, suppose we want to classify DNA strings, and the positive class is associated with the regular expression *AA??CGCG??AA*. Although we could fit this with a single hidden layer model, intuitively it will be easier to learn if the model first learns to detect the AA and CG “motifs” using the hidden units in layer 1, and then uses these features to define a simple linear classifier in layer 2, analogously to how we solved the XOR problem in Section 13.2.1. \n13.2.6 The “deep learning revolution” \nAlthough the ideas behind DNNs date back several decades, it was not until the 2010s that they started to become very widely used. The first area to adopt these methods was the field of automatic speech recognition (ASR), based on breakthrough results in [Dah+11]. This approach rapidly became the standard paradigm, and was widely adopted in academia and industry [Hin+12]. \nHowever, the moment that got the most attention was when [KSH12] showed that deep CNNs could significantly improve performance on the challenging ImageNet image classification benchmark, reducing the error rate from 26% to $1 6 %$ in a single year (see Figure 1.14b); this was a huge jump compared to the previous rate of progress of about 2% reduction per year. \nThe “explosion” in the usage of DNNs has several contributing factors. One is the availability of cheap GPUs (graphics processing units); these were originally developed to speed up image rendering for video games, but they can also massively reduce the time it takes to fit large CNNs, which involve similar kinds of matrix-vector computations. Another is the growth in large labeled datasets, which enables us to fit complex function approximators with many parameters without overfitting. (For example, ImageNet has 1.3M labeled images, and is used to fit models that have millions of parameters.) Indeed, if deep learning systems are viewed as “rockets”, then large datasets have been called the fuel.2 \nMotivated by the outstanding empirical success of DNNs, various companies started to become interested in this technology. This had led to the development of high quality open-source software libraries, such as Tensorflow (made by Google), PyTorch (made by Facebook), and MXNet (made by Amazon). These libraries support automatic differentiation (see Section 13.3) and scalable gradient-based optimization (see Section 8.4) of complex differentiable functions. We will use some of these libraries in various places throughout the book to implement a variety of models, not just DNNs.3 \nMore details on the history of the “deep learning revolution” can be found in e.g., [Sej18; Met21]. \n13.2.7 Connections with biology \nIn this section, we discuss the connections between the kinds of neural networks we have discussed above, known as artificial neural networks or ANNs, and real neural networks. The details on how real biological brains work are quite complex (see e.g., [Kan+12]), but we can give a simple “cartoon”. \n\nWe start by considering a model of a single neuron. To a first approximation, we can say that whether neuron $k$ fires, denoted by $h _ { k } in { 0 , 1 }$ , depends on the activity of its inputs, denoted by $pmb { x } in mathbb { R } ^ { D }$ , as well as the strength of the incoming connections, which we denote by ${ pmb w } _ { k } in mathbb { R } ^ { D }$ . We can compute a weighted sum of the inputs using $a _ { k } = pmb { w } _ { k } ^ { 1 } pmb { x }$ . These weights can be viewed as “wires” connecting the inputs $x _ { d }$ to neuron $h _ { k }$ ; these are analogous to dendrites in a real neuron (see Figure 13.8). This weighted sum is then compared to a threshold, $b _ { k }$ , and if the activation exceeds the threshold, the neuron fires; this is analogous to the neuron emitting an electrical output or action potential. Thus we can model the behavior of the neuron using $h _ { k } ( pmb { x } ) = H ( pmb { w } _ { k } ^ { top } pmb { x } - b _ { k } )$ , where $H ( a ) = mathbb { I } ( a > 0 )$ is the Heaviside function. This is called the McCulloch-Pitts model of the neuron, and was proposed in 1943 [MP43]. \nWe can combine multiple such neurons together to make an ANN. The result has sometimes been viewed as a model of the brain. However, ANNs differs from biological brains in many ways, including the following: \n• Most ANNs use backpropagation to modify the strength of their connections (see Section 13.3). However, real brains do not use backprop, since there is no way to send information backwards along an axon [Ben+15b; BS16; KH19]. Instead, they use local update rules for adjusting synaptic strengths. • Most ANNs are strictly feedforward, but real brains have many feedback connections. It is believed that this feedback acts like a prior, which can be combined with bottom up likelihoods from the sensory system to compute a posterior over hidden states of the world, which can then be used for \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 optimal decision making (see e.g., [Doy+07]). \n\n• Most ANNs use simplified neurons consisting of a weighted sum passed through a nonlinearity, but real biological neurons have complex dendritic tree structures (see Figure 13.8), with complex spatio-temporal dynamics.   \n• Most ANNs are smaller in size and number of connections than biological brains (see Figure 13.9). Of course, ANNs are getting larger every week, fueled by various new hardware accelerators, such as GPUs and TPUs (tensor processing units), etc. However, even if ANNs match biological brains in terms of number of units, the comparison is misleading since the processing capability of a biological neuron is much higher than an artificial neuron (see point above).   \n• Most ANNs are designed to model a single function, such as mapping an image to a label, or a sequence of words to another sequence of words. By contrast, biological brains are very complex systems, composed of multiple specialized interacting modules, which implement different kinds of functions or behaviors such as perception, control, memory, language, etc (see e.g., [Sha88; Kan+12]). \nOf course, there are efforts to make realistic models of biological brains (e.g., the Blue Brain Project [Mar06; Yon19]). However, an interesting question is whether studying the brain at this level of detail is useful for “solving AI”. It is commonly believed that the low level details of biological brains do not matter if our goal is to build “intelligent machines”, just as aeroplanes do not flap their wings. However, presumably “AIs” will follow similar “laws of intelligence” to intelligent biological agents, just as planes and birds follow the same laws of aerodynamics. \nUnfortunately, we do not yet know what the “laws of intelligence” are, or indeed if there even are such laws. In this book we make the assumption that any intelligent agent should follow the basic \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license principles of information processing and Bayesian decision theory, which is known to be the optimal way to make decisions under uncertainty (see Section 5.1). \n\nIn practice, the optimal Bayesian approach is often computationally intractable. In the natural world, biological agents have evolved various algorithmic “shortcuts” to the optimal solution; this can explain many of the heuristics that people use in everyday reasoning [KST82; GTA00; Gri20]. As the tasks we want our machines to solve become harder, we may be able to gain insights from neuroscience and cognitive science for how to solve such tasks in an approximate way (see e.g., [MWK16; Has+17; Lak+17; HG21]). However, we should also bear in mind that AI/ML systems are increasingly used for safety-critical applications, in which we might want and expect the machine to do better than a human. In such cases, we may want more than just heuristic solutions that often work; instead we may want provably reliable methods, similar to other engineering fields (see Section 1.6.3 for further discussion). \n13.3 Backpropagation \nThis section is coauthored with Mathieu Blondel. \nIn this section, we describe the famous backpropagation algorithm, which can be used to compute the gradient of a loss function applied to the output of the network wrt the parameters in each layer. This gradient can then be passed to a gradient-based optimization algorithm, as we discuss in Section 13.4. \nThe backpropagation algorithm was originally discovered in [BH69], and independently in [Wer74]. However, it was [RHW86] that brought the algorithm to the attention of the “mainstream” ML community. See the wikipedia page $^ 4$ for more historical details. \nWe initially assume the computation graph is a simple linear chain of stacked layers, as in an MLP. In this case, backprop is equivalent to repeated applications of the chain rule of calculus (see Equation (7.261)). However, the method can be generalized to arbitrary directed acyclic graphs (DAGs), as we discuss in Section 13.3.4. This general procedure is often called automatic differentiation or autodiff. \n13.3.1 Forward vs reverse mode differentiation \nConsider a mapping of the form $mathbf { delta } _ { mathbf { boldsymbol { o } } } = mathbf { mathcal { f } } ( mathbf { boldsymbol { x } } )$ , where $pmb { x } in mathbb { R } ^ { n }$ and $pmb { o } in mathbb { R } ^ { r n }$ . We assume that $f$ is defined as a composition of functions: \nwhere $f _ { 1 } : mathbb { R } ^ { n } to mathbb { R } ^ { m _ { 1 } }$ , $f _ { 2 } : mathbb { R } ^ { m _ { 1 } } to mathbb { R } ^ { m _ { 2 } }$ , $f _ { 3 } : mathbb { R } ^ { m _ { 2 } } to mathbb { R } ^ { m _ { 3 } }$ , and $f _ { 4 } : mathbb { R } ^ { m _ { 3 } }  mathbb { R } ^ { m }$ . The intermediate steps needed to compute $mathbf { delta } _ { mathbf { boldsymbol { o } } } = mathbf { mathcal { f } } ( mathbf { boldsymbol { x } } )$ are ${ pmb x } _ { 2 } = { pmb f } _ { 1 } ( { pmb x } )$ , ${ pmb x } _ { 3 } = { pmb f } _ { 2 } ( { pmb x } _ { 2 } )$ , ${ pmb x } _ { 4 } = { pmb f } _ { 3 } ( { pmb x } _ { 3 } )$ , and $pmb { o } = f _ { 4 } ( pmb { x } _ { 4 } )$ . \nWe can compute the Jacobian $begin{array} { r } { mathbf { J } _ { f } ( pmb { x } ) = frac { partial pmb { o } } { partial pmb { x } } in mathbb { R } ^ { m times n } } end{array}$ using the chain rule: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Multilayer perceptrons (MLPs)",
        "subsubsection": "Connections with biology"
    },
    {
        "content": "In practice, the optimal Bayesian approach is often computationally intractable. In the natural world, biological agents have evolved various algorithmic “shortcuts” to the optimal solution; this can explain many of the heuristics that people use in everyday reasoning [KST82; GTA00; Gri20]. As the tasks we want our machines to solve become harder, we may be able to gain insights from neuroscience and cognitive science for how to solve such tasks in an approximate way (see e.g., [MWK16; Has+17; Lak+17; HG21]). However, we should also bear in mind that AI/ML systems are increasingly used for safety-critical applications, in which we might want and expect the machine to do better than a human. In such cases, we may want more than just heuristic solutions that often work; instead we may want provably reliable methods, similar to other engineering fields (see Section 1.6.3 for further discussion). \n13.3 Backpropagation \nThis section is coauthored with Mathieu Blondel. \nIn this section, we describe the famous backpropagation algorithm, which can be used to compute the gradient of a loss function applied to the output of the network wrt the parameters in each layer. This gradient can then be passed to a gradient-based optimization algorithm, as we discuss in Section 13.4. \nThe backpropagation algorithm was originally discovered in [BH69], and independently in [Wer74]. However, it was [RHW86] that brought the algorithm to the attention of the “mainstream” ML community. See the wikipedia page $^ 4$ for more historical details. \nWe initially assume the computation graph is a simple linear chain of stacked layers, as in an MLP. In this case, backprop is equivalent to repeated applications of the chain rule of calculus (see Equation (7.261)). However, the method can be generalized to arbitrary directed acyclic graphs (DAGs), as we discuss in Section 13.3.4. This general procedure is often called automatic differentiation or autodiff. \n13.3.1 Forward vs reverse mode differentiation \nConsider a mapping of the form $mathbf { delta } _ { mathbf { boldsymbol { o } } } = mathbf { mathcal { f } } ( mathbf { boldsymbol { x } } )$ , where $pmb { x } in mathbb { R } ^ { n }$ and $pmb { o } in mathbb { R } ^ { r n }$ . We assume that $f$ is defined as a composition of functions: \nwhere $f _ { 1 } : mathbb { R } ^ { n } to mathbb { R } ^ { m _ { 1 } }$ , $f _ { 2 } : mathbb { R } ^ { m _ { 1 } } to mathbb { R } ^ { m _ { 2 } }$ , $f _ { 3 } : mathbb { R } ^ { m _ { 2 } } to mathbb { R } ^ { m _ { 3 } }$ , and $f _ { 4 } : mathbb { R } ^ { m _ { 3 } }  mathbb { R } ^ { m }$ . The intermediate steps needed to compute $mathbf { delta } _ { mathbf { boldsymbol { o } } } = mathbf { mathcal { f } } ( mathbf { boldsymbol { x } } )$ are ${ pmb x } _ { 2 } = { pmb f } _ { 1 } ( { pmb x } )$ , ${ pmb x } _ { 3 } = { pmb f } _ { 2 } ( { pmb x } _ { 2 } )$ , ${ pmb x } _ { 4 } = { pmb f } _ { 3 } ( { pmb x } _ { 3 } )$ , and $pmb { o } = f _ { 4 } ( pmb { x } _ { 4 } )$ . \nWe can compute the Jacobian $begin{array} { r } { mathbf { J } _ { f } ( pmb { x } ) = frac { partial pmb { o } } { partial pmb { x } } in mathbb { R } ^ { m times n } } end{array}$ using the chain rule: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe now discuss how to compute the Jacobian $mathbf { J } _ { f } ( pmb { x } )$ efficiently. Recall that \nwhere $nabla f _ { i } ( pmb { x } ) ^ { mathsf { T } } in mathbb { R } ^ { 1 times n }$ is the $i$ ’th row (for $i = 1 : m$ ) and $frac { partial pmb { f } } { partial x _ { j } } in mathbb { R } ^ { m }$ is the $j$ ’th column (for $j = 1 : n$ ). Note that, in our notation, when $m = 1$ , the gradient, denoted $nabla f ( { pmb x } )$ , has the same shape as $_ { x }$ . It is therefore a column vector, while $mathbf { J } _ { f } ( pmb { x } )$ is a row vector. In this case, we therefore technically have $nabla f ( { pmb x } ) = { bf J } _ { f } ( { pmb x } ) ^ { scriptscriptstyle 1 }$ . \nWe can extract the $i$ ’th row from $mathbf { J } _ { f } ( pmb { x } )$ by using a vector Jacobian product (VJP) of the form $e _ { i } ^ { top } mathbf { J } _ { f } ( { pmb x } )$ , where $e _ { i } in mathbb { R } ^ { m }$ is the unit basis vector. Similarly, we can extract the $j$ ’th column from $mathbf { J } _ { f } ( pmb { x } )$ by using a Jacobian vector product (JVP) of the form ${ bf J } _ { f } ( { pmb x } ) e _ { j }$ , where $boldsymbol { e } _ { j } in mathbb { R } ^ { n }$ . This shows that the computation of $mathbf { J } _ { f } ( pmb { x } )$ reduces to either $n$ JVPs or $m$ VJPs. \nIf $n < m$ , it is more efficient to compute $mathbf { J } _ { f } ( pmb { x } )$ for each column $j = 1 : n$ by using JVPs in a right-to-left manner. The right multiplication with a column vector $_ { v }$ is \nThis can be computed using forward mode differentiation; see Algorithm 5 for the pseudocode.   \nAssuming $m = 1$ and $n = prime n _ { 1 } = prime n _ { 2 } = prime n _ { 3 }$ , the cost of computing $mathbf { J } _ { f } ( pmb { x } )$ is $O ( n ^ { 3 } )$ . \nAlgorithm 5: Foward mode differentiation \n1 $pmb { x } _ { 1 } : = pmb { x }$   \n2 $pmb { v } _ { j } : = pmb { e } _ { j } in mathbb { R } ^ { n }$ for $j = 1 : n$   \n3 for $k = 1 : K$ do   \n4 ${ pmb x } _ { k + 1 } = { pmb f } _ { k } ( { pmb x } _ { k } )$   \n5 $pmb { v } _ { j } : = mathbf { J } _ { pmb { f } _ { k } } ( pmb { x } _ { k } ) pmb { v } _ { j }$ for $j = 1 : n$   \n6 Return $begin{array} { r } { pmb { omega } = pmb { x } _ { K + 1 } } end{array}$ , $[ mathbf { J } _ { f } ( pmb { x } ) ] _ { : , j } = pmb { v } _ { j }$ for $j = 1 : n$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf $n > m$ (e.g., if the output is a scalar), it is more efficient to compute $mathbf { J } _ { f } ( pmb { x } )$ for each row $i = 1 : m$ by using VJPs in a left-to-right manner. The left multiplication with a row vector $boldsymbol { u } ^ { intercal }$ is \nThis can be done using reverse mode differentiation; see Algorithm 6 for the pseudocode.   \nAssuming $m = 1$ and $n = prime n _ { 1 } = prime n _ { 2 } = prime n _ { 3 }$ , the cost of computing $mathbf { J } _ { f } ( pmb { x } )$ is $O ( n ^ { 2 } )$ . \nAlgorithm 6: Reverse mode differentiation \n1 $pmb { x } _ { 1 } : = pmb { x }$   \n2 for $k = 1 : K$ do   \n3 ${ pmb x } _ { k + 1 } = { pmb f } _ { k } ( { pmb x } _ { k } )$   \n4 $pmb { u } _ { i } : = pmb { e } _ { i } in mathbb { R } ^ { m }$ for $i = 1 : m$   \n5 for $k = K : 1$ do   \n6 $mathbf { Xi } perp mathbf { Xi } mathbf { { boldsymbol { u } } } _ { i } ^ { mathsf { T } } : = mathbf { boldsymbol { u } } _ { i } ^ { mathsf { T } } mathbf { mathbf { J } } _ { f _ { k } } ( mathbf { boldsymbol { x } } _ { k } )$ for $i = 1 : m$   \n7 Return $pmb { o } = pmb { x } _ { K + 1 }$ , $[ mathbf { J } _ { f } ( pmb { x } ) ] _ { i , : } = pmb { u } _ { i } ^ { 1 }$ for $i = 1 : m$ \nBoth Algorithms 5 and 6 can be adapted to compute JVPs and VJPs against any collection of input vectors, by accepting ${ pmb { v } _ { j } } _ { j = 1 , . . . , n }$ and ${ pmb { u } _ { i } } _ { i = 1 , . . . , m }$ as respective inputs. Initializing these vectors to the standard basis is useful specifically for producing the complete Jacobian as output. \n13.3.2 Reverse mode differentiation for multilayer perceptrons \nIn the previous section, we considered a simple linear-chain feedforward model where each layer does not have any learnable parameters. In this section, each layer can now have (optional) parameters $pmb { theta } _ { 1 } , ldots , pmb { theta } _ { 4 }$ . See Figure 13.10 for an illustration. We focus on the case where the mapping has the form $mathcal { L } : mathbb { R } ^ { n }  mathbb { R }$ , so the output is a scalar. For example, consider $ell _ { 2 }$ loss for a MLP with one hidden layer: \nwe can represent this as the following feedforward model: \nWe use the notation $f _ { k } ( pmb { x } _ { k } , pmb { theta } _ { k } )$ to denote the function at layer $k$ , where ${ boldsymbol { x } } _ { k }$ is the previous output and $pmb { theta } _ { k }$ are the optional parameters for this layer. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Backpropagation",
        "subsubsection": "Forward vs reverse mode differentiation"
    },
    {
        "content": "If $n > m$ (e.g., if the output is a scalar), it is more efficient to compute $mathbf { J } _ { f } ( pmb { x } )$ for each row $i = 1 : m$ by using VJPs in a left-to-right manner. The left multiplication with a row vector $boldsymbol { u } ^ { intercal }$ is \nThis can be done using reverse mode differentiation; see Algorithm 6 for the pseudocode.   \nAssuming $m = 1$ and $n = prime n _ { 1 } = prime n _ { 2 } = prime n _ { 3 }$ , the cost of computing $mathbf { J } _ { f } ( pmb { x } )$ is $O ( n ^ { 2 } )$ . \nAlgorithm 6: Reverse mode differentiation \n1 $pmb { x } _ { 1 } : = pmb { x }$   \n2 for $k = 1 : K$ do   \n3 ${ pmb x } _ { k + 1 } = { pmb f } _ { k } ( { pmb x } _ { k } )$   \n4 $pmb { u } _ { i } : = pmb { e } _ { i } in mathbb { R } ^ { m }$ for $i = 1 : m$   \n5 for $k = K : 1$ do   \n6 $mathbf { Xi } perp mathbf { Xi } mathbf { { boldsymbol { u } } } _ { i } ^ { mathsf { T } } : = mathbf { boldsymbol { u } } _ { i } ^ { mathsf { T } } mathbf { mathbf { J } } _ { f _ { k } } ( mathbf { boldsymbol { x } } _ { k } )$ for $i = 1 : m$   \n7 Return $pmb { o } = pmb { x } _ { K + 1 }$ , $[ mathbf { J } _ { f } ( pmb { x } ) ] _ { i , : } = pmb { u } _ { i } ^ { 1 }$ for $i = 1 : m$ \nBoth Algorithms 5 and 6 can be adapted to compute JVPs and VJPs against any collection of input vectors, by accepting ${ pmb { v } _ { j } } _ { j = 1 , . . . , n }$ and ${ pmb { u } _ { i } } _ { i = 1 , . . . , m }$ as respective inputs. Initializing these vectors to the standard basis is useful specifically for producing the complete Jacobian as output. \n13.3.2 Reverse mode differentiation for multilayer perceptrons \nIn the previous section, we considered a simple linear-chain feedforward model where each layer does not have any learnable parameters. In this section, each layer can now have (optional) parameters $pmb { theta } _ { 1 } , ldots , pmb { theta } _ { 4 }$ . See Figure 13.10 for an illustration. We focus on the case where the mapping has the form $mathcal { L } : mathbb { R } ^ { n }  mathbb { R }$ , so the output is a scalar. For example, consider $ell _ { 2 }$ loss for a MLP with one hidden layer: \nwe can represent this as the following feedforward model: \nWe use the notation $f _ { k } ( pmb { x } _ { k } , pmb { theta } _ { k } )$ to denote the function at layer $k$ , where ${ boldsymbol { x } } _ { k }$ is the previous output and $pmb { theta } _ { k }$ are the optional parameters for this layer. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn this example, the final layer returns a scalar, since it corresponds to a loss function ${ mathcal { L } } in mathbb { R }$ . Therefore it is more efficient to use reverse mode differentation to compute the gradient vectors. \nWe first discuss how to compute the gradient of the scalar output wrt the parameters in each layer. We can easily compute the gradient wrt the predictions in the final layer ∂xL4 . For the gradient wrt the parameters in the earlier layers, we can use the chain rule to get \nwhere each $begin{array} { r } { frac { partial mathcal { L } } { partial pmb { theta } _ { underline { { k } } } } = ( nabla _ { pmb { theta } _ { underline { { k } } } } mathcal { L } ) ^ { top } } end{array}$ is a $d _ { k }$ -dimensional gradient row vector, where $d _ { k }$ is the number of parameters in layer $k$ . We see that these can be computed recursively, by multiplying the gradient row vector at layer k by the Jacobian ∂∂xxkk1 which is an $n _ { k } times n _ { k - 1 }$ matrix, where $n _ { k }$ is the number of hidden units in layer $k$ . See Algorithm 7 for the pseudocode. \nThis algorithm computes the gradient of the loss wrt the parameters at each layer. It also computes the gradient of the loss wrt the input, $nabla _ { pmb { x } } { mathcal { L } } in mathbb { R } ^ { n }$ , where $n$ is the dimensionality of the input. This latter quantity is not needed for parameter learning, but can be useful for generating inputs to a model (see Section 14.6 for some applications). \nAll that remains is to specify how to compute the vector Jacobian product (VJP) of all supported layers. The details of this depend on the form of the function at each layer. We discuss some examples below. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n13.3.3 Vector-Jacobian product for common layers \nRecall that the Jacobian for a layer of the form $f : mathbb { R } ^ { n }  mathbb { R } ^ { m }$ . is defined by \nwhere $nabla f _ { i } ( pmb { x } ) ^ { sf I } in mathbb { R } ^ { n }$ is the $i$ ’th row (for $i = 1 : m$ ) and $frac { partial pmb { f } } { partial x _ { j } } in mathbb { R } ^ { m }$ is the $j$ ’th column (for $j = 1 : n$ In this section, we describe how to compute the VJP ${ pmb u } ^ { 1 } { bf J } _ { f } ( { pmb x } )$ for common layers. \n13.3.3.1 Cross entropy layer \nConsider a cross-entropy loss layer taking logits $_ { x }$ and target labels $mathbf { Delta } _ { mathbf { mathcal { Y } } }$ as input, and returning a scalar: \nwhere $begin{array} { r } { pmb { p } = mathrm { s o f t m a x } ( pmb { x } ) = frac { e ^ { x _ { c } } } { sum _ { c ^ { prime } = 1 } ^ { C } e ^ { x _ { c ^ { prime } } } } } end{array}$ Cexcexc′ are the predicted class probabilites, and y is the true distribution over labels (often a one-hot vector). The Jacobian wrt the input is \nTo see this, assume the target label is class $c$ . We have \nHence \nIf we define $pmb { y } = [ mathbb { I } left( i = c right) ]$ , we recover Equation (13.39). Note that the Jacobian of this layer is a row vector, since the output is a scalar. \n13.3.3.2 Elementwise nonlinearity \nConsider a layer that applies an elementwise nonlinearity, $z = f ( pmb { x } ) = varphi ( pmb { x } )$ , so $z _ { i } = varphi ( x _ { i } )$ . The $( i , j )$ element of the Jacobian is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Backpropagation",
        "subsubsection": "Reverse mode differentiation for multilayer perceptrons"
    },
    {
        "content": "13.3.3 Vector-Jacobian product for common layers \nRecall that the Jacobian for a layer of the form $f : mathbb { R } ^ { n }  mathbb { R } ^ { m }$ . is defined by \nwhere $nabla f _ { i } ( pmb { x } ) ^ { sf I } in mathbb { R } ^ { n }$ is the $i$ ’th row (for $i = 1 : m$ ) and $frac { partial pmb { f } } { partial x _ { j } } in mathbb { R } ^ { m }$ is the $j$ ’th column (for $j = 1 : n$ In this section, we describe how to compute the VJP ${ pmb u } ^ { 1 } { bf J } _ { f } ( { pmb x } )$ for common layers. \n13.3.3.1 Cross entropy layer \nConsider a cross-entropy loss layer taking logits $_ { x }$ and target labels $mathbf { Delta } _ { mathbf { mathcal { Y } } }$ as input, and returning a scalar: \nwhere $begin{array} { r } { pmb { p } = mathrm { s o f t m a x } ( pmb { x } ) = frac { e ^ { x _ { c } } } { sum _ { c ^ { prime } = 1 } ^ { C } e ^ { x _ { c ^ { prime } } } } } end{array}$ Cexcexc′ are the predicted class probabilites, and y is the true distribution over labels (often a one-hot vector). The Jacobian wrt the input is \nTo see this, assume the target label is class $c$ . We have \nHence \nIf we define $pmb { y } = [ mathbb { I } left( i = c right) ]$ , we recover Equation (13.39). Note that the Jacobian of this layer is a row vector, since the output is a scalar. \n13.3.3.2 Elementwise nonlinearity \nConsider a layer that applies an elementwise nonlinearity, $z = f ( pmb { x } ) = varphi ( pmb { x } )$ , so $z _ { i } = varphi ( x _ { i } )$ . The $( i , j )$ element of the Jacobian is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $begin{array} { r } { varphi ^ { prime } ( a ) = frac { d } { d a } varphi ( a ) } end{array}$ . In other words, the Jacobian wrt the input is \nFor an arbitrary vector $mathbf { Delta } _ { mathbf { u } }$ , we can compute $mathbf { nabla } _ { mathbf { boldsymbol { u } } } mathbf { cdot } mathbf { J }$ by elementwise multiplication of the diagonal elements of $mathbf { J }$ with $mathbf { Delta } _ { mathbf { u } }$ . For example, if \nwe have \nThe subderivative (Section 8.1.4.1) at $a = 0$ is any value in $lfloor 0 , 1 rfloor$ . It is often taken to be 0. Hence \nwhere $H$ is the Heaviside step function. \n13.3.3.3 Linear layer \nNow consider a linear layer, $z = f ( x , mathbf { W } ) = mathbf { W } x$ , where $mathbf { W } in mathbb { R } ^ { m times n }$ , so $pmb { x } in mathbb { R } ^ { n }$ and $z in mathbb { R } ^ { m }$ . We can compute the Jacobian wrt the input vector, $begin{array} { r } { mathbf { J } = frac { partial z } { partial x } in mathbb { R } ^ { m times n } } end{array}$ , as follows. Note that \nSo the $( i , j )$ entry of the Jacobian will be \nsince $begin{array} { r } { frac { partial } { partial x _ { j } } x _ { k } = mathbb { I } left( k = j right) } end{array}$ . Hence the Jacobian wrt the input is \nThe VJP between $pmb { u } ^ { vert } in mathbb { R } ^ { 1 times m }$ and $mathbf { J } in mathbb { R } ^ { m times n }$ is \nNow consider the Jacobian wrt the weight matrix, $begin{array} { r } { mathbf { J } = frac { partial z } { partial mathbf { W } } } end{array}$ . This can be represented as a $m times ( m times n )$ matrix, which is complex to deal with. So instead, let us focus on taking the gradient wrt a single \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nweight, Wij. This is easier to compute, since ∂∂Wz is a vector. To compute this, note that \nHence \nwhere the non-zero entry occurs in location $i$ . The VJP between $pmb { u } ^ { vert } in mathbb { R } ^ { 1 times m }$ and $frac { partial boldsymbol { z } } { partial mathbf { W } } in mathbb { R } ^ { m times ( m times n ) }$ can be represented as a matrix of shape $1 times ( m times n )$ . Note that \nTherefore \n13.3.3.4 Putting it all together \nFor an exercise that puts this all together, see Exercise 13.1. \n13.3.4 Computation graphs \nMLPs are a simple kind of DNN in which each layer feeds directly into the next, forming a chain structure, as shown in Figure 13.10. However, modern DNNs can combine differentiable components in much more complex ways, to create a computation graph, analogous to how programmers combine elementary functions to make more complex ones. (Indeed, some have suggested that “deep learning” \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 be called “differentiable programming”.) The only restriction is that the resulting computation graph corresponds to a directed acyclic graph (DAG), where each node is a differentiable function of all its inputs.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Backpropagation",
        "subsubsection": "Vector-Jacobian product for common layers"
    },
    {
        "content": "weight, Wij. This is easier to compute, since ∂∂Wz is a vector. To compute this, note that \nHence \nwhere the non-zero entry occurs in location $i$ . The VJP between $pmb { u } ^ { vert } in mathbb { R } ^ { 1 times m }$ and $frac { partial boldsymbol { z } } { partial mathbf { W } } in mathbb { R } ^ { m times ( m times n ) }$ can be represented as a matrix of shape $1 times ( m times n )$ . Note that \nTherefore \n13.3.3.4 Putting it all together \nFor an exercise that puts this all together, see Exercise 13.1. \n13.3.4 Computation graphs \nMLPs are a simple kind of DNN in which each layer feeds directly into the next, forming a chain structure, as shown in Figure 13.10. However, modern DNNs can combine differentiable components in much more complex ways, to create a computation graph, analogous to how programmers combine elementary functions to make more complex ones. (Indeed, some have suggested that “deep learning” \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 be called “differentiable programming”.) The only restriction is that the resulting computation graph corresponds to a directed acyclic graph (DAG), where each node is a differentiable function of all its inputs. \n\nFor example, consider the function \nWe can compute this using the DAG in Figure 13.11, with the following intermediate functions: \nNote that we have numbered the nodes in topological order (parents before children). During the backward pass, since the graph is no longer a chain, we may need to sum gradients along multiple paths. For example, since $x _ { 4 }$ influences $x _ { 5 }$ and $x _ { 7 }$ , we have \nWe can avoid repeated computation by working in reverse topological order. For example, \nIn general, we use \nwhere the sum is over all children $k$ of node $j$ , as shown in Figure 13.12. The $frac { partial o } { partial { bf x } _ { k } }$ gradient vector has already been computed for each child $k$ ; this quantity is called the adjoint. This gets multiplied by the Jacobian $frac { partial pmb { x } _ { k } } { partial pmb { x } _ { j } }$ of each child. \nThe computation graph can be computed ahead of time, by using an API to define a static graph. (This is how Tensorflow 1 worked.) Alternatively, the graph can be computed “just in time”, by tracing the execution of the function on an input argument. (This is how Tensorflow eager mode works, as well as JAX and PyTorch.) The latter approach makes it easier to work with a dynamic graph, whose shape can change depending on the values computed by the function. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFigure 13.13 shows a computation graph corresponding to an MLP with one hidden layer with weight decay. More precisely, the model computes the linear pre-activations $z = mathbf { W } ^ { ( 1 ) } x$ , the hidden activations $h = phi ( z )$ , the linear outputs ${ mathbf o } = { mathbf W } ^ { ( 2 ) } { mathbf h }$ , the loss $L = ell ( o , y )$ , the regularizer $begin{array} { r } { s = frac { lambda } { 2 } ( | | mathbf { W } ^ { ( 1 ) } | | _ { F } ^ { 2 } + | | mathbf { W } ^ { ( 2 ) } | | _ { F } ^ { 2 } ) } end{array}$ , and the total loss $boldsymbol { J } = boldsymbol { L } + boldsymbol { s }$ . \n13.4 Training neural networks \nIn this section, we discuss how to fit DNNs to data. The standard approach is to use maximum likelihood estimation, by minimizing the NLL: \nIt is also common to add a regularizer (such as the negative log prior), as we discuss in Section 13.5. In principle we can just use the backprop algorithm (Section 13.3) to compute the gradient of this loss and pass it to an off-the-shelf optimizer, such as those discussed in Chapter 8. (The Adam optimizer of Section 8.4.6.3 is a popular choice, due to its ability to scale to large datasets (by virtue of being an SGD-type algorithm), and to converge fairly quickly (by virtue of using diagonal \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 preconditioning and momentum).) However, in practice this may not work well. In this section, we discuss various problems that may arise, as well as some solutions. For more details on the practicalities of training DNNs, see various other books, such as [HG20; Zha+20; Gér19].",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Backpropagation",
        "subsubsection": "Computation graphs"
    },
    {
        "content": "In addition to practical issues, there are important theoretical issues. In particular, we note that the DNN loss is not a convex objective, so in general we will not be able to find the global optimum. Nevertheless, SGD can often find suprisingly good solutions. The research into why this is the case is still being conducted; see [Bah+20] for a recent review of some of this work. \n13.4.1 Tuning the learning rate \nIt is important to tune the learning rate (step size), to ensure convergence to a good solution. We discuss this issue in Section 8.4.3. \n13.4.2 Vanishing and exploding gradients \nWhen training very deep models, the gradient tends to become either very small (this is called the vanishing gradient problem) or very large (this is called the exploding gradient problem), because the error signal is being passed through a series of layers which either amplify or diminish it [Hoc+01]. (Similar problems arise in RNNs on long sequences, as we explain in Section 15.2.6.) \nTo explain the problem in more detail, consider the gradient of the loss wrt a node at layer $it { l }$ : \nwhere Jl = ∂∂zlz+l1 is the Jacobian matrix, and gl+1 = ∂z∂lL+1 is the gradient at the next layer. If $mathbf { J } _ { l }$ is constant across layers, it is clear that the contribution of the gradient from the final layer, $_ { g L }$ , to layer $it { l }$ will be $mathbf { J } ^ { L - l } { g _ { L } }$ . Thus the behavior of the system depends on the eigenvectors of $mathbf { J }$ . \nAlthough $mathbf { J }$ is a real-valued matrix, it is not (in general) symmetric, so its eigenvalues and eigenvectors can be complex-valued, with the imaginary components corresponding to oscillatory behavior. Let $lambda$ be the spectral radius of $mathbf { J }$ , which is the maximum of the absolute values of the eigenvalues. If this is greater than 1, the gradient can explode; if this is less than 1, the gradient can vanish. (Similarly, the spectral radius of $mathbf { W }$ , connecting $_ { z _ { l } }$ to $z _ { l + 1 }$ , determines the stability of the dynamical system when run in forwards mode.) \nThe exploding gradient problem can be ameliorated by gradient clipping, in which we cap the magnitude of the gradient if it becomes too large, i.e., we use \nThis way, the norm of $g ^ { prime }$ can never exceed $c$ , but the vector is always in the same direction as $pmb { g }$ . However, the vanishing gradient problem is more difficult to solve. There are various solutions, such as the following: \n• Modify the the activation functions at each layer to prevent the gradient from becoming too large or too small; see Section 13.4.3.   \n• Modify the architecture so that the updates are additive rather than multiplicative; see Section 13.4.4. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license • Modify the architecture to standardize the activations at each layer, so that the distribution of activations over the dataset remains constant during training; see Section 14.2.4.1.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Training neural networks",
        "subsubsection": "Tuning the learning rate"
    },
    {
        "content": "In addition to practical issues, there are important theoretical issues. In particular, we note that the DNN loss is not a convex objective, so in general we will not be able to find the global optimum. Nevertheless, SGD can often find suprisingly good solutions. The research into why this is the case is still being conducted; see [Bah+20] for a recent review of some of this work. \n13.4.1 Tuning the learning rate \nIt is important to tune the learning rate (step size), to ensure convergence to a good solution. We discuss this issue in Section 8.4.3. \n13.4.2 Vanishing and exploding gradients \nWhen training very deep models, the gradient tends to become either very small (this is called the vanishing gradient problem) or very large (this is called the exploding gradient problem), because the error signal is being passed through a series of layers which either amplify or diminish it [Hoc+01]. (Similar problems arise in RNNs on long sequences, as we explain in Section 15.2.6.) \nTo explain the problem in more detail, consider the gradient of the loss wrt a node at layer $it { l }$ : \nwhere Jl = ∂∂zlz+l1 is the Jacobian matrix, and gl+1 = ∂z∂lL+1 is the gradient at the next layer. If $mathbf { J } _ { l }$ is constant across layers, it is clear that the contribution of the gradient from the final layer, $_ { g L }$ , to layer $it { l }$ will be $mathbf { J } ^ { L - l } { g _ { L } }$ . Thus the behavior of the system depends on the eigenvectors of $mathbf { J }$ . \nAlthough $mathbf { J }$ is a real-valued matrix, it is not (in general) symmetric, so its eigenvalues and eigenvectors can be complex-valued, with the imaginary components corresponding to oscillatory behavior. Let $lambda$ be the spectral radius of $mathbf { J }$ , which is the maximum of the absolute values of the eigenvalues. If this is greater than 1, the gradient can explode; if this is less than 1, the gradient can vanish. (Similarly, the spectral radius of $mathbf { W }$ , connecting $_ { z _ { l } }$ to $z _ { l + 1 }$ , determines the stability of the dynamical system when run in forwards mode.) \nThe exploding gradient problem can be ameliorated by gradient clipping, in which we cap the magnitude of the gradient if it becomes too large, i.e., we use \nThis way, the norm of $g ^ { prime }$ can never exceed $c$ , but the vector is always in the same direction as $pmb { g }$ . However, the vanishing gradient problem is more difficult to solve. There are various solutions, such as the following: \n• Modify the the activation functions at each layer to prevent the gradient from becoming too large or too small; see Section 13.4.3.   \n• Modify the architecture so that the updates are additive rather than multiplicative; see Section 13.4.4. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license • Modify the architecture to standardize the activations at each layer, so that the distribution of activations over the dataset remains constant during training; see Section 14.2.4.1. \n\n• Carefully choose the initial values of the parameters; see Section 13.4.5. \n13.4.3 Non-saturating activation functions \nIn Section 13.2.3, we mentioned that the sigmoid activation function saturates at 0 for large negative inputs, and at 1 for large positive inputs. It turns out that the gradient signal in these regimes is $0$ , preventing backpropagation from working. \nTo see why the gradient vanishes, consider a layer which computes $z = sigma ( mathbf { W } mathbf { x } )$ , where \nIf the weights are initialized to be large (positive or negative), then it becomes very easy for $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ to take on large values, and hence for $mathscr { z }$ to saturate near 0 or $mathbf { 1 }$ , since the sigmoid saturates, as shown in Figure 13.14a. Now let us consider the gradient of the loss wrt the inputs $_ { x }$ (from an earlier layer) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Training neural networks",
        "subsubsection": "Vanishing and exploding gradients"
    },
    {
        "content": "• Carefully choose the initial values of the parameters; see Section 13.4.5. \n13.4.3 Non-saturating activation functions \nIn Section 13.2.3, we mentioned that the sigmoid activation function saturates at 0 for large negative inputs, and at 1 for large positive inputs. It turns out that the gradient signal in these regimes is $0$ , preventing backpropagation from working. \nTo see why the gradient vanishes, consider a layer which computes $z = sigma ( mathbf { W } mathbf { x } )$ , where \nIf the weights are initialized to be large (positive or negative), then it becomes very easy for $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ to take on large values, and hence for $mathscr { z }$ to saturate near 0 or $mathbf { 1 }$ , since the sigmoid saturates, as shown in Figure 13.14a. Now let us consider the gradient of the loss wrt the inputs $_ { x }$ (from an earlier layer) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nand the parameters $mathbf { W }$ . The derivative of the activation function is given by \nSee Figure 13.14b for a plot. In Section 13.3.3, we show that the gradient of the loss wrt the inputs is \nand the gradient of the loss wrt the parameters is \nHence, if $textit { textbf { z } }$ is near 0 or 1, the gradients will go to 0. \nOne of the keys to being able to train very deep models is to use non-saturating activation functions. Several different functions have been proposed: see Table 13.4 for a summary, and https://mlfromscratch.com/activation-functions-explained for more details. \n13.4.3.1 ReLU \nThe most common is rectified linear unit or ReLU, proposed in [GBB11; KSH12]. This is defined as \nThe ReLU function simply “turns off” negative inputs, and passes positive inputs unchanged. The gradient has the following form: \nNow suppose we use this in a layer to compute $z = mathrm { R e L U } ( mathbf { W } x )$ . In Section 13.3.3, we show that the gradient wrt the inputs has the form \nand wrt the parameters has the form \nHence the gradient will not vanish, as long a $mathscr { z }$ is positive. \nUnfortunately, if the weights are initialized to be large and negative, then it becomes very easy for (some components of) $mathbf { Delta } a = mathbf { W } mathbf { Delta } x$ to take on large negative values, and hence for $boldsymbol { z }$ to go to 0. This will cause the gradient for the weights to go to 0. The algorithm will never be able to escape this situation, so the hidden units (components of $_ { z }$ ) will stay permanently off. This is called the “dead ReLU” problem [Lu+19]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n13.4.3.2 Non-saturating ReLU \nThe problem of dead ReLU’s can be solved by using non-saturating variants of ReLU. One alternate is the leaky ReLU, proposed in [MHN13]. This is defined as \nwhere $0 < alpha < 1$ . The slope of this function is 1 for positive inputs, and $alpha$ for negative inputs, thus ensuring there is some signal passed back to earlier layers, even when the input is negative. See Figure 13.14b for a plot. If we allow the parameter $alpha$ to be learned, rather than fixed, the leaky ReLU is called parametric ReLU [He+15]. \nAnother popular choice is the ELU, proposed in [CUH16]. This is defined by \nThis has the advantage over leaky ReLU of being a smooth function. See Figure 13.14 for plot. A slight variant of ELU, known as SELU (self-normalizing ELU), was proposed in [Kla+17]. This has the form \nSurprisingly, they prove that by setting $alpha$ and $lambda$ to carefully chosen values, this activation function is guaranteed to ensure that the output of each layer is standardized (provided the input is also standardized), even without the use of techniques such as batchnorm (Section 14.2.4.1). This can help with model fitting. \n13.4.3.3 Other choices \nAs an alternative to manually discovering good activation functions, we can use blackbox optimization methods to search over the space of functional forms. Such an approach was used in [RZL17], where they discovered a function they call swish that seems to do well on some image classification benchmarks. It is defined by \n(The same function, under the name SiLU (for Sigmoid Linear Unit), was independently proposed in [HG16].) See Figure 13.14 for plot. \nAnother popular activation function is GELU, which stands for “Gaussian Error Linear Unit” [HG16]. This is defined as follows: \nwhere $Phi ( a )$ is the cdf of a standard normal: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe see from Figure 13.14 that this is not a convex or monontonic function, unlike most other activation functions. \nWe can think of GELU as a “soft” version of ReLU, since it replaces the step function $mathbb { I } left( a > 0 right)$ with the Gaussian cdf, $Phi ( a )$ . Alternatively, the GELU can be motivated as an adaptive version of dropout (Section 13.5.4), where we multiply the input by a binary scalar mask, $m sim operatorname { B e r } ( Phi ( a ) )$ , where the probability of being dropped is given by $1 - Phi ( a )$ . Thus the expected output is \nWe can approximate GELU using swish with a particular parameter setting, namely \n13.4.4 Residual connections \nOne solution to the vanishing gradient problem for DNNs is to use a residual network or ResNet [He+16a]. This is a feedforward model in which each layer has the form of a residual block, defined by \nwhere $mathcal { F } _ { l }$ is a standard shallow nonlinear mapping (e.g., linear-activation-linear). The inner $mathcal { F } _ { l }$ function computes the residual term or delta that needs to be added to the input $_ { x }$ to generate the desired output; it is often easier to learn to generate a small perturbation to the input than to directly predict the output. (Residual connections are usually used in conjunction with CNNs, as discussed in Section 14.3.4, but can also be used in MLPs.) \nA model with residual connections has the same number of parameters as a model without residual connections, but it is easier to train. The reason is that gradients can flow directly from the output \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license to earlier layers, as sketched in Figure 13.15b. To see this, note that the activations at the output layer can be derived in terms of any previous layer $it { l }$ using",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Training neural networks",
        "subsubsection": "Non-saturating activation functions"
    },
    {
        "content": "We see from Figure 13.14 that this is not a convex or monontonic function, unlike most other activation functions. \nWe can think of GELU as a “soft” version of ReLU, since it replaces the step function $mathbb { I } left( a > 0 right)$ with the Gaussian cdf, $Phi ( a )$ . Alternatively, the GELU can be motivated as an adaptive version of dropout (Section 13.5.4), where we multiply the input by a binary scalar mask, $m sim operatorname { B e r } ( Phi ( a ) )$ , where the probability of being dropped is given by $1 - Phi ( a )$ . Thus the expected output is \nWe can approximate GELU using swish with a particular parameter setting, namely \n13.4.4 Residual connections \nOne solution to the vanishing gradient problem for DNNs is to use a residual network or ResNet [He+16a]. This is a feedforward model in which each layer has the form of a residual block, defined by \nwhere $mathcal { F } _ { l }$ is a standard shallow nonlinear mapping (e.g., linear-activation-linear). The inner $mathcal { F } _ { l }$ function computes the residual term or delta that needs to be added to the input $_ { x }$ to generate the desired output; it is often easier to learn to generate a small perturbation to the input than to directly predict the output. (Residual connections are usually used in conjunction with CNNs, as discussed in Section 14.3.4, but can also be used in MLPs.) \nA model with residual connections has the same number of parameters as a model without residual connections, but it is easier to train. The reason is that gradients can flow directly from the output \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license to earlier layers, as sketched in Figure 13.15b. To see this, note that the activations at the output layer can be derived in terms of any previous layer $it { l }$ using \n\nWe can therefore compute the gradient of the loss wrt the parameters of the $it { Delta } l$ ’th layer as follows: \nThus we see that the gradient at layer $it { l }$ depends directly on the gradient at layer $L$ in a way that is independent of the depth of the network. \n13.4.5 Parameter initialization \nSince the objective function for DNN training is non-convex, the way that we initialize the parameters of a DNN can play a big role on what kind of solution we end up with, as well as how easy the function is to train (i.e., how well information can flow forwards and backwards through the model). In the rest of this section, we present some common heuristic methods that are used for initializing parameters. \n13.4.5.1 Heuristic initialization schemes \nIn [GB10], they show that sampling parameters from a standard normal with fixed variance can result in exploding activations or gradients. To see why, consider a linear unit with no activation function given by $begin{array} { r } { o _ { i } = sum _ { j = 1 } ^ { n _ { mathrm { i n } } } w _ { i j } x _ { j } } end{array}$ ; suppose $w _ { i j } sim mathcal { N } ( 0 , sigma ^ { 2 } )$ , and $mathbb { E } left[ x _ { j } right] = 0$ and $mathbb { V } left[ x _ { j } right] = gamma ^ { 2 }$ , where we assume $x _ { j }$ are independent of $w _ { i j }$ . The mean and variance of the output is given by \nTo keep the output variance from blowing up, we need to ensure $n _ { mathrm { i n } } sigma ^ { 2 } = 1$ (or some other constant), where $n _ { mathrm { i n } }$ is the fan-in of a unit (number of incoming connections). \nNow consider the backwards pass. By analogous reasoning, we see that the variance of the gradients can blow up unless $n _ { mathrm { o u t } } sigma ^ { 2 } = 1$ , where $n _ { mathrm { o u t } }$ is the fan-out of a unit (number of outgoing connections). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Training neural networks",
        "subsubsection": "Residual connections"
    },
    {
        "content": "We can therefore compute the gradient of the loss wrt the parameters of the $it { Delta } l$ ’th layer as follows: \nThus we see that the gradient at layer $it { l }$ depends directly on the gradient at layer $L$ in a way that is independent of the depth of the network. \n13.4.5 Parameter initialization \nSince the objective function for DNN training is non-convex, the way that we initialize the parameters of a DNN can play a big role on what kind of solution we end up with, as well as how easy the function is to train (i.e., how well information can flow forwards and backwards through the model). In the rest of this section, we present some common heuristic methods that are used for initializing parameters. \n13.4.5.1 Heuristic initialization schemes \nIn [GB10], they show that sampling parameters from a standard normal with fixed variance can result in exploding activations or gradients. To see why, consider a linear unit with no activation function given by $begin{array} { r } { o _ { i } = sum _ { j = 1 } ^ { n _ { mathrm { i n } } } w _ { i j } x _ { j } } end{array}$ ; suppose $w _ { i j } sim mathcal { N } ( 0 , sigma ^ { 2 } )$ , and $mathbb { E } left[ x _ { j } right] = 0$ and $mathbb { V } left[ x _ { j } right] = gamma ^ { 2 }$ , where we assume $x _ { j }$ are independent of $w _ { i j }$ . The mean and variance of the output is given by \nTo keep the output variance from blowing up, we need to ensure $n _ { mathrm { i n } } sigma ^ { 2 } = 1$ (or some other constant), where $n _ { mathrm { i n } }$ is the fan-in of a unit (number of incoming connections). \nNow consider the backwards pass. By analogous reasoning, we see that the variance of the gradients can blow up unless $n _ { mathrm { o u t } } sigma ^ { 2 } = 1$ , where $n _ { mathrm { o u t } }$ is the fan-out of a unit (number of outgoing connections). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo satisfy both requirements at once, we set $textstyle { frac { 1 } { 2 } } ( n _ { mathrm { i n } } + n _ { mathrm { o u t } } ) sigma ^ { 2 } = 1$ , or equivalently \nThis is known as Xavier initialization or Glorot initialization, named after the first author of [GB10]. \nA special case arises if we use $sigma ^ { 2 } = 1 / n _ { mathrm { i n } }$ ; this is known as LeCun initialization, named after Yann LeCun, who proposed it in the 1990s. This is equivalent to Glorot initialization when $r  { mathrm { = } } pi  { mathrm { i n } } = r  { mathrm { = } }  { mathrm { i } _ { mathrm { o u t } } }$ . If we use $sigma ^ { 2 } = 2 / n _ { mathrm { i n } }$ , the method is called He initialization, named after Ximing He, who proposed it in [He+15]. \nNote that it is not necessary to use a Gaussian distribution. Indeed, the above derivation just worked in terms of the first two moments (mean and variance), and made no assumptions about Gaussianity. For example, suppose we sample weights from a uniform distribution, $w _ { i j } sim mathrm { U n i f } ( - a , a )$ . The mean is 0, and the variance is $sigma ^ { 2 } = a ^ { 2 } / 3$ . Hence we should set $begin{array} { r } { a = sqrt { frac { 6 } { n _ { mathrm { i n } } + n _ { mathrm { o u t } } } } } end{array}$ \nAlthough the above derivation assumes a linear output unit, the technique works well empirically even for nonlinear units. The best choice of initialization method depends on which activation function you use. For linear, tanh, logistic, and softmax, Glorot is recommended. For ReLU and variants, He is recommended. For SELU, LeCun is recommended. See e.g., [Gér19] for more heuristics, and e.g., [HDR19] for some theory. \n13.4.5.2 Data-driven initializations \nWe can also adopt a data-driven approach to parameter initialization. For example, [MM16] proposed a simple but effective scheme known as layer-sequential unit-variance (LSUV) initialization, which works as follows. First we initialize the weights of each (fully connected or convolutional) layer with orthonormal matrices, as proposed in [SMG14]. (This can be achieved by drawing from ${ pmb w } sim mathcal { N } ( { bf 0 } , { bf I } )$ , reshaping to $mathbf { boldsymbol { w } }$ to a matrix $mathbf { W }$ , and then computing an orthonormal basis using QR or SVD decomposition.) Then, for each layer $it { l }$ , we compute the variance ${ boldsymbol { v } } _ { l }$ of the activations across a minibatch; we then rescale using $mathbf { W } _ { l } : = mathbf { W } _ { l } / sqrt { v _ { l } }$ . This scheme can be viewed as an orthonormal initialization combined with batch normalization performed only on the first mini-batch. This is faster than full batch normalization, but can sometimes work just as well. \n13.4.6 Parallel training \nIt can be quite slow to train large models on large datasets. One way to speed this process up is to use specialized hardware, such as graphics processing units (GPUs) and tensor processing units (TPUs), which are very efficient at performing matrix-matrix multiplication. If we have multiple GPUs, we can sometimes further speed things up. There are two main approaches: model parallelism, in which we partition the model between machines, and data parallelism, in which each machine has its own copy of the model, and applies it to a different set of data. \nModel parallelism can be quite complicated, since it requires tight communication between machines to ensure they compute the correct answer. We will not discuss this further. Data parallelism is generally much simpler, since it is embarassingly parallel. To use this to speed up training, at each training step $t$ , we do the following: 1) we partition the minibatch across the $K$ machines to get $mathcal { D } _ { t } ^ { k }$ ; 2) each machine $k$ computes its own gradient, $pmb { g } _ { t } ^ { k } = nabla _ { pmb { theta } } mathcal { L } ( pmb { theta } ; mathcal { D } _ { t } ^ { k } ) ; 3$ ) we collect all the local gradients \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license on a central machine (e.g., device $0$ ) and sum them using $begin{array} { r } { pmb { g } _ { t } = sum _ { k = 1 } ^ { K } pmb { g } _ { t } ^ { k } ; } end{array}$ 4) we broadcast the summed gradient back to all devices, so $tilde { pmb { g } } _ { t } ^ { k } = pmb { g } _ { t }$ ; 5) each machine updates its own copy of the parameters using $pmb { theta } _ { t } ^ { k } : = pmb { theta } _ { t } ^ { k } - eta _ { t } tilde { pmb { g } } _ { t } ^ { k }$ . See Figure 13.16 for an illustration and multi_gpu_training_jax.ipynb for some sample code.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Training neural networks",
        "subsubsection": "Parameter initialization"
    },
    {
        "content": "To satisfy both requirements at once, we set $textstyle { frac { 1 } { 2 } } ( n _ { mathrm { i n } } + n _ { mathrm { o u t } } ) sigma ^ { 2 } = 1$ , or equivalently \nThis is known as Xavier initialization or Glorot initialization, named after the first author of [GB10]. \nA special case arises if we use $sigma ^ { 2 } = 1 / n _ { mathrm { i n } }$ ; this is known as LeCun initialization, named after Yann LeCun, who proposed it in the 1990s. This is equivalent to Glorot initialization when $r  { mathrm { = } } pi  { mathrm { i n } } = r  { mathrm { = } }  { mathrm { i } _ { mathrm { o u t } } }$ . If we use $sigma ^ { 2 } = 2 / n _ { mathrm { i n } }$ , the method is called He initialization, named after Ximing He, who proposed it in [He+15]. \nNote that it is not necessary to use a Gaussian distribution. Indeed, the above derivation just worked in terms of the first two moments (mean and variance), and made no assumptions about Gaussianity. For example, suppose we sample weights from a uniform distribution, $w _ { i j } sim mathrm { U n i f } ( - a , a )$ . The mean is 0, and the variance is $sigma ^ { 2 } = a ^ { 2 } / 3$ . Hence we should set $begin{array} { r } { a = sqrt { frac { 6 } { n _ { mathrm { i n } } + n _ { mathrm { o u t } } } } } end{array}$ \nAlthough the above derivation assumes a linear output unit, the technique works well empirically even for nonlinear units. The best choice of initialization method depends on which activation function you use. For linear, tanh, logistic, and softmax, Glorot is recommended. For ReLU and variants, He is recommended. For SELU, LeCun is recommended. See e.g., [Gér19] for more heuristics, and e.g., [HDR19] for some theory. \n13.4.5.2 Data-driven initializations \nWe can also adopt a data-driven approach to parameter initialization. For example, [MM16] proposed a simple but effective scheme known as layer-sequential unit-variance (LSUV) initialization, which works as follows. First we initialize the weights of each (fully connected or convolutional) layer with orthonormal matrices, as proposed in [SMG14]. (This can be achieved by drawing from ${ pmb w } sim mathcal { N } ( { bf 0 } , { bf I } )$ , reshaping to $mathbf { boldsymbol { w } }$ to a matrix $mathbf { W }$ , and then computing an orthonormal basis using QR or SVD decomposition.) Then, for each layer $it { l }$ , we compute the variance ${ boldsymbol { v } } _ { l }$ of the activations across a minibatch; we then rescale using $mathbf { W } _ { l } : = mathbf { W } _ { l } / sqrt { v _ { l } }$ . This scheme can be viewed as an orthonormal initialization combined with batch normalization performed only on the first mini-batch. This is faster than full batch normalization, but can sometimes work just as well. \n13.4.6 Parallel training \nIt can be quite slow to train large models on large datasets. One way to speed this process up is to use specialized hardware, such as graphics processing units (GPUs) and tensor processing units (TPUs), which are very efficient at performing matrix-matrix multiplication. If we have multiple GPUs, we can sometimes further speed things up. There are two main approaches: model parallelism, in which we partition the model between machines, and data parallelism, in which each machine has its own copy of the model, and applies it to a different set of data. \nModel parallelism can be quite complicated, since it requires tight communication between machines to ensure they compute the correct answer. We will not discuss this further. Data parallelism is generally much simpler, since it is embarassingly parallel. To use this to speed up training, at each training step $t$ , we do the following: 1) we partition the minibatch across the $K$ machines to get $mathcal { D } _ { t } ^ { k }$ ; 2) each machine $k$ computes its own gradient, $pmb { g } _ { t } ^ { k } = nabla _ { pmb { theta } } mathcal { L } ( pmb { theta } ; mathcal { D } _ { t } ^ { k } ) ; 3$ ) we collect all the local gradients \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license on a central machine (e.g., device $0$ ) and sum them using $begin{array} { r } { pmb { g } _ { t } = sum _ { k = 1 } ^ { K } pmb { g } _ { t } ^ { k } ; } end{array}$ 4) we broadcast the summed gradient back to all devices, so $tilde { pmb { g } } _ { t } ^ { k } = pmb { g } _ { t }$ ; 5) each machine updates its own copy of the parameters using $pmb { theta } _ { t } ^ { k } : = pmb { theta } _ { t } ^ { k } - eta _ { t } tilde { pmb { g } } _ { t } ^ { k }$ . See Figure 13.16 for an illustration and multi_gpu_training_jax.ipynb for some sample code. \n\nNote that steps 3 and 4 are usually combined into one atomic step; this is known as an all-reduce operation (where we use sum to reduce the set of (gradient) vectors into one). If each machine blocks until receiving the centrally aggregated gradient, $mathbf { nabla } _ { mathbf { boldsymbol { mathcal { G } } } t }$ , the method is known as synchronous training. This will give the same results as training with one machine (with a larger batchsize), only faster (assuming we ignore any batch normalization layers). If we let each machine update its parameters using its own local gradient estimate, and not wait for the broadcast to/from the other machines, the method is called asynchronous training. This is not guaranteed to work, since the different machines may get out of step, and hence will be updating different versions of the parameters; this approach has therefore been called hogwild training [Niu+11]. However, if the updates are sparse, so each machine “touches” a different part of the parameter vector, one can prove that hogwild training behaves like standard synchronous SGD. \n13.5 Regularization \nIn Section 13.4 we discussed computational issues associated with training (large) neural networks. In this section, we discuss statistical issues. In particular, we focus on ways to avoid overfitting. This is crucial, since large neural networks can easily have millions of parameters. \n13.5.1 Early stopping \nPerhaps the simplest way to prevent overfitting is called early stopping, which refers to the heuristic of stopping the training procedure when the error on the validation set starts to increase (see Figure 4.8 for an example). This method works because we are restricting the ability of the optimization algorithm to transfer information from the training examples to the parameters, as explained in [AS19]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Training neural networks",
        "subsubsection": "Parallel training"
    },
    {
        "content": "Note that steps 3 and 4 are usually combined into one atomic step; this is known as an all-reduce operation (where we use sum to reduce the set of (gradient) vectors into one). If each machine blocks until receiving the centrally aggregated gradient, $mathbf { nabla } _ { mathbf { boldsymbol { mathcal { G } } } t }$ , the method is known as synchronous training. This will give the same results as training with one machine (with a larger batchsize), only faster (assuming we ignore any batch normalization layers). If we let each machine update its parameters using its own local gradient estimate, and not wait for the broadcast to/from the other machines, the method is called asynchronous training. This is not guaranteed to work, since the different machines may get out of step, and hence will be updating different versions of the parameters; this approach has therefore been called hogwild training [Niu+11]. However, if the updates are sparse, so each machine “touches” a different part of the parameter vector, one can prove that hogwild training behaves like standard synchronous SGD. \n13.5 Regularization \nIn Section 13.4 we discussed computational issues associated with training (large) neural networks. In this section, we discuss statistical issues. In particular, we focus on ways to avoid overfitting. This is crucial, since large neural networks can easily have millions of parameters. \n13.5.1 Early stopping \nPerhaps the simplest way to prevent overfitting is called early stopping, which refers to the heuristic of stopping the training procedure when the error on the validation set starts to increase (see Figure 4.8 for an example). This method works because we are restricting the ability of the optimization algorithm to transfer information from the training examples to the parameters, as explained in [AS19]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n13.5.2 Weight decay \nA common approach to reduce overfitting is to impose a prior on the parameters, and then use MAP estimation. It is standard to use a Gaussian prior for the weights $mathcal { N } ( boldsymbol { mathbf { mathit { w } } } | mathbf { mathbf { 0 } } , alpha ^ { 2 } mathbf { I } )$ and biases, $mathcal { N } ( b | mathbf { 0 } , beta ^ { 2 } mathbf { I } )$ . This is equivalent to $ell _ { 2 }$ regularization of the objective. In the neural networks literature, this is called weight decay, since it encourages small weights, and hence simpler models, as in ridge regression (Section 11.3). \n13.5.3 Sparse DNNs \nSince there are many weights in a neural network, it is often helpful to encourage sparsity. This allows us to perform model compression, which can save memory and time. To do this, we can use $ell _ { 1 }$ regularization (as in Section 11.4), or ARD (as in Section 11.7.7), or several other methods (see e.g., [Hoe+21; Bha+20] for recent reviews). As a simple example, Figure 13.17 shows a 5 layer MLP which has been fit to some 1d regression data using an $ell _ { 1 }$ regularizer on the weights. We see that the resulting graph topology is sparse. \nDespite the intuitive appeal of sparse topology, in practice these methods are not widely used, since modern GPUs are optimized for dense matrix multiplication, and there are few computational benefits to sparse weight matrices. However, if we use methods that encourage group sparsity, we can prune out whole layers of the model. This results in block sparse weight matrices, which can result in speedups and memory savings (see e.g., [Sca+17; Wen+16; MAV17; LUW17]). \n13.5.4 Dropout \nSuppose that we randomly (on a per-example basis) turn off all the outgoing connections from each neuron with probability $p$ , as illustrated in Figure 13.18. This technique is known as dropout [Sri+14]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Regularization",
        "subsubsection": "Early stopping"
    },
    {
        "content": "13.5.2 Weight decay \nA common approach to reduce overfitting is to impose a prior on the parameters, and then use MAP estimation. It is standard to use a Gaussian prior for the weights $mathcal { N } ( boldsymbol { mathbf { mathit { w } } } | mathbf { mathbf { 0 } } , alpha ^ { 2 } mathbf { I } )$ and biases, $mathcal { N } ( b | mathbf { 0 } , beta ^ { 2 } mathbf { I } )$ . This is equivalent to $ell _ { 2 }$ regularization of the objective. In the neural networks literature, this is called weight decay, since it encourages small weights, and hence simpler models, as in ridge regression (Section 11.3). \n13.5.3 Sparse DNNs \nSince there are many weights in a neural network, it is often helpful to encourage sparsity. This allows us to perform model compression, which can save memory and time. To do this, we can use $ell _ { 1 }$ regularization (as in Section 11.4), or ARD (as in Section 11.7.7), or several other methods (see e.g., [Hoe+21; Bha+20] for recent reviews). As a simple example, Figure 13.17 shows a 5 layer MLP which has been fit to some 1d regression data using an $ell _ { 1 }$ regularizer on the weights. We see that the resulting graph topology is sparse. \nDespite the intuitive appeal of sparse topology, in practice these methods are not widely used, since modern GPUs are optimized for dense matrix multiplication, and there are few computational benefits to sparse weight matrices. However, if we use methods that encourage group sparsity, we can prune out whole layers of the model. This results in block sparse weight matrices, which can result in speedups and memory savings (see e.g., [Sca+17; Wen+16; MAV17; LUW17]). \n13.5.4 Dropout \nSuppose that we randomly (on a per-example basis) turn off all the outgoing connections from each neuron with probability $p$ , as illustrated in Figure 13.18. This technique is known as dropout [Sri+14]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Regularization",
        "subsubsection": "Weight decay"
    },
    {
        "content": "13.5.2 Weight decay \nA common approach to reduce overfitting is to impose a prior on the parameters, and then use MAP estimation. It is standard to use a Gaussian prior for the weights $mathcal { N } ( boldsymbol { mathbf { mathit { w } } } | mathbf { mathbf { 0 } } , alpha ^ { 2 } mathbf { I } )$ and biases, $mathcal { N } ( b | mathbf { 0 } , beta ^ { 2 } mathbf { I } )$ . This is equivalent to $ell _ { 2 }$ regularization of the objective. In the neural networks literature, this is called weight decay, since it encourages small weights, and hence simpler models, as in ridge regression (Section 11.3). \n13.5.3 Sparse DNNs \nSince there are many weights in a neural network, it is often helpful to encourage sparsity. This allows us to perform model compression, which can save memory and time. To do this, we can use $ell _ { 1 }$ regularization (as in Section 11.4), or ARD (as in Section 11.7.7), or several other methods (see e.g., [Hoe+21; Bha+20] for recent reviews). As a simple example, Figure 13.17 shows a 5 layer MLP which has been fit to some 1d regression data using an $ell _ { 1 }$ regularizer on the weights. We see that the resulting graph topology is sparse. \nDespite the intuitive appeal of sparse topology, in practice these methods are not widely used, since modern GPUs are optimized for dense matrix multiplication, and there are few computational benefits to sparse weight matrices. However, if we use methods that encourage group sparsity, we can prune out whole layers of the model. This results in block sparse weight matrices, which can result in speedups and memory savings (see e.g., [Sca+17; Wen+16; MAV17; LUW17]). \n13.5.4 Dropout \nSuppose that we randomly (on a per-example basis) turn off all the outgoing connections from each neuron with probability $p$ , as illustrated in Figure 13.18. This technique is known as dropout [Sri+14]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Regularization",
        "subsubsection": "Sparse DNNs"
    },
    {
        "content": "13.5.2 Weight decay \nA common approach to reduce overfitting is to impose a prior on the parameters, and then use MAP estimation. It is standard to use a Gaussian prior for the weights $mathcal { N } ( boldsymbol { mathbf { mathit { w } } } | mathbf { mathbf { 0 } } , alpha ^ { 2 } mathbf { I } )$ and biases, $mathcal { N } ( b | mathbf { 0 } , beta ^ { 2 } mathbf { I } )$ . This is equivalent to $ell _ { 2 }$ regularization of the objective. In the neural networks literature, this is called weight decay, since it encourages small weights, and hence simpler models, as in ridge regression (Section 11.3). \n13.5.3 Sparse DNNs \nSince there are many weights in a neural network, it is often helpful to encourage sparsity. This allows us to perform model compression, which can save memory and time. To do this, we can use $ell _ { 1 }$ regularization (as in Section 11.4), or ARD (as in Section 11.7.7), or several other methods (see e.g., [Hoe+21; Bha+20] for recent reviews). As a simple example, Figure 13.17 shows a 5 layer MLP which has been fit to some 1d regression data using an $ell _ { 1 }$ regularizer on the weights. We see that the resulting graph topology is sparse. \nDespite the intuitive appeal of sparse topology, in practice these methods are not widely used, since modern GPUs are optimized for dense matrix multiplication, and there are few computational benefits to sparse weight matrices. However, if we use methods that encourage group sparsity, we can prune out whole layers of the model. This results in block sparse weight matrices, which can result in speedups and memory savings (see e.g., [Sca+17; Wen+16; MAV17; LUW17]). \n13.5.4 Dropout \nSuppose that we randomly (on a per-example basis) turn off all the outgoing connections from each neuron with probability $p$ , as illustrated in Figure 13.18. This technique is known as dropout [Sri+14]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nDropout can dramatically reduce overfitting and is very widely used. Intuitively, the reason dropout works well is that it prevents complex co-adaptation of the hidden units. In other words, each unit must learn to perform well even if some of the other units are missing at random. This prevents the units from learning complex, but fragile, dependencies on each other.5 A more formal explanation, in terms of Gaussian scale mixture priors, can be found in [NHLS19]. \nWe can view dropout as estimating a noisy version of the weights, $begin{array} { r } { theta _ { l i j } = w _ { l i j } epsilon _ { l i } } end{array}$ , where $epsilon _ { l i } sim$ $operatorname { B e r } ( 1 - p )$ is a Bernoulli noise term. (So if we sample $epsilon _ { l i } = 0$ , then all of the weights going out of unit $i$ in layer ${ mathit { l } } - 1$ into any $j$ in layer $it l$ will be set to 0.) At test time, we usually turn the noise off. To ensure the weights have the same expectation at test time as they did during training (so the input activation to the neurons is the same, on average), at test time we should use $w _ { l i j } = theta _ { l i j } mathbb { E } left[ epsilon _ { l i } right]$ . For Bernoulli noise, we have $mathbb { E } left[ epsilon right] = 1 - p$ , so we should multiply the weights by the keep probability, $1 - p$ , before making predictions. \nWe can, however, use dropout at test time if we wish. The result is an ensemble of networks, each with slightly different sparse graph structures. This is called Monte Carlo dropout [GG16; KG17], and has the form \nwhere $S$ is the number of samples, and we write $hat { mathbf { W } } epsilon ^ { s }$ to indicate that we are multiplying all the estimated weight matrices by a sampled noise vector. This can sometimes provide a good approximation to the Bayesian posterior predictive distribution $p ( pmb { y } | pmb { x } , mathcal { D } )$ , especially if the noise rate is optimized [GHK17]. \n13.5.5 Bayesian neural networks \nModern DNNs are usually trained using a (penalized) maximum likelihood objective to find a single setting of parameters. However, with large models, there are often many more parameters than data points, so there may be multiple possible models which fit the training data equally well, yet which generalize in different ways. It is often useful to capture the induced uncertainty in the posterior predictive distribution. This can be done by marginalizing out the parameters by computing \nThe result is known as a Bayesian neural network or BNN. It can be thought of as an infinite ensemble of differently weight neural networks. By marginalizing out the parameters, we can avoid overfitting [Mac95]. Bayesian marginalization is challenging for large neural networks, but also can lead to significant performance gains [WI20]. For more details on the topic of Bayesian deep learning, see the sequel to this book, [Mur23]. \n13.5.6 Regularization effects of (stochastic) gradient descent * \nSome optimization methods (in particular, second-order batch methods) are able to find “needles in haystacks”, corresponding to narrow but deep “holes” in the loss landscape, corresponding to parameter settings with very low loss. These are known as sharp minima, see Figure 13.19(right). From the point of view of minimizing the empirical loss, the optimizer has done a good job. However, such solutions generally correspond to a model that has overfit the data. It is better to find points that correspond to flat minima, as shown in Figure 13.19(left); such solutions are more robust and generalize better. To see why, note that flat minima correspond to regions in parameter space where there is a lot of posterior uncertainty, and hence samples from this region are less able to precisely memorize irrelevant details about the training set [AS17]. SGD often finds such flat minima by virtue of the addition of noise, which prevents it from “entering” narrow regions of the loss landscape (see e.g., [SL18]). This is called implicit regularization. It is also possible to explicitly encourage SGD to find such flat minima, using entropy SGD [Cha+17], sharpness aware minimization [For+21], stochastic weight averaging (SWA) [Izm+18], and other related techniques. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Regularization",
        "subsubsection": "Dropout"
    },
    {
        "content": "13.5.5 Bayesian neural networks \nModern DNNs are usually trained using a (penalized) maximum likelihood objective to find a single setting of parameters. However, with large models, there are often many more parameters than data points, so there may be multiple possible models which fit the training data equally well, yet which generalize in different ways. It is often useful to capture the induced uncertainty in the posterior predictive distribution. This can be done by marginalizing out the parameters by computing \nThe result is known as a Bayesian neural network or BNN. It can be thought of as an infinite ensemble of differently weight neural networks. By marginalizing out the parameters, we can avoid overfitting [Mac95]. Bayesian marginalization is challenging for large neural networks, but also can lead to significant performance gains [WI20]. For more details on the topic of Bayesian deep learning, see the sequel to this book, [Mur23]. \n13.5.6 Regularization effects of (stochastic) gradient descent * \nSome optimization methods (in particular, second-order batch methods) are able to find “needles in haystacks”, corresponding to narrow but deep “holes” in the loss landscape, corresponding to parameter settings with very low loss. These are known as sharp minima, see Figure 13.19(right). From the point of view of minimizing the empirical loss, the optimizer has done a good job. However, such solutions generally correspond to a model that has overfit the data. It is better to find points that correspond to flat minima, as shown in Figure 13.19(left); such solutions are more robust and generalize better. To see why, note that flat minima correspond to regions in parameter space where there is a lot of posterior uncertainty, and hence samples from this region are less able to precisely memorize irrelevant details about the training set [AS17]. SGD often finds such flat minima by virtue of the addition of noise, which prevents it from “entering” narrow regions of the loss landscape (see e.g., [SL18]). This is called implicit regularization. It is also possible to explicitly encourage SGD to find such flat minima, using entropy SGD [Cha+17], sharpness aware minimization [For+21], stochastic weight averaging (SWA) [Izm+18], and other related techniques. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Regularization",
        "subsubsection": "Bayesian neural networks"
    },
    {
        "content": "13.5.5 Bayesian neural networks \nModern DNNs are usually trained using a (penalized) maximum likelihood objective to find a single setting of parameters. However, with large models, there are often many more parameters than data points, so there may be multiple possible models which fit the training data equally well, yet which generalize in different ways. It is often useful to capture the induced uncertainty in the posterior predictive distribution. This can be done by marginalizing out the parameters by computing \nThe result is known as a Bayesian neural network or BNN. It can be thought of as an infinite ensemble of differently weight neural networks. By marginalizing out the parameters, we can avoid overfitting [Mac95]. Bayesian marginalization is challenging for large neural networks, but also can lead to significant performance gains [WI20]. For more details on the topic of Bayesian deep learning, see the sequel to this book, [Mur23]. \n13.5.6 Regularization effects of (stochastic) gradient descent * \nSome optimization methods (in particular, second-order batch methods) are able to find “needles in haystacks”, corresponding to narrow but deep “holes” in the loss landscape, corresponding to parameter settings with very low loss. These are known as sharp minima, see Figure 13.19(right). From the point of view of minimizing the empirical loss, the optimizer has done a good job. However, such solutions generally correspond to a model that has overfit the data. It is better to find points that correspond to flat minima, as shown in Figure 13.19(left); such solutions are more robust and generalize better. To see why, note that flat minima correspond to regions in parameter space where there is a lot of posterior uncertainty, and hence samples from this region are less able to precisely memorize irrelevant details about the training set [AS17]. SGD often finds such flat minima by virtue of the addition of noise, which prevents it from “entering” narrow regions of the loss landscape (see e.g., [SL18]). This is called implicit regularization. It is also possible to explicitly encourage SGD to find such flat minima, using entropy SGD [Cha+17], sharpness aware minimization [For+21], stochastic weight averaging (SWA) [Izm+18], and other related techniques. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nOf course, the loss landscape depends not just on the parameter values, but also on the data. Since we usually cannot afford to do full-batch gradient descent, we will get a set of loss curves, one per minibatch. If each one of these curves corresponds to a wide basin, as shown in Figure 13.20a, we are at a point in parameter space that is robust to perturbations, and will likely generalize well. However, if the overall wide basin is the result of averaging over many different narrow basins, as shown in Figure 13.20b, the resulting estimate will likely generalize less well. \nThis can be formalized using the analysis in [Smi+21; BD21]. Specifically, they consider continuous time gradient flow which approximates the behavior of (S)GD. In [BD21], they consider full-batch GD, and show that the flow has the form $pmb { dot { w } } = - nabla _ { pmb { w } } mathcal { bar { L } } _ { G D } ( pmb { w } )$ , where \nwhere $mathcal { L } ( w )$ is the original loss, $epsilon$ is the learning rate, and the second term is an implicit regularization term that penalizes solutions with large gradients (high curvature). \nIn [Smi+21], they extend this analysis to the SGD case. They show that the flow has the form $begin{array} { r } { dot { pmb w } = - nabla _ { pmb w } dot { mathcal { L } } _ { S G D } ( pmb w ) } end{array}$ , where \nwhere $m$ is the number of minibatches, and $mathcal { L } _ { k } ( w )$ is the loss on the $k$ ’th such minibatch. Comparing this to the full-batch GD loss, we see \nThe second term estimates the variance of the minibatch gradients, which is a measure of stability, and hence of generalization ability. \nThe above analysis shows that SGD not only has computational advantages (since it is faster than full-batch GD or second-order methods), but also statistical advantages. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n13.6 Other kinds of feedforward networks * \n13.6.1 Radial basis function networks \nConsider a 1 layer neural net where the hidden layer is given by the feature vector \nwhere $pmb { mu _ { k } } in mathcal X$ are a set of $K$ centroids or exemplars, and $mathcal { K } ( { pmb x } , { pmb mu } ) ge 0$ is a kernel function. We describe kernel functions in detail in Section 17.1. Here we just give an example, namely the Gaussian kernel \nThe parameter $sigma$ is known as the bandwidth of the kernel. Note that this kernel is shift invariant, meaning it is only a function of the distance $r = | | { pmb x } - { pmb c } | | _ { 2 }$ , so we can equivalently write this as \nThis is therefore called a radial basis function kernel or RBF kernel. \nA 1 layer neural net in which we use Equation (13.101) as the hidden layer, with RBF kernels, is called an RBF network [BL88]. This has the form \nwhere $theta = ( mu , w )$ . If the centroids $pmb { mu }$ are fixed, we can solve for the optimal weights $mathbf { boldsymbol { w } }$ using (regularized) least squares, as discussed in Chapter 11. If the centroids are unknown, we can estimate them by using an unsupervised clustering method, such as K-means (Section 21.3). Alternatively, we can associate one centroid per data point in the training set, to get ${ pmb { mu } } _ { n } = { pmb x } _ { n }$ , where now $K = N$ . This is an example of a non-parametric model, since the number of parameters grows (in this case linearly) with the amount of data, and is not independent of $N$ . If $K = N$ , the model can perfectly interpolate the data, and hence may overfit. However, by ensuring that the output weight vector $mathbf { boldsymbol { w } }$ is sparse, the model will only use a finite subset of the input examples; this is called a sparse kernel machine, and will be discussed in more detail in Section 17.4.1 and Section 17.3. Another way to avoid overfitting is to adopt a Bayesian approach, by integrating out the weights $mathbf { boldsymbol { w } }$ ; this gives rise to a model called a Gaussian process, which will be discussed in more detail in Section 17.2. \n13.6.1.1 RBF network for regression \nWe can use RBF networks for regression by defining $p ( y | mathbf { x } , pmb { theta } ) = mathcal { N } ( pmb { w } ^ { T } mathbf { phi } ( mathbf { x } ) , sigma ^ { 2 } )$ . For example, Figure 13.22 shows a 1d data set fit with $K = 1 0$ uniformly spaced RBF prototypes, but with the bandwidth ranging from small to large. Small values lead to very wiggly functions, since the predicted function value will only be non-zero for points $_ { x }$ that are close to one of the prototypes $pmb { mu } _ { k }$ . If the bandwidth is very large, the design matrix reduces to a constant matrix of 1’s, since each point is equally close to every prototype; hence the corresponding function is just a straight line. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Regularization",
        "subsubsection": "Regularization effects of (stochastic) gradient descent *"
    },
    {
        "content": "13.6 Other kinds of feedforward networks * \n13.6.1 Radial basis function networks \nConsider a 1 layer neural net where the hidden layer is given by the feature vector \nwhere $pmb { mu _ { k } } in mathcal X$ are a set of $K$ centroids or exemplars, and $mathcal { K } ( { pmb x } , { pmb mu } ) ge 0$ is a kernel function. We describe kernel functions in detail in Section 17.1. Here we just give an example, namely the Gaussian kernel \nThe parameter $sigma$ is known as the bandwidth of the kernel. Note that this kernel is shift invariant, meaning it is only a function of the distance $r = | | { pmb x } - { pmb c } | | _ { 2 }$ , so we can equivalently write this as \nThis is therefore called a radial basis function kernel or RBF kernel. \nA 1 layer neural net in which we use Equation (13.101) as the hidden layer, with RBF kernels, is called an RBF network [BL88]. This has the form \nwhere $theta = ( mu , w )$ . If the centroids $pmb { mu }$ are fixed, we can solve for the optimal weights $mathbf { boldsymbol { w } }$ using (regularized) least squares, as discussed in Chapter 11. If the centroids are unknown, we can estimate them by using an unsupervised clustering method, such as K-means (Section 21.3). Alternatively, we can associate one centroid per data point in the training set, to get ${ pmb { mu } } _ { n } = { pmb x } _ { n }$ , where now $K = N$ . This is an example of a non-parametric model, since the number of parameters grows (in this case linearly) with the amount of data, and is not independent of $N$ . If $K = N$ , the model can perfectly interpolate the data, and hence may overfit. However, by ensuring that the output weight vector $mathbf { boldsymbol { w } }$ is sparse, the model will only use a finite subset of the input examples; this is called a sparse kernel machine, and will be discussed in more detail in Section 17.4.1 and Section 17.3. Another way to avoid overfitting is to adopt a Bayesian approach, by integrating out the weights $mathbf { boldsymbol { w } }$ ; this gives rise to a model called a Gaussian process, which will be discussed in more detail in Section 17.2. \n13.6.1.1 RBF network for regression \nWe can use RBF networks for regression by defining $p ( y | mathbf { x } , pmb { theta } ) = mathcal { N } ( pmb { w } ^ { T } mathbf { phi } ( mathbf { x } ) , sigma ^ { 2 } )$ . For example, Figure 13.22 shows a 1d data set fit with $K = 1 0$ uniformly spaced RBF prototypes, but with the bandwidth ranging from small to large. Small values lead to very wiggly functions, since the predicted function value will only be non-zero for points $_ { x }$ that are close to one of the prototypes $pmb { mu } _ { k }$ . If the bandwidth is very large, the design matrix reduces to a constant matrix of 1’s, since each point is equally close to every prototype; hence the corresponding function is just a straight line. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n13.6.1.2 RBF network for classification \nWe can use RBF networks for binary classification by defining $p ( boldsymbol { y } | mathbf { boldsymbol { x } } , pmb { theta } ) = mathrm { B e r } big ( sigma ( mathbf { boldsymbol { w } } ^ { T } boldsymbol { phi } ( mathbf { boldsymbol { x } } ) ) big )$ . As an example, consider the data coming from the exclusive or function. This is a binary-valued function of two binary inputs. Its truth table is shown in Figure 13.21(a). In Figure 13.21(b), we have shown some data labeled by the xor function, but we have jittered the points to make the picture clearer.6 We see we cannot separate the data even using a degree 10 polynomial. However, using an RBF kernel and just 4 prototypes easily solves the problem as shown in Figure 13.21(c). \n13.6.2 Mixtures of experts \nWhen considering regression problems, it is common to assume a unimodal output distribution, such as a Gaussian or Student distribution, where the mean and variance is some function of the input, i.e., \nwhere the $f$ functions may be MLPs (possibly with some shared hidden units, as in Figure 13.5). However, this will not work well for one-to-many functions, in which each input can have multiple possible outputs. \nFigure 13.23a gives a simple example of such a function. We see that in the middle of the plot there are certain $x$ values for which there are two equally probable $y$ values. There are many real world problems of this form, e.g., 3d pose prediction of a person from a single image [Bo+08], colorization of a black and white image [Gua+17], predicting future frames of a video sequence [VT17], etc. Any model which is trained to maximize likelihood using a unimodal output density — even if the model is a flexible nonlinear model, such as neural network — will work poorly on one-to-many functions such as these, since it will just produce a blurry average output.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Other kinds of feedforward networks *",
        "subsubsection": "Radial basis function networks"
    },
    {
        "content": "13.6.1.2 RBF network for classification \nWe can use RBF networks for binary classification by defining $p ( boldsymbol { y } | mathbf { boldsymbol { x } } , pmb { theta } ) = mathrm { B e r } big ( sigma ( mathbf { boldsymbol { w } } ^ { T } boldsymbol { phi } ( mathbf { boldsymbol { x } } ) ) big )$ . As an example, consider the data coming from the exclusive or function. This is a binary-valued function of two binary inputs. Its truth table is shown in Figure 13.21(a). In Figure 13.21(b), we have shown some data labeled by the xor function, but we have jittered the points to make the picture clearer.6 We see we cannot separate the data even using a degree 10 polynomial. However, using an RBF kernel and just 4 prototypes easily solves the problem as shown in Figure 13.21(c). \n13.6.2 Mixtures of experts \nWhen considering regression problems, it is common to assume a unimodal output distribution, such as a Gaussian or Student distribution, where the mean and variance is some function of the input, i.e., \nwhere the $f$ functions may be MLPs (possibly with some shared hidden units, as in Figure 13.5). However, this will not work well for one-to-many functions, in which each input can have multiple possible outputs. \nFigure 13.23a gives a simple example of such a function. We see that in the middle of the plot there are certain $x$ values for which there are two equally probable $y$ values. There are many real world problems of this form, e.g., 3d pose prediction of a person from a single image [Bo+08], colorization of a black and white image [Gua+17], predicting future frames of a video sequence [VT17], etc. Any model which is trained to maximize likelihood using a unimodal output density — even if the model is a flexible nonlinear model, such as neural network — will work poorly on one-to-many functions such as these, since it will just produce a blurry average output. \nTo prevent this problem of regression to the mean, we can use a conditional mixture model. That is, we assume the output is a weighted mixture of $K$ different outputs, corresponding to different modes of the output distribution for each input $_ { x }$ . In the Gaussian case, this becomes \nHere $f _ { mu , k }$ predicts the mean of the $k$ ’th Gaussian, $f _ { sigma , k }$ predicts its variance terms, and $f _ { z }$ predicts which mixture component to use. This model is called a mixture of experts (MoE) [Jac+91; JJ94; YWG12; ME14]. The idea is that the $k$ ’th submodel $p ( { pmb y } | { pmb x } , z = k )$ is considered to be an “expert” in a certain region of input space. The function $p ( z = k | pmb { x } )$ is called a gating function, and decides which expert to use, depending on the input values. By picking the most likely expert for a given input $_ { x }$ , we can “activate” just a subset of the model. This is an example of conditional computation, since we decide what expert to run based on the results of earlier computations from the gating network [Sha+17]. \nWe can train this model using SGD, or using the EM algorithm (see Section 8.7.3 for details on the latter method). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n13.6.2.1 Mixture of linear experts \nIn this section, we consider a simple example in which we use linear regression experts and a linear classification gating function, i.e., the model has the form: \nwhere softmax $k$ is the $k$ ’th output from the softmax function. The individual weighting term $p ( z = k | pmb { x } )$ is called the responsibility for expert $k$ for input $_ { x }$ . In Figure 13.23b, we see how the gating networks softly partitions the input space amongst the $K = 3$ experts. \nEach expert $p ( y | mathbf { boldsymbol { x } } , z = k )$ corresponds to a linear regression model with different parameters. These are shown in Figure 13.23c. \nIf we take a weighted combination of the experts as our output, we get the red curve in Figure 13.23a, which is clearly is a bad predictor. If instead we only predict using the most active expert (i.e., the one with the highest responsibility), we get the discontinuous black curve, which is a much better predictor. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n13.6.2.2 Mixture density networks \nThe gating function and experts can be any kind of conditional probabilistic model, not just a linear model. If we make them both DNNs, then resulting model is called a mixture density network (MDN) [Bis94; ZS14] or a deep mixture of experts [CGG17]. See Figure 13.24 for a sketch of the model. \n13.6.2.3 Hierarchical MOEs \nIf each expert is itself an MoE model, the resulting model is called a hierarchical mixture of experts [JJ94]. See Figure 13.25 for an illustration of such a model with a two level hierarchy. \nAn HME with $L$ levels can be thought of as a “soft” decision tree of depth $L$ , where each example is passed through every branch of the tree, and the final prediction is a weighted average. (We discuss decision trees in Section 18.1.) \n13.7 Exercises \nExercise 13.1 [Backpropagation for a MLP] \n(Based on an exercise by Kevin Clark.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Other kinds of feedforward networks *",
        "subsubsection": "Mixtures of experts"
    },
    {
        "content": "13.6.2.2 Mixture density networks \nThe gating function and experts can be any kind of conditional probabilistic model, not just a linear model. If we make them both DNNs, then resulting model is called a mixture density network (MDN) [Bis94; ZS14] or a deep mixture of experts [CGG17]. See Figure 13.24 for a sketch of the model. \n13.6.2.3 Hierarchical MOEs \nIf each expert is itself an MoE model, the resulting model is called a hierarchical mixture of experts [JJ94]. See Figure 13.25 for an illustration of such a model with a two level hierarchy. \nAn HME with $L$ levels can be thought of as a “soft” decision tree of depth $L$ , where each example is passed through every branch of the tree, and the final prediction is a weighted average. (We discuss decision trees in Section 18.1.) \n13.7 Exercises \nExercise 13.1 [Backpropagation for a MLP] \n(Based on an exercise by Kevin Clark.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nConsider the following classification MLP with one hidden layer: \nwhere $pmb { x } in mathbb { R } ^ { D }$ , $pmb { b } _ { 1 } in mathbb { R } ^ { K }$ , $mathbf { W } in mathbb { R } ^ { K times D }$ , $pmb { b } _ { 2 } in mathbb { R } ^ { C }$ , $mathbf { V } in mathbb { R } ^ { C times K }$ , where $mathcal { D }$ is the size of the input, $K$ is the number of hidden units, and $C$ is the number of classes. Show that the gradients for the parameters and input are as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere the gradients of the loss wrt the two layers (logit and hidden) are given by the following: \nwith $H$ is the Heaviside function. Note that, in our notation, the gradient (which has the same shape as the variable with respect to which we differentiate) is equal to the Jacobian’s transpose when the variable is a vector and to the first slice of the Jacobian when the variable is a matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n14 Neural Networks for Images \n14.1 Introduction \nIn Chapter 13, we discussed multilayered perceptrons (MLPs) as a way to learn functions mapping “unstructured” input vectors $pmb { x } in mathbb { R } ^ { D }$ to outputs. In this chapter, we extend this to the case where the input $_ { x }$ has 2d spatial structure. (Similar ideas apply to 1d temporal structure, or 3d spatio-temporal structure.) \nTo see why it is not a good idea to apply MLPs directly to image data, recall that the core operation in an MLP at each hidden layer is computing the activations $z = varphi ( mathbf { W } mathbf { x } )$ , where $_ { x }$ is the input to a layer, W are the weights, and $varphi ( )$ is the nonlinear activation function. Thus the $j$ ’th element of the hidden layer has value $z _ { j } = varphi ( w _ { j } ^ { 1 } pmb { x } )$ . We can think of this inner product operation as comparing the input $_ { x }$ to a learned template or pattern ${ pmb w } _ { j }$ ; if the match is good (large positive inner product), the activation of that unit will be large (assuming a ReLU nonlinearity), signalling that the $j$ ’th pattern is present in the input. \nHowever, this does not work well if the input is a variable-sized image, $pmb { x } in mathbb { R } ^ { W H C }$ , where $W$ is the width, $H$ is the height, and $C$ is the number of input channels (e.g., $C = 3$ for RGB color). The problem is that we would need to learn a different-sized weight matrix $mathbf { W }$ for every size of input image. In addition, even if the input was fixed size, the number of parameters needed would be prohibitive for reasonably sized images, since the weight matrix would have size $( W times H times C ) times D$ , where $D$ is the number of outputs (hidden units). The final problem is that a pattern that occurs in one location may not be recognized when it occurs in a different location — that is, the model may not exhibit translation invariance — because the weights are not shared across locations (see Figure 14.1). \nTo solve these problems, we will use convolutional neural networks (CNNs), in which we replace matrix multiplication with a convolution operation. We explain this in detail in Section 14.2, but the basic idea is to divide the input into overlapping 2d image patches, and to compare each patch with a set of small weight matrices, or filters, which represent parts of an object; this is illustrated in Figure 14.2. We can think of this as a form of template matching. We will learn these templates from data, as we explain below. Because the templates are small (often just 3x3 or 5x5), the number of parameters is significantly reduced. And because we use convolution to do the template matching, instead of matrix multiplication, the model will be translationally invariant. This is useful for tasks such as image classification, where the goal is to classify if an object is present, regardless of its location. \nCNNs have many other applications besides image classification, as we will discuss later in this chapter. They can also be applied to 1d inputs (see Section 15.3) and 3d inputs; however, we mostly",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Tabular Data",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "14 Neural Networks for Images \n14.1 Introduction \nIn Chapter 13, we discussed multilayered perceptrons (MLPs) as a way to learn functions mapping “unstructured” input vectors $pmb { x } in mathbb { R } ^ { D }$ to outputs. In this chapter, we extend this to the case where the input $_ { x }$ has 2d spatial structure. (Similar ideas apply to 1d temporal structure, or 3d spatio-temporal structure.) \nTo see why it is not a good idea to apply MLPs directly to image data, recall that the core operation in an MLP at each hidden layer is computing the activations $z = varphi ( mathbf { W } mathbf { x } )$ , where $_ { x }$ is the input to a layer, W are the weights, and $varphi ( )$ is the nonlinear activation function. Thus the $j$ ’th element of the hidden layer has value $z _ { j } = varphi ( w _ { j } ^ { 1 } pmb { x } )$ . We can think of this inner product operation as comparing the input $_ { x }$ to a learned template or pattern ${ pmb w } _ { j }$ ; if the match is good (large positive inner product), the activation of that unit will be large (assuming a ReLU nonlinearity), signalling that the $j$ ’th pattern is present in the input. \nHowever, this does not work well if the input is a variable-sized image, $pmb { x } in mathbb { R } ^ { W H C }$ , where $W$ is the width, $H$ is the height, and $C$ is the number of input channels (e.g., $C = 3$ for RGB color). The problem is that we would need to learn a different-sized weight matrix $mathbf { W }$ for every size of input image. In addition, even if the input was fixed size, the number of parameters needed would be prohibitive for reasonably sized images, since the weight matrix would have size $( W times H times C ) times D$ , where $D$ is the number of outputs (hidden units). The final problem is that a pattern that occurs in one location may not be recognized when it occurs in a different location — that is, the model may not exhibit translation invariance — because the weights are not shared across locations (see Figure 14.1). \nTo solve these problems, we will use convolutional neural networks (CNNs), in which we replace matrix multiplication with a convolution operation. We explain this in detail in Section 14.2, but the basic idea is to divide the input into overlapping 2d image patches, and to compare each patch with a set of small weight matrices, or filters, which represent parts of an object; this is illustrated in Figure 14.2. We can think of this as a form of template matching. We will learn these templates from data, as we explain below. Because the templates are small (often just 3x3 or 5x5), the number of parameters is significantly reduced. And because we use convolution to do the template matching, instead of matrix multiplication, the model will be translationally invariant. This is useful for tasks such as image classification, where the goal is to classify if an object is present, regardless of its location. \nCNNs have many other applications besides image classification, as we will discuss later in this chapter. They can also be applied to 1d inputs (see Section 15.3) and 3d inputs; however, we mostly \nfocus on the 2d case in this chapter. \n14.2 Common layers \nIn this section, we discuss the basics of CNNs. \n14.2.1 Convolutional layers \nWe start by describing the basics of convolution in 1d, and then in 2d, and then describe how they are used as a key component of CNNs. \n14.2.1.1 Convolution in 1d \nThe convolution between two functions, say $f , g : mathbb { R } ^ { D }  mathbb { R }$ , is defined as \nNow suppose we replace the functions with finite-length vectors, which we can think of as functions defined on a finite set of points. For example, suppose $f$ is evaluated at the points ${ - L , - L +$ \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "focus on the 2d case in this chapter. \n14.2 Common layers \nIn this section, we discuss the basics of CNNs. \n14.2.1 Convolutional layers \nWe start by describing the basics of convolution in 1d, and then in 2d, and then describe how they are used as a key component of CNNs. \n14.2.1.1 Convolution in 1d \nThe convolution between two functions, say $f , g : mathbb { R } ^ { D }  mathbb { R }$ , is defined as \nNow suppose we replace the functions with finite-length vectors, which we can think of as functions defined on a finite set of points. For example, suppose $f$ is evaluated at the points ${ - L , - L +$ \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nFigure 14.3: Discrete convolution of $pmb { x } = [ 1 , 2 , 3 , 4 ]$ with $pmb { w } = [ 5 , 6 , 7 ]$ to yield $boldsymbol { z } = [ 5 , 1 6 , 3 4 , 5 2 , 4 5 , 2 8 ]$ . We see that this operation consists of “flipping” $textbf { em w }$ and then “dragging” it over $textbf { em x }$ , multiplying elementwise, and adding up the results. \nas \n$1 , ldots , 0 , 1 , ldots , L }$ to yield the weight vector (also called a filter or kernel) $w _ { - L } = f ( - L )$ up to $w _ { L } = f ( L )$ . Now let $g$ be evaluated at points ${ - N , ldots , N }$ to yield the feature vector $x _ { - N } = g ( - N )$ up to $x _ { N } = g ( N )$ . Then the above equation becomes \n(We discuss boundary conditions (edge effects) later on.) We see that we “flip” the weight vector $mathbf { Delta } _ { mathbf { w } }$ + (since indices of $mathbf { boldsymbol { w } }$ are reversed), and then “drag” it over the $_ { x }$ vector, summing up the local windows at each point, as illustrated in Figure 14.3. \nThere is a very closely related operation, in which we do not flip $mathbf { boldsymbol { w } }$ first: \nThis is called cross correlation; If the weight vector is symmetric, as is often the case, then cross correlation and convolution are the same. In the deep learning literature, the term “convolution” is usually used to mean cross correlation; we will follow this convention. \nWe can also evaluate the weights $mathbf { boldsymbol { w } }$ on domain ${ 0 , 1 , ldots , L - 1 }$ and the features $_ { x }$ on domain ${ 0 , 1 , ldots , N - 1 }$ , to eliminate negative indices. Then the above equation becomes \nSee Figure 14.4 for an example. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n14.2.1.2 Convolution in 2d \nIn 2d, Equation (14.4) becomes \nwhere the 2d filter $mathbf { W }$ has size $H times W$ . For example, consider convolving a $3 times 3$ input $mathbf { X }$ with a $2 times 2$ kernel $mathbf { W }$ to compute a $2 times 2$ output $mathbf { Y }$ : \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nSee Figure 14.5 for a visualization of this process. \nWe can think of 2d convolution as template matching, since the output at a point $( i , j )$ will be large if the corresponding image patch centered on $( i , j )$ is similar to $mathbf { W }$ . If the template $mathbf { W }$ corresponds to an oriented edge, then convolving with it will cause the output heat map to “light up” in regions that contain edges that match that orientation, as shown in Figure 14.6. More generally, we can think of convolution as a form of feature detection. The resulting output $mathbf { Y } = mathbf { W } circledast mathbf { X }$ is therefore called a feature map. \n14.2.1.3 Convolution as matrix-vector multiplication \nSince convolution is a linear operator, we can represent it by matrix multiplication. For example, consider Equation (14.7). We can rewrite this as matrix-vector mutiplication by flattening the 2d matrix $mathbf { X }$ into a 1d vector $_ { x }$ , and multiplying by a Toeplitz-like matrix $mathbf { C }$ derived from the kernel $mathbf { W }$ , as follows: \nWe can recover the $2 times 2$ output by reshaping the $4 times 1$ vector $pmb { y }$ back to $mathbf { Y }$ .1 \nThus we see that CNNs are like MLPs where the weight matrices have a special sparse structure, and the elements are tied across spatial locations. This implements the idea of translation invariance, and massively reduces the number of parameters compared to a weight matrix in a standard fully connected or dense layer, as used in MLPs. \n14.2.1.4 Boundary conditions and padding \nIn Equation (14.7), we saw that convolving a $3 times 3$ image with a $2 times 2$ filter resulted in a $2 times 2$ output. In general, convolving a $f _ { h } times f _ { w }$ filter over an image of size $x _ { h } times x _ { w }$ produces an output of size $( x _ { h } - f _ { h } + 1 ) times ( x _ { w } - f _ { w } + 1 )$ ; this is called valid convolution, since we only apply the filter to “valid” parts of the input, i.e., we don’t let it “slide off the ends”. If we want the output to have the same size as the input, we can use zero-padding, which means we add a border of 0s to the image, as illustrated in Figure 14.7. This is called same convolution. \nIn general, if the input has size $x _ { h } times x _ { w }$ , we use a kernel of size $f _ { h } times f _ { w }$ , we use zero padding on each side of size $p _ { h }$ and $p _ { w }$ , then the output has the following size [DV16]: \nFor example, consider Figure 14.8a. We have $p = 1$ , $f = 3$ , $x _ { h } = 5$ and $x _ { w } = 7$ , so the output has size \nIf we set $2 p = f - 1$ , then the output will have the same size as the input. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n14.2.1.5 Strided convolution \nSince each output pixel is generated by a weighted combination of inputs in its receptive field (based on the size of the filter), neighboring outputs will be very similar in value, since their inputs are overlapping. We can reduce this redundancy (and speedup computation) by skipping every $s$ ’th input. This is called strided convolution. This is illustrated in Figure 14.8b, where we convolve a $5 times 7$ image with a $3 times 3$ filter with stride 2 to get a $3 times 4$ output. \nIn general, if the input has size $x _ { h } times x _ { w }$ , we use a kernel of size $f _ { h } times f _ { w }$ , we use zero padding on each side of size $p _ { h }$ and $p _ { w }$ , and we use strides of size $s _ { h }$ and $s _ { w }$ , then the output has the following size [DV16]: \nFor example, consider Figure 14.8b, where we set the stride to $s = 2$ . Now the output is smaller than the input. \n14.2.1.6 Multiple input and output channels \nIn Figure 14.6, the input was a gray-scale image. In general, the input will have multiple channels (e.g., RGB, or hyper-spectral bands for satellite images). We can extend the definition of convolution to this case by defining a kernel for each input channel; thus now $mathbf { W }$ is a 3d weight matrix or tensor. We compute the output by convolving channel $c$ of the input with kernel $mathbf { W } _ { : , : , c }$ , and then summing over channels: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $s$ is the stride (which we assume is the same for both height and width, for simplicity), and $b$ is the bias term. This is illustrated in Figure 14.9. \n\nEach weight matrix can detect a single kind of feature. We typically want to detect multiple kinds of features, as illustrated in Figure 14.2. We can do this by making $mathbf { W }$ into a 4d weight matrix. The filter to detect feature type $d$ in input channel $c$ is stored in $mathbf { W } _ { : , : , c , d }$ . We extend the definition of convolution to this case as follows: \nThis is illustrated in Figure 14.10. Each vertical cylindrical column denotes the set of output features at a given location, $z _ { i , j , 1 : D }$ ; this is sometimes called a hypercolumn. Each element is a different weighted combination of the $C$ features in the receptive field of each of the feature maps in the layer below.2 \n14.2.1.7 $mathbf { 1 } times mathbf { 1 }$ (pointwise) convolution \nSometimes we just want to take a weighted combination of the features at a given location, rather than across locations. This can be done using 1x1 convolution, also called pointwise convolution. \nThis changes the number of channels from $C$ to $D$ , without changing the spatial dimensionality: \nThis can be thought of as a single layer MLP applied to each feature column in parallel. \n14.2.2 Pooling layers \nConvolution will preserve information about the location of input features (modulo reduced resolution), a property known as equivariance. In some case we want to be invariant to the location. For example, when performing image classification, we may just want to know if an object of interest (e.g., a face) is present anywhere in the image. \nOne simple way to achieve this is called max pooling, which just computes the maximum over its incoming values, as illustrated in Figure 14.12. An alternative is to use average pooling, which replaces the max by the mean. In either case, the output neuron has the same response no matter \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where the input pattern occurs within its receptive field. (Note that we apply pooling to each feature channel independently.)",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common layers",
        "subsubsection": "Convolutional layers"
    },
    {
        "content": "This changes the number of channels from $C$ to $D$ , without changing the spatial dimensionality: \nThis can be thought of as a single layer MLP applied to each feature column in parallel. \n14.2.2 Pooling layers \nConvolution will preserve information about the location of input features (modulo reduced resolution), a property known as equivariance. In some case we want to be invariant to the location. For example, when performing image classification, we may just want to know if an object of interest (e.g., a face) is present anywhere in the image. \nOne simple way to achieve this is called max pooling, which just computes the maximum over its incoming values, as illustrated in Figure 14.12. An alternative is to use average pooling, which replaces the max by the mean. In either case, the output neuron has the same response no matter \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where the input pattern occurs within its receptive field. (Note that we apply pooling to each feature channel independently.) \n\nIf we average over all the locations in a feature map, the method is called global average pooling. Thus we can convert a $H times W times D$ feature map into a $1 times 1 times D$ dimensional feature map; this can be reshaped to a $D$ -dimensional vector, which can be passed into a fully connected layer to map it to a $C$ -dimensional vector before passing into a softmax output. The use of global average pooling means we can apply the classifier to an image of any size, since the final feature map will always be converted to a fixed $D$ -dimensional vector before being mapped to a distribution over the $C$ classes. \n14.2.3 Putting it all together \nA common design pattern is to create a CNN by alternating convolutional layers with max pooling layers, followed by a final linear classification layer at the end. This is illustrated in Figure 14.13. (We omit normalization layers in this example, since the model is quite shallow.) This design pattern first appeared in Fukushima’s neocognitron [Fuk75], and was inspired by Hubel and Wiesel’s model of simple and complex cells in the human visual cortex [HW62]. In 1998 Yann LeCun used a similar design in his eponynous LeNet model [LeC+98], which used backpropagation and SGD to estimate the parameters. This design pattern continues to be popular in neurally-inspired models of visual object recognition [RP99], as well as various practical applications (see Section 14.3 and Section 14.5). \n14.2.4 Normalization layers \nThe basic design in Figure 14.13 works well for shallow CNNs, but it can be difficult to scale it to deeper models, due to problems with vanishing or exploding gradients, as explained in Section 13.4.2. A common solution to this problem is to add extra layers to the model, to standardize the statistics of the hidden units (i.e., to ensure they are zero mean and unit variance), just like we do to the inputs of many models. We discuss various kinds of normalization layers below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common layers",
        "subsubsection": "Pooling layers"
    },
    {
        "content": "If we average over all the locations in a feature map, the method is called global average pooling. Thus we can convert a $H times W times D$ feature map into a $1 times 1 times D$ dimensional feature map; this can be reshaped to a $D$ -dimensional vector, which can be passed into a fully connected layer to map it to a $C$ -dimensional vector before passing into a softmax output. The use of global average pooling means we can apply the classifier to an image of any size, since the final feature map will always be converted to a fixed $D$ -dimensional vector before being mapped to a distribution over the $C$ classes. \n14.2.3 Putting it all together \nA common design pattern is to create a CNN by alternating convolutional layers with max pooling layers, followed by a final linear classification layer at the end. This is illustrated in Figure 14.13. (We omit normalization layers in this example, since the model is quite shallow.) This design pattern first appeared in Fukushima’s neocognitron [Fuk75], and was inspired by Hubel and Wiesel’s model of simple and complex cells in the human visual cortex [HW62]. In 1998 Yann LeCun used a similar design in his eponynous LeNet model [LeC+98], which used backpropagation and SGD to estimate the parameters. This design pattern continues to be popular in neurally-inspired models of visual object recognition [RP99], as well as various practical applications (see Section 14.3 and Section 14.5). \n14.2.4 Normalization layers \nThe basic design in Figure 14.13 works well for shallow CNNs, but it can be difficult to scale it to deeper models, due to problems with vanishing or exploding gradients, as explained in Section 13.4.2. A common solution to this problem is to add extra layers to the model, to standardize the statistics of the hidden units (i.e., to ensure they are zero mean and unit variance), just like we do to the inputs of many models. We discuss various kinds of normalization layers below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common layers",
        "subsubsection": "Putting it all together"
    },
    {
        "content": "If we average over all the locations in a feature map, the method is called global average pooling. Thus we can convert a $H times W times D$ feature map into a $1 times 1 times D$ dimensional feature map; this can be reshaped to a $D$ -dimensional vector, which can be passed into a fully connected layer to map it to a $C$ -dimensional vector before passing into a softmax output. The use of global average pooling means we can apply the classifier to an image of any size, since the final feature map will always be converted to a fixed $D$ -dimensional vector before being mapped to a distribution over the $C$ classes. \n14.2.3 Putting it all together \nA common design pattern is to create a CNN by alternating convolutional layers with max pooling layers, followed by a final linear classification layer at the end. This is illustrated in Figure 14.13. (We omit normalization layers in this example, since the model is quite shallow.) This design pattern first appeared in Fukushima’s neocognitron [Fuk75], and was inspired by Hubel and Wiesel’s model of simple and complex cells in the human visual cortex [HW62]. In 1998 Yann LeCun used a similar design in his eponynous LeNet model [LeC+98], which used backpropagation and SGD to estimate the parameters. This design pattern continues to be popular in neurally-inspired models of visual object recognition [RP99], as well as various practical applications (see Section 14.3 and Section 14.5). \n14.2.4 Normalization layers \nThe basic design in Figure 14.13 works well for shallow CNNs, but it can be difficult to scale it to deeper models, due to problems with vanishing or exploding gradients, as explained in Section 13.4.2. A common solution to this problem is to add extra layers to the model, to standardize the statistics of the hidden units (i.e., to ensure they are zero mean and unit variance), just like we do to the inputs of many models. We discuss various kinds of normalization layers below. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n14.2.4.1 Batch normalization \nThe most popular normalization layer is called batch normalization (BN) [IS15]. This ensures the distribution of the activations within a layer has zero mean and unit variance, when averaged across the samples in a minibatch. More precisely, we replace the activation vector $z _ { n }$ (or sometimes the pre-activation vector $mathbf { Delta } _ { mathbf { u } }$ ) for example $n$ (in some layer) with $tilde { z } _ { n }$ , which is computed as follows: \nwhere $boldsymbol { B }$ is the minibatch containing example $n$ , $pmb { mu } _ { B }$ is the mean of the activations for this batch3, ${ pmb { sigma } } _ { B } ^ { 2 }$ is the corresponding variance, $hat { z } _ { n }$ is the standardized activation vector, $tilde { z } _ { n }$ is the shifted and scaled version (the output of the BN layer), $beta$ and $gamma$ are learnable parameters for this layer, and $epsilon > 0$ is a small constant. Since this transformation is differentiable, we can easily pass gradients back to the input of the layer and to the BN parameters $beta$ and $gamma$ . \nWhen applied to the input layer, batch normalization is equivalent to the usual standardization procedure we discussed in Section 10.2.8. Note that the mean and variance for the input layer can be computed once, since the data is static. However, the empirical means and variances of the internal layers keep changing, as the parameters adapt. (This is sometimes called “internal covariate shift”.) This is why we need to recompute $pmb { mu }$ and $sigma ^ { 2 }$ on each minibatch. \nAt test time, we may have a single input, so we cannot compute batch statistics. The standard solution to this is as follows: after training, compute $pmb { mu } _ { l }$ and $sigma _ { l } ^ { 2 }$ for layer $it { l }$ across all the examples in the training set (i.e. using the full batch), and then “freeze” these parameters, and add them to the list of other parameters for the layer, namely $beta _ { l }$ and $gamma _ { l }$ . At test time, we then use these frozen training values for $pmb { mu } _ { l }$ and $sigma _ { l } ^ { 2 }$ , rather than computing statistics from the test batch. Thus when using a model with BN, we need to specify if we are using it for inference or training. (See batchnorm_jax.ipynb for some sample code.) \nFor speed, we can combine a frozen batch norm layer with the previous layer. In particular suppose the previous layer computes $mathbf { X } mathbf { W } + pmb { b }$ ; combining this with BN gives $gamma odot ( mathbf { X } mathbf { W } + pmb { b } - pmb { mu } ) / pmb { sigma } + beta$ . If we define $mathbf { W } ^ { prime } = gamma odot mathbf { W } / sigma$ and $b ^ { prime } = gamma odot ( b - pmb { mu } ) / pmb { sigma } + beta$ , then we can write the combined layers as $mathbf { X } mathbf { W } ^ { prime } + pmb { b } ^ { prime }$ . This is called fused batchnorm. Similar tricks can be developed to speed up BN during training [Jun+19]. \nThe benefits of batch normalization (in terms of training speed and stability) can be quite dramatic, especially for deep CNNs. The exact reasons for this are still unclear, but BN seems to make the optimization landscape significantly smoother [San+18b]. It also reduces the sensitivity to the learning rate [ALL18]. In addition to computational advantages, it has statistical advantages. In particular, BN acts like a regularizer; indeed it can be shown to be equivalent to a form of approximate Bayesian inference [TAS18; Luo+19]. \n\nHowever, the reliance on a minibatch of data causes several problems. In particular, it can result in unstable estimates of the parameters when training with small batch sizes, although a more recent version of the method, known as batch renormalization [Iof17], partially addresses this. We discuss some other alternatives to batch norm below. \n14.2.4.2 Other kinds of normalization layer \nIn Section 14.2.4.1 we discussed batch normalization, which standardizes all the activations within a given feature channel to be zero mean and unit variance. This can significantly help with training, and allow for a larger learning rate. (See batchnorm_jax.ipynb for some sample code.) \nAlthough batch normalization works well, it struggles when the batch size is small, since the estimated mean and variance parameters can be unreliable. One solution is to compute the mean and variance by pooling statistics across other dimensions of the tensor, but not across examples in the batch. More precisely, let $z _ { i }$ refer to the $i$ ’th element of a tensor; in the case of 2d images, the index $i$ has 4 components, indicating batch, height, width and channel, $i = ( i _ { N } , i _ { H } , i _ { W } , i _ { C } )$ . We compute the mean and standard deviation for each index $z _ { i }$ as follows: \nwhere $S _ { i }$ is the set of elements we average over. We then compute $hat { z } _ { i } = ( z _ { i } - mu _ { i } ) / sigma _ { i }$ and $widetilde z _ { i } = gamma _ { c } hat { z } _ { i } + beta _ { c }$ , where $c$ is the channel corresponding to index $i$ . \nIn batch norm, we pool over batch, height, width, so $S _ { i }$ is the set of all location in the tensor that match the channel index of $i$ . To avoid problems with small batches, we can instead pool over channel, height and width, but match on the batch index. This is known as layer normalization [BKH16]. (See layer_norm_jax.ipynb for some sample code.) Alternatively, we can have separate normalization parameters for each example in the batch and for each channel. This is known as instance normalization [UVL16]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nA natural generalization of the above methods is known as group normalization [WH18], where we pool over all locations whose channel is in the same group as $i$ ’s. This is illustrated in Figure 14.14. Layer normalization is a special case in which there is a single group, containing all the channels. Instance normalization is a special case in which there are $C$ groups, one per channel. In [WH18], they show experimentally that it can be better (in terms of training speed, as well as training and test accuracies) to use groups that are larger than individual channels, but smaller than all the channels. \nMore recently, [SK20] proposed filter response normalization which is an alternative to batch norm that works well even with a minibatch size of 1. The idea is to define each group as all locations with a single channel and batch sample (as in instance normalization), but then to just divide by the mean squared norm instead of standardizing. That is, if the input (for a given channel and batch entry) is $boldsymbol { z } = mathbf { Z } _ { b , : , : , c } in mathbb { R } ^ { N }$ , we compute $hat { z } = z / sqrt { nu ^ { 2 } + epsilon }$ , where $begin{array} { r } { nu ^ { 2 } = sum _ { i j } z _ { b i j c } ^ { 2 } / N } end{array}$ , and then $tilde { z } = gamma _ { c } hat { z } + beta _ { c }$ . Since there is no mean centering, the activations can drift away from $0$ , which can have detrimental effects, especially with ReLU activations. To compensate for this, the authors propose to add a thresholded linear unit at the output. This has the form ${ pmb y } = operatorname* { m a x } ( { pmb x } , tau )$ , where $tau$ is a learnable offset. The combination of FRN and TLU results in good performance on image classification and object detection even with a batch size of 1. \n14.2.4.3 Normalizer-free networks \nRecently, [Bro+21] have proposed a method called normalizer-free networks, which is a way to train deep residual networks without using batchnorm or any other form of normalization layer. The key is to replace it with adaptive gradient clipping, as an alternative way to avoid training instabilities. That is, we use Equation (13.70), but adapt the clipping strength dynamically. The resulting model is faster to train, and more accurate, than other competitive models trained with batchnorm. \n14.3 Common architectures for image classification \nIt is common to use CNNs to perform image classification, which is the task of estimating the function $f : mathbb { R } ^ { H times W times K }  { 0 , 1 } ^ { C }$ , where $K$ is the number of input channels (e.g., $K = 3$ for RGB images), and $C$ is the number of class labels. \nIn this section, we briefly review various CNNs that have been developed over the years to solve image classification tasks. See e.g., [Kha+20] for a more extensive review of CNNs, and e.g., https://github.com/rwightman/pytorch-image-models for an up-to-date repository of code and models (in PyTorch). \n14.3.1 LeNet \nOne of the earliest CNNs, created in 1998, is known as LeNet [LeC+98], named after its creator, Yann LeCun. It was designed to classify images of handwritten digits, and was trained on the MNIST dataset introduced in Section 3.5.2. The model is shown in Figure 14.15. (See also Figure 14.16a for a more compact representation of the model.) Some predictions of this model are shown in Figure 14.17. After just 1 epoch, the test accuracy is already 98.8%. By contrast, the MLP in Section 13.2.4.2 had an accuracy of $9 5 . 9 %$ after 1 epoch. More rounds of training can further increase accuracy to a point where performance is indistinguishable from label noise. (See lenet_jax.ipynb for some sample code.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common layers",
        "subsubsection": "Normalization layers"
    },
    {
        "content": "A natural generalization of the above methods is known as group normalization [WH18], where we pool over all locations whose channel is in the same group as $i$ ’s. This is illustrated in Figure 14.14. Layer normalization is a special case in which there is a single group, containing all the channels. Instance normalization is a special case in which there are $C$ groups, one per channel. In [WH18], they show experimentally that it can be better (in terms of training speed, as well as training and test accuracies) to use groups that are larger than individual channels, but smaller than all the channels. \nMore recently, [SK20] proposed filter response normalization which is an alternative to batch norm that works well even with a minibatch size of 1. The idea is to define each group as all locations with a single channel and batch sample (as in instance normalization), but then to just divide by the mean squared norm instead of standardizing. That is, if the input (for a given channel and batch entry) is $boldsymbol { z } = mathbf { Z } _ { b , : , : , c } in mathbb { R } ^ { N }$ , we compute $hat { z } = z / sqrt { nu ^ { 2 } + epsilon }$ , where $begin{array} { r } { nu ^ { 2 } = sum _ { i j } z _ { b i j c } ^ { 2 } / N } end{array}$ , and then $tilde { z } = gamma _ { c } hat { z } + beta _ { c }$ . Since there is no mean centering, the activations can drift away from $0$ , which can have detrimental effects, especially with ReLU activations. To compensate for this, the authors propose to add a thresholded linear unit at the output. This has the form ${ pmb y } = operatorname* { m a x } ( { pmb x } , tau )$ , where $tau$ is a learnable offset. The combination of FRN and TLU results in good performance on image classification and object detection even with a batch size of 1. \n14.2.4.3 Normalizer-free networks \nRecently, [Bro+21] have proposed a method called normalizer-free networks, which is a way to train deep residual networks without using batchnorm or any other form of normalization layer. The key is to replace it with adaptive gradient clipping, as an alternative way to avoid training instabilities. That is, we use Equation (13.70), but adapt the clipping strength dynamically. The resulting model is faster to train, and more accurate, than other competitive models trained with batchnorm. \n14.3 Common architectures for image classification \nIt is common to use CNNs to perform image classification, which is the task of estimating the function $f : mathbb { R } ^ { H times W times K }  { 0 , 1 } ^ { C }$ , where $K$ is the number of input channels (e.g., $K = 3$ for RGB images), and $C$ is the number of class labels. \nIn this section, we briefly review various CNNs that have been developed over the years to solve image classification tasks. See e.g., [Kha+20] for a more extensive review of CNNs, and e.g., https://github.com/rwightman/pytorch-image-models for an up-to-date repository of code and models (in PyTorch). \n14.3.1 LeNet \nOne of the earliest CNNs, created in 1998, is known as LeNet [LeC+98], named after its creator, Yann LeCun. It was designed to classify images of handwritten digits, and was trained on the MNIST dataset introduced in Section 3.5.2. The model is shown in Figure 14.15. (See also Figure 14.16a for a more compact representation of the model.) Some predictions of this model are shown in Figure 14.17. After just 1 epoch, the test accuracy is already 98.8%. By contrast, the MLP in Section 13.2.4.2 had an accuracy of $9 5 . 9 %$ after 1 epoch. More rounds of training can further increase accuracy to a point where performance is indistinguishable from label noise. (See lenet_jax.ipynb for some sample code.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nOf course, classifying isolated digits is of limited applicability: in the real world, people usually write strings of digits or other letters. This requires both segmentation and classification. LeCun and colleagues devised a way to combine convolutional neural networks with a model similar to a conditional random field to solve this problem. The system was deployed by the US postal service. See [LeC+98] for a more detailed account of the system. \n14.3.2 AlexNet \nAlthough CNNs have been around for many years, it was not until the paper of [KSH12] in 2012 that mainstream computer vision researchers paid attention to them. In that paper, the authors showed how to reduce the (top 5) error rate on the ImageNet challenge (Section 1.5.1.2) from the previous best of 26% to $1 5 %$ , which was a dramatic improvement. This model became known as AlexNet model, named after its creator, Alex Krizhevsky. \nFigure 14.16b(b) shows the architecture. It is very similar to LeNet, shown in Figure 14.16a, with the following differences: it is deeper (8 layers of adjustable parameters (i.e., excluding the pooling layers) instead of 5); it uses ReLU nonlinearities instead of tanh (see Section 13.2.3 for why this is important); it uses dropout (Section 13.5.4) for regularization instead of weight decay; and it stacks several convolutional layers on top of each other, rather than strictly alternating between convolution and pooling. Stacking multiple convolutional layers together has the advantage that the receptive fields become larger as the output of one layer is fed into another (for example, three $3 times 3$ filters in a row will have a receptive field size of $7 times 7$ ). This is better than using a single layer with a larger receptive field, since the multiple layers also have nonlinearities in between. Also, three $3 times 3$ filters have fewer parameters than one $7 times 7$ . \nNote that AlexNet has 60M free parameters (which is much more than the 1M labeled examples), mostly due to the three fully connected layers at the output. Fitting this model relied on using two GPUs (due to limited memory of GPUs at that time), and is widely considered an engineering tour de force.4 Figure 1.14a shows some predictions made by the model on some images from ImageNet.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common architectures for image classification",
        "subsubsection": "LeNet"
    },
    {
        "content": "Of course, classifying isolated digits is of limited applicability: in the real world, people usually write strings of digits or other letters. This requires both segmentation and classification. LeCun and colleagues devised a way to combine convolutional neural networks with a model similar to a conditional random field to solve this problem. The system was deployed by the US postal service. See [LeC+98] for a more detailed account of the system. \n14.3.2 AlexNet \nAlthough CNNs have been around for many years, it was not until the paper of [KSH12] in 2012 that mainstream computer vision researchers paid attention to them. In that paper, the authors showed how to reduce the (top 5) error rate on the ImageNet challenge (Section 1.5.1.2) from the previous best of 26% to $1 5 %$ , which was a dramatic improvement. This model became known as AlexNet model, named after its creator, Alex Krizhevsky. \nFigure 14.16b(b) shows the architecture. It is very similar to LeNet, shown in Figure 14.16a, with the following differences: it is deeper (8 layers of adjustable parameters (i.e., excluding the pooling layers) instead of 5); it uses ReLU nonlinearities instead of tanh (see Section 13.2.3 for why this is important); it uses dropout (Section 13.5.4) for regularization instead of weight decay; and it stacks several convolutional layers on top of each other, rather than strictly alternating between convolution and pooling. Stacking multiple convolutional layers together has the advantage that the receptive fields become larger as the output of one layer is fed into another (for example, three $3 times 3$ filters in a row will have a receptive field size of $7 times 7$ ). This is better than using a single layer with a larger receptive field, since the multiple layers also have nonlinearities in between. Also, three $3 times 3$ filters have fewer parameters than one $7 times 7$ . \nNote that AlexNet has 60M free parameters (which is much more than the 1M labeled examples), mostly due to the three fully connected layers at the output. Fitting this model relied on using two GPUs (due to limited memory of GPUs at that time), and is widely considered an engineering tour de force.4 Figure 1.14a shows some predictions made by the model on some images from ImageNet. \n双 5 又2 + + F + SES → + GrRf\n14.3.3 GoogLeNet (Inception) \nGoogle who developed a model known as GoogLeNet [Sze+15a]. (The name is a pun on Google and LeNet.) The main difference from earlier models is that GoogLeNet used a new kind of block, known as an inception block $^ { 5 }$ , that employs multiple parallel pathways, each of which has a convolutional filter of a different size. See Figure 14.18 for an illustration. This lets the model learn what the optimal filter size should be at each level. The overall model consists of 9 inception blocks followed by global average pooling. See Figure 14.19 for an illustration. Since this model first came out, various extensions were proposed; details can be found in [IS15; Sze+15b; SIV17]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common architectures for image classification",
        "subsubsection": "AlexNet"
    },
    {
        "content": "双 5 又2 + + F + SES → + GrRf\n14.3.3 GoogLeNet (Inception) \nGoogle who developed a model known as GoogLeNet [Sze+15a]. (The name is a pun on Google and LeNet.) The main difference from earlier models is that GoogLeNet used a new kind of block, known as an inception block $^ { 5 }$ , that employs multiple parallel pathways, each of which has a convolutional filter of a different size. See Figure 14.18 for an illustration. This lets the model learn what the optimal filter size should be at each level. The overall model consists of 9 inception blocks followed by global average pooling. See Figure 14.19 for an illustration. Since this model first came out, various extensions were proposed; details can be found in [IS15; Sze+15b; SIV17]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n2 DT II-1I-I-- T \n14.3.4 ResNet \nThe winner of the 2015 ImageNet classification challenge was a team at Microsoft, who proposed a model known as ResNet [He+16a]. The key idea is to replace ${ pmb x } _ { l + 1 } = mathcal { F } _ { l } ( { pmb x } _ { l } )$ with \nThis is known as a residual block, since $mathcal { F } _ { l }$ only needs to learn the residual, or difference, between input and output of this layer, which is a simpler task. In [He+16a], $mathcal { F }$ has the form conv-BN-reluconv-BN, where conv is a convolutional layer, and BN is a batch norm layer (Section 14.2.4.1). See Figure 14.20(left) for an illustration.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common architectures for image classification",
        "subsubsection": "GoogLeNet (Inception)"
    },
    {
        "content": "2 DT II-1I-I-- T \n14.3.4 ResNet \nThe winner of the 2015 ImageNet classification challenge was a team at Microsoft, who proposed a model known as ResNet [He+16a]. The key idea is to replace ${ pmb x } _ { l + 1 } = mathcal { F } _ { l } ( { pmb x } _ { l } )$ with \nThis is known as a residual block, since $mathcal { F } _ { l }$ only needs to learn the residual, or difference, between input and output of this layer, which is a simpler task. In [He+16a], $mathcal { F }$ has the form conv-BN-reluconv-BN, where conv is a convolutional layer, and BN is a batch norm layer (Section 14.2.4.1). See Figure 14.20(left) for an illustration. \nWe can ensure the spatial dimensions of the output $mathcal { F } _ { l } ( pmb { x } _ { l } )$ of the convolutional layer match those of the input ${ boldsymbol { mathbf { mathit { x } } } } _ { l }$ by using padding. However, if we want to allow for the output of the convolutional layer to have a different number of channels, we need to add $1 times 1$ convolution to the skip connection on $boldsymbol { x } _ { l }$ . See Figure 14.20(right) for an illustration. \nThe use of residual blocks allows us to train very deep models. The reason this is possible is that gradient can flow directly from the output to earlier layers, via the skip connections, for reasons explained in Section 13.4.4. \nIn [He+16a] trained a 152 layer ResNet on ImageNet. However, it is common to use shallower models. For example, Figure 14.21 shows the ResNet-18 architecture, which has 18 trainable layers: there are 2 3x3 conv layers in each residual block, and there are 8 such blocks, with an initial 7x7 conv (stride 2) and a final fully connected layer. Symbolically, we can define the model as follows: \n(Conv : BN : Max) : (R : R) : (R’ : R) : (R’ : R) : (R’ : R) : Avg : FC where R is a residual block, R’ is a residual block with skip connection (due to the change in the number of channels) with stride 2, FC is fully connected (dense) layer, and : denotes concatenation. Note that the input size gets reduced spatially by a factor of $2 ^ { 5 } = 3 2$ (factor of 2 for each R’ block, plus the initial Conv-7x7(2) and Max-pool), so a 224x224 images becomes a 7x7 image before going into the global average pooling layer. \n\nSome code to fit these models can be found online.6 \nIn [He+16b], they showed how a small modification of the above scheme allows us to train models with up to 1001 layers. The key insight is that the signal on the skip connections is still being attentuated due to the use of the nonlinear activation function after the addition step, $pmb { x } _ { l + 1 } = varphi ( pmb { x } _ { l } + mathcal { F } ( pmb { x } _ { l } ) )$ . They showed that it is better to use \nThis is called a preactivation resnet or PreResnet for short. Now it is very easy for the network to learn the identity function at a given layer: if we use ReLU activations, we just need to ensure that $mathcal { F } _ { l } ( pmb { x } _ { l } ) = mathbf { 0 }$ , which we can do by setting the weights and biases to 0. \nAn alternative to using a very deep model is to use a very “wide” model, with lots of feature channels per layer. This is the idea behind the wide resnet model [ZK16], which is quite popular. \n14.3.5 DenseNet \nIn a residual net, we add the output of each function to its input. An alternative approach would be to concatenate the output with the input, as illustrated in Figure 14.22a. If we stack a series of such blocks, we can get an architecture similar to Figure 14.22b. This is known as a DenseNets [Hua+17a], since each layer densely depends on all previous layers. Thus the overall model is computing a function of the form",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common architectures for image classification",
        "subsubsection": "ResNet"
    },
    {
        "content": "We can ensure the spatial dimensions of the output $mathcal { F } _ { l } ( pmb { x } _ { l } )$ of the convolutional layer match those of the input ${ boldsymbol { mathbf { mathit { x } } } } _ { l }$ by using padding. However, if we want to allow for the output of the convolutional layer to have a different number of channels, we need to add $1 times 1$ convolution to the skip connection on $boldsymbol { x } _ { l }$ . See Figure 14.20(right) for an illustration. \nThe use of residual blocks allows us to train very deep models. The reason this is possible is that gradient can flow directly from the output to earlier layers, via the skip connections, for reasons explained in Section 13.4.4. \nIn [He+16a] trained a 152 layer ResNet on ImageNet. However, it is common to use shallower models. For example, Figure 14.21 shows the ResNet-18 architecture, which has 18 trainable layers: there are 2 3x3 conv layers in each residual block, and there are 8 such blocks, with an initial 7x7 conv (stride 2) and a final fully connected layer. Symbolically, we can define the model as follows: \n(Conv : BN : Max) : (R : R) : (R’ : R) : (R’ : R) : (R’ : R) : Avg : FC where R is a residual block, R’ is a residual block with skip connection (due to the change in the number of channels) with stride 2, FC is fully connected (dense) layer, and : denotes concatenation. Note that the input size gets reduced spatially by a factor of $2 ^ { 5 } = 3 2$ (factor of 2 for each R’ block, plus the initial Conv-7x7(2) and Max-pool), so a 224x224 images becomes a 7x7 image before going into the global average pooling layer. \n\nSome code to fit these models can be found online.6 \nIn [He+16b], they showed how a small modification of the above scheme allows us to train models with up to 1001 layers. The key insight is that the signal on the skip connections is still being attentuated due to the use of the nonlinear activation function after the addition step, $pmb { x } _ { l + 1 } = varphi ( pmb { x } _ { l } + mathcal { F } ( pmb { x } _ { l } ) )$ . They showed that it is better to use \nThis is called a preactivation resnet or PreResnet for short. Now it is very easy for the network to learn the identity function at a given layer: if we use ReLU activations, we just need to ensure that $mathcal { F } _ { l } ( pmb { x } _ { l } ) = mathbf { 0 }$ , which we can do by setting the weights and biases to 0. \nAn alternative to using a very deep model is to use a very “wide” model, with lots of feature channels per layer. This is the idea behind the wide resnet model [ZK16], which is quite popular. \n14.3.5 DenseNet \nIn a residual net, we add the output of each function to its input. An alternative approach would be to concatenate the output with the input, as illustrated in Figure 14.22a. If we stack a series of such blocks, we can get an architecture similar to Figure 14.22b. This is known as a DenseNets [Hua+17a], since each layer densely depends on all previous layers. Thus the overall model is computing a function of the form \nThe dense connectivity increases the number of parameters, since the channels get stacked depthwise. We can compensate for this by adding $1 times 1$ convolution layers in between. We can also add pooling layers with a stride of 2 to reduce the spatial resolution. (See densenet_jax.ipynb for some sample code.) \nDenseNets can perform better than ResNets, since all previously computed features are directly accessible to the output layer. However, they can be more computationally expensive. \n14.3.6 Neural architecture search \nWe have seen how many CNNs are fairly similar in their design, and simply rearrange various building blocks (such as convolutional or pooling layers) in different topologies, and adjust various parameter settings (e.g., stride, number of channels, or learning rate). Indeed, the recent ConvNeXt model of [Liu+22] — which, at the time of writing (April 2022) is considered the state of the art CNN architecture for a wide variety of vision tasks — was created by combining multiple such small improvements on top of a standard ResNet architecture. \nWe can automate this design process using blackbox (derivative free) optimization methods to find architectures that minimize the validation loss. This is called Auto-ML; in the context of neural nets, it is called neural architecture search (NAS). \nWhen performing NAS, we can optimize for multiple objectives at the same time, such as accuracy, model size, training or inference speed, etc (this is how EfficientNetv2 is created [TL21]). The main challenge arises due to the expense of computing the objective (since it requires training each candidate point in model space). One way to reduce the number of calls to the objective function is to use Bayesian optimization (see e.g., [WNS19]). Another approach is to create differentiable approximations to the loss (see e.g., [LSY19; Wan+21]), or to convert the architecture into a kernel function (using the neural tangent kernel method, Section 17.2.8), and then to analyze properties of its eigenvalues, which can predict performance without actually training the model [CGW21]. The field of NAS is very large and still growing. See [EMH19] for a more thorough review. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common architectures for image classification",
        "subsubsection": "DenseNet"
    },
    {
        "content": "The dense connectivity increases the number of parameters, since the channels get stacked depthwise. We can compensate for this by adding $1 times 1$ convolution layers in between. We can also add pooling layers with a stride of 2 to reduce the spatial resolution. (See densenet_jax.ipynb for some sample code.) \nDenseNets can perform better than ResNets, since all previously computed features are directly accessible to the output layer. However, they can be more computationally expensive. \n14.3.6 Neural architecture search \nWe have seen how many CNNs are fairly similar in their design, and simply rearrange various building blocks (such as convolutional or pooling layers) in different topologies, and adjust various parameter settings (e.g., stride, number of channels, or learning rate). Indeed, the recent ConvNeXt model of [Liu+22] — which, at the time of writing (April 2022) is considered the state of the art CNN architecture for a wide variety of vision tasks — was created by combining multiple such small improvements on top of a standard ResNet architecture. \nWe can automate this design process using blackbox (derivative free) optimization methods to find architectures that minimize the validation loss. This is called Auto-ML; in the context of neural nets, it is called neural architecture search (NAS). \nWhen performing NAS, we can optimize for multiple objectives at the same time, such as accuracy, model size, training or inference speed, etc (this is how EfficientNetv2 is created [TL21]). The main challenge arises due to the expense of computing the objective (since it requires training each candidate point in model space). One way to reduce the number of calls to the objective function is to use Bayesian optimization (see e.g., [WNS19]). Another approach is to create differentiable approximations to the loss (see e.g., [LSY19; Wan+21]), or to convert the architecture into a kernel function (using the neural tangent kernel method, Section 17.2.8), and then to analyze properties of its eigenvalues, which can predict performance without actually training the model [CGW21]. The field of NAS is very large and still growing. See [EMH19] for a more thorough review. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n14.4 Other forms of convolution * \nWe discussed the basics of convolution in Section 14.2. In this section, we discuss some extensions, which are needed for applications such as image segmentation and image generation. \n14.4.1 Dilated convolution \nConvolution is an operation that combines the pixel values in a local neighborhood. By using striding, and stacking many layers of convolution together, we can enlarge the receptive field of each neuron, which is the region of input space that each neuron responds to. However, we would need many layers to give each neuron enough context to cover the entire image (unless we used very large filters, which would be slow and require too many parameters). \nAs an alternative, we can use convolution with holes [Mal99], sometimes known by the French term à trous algorithm, and recently renamed dilated convolution [YK16]. This method simply takes every $r$ ’th input element when performing convolution, where $r$ is known as the rate or dilation factor. For example, in 1d, convolving with filter $mathbf { boldsymbol { w } }$ using rate $r = 2$ is equivalent to regular convolution using the filter $tilde { pmb { w } } = [ w _ { 1 } , 0 , w _ { 2 } , 0 , w _ { 3 } ]$ , where we have inserted 0s to expand the receptive field (hence the term “convolution with holes”). This allows us to get the benefit of increased receptive fields without increasing the number of parameters or the amount of compute. See Figure 14.23 for an illustration. \nMore precisely, dilated convolution in 2d is defined as follows: \nwhere we assume the same rate $r$ for both height and width, for simplicity. Compare this to Equation (14.15), where the stride parameter uses xsi+u,sj+v,c. \n14.4.2 Transposed convolution \nIn convolution, we reduce from a large input $mathbf { X }$ to a small output $mathbf { Y }$ by taking a weighted combination of the input pixels and the convolutional kernel $mathbf { K }$ . This is easiest to explain in code: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 def conv(X, K): h, $texttt { w } = texttt { K }$ .shape $textsf { Y } =$ zeros((X.shape[0] - $texttt { h + 1 }$ , X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] $mathbf { Sigma } = mathbf { Sigma }$ (X[i:i + h, j:j + w] * K).sum() return Y",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Common architectures for image classification",
        "subsubsection": "Neural architecture search"
    },
    {
        "content": "14.4 Other forms of convolution * \nWe discussed the basics of convolution in Section 14.2. In this section, we discuss some extensions, which are needed for applications such as image segmentation and image generation. \n14.4.1 Dilated convolution \nConvolution is an operation that combines the pixel values in a local neighborhood. By using striding, and stacking many layers of convolution together, we can enlarge the receptive field of each neuron, which is the region of input space that each neuron responds to. However, we would need many layers to give each neuron enough context to cover the entire image (unless we used very large filters, which would be slow and require too many parameters). \nAs an alternative, we can use convolution with holes [Mal99], sometimes known by the French term à trous algorithm, and recently renamed dilated convolution [YK16]. This method simply takes every $r$ ’th input element when performing convolution, where $r$ is known as the rate or dilation factor. For example, in 1d, convolving with filter $mathbf { boldsymbol { w } }$ using rate $r = 2$ is equivalent to regular convolution using the filter $tilde { pmb { w } } = [ w _ { 1 } , 0 , w _ { 2 } , 0 , w _ { 3 } ]$ , where we have inserted 0s to expand the receptive field (hence the term “convolution with holes”). This allows us to get the benefit of increased receptive fields without increasing the number of parameters or the amount of compute. See Figure 14.23 for an illustration. \nMore precisely, dilated convolution in 2d is defined as follows: \nwhere we assume the same rate $r$ for both height and width, for simplicity. Compare this to Equation (14.15), where the stride parameter uses xsi+u,sj+v,c. \n14.4.2 Transposed convolution \nIn convolution, we reduce from a large input $mathbf { X }$ to a small output $mathbf { Y }$ by taking a weighted combination of the input pixels and the convolutional kernel $mathbf { K }$ . This is easiest to explain in code: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 def conv(X, K): h, $texttt { w } = texttt { K }$ .shape $textsf { Y } =$ zeros((X.shape[0] - $texttt { h + 1 }$ , X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] $mathbf { Sigma } = mathbf { Sigma }$ (X[i:i + h, j:j + w] * K).sum() return Y",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Other forms of convolution *",
        "subsubsection": "Dilated convolution"
    },
    {
        "content": "14.4 Other forms of convolution * \nWe discussed the basics of convolution in Section 14.2. In this section, we discuss some extensions, which are needed for applications such as image segmentation and image generation. \n14.4.1 Dilated convolution \nConvolution is an operation that combines the pixel values in a local neighborhood. By using striding, and stacking many layers of convolution together, we can enlarge the receptive field of each neuron, which is the region of input space that each neuron responds to. However, we would need many layers to give each neuron enough context to cover the entire image (unless we used very large filters, which would be slow and require too many parameters). \nAs an alternative, we can use convolution with holes [Mal99], sometimes known by the French term à trous algorithm, and recently renamed dilated convolution [YK16]. This method simply takes every $r$ ’th input element when performing convolution, where $r$ is known as the rate or dilation factor. For example, in 1d, convolving with filter $mathbf { boldsymbol { w } }$ using rate $r = 2$ is equivalent to regular convolution using the filter $tilde { pmb { w } } = [ w _ { 1 } , 0 , w _ { 2 } , 0 , w _ { 3 } ]$ , where we have inserted 0s to expand the receptive field (hence the term “convolution with holes”). This allows us to get the benefit of increased receptive fields without increasing the number of parameters or the amount of compute. See Figure 14.23 for an illustration. \nMore precisely, dilated convolution in 2d is defined as follows: \nwhere we assume the same rate $r$ for both height and width, for simplicity. Compare this to Equation (14.15), where the stride parameter uses xsi+u,sj+v,c. \n14.4.2 Transposed convolution \nIn convolution, we reduce from a large input $mathbf { X }$ to a small output $mathbf { Y }$ by taking a weighted combination of the input pixels and the convolutional kernel $mathbf { K }$ . This is easiest to explain in code: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 def conv(X, K): h, $texttt { w } = texttt { K }$ .shape $textsf { Y } =$ zeros((X.shape[0] - $texttt { h + 1 }$ , X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] $mathbf { Sigma } = mathbf { Sigma }$ (X[i:i + h, j:j + w] * K).sum() return Y \n\nIn transposed convolution, we do the opposite, in order to produce a larger output from a smaller input: \ndef trans_conv(X, K): h, $texttt { w } = texttt { K }$ .shape $textsf { Y } =$ zeros((X.shape[0] + h - 1, X.shape[1] + w - 1)) for i in range(X.shape[0]): for j in range(X.shape[1]): $texttt { Y } [ texttt { i : i } + texttt { h }$ , j:j + w] += X[i, j] * K return Y \nThis is equivalent to padding the input image with $( h - 1 , w - 1 )$ 0s (on the bottom right), where $( h , w )$ is the kernel size, then placing a weighted copy of the kernel on each one of the input locations, where the weight is the corresponding pixel value, and then adding up. This process is illustrated in Figure 14.24. We can think of the kernel as a “stencil” that is used to generate the output, modulated by the weights in the input. \nThe term “transposed convolution” comes from the interpretation of convolution as matrix multiplication, which we discussed in Section 14.2.1.3. If $mathbf { W }$ is the matrix derived from kernel $mathbf { K }$ using the process illustrated in Equation (14.9), then one can show that Y = transposed- $mathrm { c o n v } ( mathbf { X } , mathbf { K } )$ is equivalent to $mathbf { Y } = mathrm { r e s h a p e } ( mathbf { W } ^ { mathsf { T } } mathrm { v e c } ( mathbf { X } ) )$ . See transposed_conv_jax.ipynb for a demo. \nNote that transposed convolution is also sometimes called deconvolution, but this is an incorrect usage of the term: deconvolution is the process of “undoing” the effect of convolution with a known filter, such as a blur filter, to recover the original input, as illustrated in Figure 14.25. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n14.4.3 Depthwise separable convolution \nStandard convolution uses a filter of size $H times W times C times D$ , which requires a lot of data to learn and a lot of time to compute with. A simplification, known as depthwise separable convolution, first convolves each input channel by a corresponding 2d filter $mathbf { boldsymbol { w } }$ , and then maps these $C$ channels to $D$ channels using $1 times 1$ convolution $mathbf { Delta } mathbf { w ^ { prime } }$ : \nSee Figure 14.26 for an illustration. \nTo see the advantage of this, let us consider a simple numerical example.7 Regular convolution of a $1 2 times 1 2 times 3$ input with a $5 times 5 times 3 times 2 5 6$ filter gives a $8 times 8 times 2 5 6$ output (assuming valid convolution: 12-5+1=8), as illustrated in Figure 14.13. With separable convolution, we start with $1 2 times 1 2 times 3$ input, convolve with a $5 times 5 times 1 times 1$ filter (across space but not channels) to get $8 times 8 times 3$ , then pointwise convolve (across channels but not space) with a $1 times 1 times 3 times 2 5 6$ filter to get a $8 times 8 times 2 5 6$ output. So the output has the same size as before, but we used many fewer parameters to define the layer, and used much less compute. For this reason, separable convolution is often used in lightweight CNN models, such as the MobileNet model [How+17; San+18a] and other edge devices. \n14.5 Solving other discriminative vision tasks with CNNs * \nIn this section, we briefly discuss how to tackle various other vision tasks using CNNs. Each task also introduces a new architectural innovation to the library of basic building blocks we have already seen. More details on CNNs for computer vision can be found in e.g., [Bro19]. \n14.5.1 Image tagging \nImage classification associates a single label with the whole image, i.e., the outputs are assumed to be mutually exclusive. In many problems, there may be multiple objects present, and we want to label all of them. This is known as image tagging, and is an application of multi-label prediction. In this case, we define the output space as $mathcal { Y } = { 0 , 1 } ^ { C }$ , where $C$ is the number of tag types. Since the output bits are independent (given the image), we should replace the final softmax with a set of $C$ logistic units.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Other forms of convolution *",
        "subsubsection": "Transposed convolution"
    },
    {
        "content": "14.4.3 Depthwise separable convolution \nStandard convolution uses a filter of size $H times W times C times D$ , which requires a lot of data to learn and a lot of time to compute with. A simplification, known as depthwise separable convolution, first convolves each input channel by a corresponding 2d filter $mathbf { boldsymbol { w } }$ , and then maps these $C$ channels to $D$ channels using $1 times 1$ convolution $mathbf { Delta } mathbf { w ^ { prime } }$ : \nSee Figure 14.26 for an illustration. \nTo see the advantage of this, let us consider a simple numerical example.7 Regular convolution of a $1 2 times 1 2 times 3$ input with a $5 times 5 times 3 times 2 5 6$ filter gives a $8 times 8 times 2 5 6$ output (assuming valid convolution: 12-5+1=8), as illustrated in Figure 14.13. With separable convolution, we start with $1 2 times 1 2 times 3$ input, convolve with a $5 times 5 times 1 times 1$ filter (across space but not channels) to get $8 times 8 times 3$ , then pointwise convolve (across channels but not space) with a $1 times 1 times 3 times 2 5 6$ filter to get a $8 times 8 times 2 5 6$ output. So the output has the same size as before, but we used many fewer parameters to define the layer, and used much less compute. For this reason, separable convolution is often used in lightweight CNN models, such as the MobileNet model [How+17; San+18a] and other edge devices. \n14.5 Solving other discriminative vision tasks with CNNs * \nIn this section, we briefly discuss how to tackle various other vision tasks using CNNs. Each task also introduces a new architectural innovation to the library of basic building blocks we have already seen. More details on CNNs for computer vision can be found in e.g., [Bro19]. \n14.5.1 Image tagging \nImage classification associates a single label with the whole image, i.e., the outputs are assumed to be mutually exclusive. In many problems, there may be multiple objects present, and we want to label all of them. This is known as image tagging, and is an application of multi-label prediction. In this case, we define the output space as $mathcal { Y } = { 0 , 1 } ^ { C }$ , where $C$ is the number of tag types. Since the output bits are independent (given the image), we should replace the final softmax with a set of $C$ logistic units.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Other forms of convolution *",
        "subsubsection": "Depthwise separable convolution"
    },
    {
        "content": "14.4.3 Depthwise separable convolution \nStandard convolution uses a filter of size $H times W times C times D$ , which requires a lot of data to learn and a lot of time to compute with. A simplification, known as depthwise separable convolution, first convolves each input channel by a corresponding 2d filter $mathbf { boldsymbol { w } }$ , and then maps these $C$ channels to $D$ channels using $1 times 1$ convolution $mathbf { Delta } mathbf { w ^ { prime } }$ : \nSee Figure 14.26 for an illustration. \nTo see the advantage of this, let us consider a simple numerical example.7 Regular convolution of a $1 2 times 1 2 times 3$ input with a $5 times 5 times 3 times 2 5 6$ filter gives a $8 times 8 times 2 5 6$ output (assuming valid convolution: 12-5+1=8), as illustrated in Figure 14.13. With separable convolution, we start with $1 2 times 1 2 times 3$ input, convolve with a $5 times 5 times 1 times 1$ filter (across space but not channels) to get $8 times 8 times 3$ , then pointwise convolve (across channels but not space) with a $1 times 1 times 3 times 2 5 6$ filter to get a $8 times 8 times 2 5 6$ output. So the output has the same size as before, but we used many fewer parameters to define the layer, and used much less compute. For this reason, separable convolution is often used in lightweight CNN models, such as the MobileNet model [How+17; San+18a] and other edge devices. \n14.5 Solving other discriminative vision tasks with CNNs * \nIn this section, we briefly discuss how to tackle various other vision tasks using CNNs. Each task also introduces a new architectural innovation to the library of basic building blocks we have already seen. More details on CNNs for computer vision can be found in e.g., [Bro19]. \n14.5.1 Image tagging \nImage classification associates a single label with the whole image, i.e., the outputs are assumed to be mutually exclusive. In many problems, there may be multiple objects present, and we want to label all of them. This is known as image tagging, and is an application of multi-label prediction. In this case, we define the output space as $mathcal { Y } = { 0 , 1 } ^ { C }$ , where $C$ is the number of tag types. Since the output bits are independent (given the image), we should replace the final softmax with a set of $C$ logistic units. \nUsers of social media sites like Instagram often create hashtags for their images; this therefore provides a “free” way of creating large supervised datasets. Of course, many tags may be quite sparsely used, and their meaning may not be well-defined visually. (For example, someone may take a photo of themselves after they get a COVID test and tag the image “#covid”; however, visually it just looks like any other image of a person.) Thus this kind of user-generated labeling is usually considered quite noisy. However, it can be useful for “pre-training”, as discussed in [Mah+18]. \nFinally, it is worth noting that image tagging is often a much more sensible objective than image classification, since many images have multiple objects in them, and it can be hard to know which one we should be labeling. Indeed, Andrej Karpathy, who created the “human performance benchmark” on ImageNet, noted the following:8 \nBoth [CNNs] and humans struggle with images that contain multiple ImageNet classes (usually many more than five), with little indication of which object is the focus of the image. This error is only present in the classification setting, since every image is constrained to have exactly one correct label. In total, we attribute 16% of human errors to this category. \n14.5.2 Object detection \nIn some cases, we want to produce a variable number of outputs, corresponding to a variable number of objects of interest that may be present in the image. (This is an example of an open world problem, with an unknown number of objects.) \nA canonical example of this is object detection, in which we must return a set of bounding boxes representing the locations of objects of interest, together with their class labels. A special case of this is face detection, where there is only one class of interest. This is illustrated in Figure 14.27a.9",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Solving other discriminative vision tasks with CNNs *",
        "subsubsection": "Image tagging"
    },
    {
        "content": "Users of social media sites like Instagram often create hashtags for their images; this therefore provides a “free” way of creating large supervised datasets. Of course, many tags may be quite sparsely used, and their meaning may not be well-defined visually. (For example, someone may take a photo of themselves after they get a COVID test and tag the image “#covid”; however, visually it just looks like any other image of a person.) Thus this kind of user-generated labeling is usually considered quite noisy. However, it can be useful for “pre-training”, as discussed in [Mah+18]. \nFinally, it is worth noting that image tagging is often a much more sensible objective than image classification, since many images have multiple objects in them, and it can be hard to know which one we should be labeling. Indeed, Andrej Karpathy, who created the “human performance benchmark” on ImageNet, noted the following:8 \nBoth [CNNs] and humans struggle with images that contain multiple ImageNet classes (usually many more than five), with little indication of which object is the focus of the image. This error is only present in the classification setting, since every image is constrained to have exactly one correct label. In total, we attribute 16% of human errors to this category. \n14.5.2 Object detection \nIn some cases, we want to produce a variable number of outputs, corresponding to a variable number of objects of interest that may be present in the image. (This is an example of an open world problem, with an unknown number of objects.) \nA canonical example of this is object detection, in which we must return a set of bounding boxes representing the locations of objects of interest, together with their class labels. A special case of this is face detection, where there is only one class of interest. This is illustrated in Figure 14.27a.9 \nThe simplest way to tackle such detection problems is to convert it into a closed world problem, in which there is a finite number of possible locations (and orientations) any object can be in. These candidate locations are known as anchor boxes. We can create boxes at multiple locations, scales and aspect ratios, as illustrated in Figure 14.27b. For each box, we train the system to predict what category of object it contains (if any); we can also perform regression to predict the offset of the object location from the center of the anchor. (These residual regression terms allow sub-grid spatial localization.) \nAbstractly, we are learning a function of the form \nwhere $K$ is the number of input channels, $A$ is the number of anchor boxes in each dimension, and $C$ is the number of object types (class labels). For each box location $( i , j )$ , we predict three outputs: an object presence probability, $p _ { i j } in [ 0 , 1 ]$ , an object category, $y _ { i j } in { 1 , ldots , C }$ , and two 2d offset vectors, $pmb { delta } _ { i j } in mathbb { R } ^ { 4 }$ , which can be added to the centroid of the box to get the top left and bottom right corners. \nSeveral models of this type have been proposed, including the single shot detector model of [Liu+16], and the YOLO (you only look once) model of [Red+16]. Many other methods for object detection have been proposed over the years. These models make different tradeoffs between speed, accuracy, simplicity, etc. See [Hua+17b] for an empirical comparison, and [Zha+18] for a more recent review. \n14.5.3 Instance segmentation \nIn object detection, we predict a label and bounding box for each object. In instance segmentation, the goal is to predict the label and 2d shape mask of each object instance in the image, as illustrated in Figure 14.28. This can be done by applying a semantic segmentation model to each detected box,",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Solving other discriminative vision tasks with CNNs *",
        "subsubsection": "Object detection"
    },
    {
        "content": "The simplest way to tackle such detection problems is to convert it into a closed world problem, in which there is a finite number of possible locations (and orientations) any object can be in. These candidate locations are known as anchor boxes. We can create boxes at multiple locations, scales and aspect ratios, as illustrated in Figure 14.27b. For each box, we train the system to predict what category of object it contains (if any); we can also perform regression to predict the offset of the object location from the center of the anchor. (These residual regression terms allow sub-grid spatial localization.) \nAbstractly, we are learning a function of the form \nwhere $K$ is the number of input channels, $A$ is the number of anchor boxes in each dimension, and $C$ is the number of object types (class labels). For each box location $( i , j )$ , we predict three outputs: an object presence probability, $p _ { i j } in [ 0 , 1 ]$ , an object category, $y _ { i j } in { 1 , ldots , C }$ , and two 2d offset vectors, $pmb { delta } _ { i j } in mathbb { R } ^ { 4 }$ , which can be added to the centroid of the box to get the top left and bottom right corners. \nSeveral models of this type have been proposed, including the single shot detector model of [Liu+16], and the YOLO (you only look once) model of [Red+16]. Many other methods for object detection have been proposed over the years. These models make different tradeoffs between speed, accuracy, simplicity, etc. See [Hua+17b] for an empirical comparison, and [Zha+18] for a more recent review. \n14.5.3 Instance segmentation \nIn object detection, we predict a label and bounding box for each object. In instance segmentation, the goal is to predict the label and 2d shape mask of each object instance in the image, as illustrated in Figure 14.28. This can be done by applying a semantic segmentation model to each detected box, \n14.5. Solving other discriminative vision tasks with CNNs * \nwhich has to label each pixel as foreground or background. (See Section 14.5.4 for more details on semantic segmentation.) \n14.5.4 Semantic segmentation \nIn semantic segmentation, we have to predict a class label $y _ { i } in { 1 , ldots , C }$ for each pixel, where the classes may represent things like sky, road, car, etc. In contrast to instance segmentation, which we discussed in Section 14.5.3, all car pixels get the same label, so semantic segmentation does not differentiate between objects. We can combine semantic segmentation of “stuff” (like sky, road) and instance segmentation of “things” (like car, person) into a coherent framework called “panoptic segmentation” [Kir+19]. \nA common way to tackle semantic segmentation is to use an encoder-decoder architecture, as illustrated in Figure 14.29. The encoder uses standard convolution to map the input into a small 2d bottleneck, which captures high level properties of the input at a coarse spatial resolution. (This typically uses a technique called dilated convolution that we explain in Section 14.4.1, to capture a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license large field of view, i.e., more context.) The decoder maps the small 2d bottleneck back to a full-sized output image using a technique called transposed convolution that we explain in Section 14.4.2. Since the bottleneck loses information, we can also add skip connections from input layers to output layers. We can redraw this model as shown in Figure 14.30. Since the overall structure resembles the letter U, this is also known as a U-net [RFB15].",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Solving other discriminative vision tasks with CNNs *",
        "subsubsection": "Instance segmentation"
    },
    {
        "content": "14.5. Solving other discriminative vision tasks with CNNs * \nwhich has to label each pixel as foreground or background. (See Section 14.5.4 for more details on semantic segmentation.) \n14.5.4 Semantic segmentation \nIn semantic segmentation, we have to predict a class label $y _ { i } in { 1 , ldots , C }$ for each pixel, where the classes may represent things like sky, road, car, etc. In contrast to instance segmentation, which we discussed in Section 14.5.3, all car pixels get the same label, so semantic segmentation does not differentiate between objects. We can combine semantic segmentation of “stuff” (like sky, road) and instance segmentation of “things” (like car, person) into a coherent framework called “panoptic segmentation” [Kir+19]. \nA common way to tackle semantic segmentation is to use an encoder-decoder architecture, as illustrated in Figure 14.29. The encoder uses standard convolution to map the input into a small 2d bottleneck, which captures high level properties of the input at a coarse spatial resolution. (This typically uses a technique called dilated convolution that we explain in Section 14.4.1, to capture a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license large field of view, i.e., more context.) The decoder maps the small 2d bottleneck back to a full-sized output image using a technique called transposed convolution that we explain in Section 14.4.2. Since the bottleneck loses information, we can also add skip connections from input layers to output layers. We can redraw this model as shown in Figure 14.30. Since the overall structure resembles the letter U, this is also known as a U-net [RFB15]. \n\nA similar encoder-decoder architecture can be used for other dense prediction or image-toimage tasks, such as depth prediction (predict the distance from the camera, $z _ { i } in mathbb { R }$ , for each pixel $textit { textbf { l } }$ ), surface normal prediction (predict the orientation of the surface, $z _ { i } in mathbb { R } ^ { 3 }$ , at each image patch), etc. We can of course train one model to solve all of these tasks simultaneously, using multiple output heads, as illustrated in Figure 14.31. (See e.g., [Kok17] for details.) \n14.5.5 Human pose estimation \nWe can train an object detector to detect people, and to predict their 2d shape, as represented by a mask. However, we can also train the model to predict the location of a fixed set of skeletal keypoints, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 e.g., the location of the head or hands. This is called human pose estimation. See Figure 14.32 for an example. There are several techniques for this, e.g., PersonLab [Pap+18] and OpenPose [Cao+18]. See [Bab19] for a recent review.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Solving other discriminative vision tasks with CNNs *",
        "subsubsection": "Semantic segmentation"
    },
    {
        "content": "A similar encoder-decoder architecture can be used for other dense prediction or image-toimage tasks, such as depth prediction (predict the distance from the camera, $z _ { i } in mathbb { R }$ , for each pixel $textit { textbf { l } }$ ), surface normal prediction (predict the orientation of the surface, $z _ { i } in mathbb { R } ^ { 3 }$ , at each image patch), etc. We can of course train one model to solve all of these tasks simultaneously, using multiple output heads, as illustrated in Figure 14.31. (See e.g., [Kok17] for details.) \n14.5.5 Human pose estimation \nWe can train an object detector to detect people, and to predict their 2d shape, as represented by a mask. However, we can also train the model to predict the location of a fixed set of skeletal keypoints, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 e.g., the location of the head or hands. This is called human pose estimation. See Figure 14.32 for an example. There are several techniques for this, e.g., PersonLab [Pap+18] and OpenPose [Cao+18]. See [Bab19] for a recent review. \n\nWe can also predict 3d properties of each detected object. The main limitation is the ability to collect enough labeled training data, since it is difficult for human annotators to label things in 3d. However, we can use computer graphics engines to create simulated images with infinite ground truth 3d annotations (see e.g., [GNK18]). \n14.6 Generating images by inverting CNNs * \nA CNN trained for image classification is a discriminative model of the form $p ( boldsymbol { y } | boldsymbol { x } )$ , which takes as input an image, and returns as output a probability distribution over $C$ class labels. In this section we discuss how to “invert” this model, by converting it into a (conditional) generative image model of the form $p ( { pmb x } | y )$ . This will allow us to generate images that belong to a specific class. (We discuss more principled approaches to creating generative models for images in the sequel to this book, [Mur23].) \n14.6.1 Converting a trained classifier into a generative model \nWe can define a joint distribution over images and labels using $p ( pmb { x } , y ) = p ( pmb { x } ) p ( y | pmb { x } )$ , where $p ( boldsymbol { y } | boldsymbol { x } )$ is the CNN classifier, and $p ( { pmb x } )$ is some prior over images. If we then clamp the class label to a specific value, we can create a conditional generative model using $p ( { pmb x } | y ) propto p ( { pmb x } ) p ( y | { pmb x } )$ . Note that the discriminative classifier $p ( boldsymbol { y } | boldsymbol { x } )$ was trained to “throw away” information, so $p ( boldsymbol { y } | boldsymbol { x } )$ is not an invertible function. Thus the prior term $p ( { pmb x } )$ will play an important role in regularizing this process, as we see in Section 14.6.2. \nOne way to sample from this model is to use the Metropolis Hastings algorithm (Section 4.6.8.4), treating $begin{array} { r } { mathcal { E } _ { c } ( pmb { x } ) = log p ( y = c vert pmb { x } ) + log p ( pmb { x } ) } end{array}$ as the energy function. Since gradient information is available, we can use a proposal of the form $q ( pmb { x } ^ { prime } | pmb { x } ) = mathcal { N } ( pmb { mu } ( pmb { x } ) , epsilon mathbf { I } )$ , where $begin{array} { r } { pmb { mu } ( pmb { x } ) = pmb { x } + frac { epsilon } { 2 } nabla log mathcal { E } _ { c } ( pmb { x } ) } end{array}$ . This is called the Metropolis-adjusted Langevin algorithm (MALA). As an approximation, we can ignore the rejection step, and accept every proposal. This is called the unadjusted Langevin algorithm, and was used in [Ngu+17] for conditional image generation. In addition, we can scale \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license the gradient of the log prior and log likelihood independently. Thus we get an update over the space of images that looks like a noisy version of SGD, except we take derivatives wrt the input pixels (using Equation (13.50)), instead of the parameters:",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Solving other discriminative vision tasks with CNNs *",
        "subsubsection": "Human pose estimation"
    },
    {
        "content": "We can also predict 3d properties of each detected object. The main limitation is the ability to collect enough labeled training data, since it is difficult for human annotators to label things in 3d. However, we can use computer graphics engines to create simulated images with infinite ground truth 3d annotations (see e.g., [GNK18]). \n14.6 Generating images by inverting CNNs * \nA CNN trained for image classification is a discriminative model of the form $p ( boldsymbol { y } | boldsymbol { x } )$ , which takes as input an image, and returns as output a probability distribution over $C$ class labels. In this section we discuss how to “invert” this model, by converting it into a (conditional) generative image model of the form $p ( { pmb x } | y )$ . This will allow us to generate images that belong to a specific class. (We discuss more principled approaches to creating generative models for images in the sequel to this book, [Mur23].) \n14.6.1 Converting a trained classifier into a generative model \nWe can define a joint distribution over images and labels using $p ( pmb { x } , y ) = p ( pmb { x } ) p ( y | pmb { x } )$ , where $p ( boldsymbol { y } | boldsymbol { x } )$ is the CNN classifier, and $p ( { pmb x } )$ is some prior over images. If we then clamp the class label to a specific value, we can create a conditional generative model using $p ( { pmb x } | y ) propto p ( { pmb x } ) p ( y | { pmb x } )$ . Note that the discriminative classifier $p ( boldsymbol { y } | boldsymbol { x } )$ was trained to “throw away” information, so $p ( boldsymbol { y } | boldsymbol { x } )$ is not an invertible function. Thus the prior term $p ( { pmb x } )$ will play an important role in regularizing this process, as we see in Section 14.6.2. \nOne way to sample from this model is to use the Metropolis Hastings algorithm (Section 4.6.8.4), treating $begin{array} { r } { mathcal { E } _ { c } ( pmb { x } ) = log p ( y = c vert pmb { x } ) + log p ( pmb { x } ) } end{array}$ as the energy function. Since gradient information is available, we can use a proposal of the form $q ( pmb { x } ^ { prime } | pmb { x } ) = mathcal { N } ( pmb { mu } ( pmb { x } ) , epsilon mathbf { I } )$ , where $begin{array} { r } { pmb { mu } ( pmb { x } ) = pmb { x } + frac { epsilon } { 2 } nabla log mathcal { E } _ { c } ( pmb { x } ) } end{array}$ . This is called the Metropolis-adjusted Langevin algorithm (MALA). As an approximation, we can ignore the rejection step, and accept every proposal. This is called the unadjusted Langevin algorithm, and was used in [Ngu+17] for conditional image generation. In addition, we can scale \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license the gradient of the log prior and log likelihood independently. Thus we get an update over the space of images that looks like a noisy version of SGD, except we take derivatives wrt the input pixels (using Equation (13.50)), instead of the parameters: \n\nWe can interpret each term in this equation as follows: the $epsilon _ { 1 }$ term ensures the image is plausible under the prior, the $epsilon _ { 2 }$ term ensures the image is plausible under the likelihood, and the $epsilon _ { 3 }$ term is a noise term, in order to generate diverse samples. If we set $epsilon _ { 3 } = 0$ , the method becomes a deterministic algorithm to (approximately) generate the “most likely image” for this class. \n14.6.2 Image priors \nIn this section, we discuss various kinds of image priors that we can use to regularize the ill-posed problem of inverting a classifier. These priors, together with the image that we start the optimization from, will determine the kinds of outputs that we generate. \n14.6.2.1 Gaussian prior \nJust specifying the class label is not enough information to specify the kind of images we want. We also need a prior $p ( { pmb x } )$ over what constitutes a “plausible” image. The prior can have a large effect on the quality of the resulting image, as we show below. \nArguably the simplest prior is $p ( pmb { x } ) = mathcal { N } ( pmb { x } | mathbf { 0 } , mathbf { I } )$ , as suggested in [SVZ14]. (This assumes the image pixels have been centered.) This can prevent pixels from taking on extreme values. In this case, the update due to the prior term has the form \nThus the overall update (assuming $epsilon _ { 2 } = 1$ and $epsilon _ { 3 } = 0$ ) has the form \nSee Figure 14.33 for some samples generated by this method. \n14.6.2.2 Total variation (TV) prior \nWe can generate slightly more realistic looking images if we use additional regularizers. [MV15; MV16] suggested computing the total variation or TV norm of the image. This is equal to the integral of the per-pixel gradients, which can be approximated as follows: \nwhere $x _ { i j k }$ is the pixel value in row $i$ , column $j$ and channel $k$ (for RGB images). We can rewrite this in terms of the horizontal and vertical Sobel edge detector applied to each channel: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Generating images by inverting CNNs *",
        "subsubsection": "Converting a trained classifier into a generative model"
    },
    {
        "content": "We can interpret each term in this equation as follows: the $epsilon _ { 1 }$ term ensures the image is plausible under the prior, the $epsilon _ { 2 }$ term ensures the image is plausible under the likelihood, and the $epsilon _ { 3 }$ term is a noise term, in order to generate diverse samples. If we set $epsilon _ { 3 } = 0$ , the method becomes a deterministic algorithm to (approximately) generate the “most likely image” for this class. \n14.6.2 Image priors \nIn this section, we discuss various kinds of image priors that we can use to regularize the ill-posed problem of inverting a classifier. These priors, together with the image that we start the optimization from, will determine the kinds of outputs that we generate. \n14.6.2.1 Gaussian prior \nJust specifying the class label is not enough information to specify the kind of images we want. We also need a prior $p ( { pmb x } )$ over what constitutes a “plausible” image. The prior can have a large effect on the quality of the resulting image, as we show below. \nArguably the simplest prior is $p ( pmb { x } ) = mathcal { N } ( pmb { x } | mathbf { 0 } , mathbf { I } )$ , as suggested in [SVZ14]. (This assumes the image pixels have been centered.) This can prevent pixels from taking on extreme values. In this case, the update due to the prior term has the form \nThus the overall update (assuming $epsilon _ { 2 } = 1$ and $epsilon _ { 3 } = 0$ ) has the form \nSee Figure 14.33 for some samples generated by this method. \n14.6.2.2 Total variation (TV) prior \nWe can generate slightly more realistic looking images if we use additional regularizers. [MV15; MV16] suggested computing the total variation or TV norm of the image. This is equal to the integral of the per-pixel gradients, which can be approximated as follows: \nwhere $x _ { i j k }$ is the pixel value in row $i$ , column $j$ and channel $k$ (for RGB images). We can rewrite this in terms of the horizontal and vertical Sobel edge detector applied to each channel: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nSee Figure 14.34 for an illustration of these edge detectors. Using $p ( pmb { x } ) propto exp ( - mathrm { T V } ( pmb { x } ) )$ discourages images from having high frequency artefacts. In [Yos+15], they use Gaussian blur instead of TV norm, but this has a similar effect. \nIn Figure 14.35 we show some results of optimizing $log p ( y = c , pmb { x } )$ using a TV prior and a CNN likelihood for different class labels $c$ starting from random noise. \n14.6.3 Visualizing the features learned by a CNN \nIt is interesting to ask what the “neurons” in a CNN are learning. One way to do this is to start with a random image, and then to optimize the input pixels so as to maximize the average activation of a particular neuron. This is called activation maximization (AM), and uses the same technique as in Section 14.6.1 but fixes an internal node to a specific value, rather than clamping the output class label. \nFigure 14.36 illustrates the output of this method (with the TV prior) when applied to the AlexNet CNN trained on Imagenet classification. We see that, as the depth increases, neurons are learning to recognize simple edges/blobs, then texture patterns, then object parts, and finally whole objects. This is believed to be roughly similar to the hierarchical structure of the visual cortex (see e.g., [Kan+12]). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Generating images by inverting CNNs *",
        "subsubsection": "Image priors"
    },
    {
        "content": "See Figure 14.34 for an illustration of these edge detectors. Using $p ( pmb { x } ) propto exp ( - mathrm { T V } ( pmb { x } ) )$ discourages images from having high frequency artefacts. In [Yos+15], they use Gaussian blur instead of TV norm, but this has a similar effect. \nIn Figure 14.35 we show some results of optimizing $log p ( y = c , pmb { x } )$ using a TV prior and a CNN likelihood for different class labels $c$ starting from random noise. \n14.6.3 Visualizing the features learned by a CNN \nIt is interesting to ask what the “neurons” in a CNN are learning. One way to do this is to start with a random image, and then to optimize the input pixels so as to maximize the average activation of a particular neuron. This is called activation maximization (AM), and uses the same technique as in Section 14.6.1 but fixes an internal node to a specific value, rather than clamping the output class label. \nFigure 14.36 illustrates the output of this method (with the TV prior) when applied to the AlexNet CNN trained on Imagenet classification. We see that, as the depth increases, neurons are learning to recognize simple edges/blobs, then texture patterns, then object parts, and finally whole objects. This is believed to be roughly similar to the hierarchical structure of the visual cortex (see e.g., [Kan+12]). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nAn alternative to optimizing in pixel space is to search the training set for images that maximally activate a given neuron. This is illustrated in Figure 14.36 for the Conv5 layer. For more information on feature visualization see e.g., [OMS17]. \n14.6.4 Deep Dream \nSo far we have focused on generating images which maximize the class label or some other neuron of interest. In this section we tackle a more artistic application, in which we want to generate versions of an input image that emphasize certain features. \nTo do this, we view our pre-trained image classifier as a feature extractor. Based on the results in Section 14.6.3, we know the activity of neurons in different layers correspond to different kinds \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 of features in the image. Suppose we are interested in “amplifying” features from layers $textit { l } in textit { L }$ . We can do this by defining an energy or loss function of the form $begin{array} { r } { mathcal { L } ( pmb { x } ) = sum _ { l in mathcal { L } } overline { { phi } } _ { l } ( pmb { x } ) } end{array}$ , where $begin{array} { r } { overline { { phi } } _ { l } = frac { 1 } { H W C } sum _ { h w c } phi _ { l h w c } ( { pmb x } ) } end{array}$ is the feature vector for layer $it { l }$ . We can now use gradient descent to optimize this energy. The resulting process is called DeepDream [MOT15], since the model amplifies features that were only hinted at in the original image and then creates images with more and more of them.10",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Generating images by inverting CNNs *",
        "subsubsection": "Visualizing the features learned by a CNN"
    },
    {
        "content": "An alternative to optimizing in pixel space is to search the training set for images that maximally activate a given neuron. This is illustrated in Figure 14.36 for the Conv5 layer. For more information on feature visualization see e.g., [OMS17]. \n14.6.4 Deep Dream \nSo far we have focused on generating images which maximize the class label or some other neuron of interest. In this section we tackle a more artistic application, in which we want to generate versions of an input image that emphasize certain features. \nTo do this, we view our pre-trained image classifier as a feature extractor. Based on the results in Section 14.6.3, we know the activity of neurons in different layers correspond to different kinds \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 of features in the image. Suppose we are interested in “amplifying” features from layers $textit { l } in textit { L }$ . We can do this by defining an energy or loss function of the form $begin{array} { r } { mathcal { L } ( pmb { x } ) = sum _ { l in mathcal { L } } overline { { phi } } _ { l } ( pmb { x } ) } end{array}$ , where $begin{array} { r } { overline { { phi } } _ { l } = frac { 1 } { H W C } sum _ { h w c } phi _ { l h w c } ( { pmb x } ) } end{array}$ is the feature vector for layer $it { l }$ . We can now use gradient descent to optimize this energy. The resulting process is called DeepDream [MOT15], since the model amplifies features that were only hinted at in the original image and then creates images with more and more of them.10 \n\nFigure 14.37 shows an example. We start with an image of a jellyfish, which we pass into a CNN that was trained to classify ImageNet images. After several iterations, we generate some image which is a hybrid of the input and the kinds of “hallucinations” we saw in Figure 14.33; these hallucinations involve dog parts, since ImageNet has so many kinds of dogs in its label set. See [Tho16] for details, and https://deepdreamgenerator.com for a fun web-based demo. \n14.6.5 Neural style transfer \nThe DeepDream system in Figure 14.37 shows one way that CNNs can be used to create “art”. However, it is rather creepy. In this section, we discuss a related approach that gives the user more control. In particular, the user has to specify a reference “style image” $scriptstyle { mathbf { x } } _ { s }$ and “content image” $x _ { c }$ . The system will then try to generate a new image $_ { x }$ that “re-renders” $x _ { c }$ in the style of $x _ { s }$ . This is called neural style transfer, and is illustrated in Figure 14.38 and Figure 14.39. This technique was first proposed in [GEB16], and there are now many papers on this topic; see [Jin+17] for a recent review. \n14.6.5.1 How it works \nStyle transfer works by optimizing the following energy function: \nSee Figure 14.40 for a high level illustration.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Generating images by inverting CNNs *",
        "subsubsection": "Deep Dream"
    },
    {
        "content": "Figure 14.37 shows an example. We start with an image of a jellyfish, which we pass into a CNN that was trained to classify ImageNet images. After several iterations, we generate some image which is a hybrid of the input and the kinds of “hallucinations” we saw in Figure 14.33; these hallucinations involve dog parts, since ImageNet has so many kinds of dogs in its label set. See [Tho16] for details, and https://deepdreamgenerator.com for a fun web-based demo. \n14.6.5 Neural style transfer \nThe DeepDream system in Figure 14.37 shows one way that CNNs can be used to create “art”. However, it is rather creepy. In this section, we discuss a related approach that gives the user more control. In particular, the user has to specify a reference “style image” $scriptstyle { mathbf { x } } _ { s }$ and “content image” $x _ { c }$ . The system will then try to generate a new image $_ { x }$ that “re-renders” $x _ { c }$ in the style of $x _ { s }$ . This is called neural style transfer, and is illustrated in Figure 14.38 and Figure 14.39. This technique was first proposed in [GEB16], and there are now many papers on this topic; see [Jin+17] for a recent review. \n14.6.5.1 How it works \nStyle transfer works by optimizing the following energy function: \nSee Figure 14.40 for a high level illustration. \nThe first term in Equation (14.33) is the total variation prior discussed in Section 14.6.2.2. The second term measures how similar $_ { x }$ is to $x _ { c }$ by comparing feature maps of a pre-trained CNN $phi ( { pmb x } )$ in the relevant “content layer” $it { l }$ : \nFinally we have to define the style term. We can interpret visual style as the statistical distribution of certain kinds of image features. The location of these features in the image may not matter, but their co-occurence does. This is illustrated in Figure 14.41. It is clear (to a human) that image 1 is more similar in style to image 2 than to image 3. Intuitively this is because both image 1 and image 2 have spiky green patches in them, whereas image 3 has spiky things that are not green. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nTo capture the co-occurence statistics we compute the Gram matrix for an image using feature maps from a specific layer $ell$ : \nThe Gram matrix is a $C _ { ell } times C _ { ell }$ matrix which is proportional to the uncentered covariance of the $C _ { ell }$ -dimensional feature vectors sampled over each of the $H _ { ell } W _ { ell }$ locations. \nGiven this, we define the style loss for layer $ell$ as follows: \nFinally, we define the overall style loss as a sum over the losses for a set $boldsymbol { S }$ of layers: \nFor example, in Figure 14.40, we compute the style loss at layers 1 and 3. (Lower layers will capture visual texture, and higher layers will capture object layout.) \n14.6.5.2 Speeding up the method \nIn [GEB16], they used L-BFGS (Section 8.3.2) to optimize Equation (14.33), starting from white noise. We can get faster results if we use an optimizer such as Adam instead of BFGS, and initialize from the content image instead of white noise. Nevertheless, running an optimizer for every new style and content image is slow. Several papers (see e.g., [JAFF16; Uly+16; UVL16; LW16]) have proposed to train a neural network to directly predict the outcome of this optimization, rather than solving it for each new image pair. (This can be viewed as a form of amortized optimization.) In particular, for every style image $mathbf { boldsymbol { x } } _ { s }$ , we fit a model $f _ { s }$ such that $f _ { s } ( pmb { x } _ { c } ) = mathrm { a r g m i n } _ { pmb { x } } mathcal { L } ( pmb { x } | pmb { x } _ { s } , pmb { x } _ { c } )$ . We can then apply this model to new content images without having to reoptimize. \nMore recently, [DSK16] has shown how it is possible to train a single network that takes as input both the content and a discrete representation $s$ of the style, and then produces $f ( { pmb x } _ { c } , s ) =$ argminx $mathcal { L } ( pmb { x } | s , pmb { x } _ { c } )$ as the output. This avoids the need to train a separate network for every style image. The key idea is to standardize the features at a given layer using scale and shift parameters that are style specific. In particular, we use the following conditional instance normalization transformation: \nwhere $mu ( phi ( { pmb x } _ { c } ) )$ is the mean of the features in a given layer, $sigma ( phi ( { pmb x } _ { c } ) )$ is the standard deviation, and $beta _ { s }$ and $gamma _ { s }$ are parameters for style type $s$ . (See Section 14.2.4.2 for more details on instance normalization.) Surprisingly, this simple trick is enough to capture many kinds of styles. \nThe drawback of the above technique is that it only works for a fixed number of discrete styles. [HB17] proposed to generalize this by replacing the constants $beta _ { s }$ and $gamma _ { s }$ by the output of another CNN, which takes an arbitrary style image $scriptstyle { mathbf { x } } _ { s }$ as input. That is, in Equation (14.38), we set $beta _ { s } = f _ { beta } ( phi ( pmb { x } _ { s } ) )$ \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nand $gamma _ { s } = f _ { gamma } ( phi ( pmb { x } _ { s } ) )$ , and we learn the parameters $beta$ and $gamma$ along with all the other parameters. The model becomes \nThey call their method adaptive instance normalization. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n15 Neural Networks for Sequences \n15.1 Introduction \nIn this chapter, we discuss various kinds of neural networks for sequences. We will consider the case where the input is a sequence, the output is a sequence, or both are sequences. Such models have many applications, such as machine translation, speech recognition, text classification, image captioning, etc. Our presentation borrows from parts of [Zha+20], which should be consulted for more details. \n15.2 Recurrent neural networks (RNNs) \nA recurrent neural network or RNN is a neural network which maps from an input space of sequences to an output space of sequences in a stateful way. That is, the prediction of output ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ depends not only on the input $scriptstyle { mathbf { x } } _ { t }$ , but also on the hidden state of the system, $boldsymbol { h } _ { t }$ , which gets updated over time, as the sequence is processed. Such models can be used for sequence generation, sequence classification, and sequence translation, as we explain below.1 \n15.2.1 Vec2Seq (sequence generation) \nIn this section, we discuss how to learn functions of the form $f _ { pmb theta } : mathbb R ^ { D }  mathbb R ^ { N _ { infty } C }$ , where $D$ is the size of the input vector, and the output is an arbitrary-length sequence of vectors, each of size $C$ . (Note that words are discrete tokens, but can be converted to real-valued vectors as we discuss in Section 1.5.4.) We call these vec2seq models, since they map a vector to a sequence. \nThe output sequence $mathbf { pmb { y } } _ { 1 : T }$ is generated one token at a time. At each step we sample $tilde { y } _ { t }$ from the hidden state $mathbf { } h _ { t }$ of the model, and then “feed it back in” to the model to get the new state $boldsymbol { h } _ { t + 1 }$ (which also depends on the input $_ { x }$ ). See Figure 15.1 for an illustration. In this way the model defines a conditional generative model of the form $p ( { pmb y } _ { 1 : T } | { pmb x } )$ , which captures dependencies between the output tokens. We explain this in more detail below.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Images",
        "subsection": "Generating images by inverting CNNs *",
        "subsubsection": "Neural style transfer"
    },
    {
        "content": "15 Neural Networks for Sequences \n15.1 Introduction \nIn this chapter, we discuss various kinds of neural networks for sequences. We will consider the case where the input is a sequence, the output is a sequence, or both are sequences. Such models have many applications, such as machine translation, speech recognition, text classification, image captioning, etc. Our presentation borrows from parts of [Zha+20], which should be consulted for more details. \n15.2 Recurrent neural networks (RNNs) \nA recurrent neural network or RNN is a neural network which maps from an input space of sequences to an output space of sequences in a stateful way. That is, the prediction of output ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ depends not only on the input $scriptstyle { mathbf { x } } _ { t }$ , but also on the hidden state of the system, $boldsymbol { h } _ { t }$ , which gets updated over time, as the sequence is processed. Such models can be used for sequence generation, sequence classification, and sequence translation, as we explain below.1 \n15.2.1 Vec2Seq (sequence generation) \nIn this section, we discuss how to learn functions of the form $f _ { pmb theta } : mathbb R ^ { D }  mathbb R ^ { N _ { infty } C }$ , where $D$ is the size of the input vector, and the output is an arbitrary-length sequence of vectors, each of size $C$ . (Note that words are discrete tokens, but can be converted to real-valued vectors as we discuss in Section 1.5.4.) We call these vec2seq models, since they map a vector to a sequence. \nThe output sequence $mathbf { pmb { y } } _ { 1 : T }$ is generated one token at a time. At each step we sample $tilde { y } _ { t }$ from the hidden state $mathbf { } h _ { t }$ of the model, and then “feed it back in” to the model to get the new state $boldsymbol { h } _ { t + 1 }$ (which also depends on the input $_ { x }$ ). See Figure 15.1 for an illustration. In this way the model defines a conditional generative model of the form $p ( { pmb y } _ { 1 : T } | { pmb x } )$ , which captures dependencies between the output tokens. We explain this in more detail below.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "15 Neural Networks for Sequences \n15.1 Introduction \nIn this chapter, we discuss various kinds of neural networks for sequences. We will consider the case where the input is a sequence, the output is a sequence, or both are sequences. Such models have many applications, such as machine translation, speech recognition, text classification, image captioning, etc. Our presentation borrows from parts of [Zha+20], which should be consulted for more details. \n15.2 Recurrent neural networks (RNNs) \nA recurrent neural network or RNN is a neural network which maps from an input space of sequences to an output space of sequences in a stateful way. That is, the prediction of output ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ depends not only on the input $scriptstyle { mathbf { x } } _ { t }$ , but also on the hidden state of the system, $boldsymbol { h } _ { t }$ , which gets updated over time, as the sequence is processed. Such models can be used for sequence generation, sequence classification, and sequence translation, as we explain below.1 \n15.2.1 Vec2Seq (sequence generation) \nIn this section, we discuss how to learn functions of the form $f _ { pmb theta } : mathbb R ^ { D }  mathbb R ^ { N _ { infty } C }$ , where $D$ is the size of the input vector, and the output is an arbitrary-length sequence of vectors, each of size $C$ . (Note that words are discrete tokens, but can be converted to real-valued vectors as we discuss in Section 1.5.4.) We call these vec2seq models, since they map a vector to a sequence. \nThe output sequence $mathbf { pmb { y } } _ { 1 : T }$ is generated one token at a time. At each step we sample $tilde { y } _ { t }$ from the hidden state $mathbf { } h _ { t }$ of the model, and then “feed it back in” to the model to get the new state $boldsymbol { h } _ { t + 1 }$ (which also depends on the input $_ { x }$ ). See Figure 15.1 for an illustration. In this way the model defines a conditional generative model of the form $p ( { pmb y } _ { 1 : T } | { pmb x } )$ , which captures dependencies between the output tokens. We explain this in more detail below. \n15.2.1.1 Models \nFor notational simplicity, let $T$ be the length of the output (with the understanding that this is chosen dynamically). The RNN then corresponds to the following conditional generative model: \nwhere $mathbf { } h _ { t }$ is the hidden state, and where we define $p ( h _ { 1 } | h _ { 0 } , y _ { 0 } , x ) = p ( h _ { 1 } | x )$ as the initial hidden state distribution (often deterministic). \nThe output distribution is usually given by \nwhere $mathbf { W } _ { h y }$ are the hidden-to-output weights, and $b _ { y }$ is the bias term. However, for real-valued outputs, we can use \nWe assume the hidden state is computed deterministically as follows: \nfor some deterministic function $f$ . The update function $f$ is usually given by \nwhere $mathbf { W } _ { h h }$ are the hidden-to-hidden weights, $mathbf { W } _ { x h }$ are the input-to-hidden weights, and $ { boldsymbol { b } } _ { h }$ are the bias terms. See Figure 15.1 for an illustration, and rnn_jax.ipynb for some code. \nNote that ${ mathbf { } } _ { mathbf { } } mathbf { mathcal { { y } } } _ { t }$ depends on $mathbf { } h _ { t }$ , which depends on ${ mathbf { } } ^ { y _ { t - 1 } }$ , which depends on $pmb { h } _ { t - 1 }$ , and so on. Thus ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf nabla nabla mathbf { } nabla nabla mathbf  nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ implicitly depends on all past observations (as well as the optional fixed input $_ { x }$ ). Thus an RNN overcomes the limitations of standard Markov models, in that they can have unbounded memory. This makes RNNs theoretically as powerful as a Turing machine [SS95; PMB19]. In practice, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nthe githa some thong the time traveller held in his hand was a glitteringmetallic framework scarcely larger than a small clock and verydelicately made there was ivory in it and the latter than s bettyre tat howhong s ie time thave ler simk you a dimensions le ghat dionthat shall travel indifferently in any direction of space and timeas the driver determinesfilby contented himself with laughterbut i have experimental verification said the time travellerit would be remarkably convenient for the histo \nFigure 15.2: Example output of length 500 generated from a character level RNN when given the prefix “the”. We use greedy decoding, in which the most likely character at each step is computed, and then fed back into the model. The model is trained on the book The Time Machine by H. G. Wells. Generated by rnn_jax.ipynb. \nhowever, the memory length is determined by the size of the latent state and the strength of the parameters; see Section 15.2.7 for further discussion of this point. \nWhen we generate from an RNN, we sample from $tilde { pmb { y } } _ { t } sim p ( pmb { y } _ { t } | pmb { h } _ { t } )$ , and then “feed in” the sampled value into the hidden state, to deterministically compute $h _ { t + 1 } = f ( h _ { t } , tilde { y } _ { t } , boldsymbol { x } )$ , from which we sample $tilde { pmb { y } } _ { t + 1 } sim p ( pmb { y } _ { t + 1 } | pmb { h } _ { t + 1 } )$ , etc. Thus the only stochasticity in the system comes from the noise in the observation (output) model, which is fed back to the system in each step. (However, there is a variant, known as a variational RNN [Chu+15], that adds stochasticity to the dynamics of $mathbf { } h _ { t }$ independent of the observation noise.) \n15.2.1.2 Applications \nRNNs can be used to generate sequences unconditionally (by setting ${ pmb x } = emptyset$ ) or conditionally on $_ { x }$ . Unconditional sequence generation is often called language modeling; this refers to learning joint probability distributions over sequences of discrete tokens, i.e., models of the form $p ( y _ { 1 } , dots , y _ { T } )$ . (See also Section 3.6.1.2, where we discuss using Markov chains for language modeling.) \nFigure 15.2 shows a sequence generated from a simple RNN trained on the book The Time Machine by H. G. Wells. (This is a short science fiction book, with just 32,000 words and 170k characters.) We see that the generated sequence looks plausible, even though it is not very meaningful. By using more sophisticated RNN models (such as those that we discuss in Section 15.2.7.1 and Section 15.2.7.2), and by training on more data, we can create RNNs that give state-of-the-art performance on the language modeling task [CNB17]. (In the language modeling community, performance is usually measured by perplexity, which is just the exponential of the average per-token negative log likelihood; see Section 6.1.5 for more information.) \nWe can also make the generated sequence depend on some kind of input vector $_ { x }$ . For example, consider the task of image captioning: in this case, $_ { x }$ is some embedding of the image computed by a CNN, as illustrated in Figure 15.3. See e.g., [Hos+19; LXW19] for a review of image captioning methods, and https://bit.ly/2Wvs1GK for a tutorial with code. \nIt is also possible to use RNNs to generate sequences of real-valued feature vectors, such as pen strokes for hand-written characters [Gra13] and hand-drawn shapes [HE18]. This can also be useful for time series forecasting real-value sequences. \n15.2.2 Seq2Vec (sequence classification) \nIn this section, we assume we have a single fixed-length output vector $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ we want to predict, given a variable length sequence as input. Thus we want to learn a function of the form $f _ { pmb theta } : mathbb R ^ { T ^ { smash { prime } } D }  mathbb R ^ { C }$ . We \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Vec2Seq (sequence generation)"
    },
    {
        "content": "the githa some thong the time traveller held in his hand was a glitteringmetallic framework scarcely larger than a small clock and verydelicately made there was ivory in it and the latter than s bettyre tat howhong s ie time thave ler simk you a dimensions le ghat dionthat shall travel indifferently in any direction of space and timeas the driver determinesfilby contented himself with laughterbut i have experimental verification said the time travellerit would be remarkably convenient for the histo \nFigure 15.2: Example output of length 500 generated from a character level RNN when given the prefix “the”. We use greedy decoding, in which the most likely character at each step is computed, and then fed back into the model. The model is trained on the book The Time Machine by H. G. Wells. Generated by rnn_jax.ipynb. \nhowever, the memory length is determined by the size of the latent state and the strength of the parameters; see Section 15.2.7 for further discussion of this point. \nWhen we generate from an RNN, we sample from $tilde { pmb { y } } _ { t } sim p ( pmb { y } _ { t } | pmb { h } _ { t } )$ , and then “feed in” the sampled value into the hidden state, to deterministically compute $h _ { t + 1 } = f ( h _ { t } , tilde { y } _ { t } , boldsymbol { x } )$ , from which we sample $tilde { pmb { y } } _ { t + 1 } sim p ( pmb { y } _ { t + 1 } | pmb { h } _ { t + 1 } )$ , etc. Thus the only stochasticity in the system comes from the noise in the observation (output) model, which is fed back to the system in each step. (However, there is a variant, known as a variational RNN [Chu+15], that adds stochasticity to the dynamics of $mathbf { } h _ { t }$ independent of the observation noise.) \n15.2.1.2 Applications \nRNNs can be used to generate sequences unconditionally (by setting ${ pmb x } = emptyset$ ) or conditionally on $_ { x }$ . Unconditional sequence generation is often called language modeling; this refers to learning joint probability distributions over sequences of discrete tokens, i.e., models of the form $p ( y _ { 1 } , dots , y _ { T } )$ . (See also Section 3.6.1.2, where we discuss using Markov chains for language modeling.) \nFigure 15.2 shows a sequence generated from a simple RNN trained on the book The Time Machine by H. G. Wells. (This is a short science fiction book, with just 32,000 words and 170k characters.) We see that the generated sequence looks plausible, even though it is not very meaningful. By using more sophisticated RNN models (such as those that we discuss in Section 15.2.7.1 and Section 15.2.7.2), and by training on more data, we can create RNNs that give state-of-the-art performance on the language modeling task [CNB17]. (In the language modeling community, performance is usually measured by perplexity, which is just the exponential of the average per-token negative log likelihood; see Section 6.1.5 for more information.) \nWe can also make the generated sequence depend on some kind of input vector $_ { x }$ . For example, consider the task of image captioning: in this case, $_ { x }$ is some embedding of the image computed by a CNN, as illustrated in Figure 15.3. See e.g., [Hos+19; LXW19] for a review of image captioning methods, and https://bit.ly/2Wvs1GK for a tutorial with code. \nIt is also possible to use RNNs to generate sequences of real-valued feature vectors, such as pen strokes for hand-written characters [Gra13] and hand-drawn shapes [HE18]. This can also be useful for time series forecasting real-value sequences. \n15.2.2 Seq2Vec (sequence classification) \nIn this section, we assume we have a single fixed-length output vector $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ we want to predict, given a variable length sequence as input. Thus we want to learn a function of the form $f _ { pmb theta } : mathbb R ^ { T ^ { smash { prime } } D }  mathbb R ^ { C }$ . We \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \ncall this a seq2vec model. We will focus on the case where the output is a class label, $y in { 1 , ldots , C }$ , for notational simplicity. \nThe simplest approach is to use the final state of the RNN as input to the classifier: \nSee Figure 15.4a for an illustration. \nWe can often get better results if we let the hidden states of the RNN depend on the past and future context. To do this, we create two RNNs, one which recursively computes hidden states in the forwards direction, and one which recursively computes hidden states in the backwards direction. This is called a bidirectional RNN [SP97]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nMore precisely, the model is defined as follows: \nWe can then define ${ pmb h } _ { t } = [ { pmb h } _ { t } ^ { right. } , { pmb h } _ { t } ^ { left. } ]$ to be the representation of the state at time $t$ , taking into account past and future information. Finally we average pool over these hidden states to get the final classifier: \nSee Figure 15.4b for an illustration, and rnn_sentiment_jax.ipynb for some code. (This is similar to the 1d CNN text classifier1 in Section 15.3.1.) \n15.2.3 Seq2Seq (sequence translation) \nIn this section, we consider learning functions of the form $f _ { pmb { theta } } : mathbb { R } ^ { T D }  mathbb { R } ^ { T ^ { prime } C }$ . We consider two cases: one in which $T ^ { prime } = T$ , so the input and output sequences have the same length (and hence are aligned), and one in which $T ^ { prime } neq T$ , so the input and output sequences have different lengths. This is called a seq2seq problem. \n15.2.3.1 Aligned case \nIn this section, we consider the case where the input and output sequences are aligned. We can also think of it as dense sequence labeling, since we predict one label per location. It is straightforward to modify an RNN to solve this task, as shown in Figure 15.5a. This corresponds to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Seq2Vec (sequence classification)"
    },
    {
        "content": "More precisely, the model is defined as follows: \nWe can then define ${ pmb h } _ { t } = [ { pmb h } _ { t } ^ { right. } , { pmb h } _ { t } ^ { left. } ]$ to be the representation of the state at time $t$ , taking into account past and future information. Finally we average pool over these hidden states to get the final classifier: \nSee Figure 15.4b for an illustration, and rnn_sentiment_jax.ipynb for some code. (This is similar to the 1d CNN text classifier1 in Section 15.3.1.) \n15.2.3 Seq2Seq (sequence translation) \nIn this section, we consider learning functions of the form $f _ { pmb { theta } } : mathbb { R } ^ { T D }  mathbb { R } ^ { T ^ { prime } C }$ . We consider two cases: one in which $T ^ { prime } = T$ , so the input and output sequences have the same length (and hence are aligned), and one in which $T ^ { prime } neq T$ , so the input and output sequences have different lengths. This is called a seq2seq problem. \n15.2.3.1 Aligned case \nIn this section, we consider the case where the input and output sequences are aligned. We can also think of it as dense sequence labeling, since we predict one label per location. It is straightforward to modify an RNN to solve this task, as shown in Figure 15.5a. This corresponds to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere we define $h _ { 1 } = f ( h _ { 0 } , x _ { 1 } ) = f _ { 0 } ( { pmb x } _ { 1 } )$ to be the initial state. \nNote that ${ mathbf { } } _ { mathbf { } } mathbf { nabla } _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla _  mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla nabla mathbf  nabla nabla nabla mathbf { } nabla nabla mathbf { } nabla nabla mathbf { nabla nabla nabla nabla nabla mathbf { } nabla nabla mathbf nabla nabla nabla mathbf { } nabla nabla nabla nabla nabla mathbf nabla nabla nabla nabla mathbf } nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla nabla$ depends on $mathbf { } h _ { t }$ which only depends on the past inputs, ${ boldsymbol { x } } _ { 1 : t }$ . We can get better results if we let the decoder look into the “future” of $_ { x }$ as well as the past, by using a bidirectional RNN, as shown in Figure 15.5b. \nWe can create more expressive models by stacking multiple hidden chains on top of each other, as shown in Figure 15.6. The hidden units for layer $it { l }$ at time $t$ are computed using \nThe output is given by \n15.2.3.2 Unaligned case \nIn this section, we discuss how to learn a mapping from one sequence of length $T$ to another of length $T ^ { prime }$ . We first encode the input sequence to get the context vector ${ pmb { c } } = f _ { e } ( { pmb { x } } _ { 1 : T } )$ , using the last state of an RNN (or average pooling over a biRNN). We then generate the output sequence using an RNN \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 decoder ${ pmb y } _ { 1 : T ^ { prime } } = f _ { d } ( { pmb c } )$ . This is called an encoder-decoder architecture [SVL14; Cho+14a]. See Figure 15.7 for an illustration. \n\nAn important application of this is machine translation. When this is tackled using RNNs, it is called neural machine translation (as opposed to the older approach called statistical machine translation, that did not use neural networks). See Figure 15.8a for the basic idea, and nmt_jax.ipynb for some code which has more details. For a review of the NMT literature, see [Luo16; Neu17]. \n15.2.4 Teacher forcing \nWhen training a language model, the likelihood of a sequence of words $w _ { 1 } , w _ { 2 } , ldots , w _ { T }$ , is given by \nIn an RNN, we therefore set the input to $x _ { t } = w _ { t - 1 }$ and the output to $y _ { t } = w _ { t }$ . Note that we condition on the ground truth labels from the past, $pmb { w } _ { 1 : t - 1 }$ , not labels generated from the model. This is called teacher forcing, since the teacher’s values are “force fed” into the model as input at each step (i.e., $x _ { t }$ is set to $w _ { t - 1 }$ ). \nUnfortunately, teacher forcing can sometimes result in models that perform poorly at test time. The reason is that the model has only ever been trained on inputs that are “correct”, so it may not know what to do if, at test time, it encounters an input sequence $pmb { w } _ { 1 : t - 1 }$ generated from the previous step that deviates from what it saw in training. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Seq2Seq (sequence translation)"
    },
    {
        "content": "An important application of this is machine translation. When this is tackled using RNNs, it is called neural machine translation (as opposed to the older approach called statistical machine translation, that did not use neural networks). See Figure 15.8a for the basic idea, and nmt_jax.ipynb for some code which has more details. For a review of the NMT literature, see [Luo16; Neu17]. \n15.2.4 Teacher forcing \nWhen training a language model, the likelihood of a sequence of words $w _ { 1 } , w _ { 2 } , ldots , w _ { T }$ , is given by \nIn an RNN, we therefore set the input to $x _ { t } = w _ { t - 1 }$ and the output to $y _ { t } = w _ { t }$ . Note that we condition on the ground truth labels from the past, $pmb { w } _ { 1 : t - 1 }$ , not labels generated from the model. This is called teacher forcing, since the teacher’s values are “force fed” into the model as input at each step (i.e., $x _ { t }$ is set to $w _ { t - 1 }$ ). \nUnfortunately, teacher forcing can sometimes result in models that perform poorly at test time. The reason is that the model has only ever been trained on inputs that are “correct”, so it may not know what to do if, at test time, it encounters an input sequence $pmb { w } _ { 1 : t - 1 }$ generated from the previous step that deviates from what it saw in training. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nA common solution to this is known as scheduled sampling [Ben+15a]. This starts off using teacher forcing, but at random time steps, feeds in samples from the model instead; the fraction of time this happens is gradually increased. \nAn alternative solution is to use other kinds of models where MLE training works better, such as 1d CNNs (Section 15.3) and transformers (Section 15.5). \n15.2.5 Backpropagation through time \nWe can compute the maximum likelihood estimate of the parameters for an RNN by solving $pmb { theta } ^ { * } = mathrm { a r g m a x } _ { pmb { theta } } p ( pmb { y } _ { 1 : T } | pmb { x } _ { 1 : T } , pmb { theta } )$ , where we have assumed a single training sequence for notational simplicity. To compute the MLE, we have to compute gradients of the loss wrt the parameters. To do this, we can unroll the computation graph, as shown in Figure 15.9, and then apply the backpropagation algorithm. This is called backpropagation through time (BPTT) [Wer90]. \nMore precisely, consider the following model: \nwhere $mathbf { } _ { mathbf { } } mathbf { o } _ { t }$ are the output logits, and where we drop the bias terms for notational simplicity. We assume $y _ { y }$ are the true target labels for each time step, so we define the loss to be \nWe need to compute the derivatives ∂ ∂WLhx , ∂ ∂WLhh , and ∂ ∂WL . The latter term is easy, since it is local to each time step. However, the first two terms depend on the hidden state, and thus require working backwards in time. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Teacher forcing"
    },
    {
        "content": "A common solution to this is known as scheduled sampling [Ben+15a]. This starts off using teacher forcing, but at random time steps, feeds in samples from the model instead; the fraction of time this happens is gradually increased. \nAn alternative solution is to use other kinds of models where MLE training works better, such as 1d CNNs (Section 15.3) and transformers (Section 15.5). \n15.2.5 Backpropagation through time \nWe can compute the maximum likelihood estimate of the parameters for an RNN by solving $pmb { theta } ^ { * } = mathrm { a r g m a x } _ { pmb { theta } } p ( pmb { y } _ { 1 : T } | pmb { x } _ { 1 : T } , pmb { theta } )$ , where we have assumed a single training sequence for notational simplicity. To compute the MLE, we have to compute gradients of the loss wrt the parameters. To do this, we can unroll the computation graph, as shown in Figure 15.9, and then apply the backpropagation algorithm. This is called backpropagation through time (BPTT) [Wer90]. \nMore precisely, consider the following model: \nwhere $mathbf { } _ { mathbf { } } mathbf { o } _ { t }$ are the output logits, and where we drop the bias terms for notational simplicity. We assume $y _ { y }$ are the true target labels for each time step, so we define the loss to be \nWe need to compute the derivatives ∂ ∂WLhx , ∂ ∂WLhh , and ∂ ∂WL . The latter term is easy, since it is local to each time step. However, the first two terms depend on the hidden state, and thus require working backwards in time. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe simplify the notation by defining \nwhere wh is the flattened version of Whh and Whx stacked together. We focus on computing ∂wLh . By the chain rule, we have \nWe can expand the last term as follows: \nIf we expand this recursively, we find the following result (see the derivation in [Zha+20, Sec 8.7]) \nUnfortunately, this takes $O ( T )$ time to compute per time step, for a total of $O ( T ^ { 2 } )$ overall. It is therefore standard to truncate the sum to the most recent $K$ terms. It is possible to adaptively pick a suitable truncation parameter $K$ [AFF19]; however, it is usually set equal to the length of the subsequence in the current minibatch. \nWhen using truncated BPTT, we can train the model with batches of short sequences, usually created by extracting non-overlapping subsequences (windows) from the original sequence. If the previous subsequence ends at time $t - 1$ , and the current subsequence starts at time $t$ , we can “carry over” the hidden state of the RNN across batch updates during training. However, if the subsequences are not ordered, we need to reset the hidden state. See rnn_jax.ipynb for some sample code that illustrates these details. \n15.2.6 Vanishing and exploding gradients \nUnforunately, the activations in an RNN can decay or explode as we go forwards in time, since we multiply by the weight matrix ${ mathbf W } _ { h h }$ at each time step. Similarly, the gradients in an RNN can decay or explode as we go backwards in time, since we multiply the Jacobians at each time step (see Section 13.4.2 for details). A simple heuristic is to use gradient clipping (Equation (13.70)). More sophisticated methods attempt to control the spectral radius $lambda$ of the forward mapping, ${ mathbf W } _ { h h }$ , as well as the backwards mapping, given by the Jacobian $mathbf { J } _ { h h }$ . \nThe simplest way to control the spectral radius is to randomly initialize $mathbf { W } _ { h h }$ in such a way as to ensure $lambda approx 1$ , and then keep it fixed (i.e., we do not learn ${ mathbf W } _ { h h }$ ). In this case, only the output matrix $mathbf { W } _ { h o }$ needs to be learned, resulting in a convex optimization problem. This is called an echo state network [JH04]. A closely related approach, known as a liquid state machine [MNM02], uses binary-valued (spiking) neurons instead of real-valued neurons. A generic term for both ESNs \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license and LSMs is reservoir computing [LJ09]. Another approach to this problem is use constrained optimization to ensure the $mathbf { W } _ { h h }$ matrix remains orthogonal [Vor+17].",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Backpropagation through time"
    },
    {
        "content": "We simplify the notation by defining \nwhere wh is the flattened version of Whh and Whx stacked together. We focus on computing ∂wLh . By the chain rule, we have \nWe can expand the last term as follows: \nIf we expand this recursively, we find the following result (see the derivation in [Zha+20, Sec 8.7]) \nUnfortunately, this takes $O ( T )$ time to compute per time step, for a total of $O ( T ^ { 2 } )$ overall. It is therefore standard to truncate the sum to the most recent $K$ terms. It is possible to adaptively pick a suitable truncation parameter $K$ [AFF19]; however, it is usually set equal to the length of the subsequence in the current minibatch. \nWhen using truncated BPTT, we can train the model with batches of short sequences, usually created by extracting non-overlapping subsequences (windows) from the original sequence. If the previous subsequence ends at time $t - 1$ , and the current subsequence starts at time $t$ , we can “carry over” the hidden state of the RNN across batch updates during training. However, if the subsequences are not ordered, we need to reset the hidden state. See rnn_jax.ipynb for some sample code that illustrates these details. \n15.2.6 Vanishing and exploding gradients \nUnforunately, the activations in an RNN can decay or explode as we go forwards in time, since we multiply by the weight matrix ${ mathbf W } _ { h h }$ at each time step. Similarly, the gradients in an RNN can decay or explode as we go backwards in time, since we multiply the Jacobians at each time step (see Section 13.4.2 for details). A simple heuristic is to use gradient clipping (Equation (13.70)). More sophisticated methods attempt to control the spectral radius $lambda$ of the forward mapping, ${ mathbf W } _ { h h }$ , as well as the backwards mapping, given by the Jacobian $mathbf { J } _ { h h }$ . \nThe simplest way to control the spectral radius is to randomly initialize $mathbf { W } _ { h h }$ in such a way as to ensure $lambda approx 1$ , and then keep it fixed (i.e., we do not learn ${ mathbf W } _ { h h }$ ). In this case, only the output matrix $mathbf { W } _ { h o }$ needs to be learned, resulting in a convex optimization problem. This is called an echo state network [JH04]. A closely related approach, known as a liquid state machine [MNM02], uses binary-valued (spiking) neurons instead of real-valued neurons. A generic term for both ESNs \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license and LSMs is reservoir computing [LJ09]. Another approach to this problem is use constrained optimization to ensure the $mathbf { W } _ { h h }$ matrix remains orthogonal [Vor+17]. \n\nAn alternative to explicitly controlling the spectral radius is to modify the RNN architecture itself, to use additive rather than multiplicative updates to the hidden states, as we discuss in Section 15.2.7. This significantly improves training stability. \n15.2.7 Gating and long term memory \nRNNs with enough hidden units can in principle remember inputs from long in the past. However, in practice “vanilla” RNNs fail to do this because of the vanishing gradient problem (Section 13.4.2). In this section we give a solution to this in which we update the hidden state in an additive way, similar to a residual net (Section 14.3.4). \n15.2.7.1 Gated recurrent units (GRU) \nIn this section, we discuss models which use gated recurrent units (GRU), as proposed in [Cho+14a]. The key idea is to learn when to update the hidden state, by using a gating unit. This can be used to selectively “remember” important pieces of information when they are first seen. The model can also learn when to reset the hidden state, and thus forget things that are no longer useful. To explain the model in more detail, we present it in two steps, following the presentation of [Zha+20, Sec 8.8]. We assume $mathbf { X } _ { t }$ is a $N times D$ matrix, where $N$ is the batch size, and $D$ is the vocabulary size. Similarly, $mathbf { H } _ { t }$ is a $N times H$ matrix, where $H$ is the number of hidden units at time $t$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Vanishing and exploding gradients"
    },
    {
        "content": "An alternative to explicitly controlling the spectral radius is to modify the RNN architecture itself, to use additive rather than multiplicative updates to the hidden states, as we discuss in Section 15.2.7. This significantly improves training stability. \n15.2.7 Gating and long term memory \nRNNs with enough hidden units can in principle remember inputs from long in the past. However, in practice “vanilla” RNNs fail to do this because of the vanishing gradient problem (Section 13.4.2). In this section we give a solution to this in which we update the hidden state in an additive way, similar to a residual net (Section 14.3.4). \n15.2.7.1 Gated recurrent units (GRU) \nIn this section, we discuss models which use gated recurrent units (GRU), as proposed in [Cho+14a]. The key idea is to learn when to update the hidden state, by using a gating unit. This can be used to selectively “remember” important pieces of information when they are first seen. The model can also learn when to reset the hidden state, and thus forget things that are no longer useful. To explain the model in more detail, we present it in two steps, following the presentation of [Zha+20, Sec 8.8]. We assume $mathbf { X } _ { t }$ is a $N times D$ matrix, where $N$ is the batch size, and $D$ is the vocabulary size. Similarly, $mathbf { H } _ { t }$ is a $N times H$ matrix, where $H$ is the number of hidden units at time $t$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThe reset gate ${ bf R } _ { t } in { mathbb R } ^ { N times H }$ and update gate ${ bf Z } _ { t } in mathbb { R } ^ { N times H }$ are computed using \nwhere the weight matrix Note that each element of $mathbf { R } _ { t }$ and $mathbf { Z } _ { t }$ is in $[ 0 , 1 ]$ , because of the sigmoid function. \nGiven this, we we define a “candidate” next state vector using \nThis combines the old memories that are not reset (computed using $mathbf { R } _ { t } odot mathbf { H } _ { t - 1 }$ ) with the new inputs $mathbf { X } _ { t }$ . We pass the resulting linear combination through a tanh function to ensure the hidden units remain in the interval $( - 1 , 1 )$ . If the entries of the reset gate $mathbf { R } _ { t }$ are close to 1, we recover the standard RNN update rule. If the entries are close to 0, the model acts more like an MLP applied to $mathbf { X } _ { t }$ . Thus the reset gate can capture new, short-term information. \nOnce we have computed the candidate new state, the model computes the actual new state by using the dimensions from the candidate state $tilde { mathbf { H } } _ { t }$ chosen by the update gate, $mathbf { l } - mathbf { Z } _ { t }$ , and keeping the remaining dimensions at their old values of $mathbf { H } _ { t - 1 }$ : \nWhen $Z _ { t d } = 1$ , we pass $H _ { t - 1 , d }$ through unchanged, and ignore $mathbf { X } _ { t }$ . Thus the update gate can capture long-term dependencies. \nSee Figure 15.10 for an illustration of the overall architecture, and gru_jax.ipynb for some sample code. \n15.2.7.2 Long short term memory (LSTM) \nIn this section, we discuss the long short term memory (LSTM) model of [HS97b], which is a more sophisticated version of the GRU (and pre-dates it by almost 20 years). For a more detailed introduction, see https://colah.github.io/posts/2015-08-Understanding-LSTMs. \nThe basic idea is to augment the hidden state $pmb { h } _ { t }$ with a memory cell $boldsymbol { c } _ { t }$ . We need three gates to control this cell: the output gate $mathbf { O } _ { t }$ determines what gets read out; the input gate $mathbf { I } _ { t }$ determines what gets read in; and the forget gate $mathbf { F } _ { t }$ determines when we should reset the cell. These gates are computed as follows: \nWe then compute a candidate cell state: \nThe actual update to the cell is either the candidate cell (if the input gate is on) or the old cell (if the not-forget gate is on): \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf $mathbf { F } _ { t } = 1$ and $mathbf I _ { t } = 0$ , this can remember long term memories.2 \nFinally, we compute the hidden state to be a transformed version of the cell, provided the output gate is on: \nNote that $mathbf { H } _ { t }$ is used as the output of the unit as well as the hidden state for the next time step. This lets the model remember what it has just output (short-term memory), whereas the cell $mathbf { C } _ { t }$ acts as a long-term memory. See Figure 15.11 for an illustration of the overall model, and lstm_jax.ipynb for some sample code. \nSometimes we add peephole connections, where we pass the cell state as an additional input to the gates. Many other variants have been proposed. In fact, [JZS15] used genetic algorithms to test over 10,000 different architectures. Some of these worked better than LSTMs or GRUs, but in general, LSTMs seemed to do consistently well across most tasks. Similar conclusions were reached in [Gre+17]. More recently, [ZL17] used an RNN controller to generate strings which specify RNN architectures, and then trained the controller using reinforcement learning. This resulted in a novel cell structure that outperformed LSTM. However, it is rather complex and has not been adopted by the community. \nTime step 1 2 3 4 Time step 1 2 3 4   \nA 0.5 0.1 0.2 0.0 A 0.5 0.1 0.1 0.1   \nB 0.2 0.4 0.2 0.2 B 0.2 0.4 0.6 0.2   \nC 0.2 0.3 0.4 0.2 C 0.2 0.3 0.2 0.1   \n<eos> 0.1 0.2 0.2 0.6 <eos> 0.1 0.2 0.1 0.6   \n(a) (b) \n15.2.8 Beam search \nThe simplest way to generate from an RNN is to use greedy decoding, in which we compute $hat { y } _ { t } = operatorname { a r g m a x } _ { y } p ( y _ { t } = y | hat { y } _ { 1 : t } , pmb { x } )$ at each step. We can repeat this process until we generate the end-of-sentence token. See Figure 15.8b for an illustration of this method applied to NMT. \nUnfortunately greedy decoding will not generate the MAP sequence, which is defined by $y _ { 1 : T } ^ { * } =$ argmaxy1:T p(y1:T |x). The reason is that the locally optimal symbol at step t might not be on the globally optimal path. \nAs an example, consider Figure 15.12a. We greedily pick the MAP symbol at step 1, which is A. Conditional on this, suppose we have $p ( y _ { 2 } | y _ { 1 } = A ) = [ 0 . 1 , 0 . 4 , 0 . 3 , 0 . 2 ]$ , as shown. We greedily pick the MAP symbol from this, which is B. Conditional on this, suppose we have $p ( y _ { 3 } | y _ { 1 } = A , y _ { 2 } = B ) =$ [0.2, 0.2, 0.4, 0.2], as shown. We greedily pick the MAP symbol from this, which is C. Conditional on this, suppose we have $p ( y _ { 4 } | y _ { 1 } = A , y _ { 2 } = B , y _ { 3 } = C ) = [ 0 . 0 , 0 . 2 , 0 . 2 , 0 . 6 ]$ , as shown. We greedily pick the MAP symbol from this, which is eos (end of sentence), so we stop generating. The overall probability of the generated sequence is $0 . 5 times 0 . 4 times 0 . 4 times 0 . 6 = 0 . 0 4 8$ . \nNow consider Figure 15.12b. At step 2, suppose we pick the second most probable token, namely C. Conditional on this, suppose we have $p ( y _ { 3 } | y _ { 1 } = A , y _ { 2 } = C )  =  [ 0 . 1 , 0 . 6 , 0 . 2 , 0 . 1 ]$ , as shown. We greedily pick the MAP symbol from this, which is $mathrm { B }$ . Conditional on this, suppose we have $p ( y _ { 4 } | y _ { 1 } = A , y _ { 2 } = C , y _ { 3 } = B ) = [ 0 . 1 , 0 . 2 , 0 . 1 , 0 . 6 ]$ , as shown. We greedily pick the MAP symbol from this, which is eos (end of sentence), so we stop generating. The overall probability of the generated sequence is $0 . 5 times 0 . 3 times 0 . 6 times 0 . 6 = 0 . 0 5 4$ . So by being less greedy, we found a sequence with overall higher likelihood. \nFor hidden Markov models, we can use an algorithm called Viterbi decoding (which is an example of dynamic programming) to compute the globally optimal sequence in $O ( T V ^ { 2 } )$ time, where $V$ is the number of words in the vocabulary. (See [Mur23] for details.) But for RNNs, computing the global optimum takes $O ( V ^ { T } )$ , since the hidden state is not a sufficient statistic for the data. \nBeam search is a much faster heuristic method. In this approach, we compute the top $K$ candidate outputs at each step; we then expand each one in all $V$ possible ways, to generate $V K$ candidates, from which we pick the top $K$ again. This process is illustrated in Figure 15.13. \nIt is also possible to extend the algorithm to sample the top $K$ sequences without replacement \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license (i.e., pick the top one, renormalize, pick the new top one, etc.), using a method called stochastic beam search. This perturbs the model’s partial probabilities at each step with Gumbel noise. See [KHW19] for details. and [SBS20] for a sequential alternative. These sampling methods can improve diversity of the outputs. (See also the deterministic diverse beam search method of [Vij+18].)",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Gating and long term memory"
    },
    {
        "content": "Time step 1 2 3 4 Time step 1 2 3 4   \nA 0.5 0.1 0.2 0.0 A 0.5 0.1 0.1 0.1   \nB 0.2 0.4 0.2 0.2 B 0.2 0.4 0.6 0.2   \nC 0.2 0.3 0.4 0.2 C 0.2 0.3 0.2 0.1   \n<eos> 0.1 0.2 0.2 0.6 <eos> 0.1 0.2 0.1 0.6   \n(a) (b) \n15.2.8 Beam search \nThe simplest way to generate from an RNN is to use greedy decoding, in which we compute $hat { y } _ { t } = operatorname { a r g m a x } _ { y } p ( y _ { t } = y | hat { y } _ { 1 : t } , pmb { x } )$ at each step. We can repeat this process until we generate the end-of-sentence token. See Figure 15.8b for an illustration of this method applied to NMT. \nUnfortunately greedy decoding will not generate the MAP sequence, which is defined by $y _ { 1 : T } ^ { * } =$ argmaxy1:T p(y1:T |x). The reason is that the locally optimal symbol at step t might not be on the globally optimal path. \nAs an example, consider Figure 15.12a. We greedily pick the MAP symbol at step 1, which is A. Conditional on this, suppose we have $p ( y _ { 2 } | y _ { 1 } = A ) = [ 0 . 1 , 0 . 4 , 0 . 3 , 0 . 2 ]$ , as shown. We greedily pick the MAP symbol from this, which is B. Conditional on this, suppose we have $p ( y _ { 3 } | y _ { 1 } = A , y _ { 2 } = B ) =$ [0.2, 0.2, 0.4, 0.2], as shown. We greedily pick the MAP symbol from this, which is C. Conditional on this, suppose we have $p ( y _ { 4 } | y _ { 1 } = A , y _ { 2 } = B , y _ { 3 } = C ) = [ 0 . 0 , 0 . 2 , 0 . 2 , 0 . 6 ]$ , as shown. We greedily pick the MAP symbol from this, which is eos (end of sentence), so we stop generating. The overall probability of the generated sequence is $0 . 5 times 0 . 4 times 0 . 4 times 0 . 6 = 0 . 0 4 8$ . \nNow consider Figure 15.12b. At step 2, suppose we pick the second most probable token, namely C. Conditional on this, suppose we have $p ( y _ { 3 } | y _ { 1 } = A , y _ { 2 } = C )  =  [ 0 . 1 , 0 . 6 , 0 . 2 , 0 . 1 ]$ , as shown. We greedily pick the MAP symbol from this, which is $mathrm { B }$ . Conditional on this, suppose we have $p ( y _ { 4 } | y _ { 1 } = A , y _ { 2 } = C , y _ { 3 } = B ) = [ 0 . 1 , 0 . 2 , 0 . 1 , 0 . 6 ]$ , as shown. We greedily pick the MAP symbol from this, which is eos (end of sentence), so we stop generating. The overall probability of the generated sequence is $0 . 5 times 0 . 3 times 0 . 6 times 0 . 6 = 0 . 0 5 4$ . So by being less greedy, we found a sequence with overall higher likelihood. \nFor hidden Markov models, we can use an algorithm called Viterbi decoding (which is an example of dynamic programming) to compute the globally optimal sequence in $O ( T V ^ { 2 } )$ time, where $V$ is the number of words in the vocabulary. (See [Mur23] for details.) But for RNNs, computing the global optimum takes $O ( V ^ { T } )$ , since the hidden state is not a sufficient statistic for the data. \nBeam search is a much faster heuristic method. In this approach, we compute the top $K$ candidate outputs at each step; we then expand each one in all $V$ possible ways, to generate $V K$ candidates, from which we pick the top $K$ again. This process is illustrated in Figure 15.13. \nIt is also possible to extend the algorithm to sample the top $K$ sequences without replacement \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license (i.e., pick the top one, renormalize, pick the new top one, etc.), using a method called stochastic beam search. This perturbs the model’s partial probabilities at each step with Gumbel noise. See [KHW19] for details. and [SBS20] for a sequential alternative. These sampling methods can improve diversity of the outputs. (See also the deterministic diverse beam search method of [Vij+18].) \n\n15.3 1d CNNs \nConvolutional neural networks (Chapter 14) compute a function of some local neighborhood for each input using tied weights, and return an output. They are usually used for 2d inputs, but can also be applied in the 1d case, as we discuss below. They are an interesting alternative to RNNs that are much easier to train, because they don’t have to maintain long term hidden state. \n15.3.1 1d CNNs for sequence classification \nIn this section, we discuss the use of 1d CNNs for learning a mapping from variable-length sequences to a fixed length output, i.e., a function of the form $f _ { pmb theta } : mathbb { R } ^ { D T }  mathbb { R } ^ { C }$ , where $T$ is the length of the input, $D$ is the number of features per input, and $C$ is the size of the output vector (e.g., class logits). A basic 1d convolution operation applied to a 1d sequence is shown in Figure 14.4. Typically the input sequence will have $D > 1$ input channels (feature dimensions). In this case, we can convolve each channel separately and add up the result, using a different 1d filter (kernel) for each input channel to get $begin{array} { r } { boldsymbol { z } _ { i } = sum _ { d } pmb { x } _ { i - k : i + k , d } ^ { mathsf { T } } mathbf { w } _ { d } } end{array}$ , where $k$ is size of the 1d receptive field, and ${ pmb w } _ { d }$ is the filter for input channel $d$ . This produces a 1d vector $z in mathbb { R } ^ { T }$ encoding the input (ignoring boundary effects). We can create a vector representation for each location using a different weight vector for each output channel $c$ to get $begin{array} { r } { z _ { i c } = sum _ { d } pmb { x } _ { i - k : i + k , d } ^ { 1 } pmb { w } _ { d , c } } end{array}$ . This implements a mapping from $T D$ to $T C$ . To reduce this to a fixed sized vector, $z in mathbb { R } ^ { C }$ , we can use max-pooling over time to get $z _ { c } = mathrm { m a x } _ { i } z _ { i c }$ . We can \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Recurrent neural networks (RNNs)",
        "subsubsection": "Beam search"
    },
    {
        "content": "15.3 1d CNNs \nConvolutional neural networks (Chapter 14) compute a function of some local neighborhood for each input using tied weights, and return an output. They are usually used for 2d inputs, but can also be applied in the 1d case, as we discuss below. They are an interesting alternative to RNNs that are much easier to train, because they don’t have to maintain long term hidden state. \n15.3.1 1d CNNs for sequence classification \nIn this section, we discuss the use of 1d CNNs for learning a mapping from variable-length sequences to a fixed length output, i.e., a function of the form $f _ { pmb theta } : mathbb { R } ^ { D T }  mathbb { R } ^ { C }$ , where $T$ is the length of the input, $D$ is the number of features per input, and $C$ is the size of the output vector (e.g., class logits). A basic 1d convolution operation applied to a 1d sequence is shown in Figure 14.4. Typically the input sequence will have $D > 1$ input channels (feature dimensions). In this case, we can convolve each channel separately and add up the result, using a different 1d filter (kernel) for each input channel to get $begin{array} { r } { boldsymbol { z } _ { i } = sum _ { d } pmb { x } _ { i - k : i + k , d } ^ { mathsf { T } } mathbf { w } _ { d } } end{array}$ , where $k$ is size of the 1d receptive field, and ${ pmb w } _ { d }$ is the filter for input channel $d$ . This produces a 1d vector $z in mathbb { R } ^ { T }$ encoding the input (ignoring boundary effects). We can create a vector representation for each location using a different weight vector for each output channel $c$ to get $begin{array} { r } { z _ { i c } = sum _ { d } pmb { x } _ { i - k : i + k , d } ^ { 1 } pmb { w } _ { d , c } } end{array}$ . This implements a mapping from $T D$ to $T C$ . To reduce this to a fixed sized vector, $z in mathbb { R } ^ { C }$ , we can use max-pooling over time to get $z _ { c } = mathrm { m a x } _ { i } z _ { i c }$ . We can \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nthen pass this into a softmax layer. \nIn [Kim14], they applied this model to sequence classification. The idea is to embed each word using an embedding layer, and then to compute various features using 1d kernels of different widths, to capture patterns of different length scales. We then apply max pooling over time, and concatenate the results, and pass to a fully connected layer. See Figure 15.14 for an illustration, and cnn1d_sentiment_jax.ipynb for some code. \n15.3.2 Causal 1d CNNs for sequence generation \nTo use 1d CNNs in a generative setting, we must convert them to a causal CNN, in which each output variable only depends on previously generated variables. (This is also called a convolutional Markov model.) In particular, we define the model as follows: \nwhere is the convolutional filter of size $k$ , and we have assumed a single nonlinearity and $mathbf { boldsymbol { w } }$ $varphi$ categorical output, for notational simplicity. This is like regular 1d convolution except we “mask out” future inputs, so that $y _ { t }$ only depends on the past values, rather than past and future values. This is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license called causal convolution. ,W2e, 4c,a. .n ,of51c2o, u1,rs2,e 4,u.s.e ,d5e1e2p,e1r, 2m, 4o,d. .el.s,,51a2n.d we can condition on input features $_ { x }$ .e",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "1d CNNs",
        "subsubsection": "1d CNNs for sequence classification"
    },
    {
        "content": "then pass this into a softmax layer. \nIn [Kim14], they applied this model to sequence classification. The idea is to embed each word using an embedding layer, and then to compute various features using 1d kernels of different widths, to capture patterns of different length scales. We then apply max pooling over time, and concatenate the results, and pass to a fully connected layer. See Figure 15.14 for an illustration, and cnn1d_sentiment_jax.ipynb for some code. \n15.3.2 Causal 1d CNNs for sequence generation \nTo use 1d CNNs in a generative setting, we must convert them to a causal CNN, in which each output variable only depends on previously generated variables. (This is also called a convolutional Markov model.) In particular, we define the model as follows: \nwhere is the convolutional filter of size $k$ , and we have assumed a single nonlinearity and $mathbf { boldsymbol { w } }$ $varphi$ categorical output, for notational simplicity. This is like regular 1d convolution except we “mask out” future inputs, so that $y _ { t }$ only depends on the past values, rather than past and future values. This is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license called causal convolution. ,W2e, 4c,a. .n ,of51c2o, u1,rs2,e 4,u.s.e ,d5e1e2p,e1r, 2m, 4o,d. .el.s,,51a2n.d we can condition on input features $_ { x }$ .e \n\nIn ordresrultos icnaepxtpuorneelnotinalg-rreacenpgteivedefipeledndgreonwctihes,witwhe dceapnthu(sYeud&i aKtoeldtucn,on20v1ol6)u. Foon (eSxeacmtiploene1ac4.h4.1), as illustrat1e,d2,i4n .F.i.g,u5r1e2 1bl5o.c1k5.haTshriescepmtiovdeelfiehldasofbseiezen 1s0u2c4c,easnsfduclalyn buesesedetnoascraeamtoereaefsfitcaitentoafntdhde sa-rt text to speech (TTS) synthesis system known1a⇥s1w02a4venet [oor+16]. In particular, they stack 10 causal 1d convolutional layers with dilation rates $1 , 2 , 4 , ldots , 2 5 6 , 5 1 2$ to get a convolutional block with an effective receptive field of 1024. (They left-padded the input sequences with a number of zeros equal to the dilation rate before every layer, so that every layer has the same length.) They then repeat this blocOkne3 atpipmroeasctho ocomopdeultiengdtehepceronfdeiatitounraels.d \nIn wavuednioets,atmhpelescownoduiltdiobneitnoguisenfaormixattuiroenm $_ { x }$ diesl saucshetasoaf lmiinxgturiestdiecnfsietaytnuertewsordker(iBvieshdofpr,o19m94a)n input sequenceo ofmiwxtourdeso; tchoendmitioodneal tGhaeunssgiaenesrcaltesmrixatwu easu(diMoCuGsSinMg t(hTeheaisbo&veBemthogde,el2.0I1t5)i. aHloswoepveors,sible to create adfatualliys iemnpdl-itciot-lyencdontaipnuporousac(ahs, swthiecchasetafrotrs mwaigtehpriaxewl iwntoerndsist resatohrearudtihoasna lmipnlgeuviasltuiecs)f.eaOtnueres (see [Wan+1o7f]t)h. \nAlthoduisgtrhibuwtaiovnesnbeet apursoeditucmeaskehsingoh sqsuamliptiyonsspaebeocuht, hite rishtaopoe slow for use in production systems. However, it can be “distilled” into a parallel generative model [Oor+18]. We discuss these kinds of parallel sgoeftnmearxatliavye mwoduledlsnienedtthoeosuetqputel65t,o53t6hips oboaobikl,i i[eMs puerr2t3i]m. \n15.4 Attention \nIn all of the neural networks we have considered so far, the hidden activations are a linear combination of the input activations, followed by a nonlinearity: $z = varphi ( mathbf { W } v )$ , where $pmb { v } in mathbb { R } ^ { v }$ are the hidden feature vectors, and $mathbf { W } in mathbb { R } ^ { v ^ { prime } times v }$ are a fixed set of weights3that are learned on a training set. \nHowever, we can imagine a more flexible model in which we have a set of $m$ feature vectors or values $mathbf { V } in mathbb { R } ^ { m times v }$ , and the model dynamically decides (in an input dependenent way) which one to use, based on how similar the input query vector $pmb q in mathbb { R } ^ { q }$ is to a set of $m$ keys $mathbf { K } in mathbb { R } ^ { m times k }$ . If $mathbf { pmb { q } }$ is most similar to key $i$ , then we use value ${ pmb v } _ { i }$ . This is the basic idea behind attention mechanisms. This idea was originally developed for sequence models, and we will therefore explain it in this context. However, it can be more generally applied. Our presentation in the following sections is based on [Zha+20, Chap 10.]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "1d CNNs",
        "subsubsection": "Causal 1d CNNs for sequence generation"
    },
    {
        "content": "15.4.1 Attention as soft dictionary lookup \nWe can think of attention as a dictionary lookup, in which we compare the query $mathbf { pmb { q } }$ to each key $pmb { k } _ { i }$ , and then retrieve the corresponding value ${ pmb v } _ { i }$ . To make this lookup operation differentiable, instead of retrieving a single value ${ pmb v } _ { i }$ , we compute a convex combination of the values, as follows: \nwhere $alpha _ { i } ( pmb q , pmb k _ { 1 : m } )$ is the $i$ ’th attention weight; these weights satisfy $0 leq alpha _ { i } ( pmb { q } , pmb { k } _ { 1 : m } ) leq 1$ for each $i$ and $begin{array} { r } { sum _ { i } alpha _ { i } ( { pmb q } , { pmb k } _ { 1 : m } ) = 1 } end{array}$ . \nThe attention weights can be computed from an attention score function $a ( pmb q , pmb k _ { i } ) in mathbb { R }$ , that computes the similarity of query $pmb q$ to key $pmb { k } _ { i }$ . We will discuss several such score function below. Given the scores, we can compute the attention weights using the softmax function: \nSee Figure 15.16 for an illustration. \nIn some cases, we want to restrict attention to a subset of the dictionary, corresponding to valid entries. For example, we might want to pad sequences to a fixed length (for efficient minibatching), in which case we should “mask out” the padded locations. This is called masked attention. We can implement this efficiently by setting the attention score for the masked entries to a large negative number, such as $- 1 0 ^ { 6 }$ , so that the corresponding softmax weights will be 0. (This is analogous to causal convolution, discussed in Section 15.3.2.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n15.4.2 Kernel regression as non-parametric attention \nIn Section 16.3.5, we discuss kernel regression, which is a nonparametric model of the form \nwhere $alpha _ { i } ( x , x _ { 1 : n } ) geq 0$ measures the normalized similarity of test input $x$ to training input $x _ { i }$ . This similarity measure is usually computed by defining the attention score in terms of a density kernel, such as the Gaussian: \nwhere $sigma$ is called the bandwidth. We then define $a ( x , x _ { i } ) = K _ { sigma } ( x - x _ { i } )$ . \nBecause the scores are normalized, we can drop the √21πσ2 term. In addition, to maintain notation consistency with [Zha+20, Ch. 10], we rewrite the term inside the exponential as follows: \nPlugging this in to Equation (15.36), we get \nWe can interpret this as a form of nonparametric attention, where the queries are the test points $x$ , the keys are the training inputs $x _ { i }$ , and the values are the training labels $y _ { i }$ . \nIf we set $w = 1$ , the resulting attention matrix $A _ { j i } = alpha _ { i } ( x _ { j } , x _ { 1 : n } )$ for test input $j$ is shown in Figure 15.17a. The resulting predicted curve is shown in Figure 15.17b. \nThe size of the diagonal band in Figure 15.17a, and hence the sparsity of the attention mechanism, dependends on the parameter $w$ . If we increase $w$ , corresponding to reducing the kernel bandwidth, the band will get narrower, but the model will start to overfit. \n15.4.3 Parametric attention \nIn Section 15.4.2, we defined the attention score in terms of the Gaussian kernel, comparing a scalar query (test point) to each of the scalar values in the training set. This does not scale well to large training sets, or high-dimensional inputs. We will therefore turn our attention to parametric models, where we have a fixed set of keys and values, and where we compare queries and keys in a learned embedding space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Attention as soft dictionary lookup"
    },
    {
        "content": "15.4.2 Kernel regression as non-parametric attention \nIn Section 16.3.5, we discuss kernel regression, which is a nonparametric model of the form \nwhere $alpha _ { i } ( x , x _ { 1 : n } ) geq 0$ measures the normalized similarity of test input $x$ to training input $x _ { i }$ . This similarity measure is usually computed by defining the attention score in terms of a density kernel, such as the Gaussian: \nwhere $sigma$ is called the bandwidth. We then define $a ( x , x _ { i } ) = K _ { sigma } ( x - x _ { i } )$ . \nBecause the scores are normalized, we can drop the √21πσ2 term. In addition, to maintain notation consistency with [Zha+20, Ch. 10], we rewrite the term inside the exponential as follows: \nPlugging this in to Equation (15.36), we get \nWe can interpret this as a form of nonparametric attention, where the queries are the test points $x$ , the keys are the training inputs $x _ { i }$ , and the values are the training labels $y _ { i }$ . \nIf we set $w = 1$ , the resulting attention matrix $A _ { j i } = alpha _ { i } ( x _ { j } , x _ { 1 : n } )$ for test input $j$ is shown in Figure 15.17a. The resulting predicted curve is shown in Figure 15.17b. \nThe size of the diagonal band in Figure 15.17a, and hence the sparsity of the attention mechanism, dependends on the parameter $w$ . If we increase $w$ , corresponding to reducing the kernel bandwidth, the band will get narrower, but the model will start to overfit. \n15.4.3 Parametric attention \nIn Section 15.4.2, we defined the attention score in terms of the Gaussian kernel, comparing a scalar query (test point) to each of the scalar values in the training set. This does not scale well to large training sets, or high-dimensional inputs. We will therefore turn our attention to parametric models, where we have a fixed set of keys and values, and where we compare queries and keys in a learned embedding space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Kernel regression as non-parametric attention"
    },
    {
        "content": "15.4.2 Kernel regression as non-parametric attention \nIn Section 16.3.5, we discuss kernel regression, which is a nonparametric model of the form \nwhere $alpha _ { i } ( x , x _ { 1 : n } ) geq 0$ measures the normalized similarity of test input $x$ to training input $x _ { i }$ . This similarity measure is usually computed by defining the attention score in terms of a density kernel, such as the Gaussian: \nwhere $sigma$ is called the bandwidth. We then define $a ( x , x _ { i } ) = K _ { sigma } ( x - x _ { i } )$ . \nBecause the scores are normalized, we can drop the √21πσ2 term. In addition, to maintain notation consistency with [Zha+20, Ch. 10], we rewrite the term inside the exponential as follows: \nPlugging this in to Equation (15.36), we get \nWe can interpret this as a form of nonparametric attention, where the queries are the test points $x$ , the keys are the training inputs $x _ { i }$ , and the values are the training labels $y _ { i }$ . \nIf we set $w = 1$ , the resulting attention matrix $A _ { j i } = alpha _ { i } ( x _ { j } , x _ { 1 : n } )$ for test input $j$ is shown in Figure 15.17a. The resulting predicted curve is shown in Figure 15.17b. \nThe size of the diagonal band in Figure 15.17a, and hence the sparsity of the attention mechanism, dependends on the parameter $w$ . If we increase $w$ , corresponding to reducing the kernel bandwidth, the band will get narrower, but the model will start to overfit. \n15.4.3 Parametric attention \nIn Section 15.4.2, we defined the attention score in terms of the Gaussian kernel, comparing a scalar query (test point) to each of the scalar values in the training set. This does not scale well to large training sets, or high-dimensional inputs. We will therefore turn our attention to parametric models, where we have a fixed set of keys and values, and where we compare queries and keys in a learned embedding space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThere are several ways to do this. In the general case, the query $q in mathbb { R } ^ { q }$ and the key $boldsymbol { k } in mathbb { R } ^ { k }$ may have different sizes. To compare them, we can map them to a common embedding space of size $h$ by computing $mathbf { W } _ { q } pmb { q }$ and ${ bf W } _ { k } { bf k }$ . where $mathbf { W } _ { q } in mathbb { R } ^ { h times q }$ and $mathbf { W } _ { k } in mathbb { R } ^ { h times k }$ . We can then pass these into an MLP to get the following additive attention scoring function: \nA more computationally efficient approach is to assume the queries and keys both have length $d$ , so we can compute $pmb { q } ^ { top } pmb { k }$ directly. If we assume these are independent random variables with $0$ mean and unit variance, the mean of their inner product is $0$ , and the variance is $d$ . (This follows from Equation (2.34) and Equation (2.39).) To ensure the variance of the inner product remains 1 regardless of the size of the inputs, it is standard to divide by $sqrt { d }$ . This gives rise to the scaled dot-product attention: \nIn practice, we usually deal with minibatches of $n$ vectors at a time. Let the corresponding matrices of queries, keys and values be denoted by $mathbf { Q } in mathbb { R } ^ { n times d }$ , $mathbf { K } in mathbb { R } ^ { m times d }$ , $mathbf { V } in mathbb { R } ^ { m times v }$ . Then we can compute the attention-weighted outputs as follows: \nwhere the softmax function softmax is applied row-wise. See attention_jax.ipynb for some sample code. \n15.4.4 Seq2Seq with attention \nRecall the seq2seq model from Section 15.2.3. This uses an RNN decoder of the form $h _ { t } ^ { d }  =$ $f _ { d } ( pmb { h } _ { t - 1 } ^ { d } , pmb { y } _ { t - 1 } , pmb { c } )$ , where $mathbf { nabla } c$ is a fixed-length context vector, representing the encoding of the input $pmb { x } _ { 1 : T }$ . Usually we set ${ bf c } = { bf h } _ { T } ^ { e }$ , which is the final state of the encoder RNN (or we use a bidirectional \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Parametric attention"
    },
    {
        "content": "There are several ways to do this. In the general case, the query $q in mathbb { R } ^ { q }$ and the key $boldsymbol { k } in mathbb { R } ^ { k }$ may have different sizes. To compare them, we can map them to a common embedding space of size $h$ by computing $mathbf { W } _ { q } pmb { q }$ and ${ bf W } _ { k } { bf k }$ . where $mathbf { W } _ { q } in mathbb { R } ^ { h times q }$ and $mathbf { W } _ { k } in mathbb { R } ^ { h times k }$ . We can then pass these into an MLP to get the following additive attention scoring function: \nA more computationally efficient approach is to assume the queries and keys both have length $d$ , so we can compute $pmb { q } ^ { top } pmb { k }$ directly. If we assume these are independent random variables with $0$ mean and unit variance, the mean of their inner product is $0$ , and the variance is $d$ . (This follows from Equation (2.34) and Equation (2.39).) To ensure the variance of the inner product remains 1 regardless of the size of the inputs, it is standard to divide by $sqrt { d }$ . This gives rise to the scaled dot-product attention: \nIn practice, we usually deal with minibatches of $n$ vectors at a time. Let the corresponding matrices of queries, keys and values be denoted by $mathbf { Q } in mathbb { R } ^ { n times d }$ , $mathbf { K } in mathbb { R } ^ { m times d }$ , $mathbf { V } in mathbb { R } ^ { m times v }$ . Then we can compute the attention-weighted outputs as follows: \nwhere the softmax function softmax is applied row-wise. See attention_jax.ipynb for some sample code. \n15.4.4 Seq2Seq with attention \nRecall the seq2seq model from Section 15.2.3. This uses an RNN decoder of the form $h _ { t } ^ { d }  =$ $f _ { d } ( pmb { h } _ { t - 1 } ^ { d } , pmb { y } _ { t - 1 } , pmb { c } )$ , where $mathbf { nabla } c$ is a fixed-length context vector, representing the encoding of the input $pmb { x } _ { 1 : T }$ . Usually we set ${ bf c } = { bf h } _ { T } ^ { e }$ , which is the final state of the encoder RNN (or we use a bidirectional \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nRNN with average pooling). However, for tasks such as machine translation, this can result in poor performance, since the output does not have access to the input words themselves. We can avoid this bottleneck by allowing the output words to directly “look at” the input words. But which inputs should it look at? After all, word order is not always preserved across languages (e.g., German often puts verbs at the end of a sentence), so we need to infer the alignment between source and target. We can solve this problem (in a differentiable way) by using (soft) attention, as first proposed in [BCB15; LPM15]. In particular, we can replace the fixed context vector $_ c$ in the decoder with a dynamic context vector $mathbf { Delta } mathbf { c } _ { t }$ computed as follows: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nPatientTimeline \nThis uses attention where the query is the hidden state of the decoder at the previous step, $h _ { t - 1 } ^ { d }$ , the keys are all the hidden states from the encoder, and the values are also the hidden states from the encoder. (When the RNN has multiple hidden layers, we usually take the top layer from the encoder, as the keys and values, and the top layer of the decoder as the query.) This context vector is concatenated with the input vector of the decoder, ${ mathbf { } } _ { { mathbf { } } } { mathbf { } } _ { { mathbf { } } _ { mathbf { } } } { mathbf { } } _ { { mathbf { } } _ { mathbf { } } } { mathbf { } } _ { { mathbf { } } _ { mathbf { } } } { mathbf { } } _ { { mathbf { } } _ { mathbf { } } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } _ { mathbf { } } { mathbf { } } _ { mathbf { } } _ { mathbf { } } { mathbf } _ { mathbf { } } _ { mathbf { } } _ { mathbf { } mathbf { } } _ { mathbf { } } _ { mathbf } { mathbf { } } _ { mathbf } { mathbf } _ { } mathbf { } _ { mathbf } { } mathbf { } _ { mathbf }  _ { mathbf { } mathbf { } mathbf } _ { } _ { mathbf } { mathbf } _ { mathbf } { mathbf } _ { mathbf } _ { mathbf } { } _ { mathbf } _ { mathbf } mathbf { } _ { mathbf } _ { mathbf } mathbf { } _ mathbf { } _ mathbf { } mathbf mathbf { } _ mathbf { } mathbf _ { } mathbf mathbf { } _ mathbf { } mathbf mathbf mathbf { } _ mathbf { } mathbf mathbf mathbf { } _ mathbf mathbf { } mathbf mathbf mathbf { } _ mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf { } mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf mathbf$ , and fed into the decoder, along with the previous hidden state $h _ { t - 1 } ^ { d }$ , to create $mathbf { delta } _ { h _ { t } ^ { d } }$ . See Figure 15.18 for an illustration of the overall model. \nWe can train this model in the usual way on sentence pairs, and then use it to perform machine translation. (See nmt_attention_jax.ipynb for some sample code.) We can also visualize the attention weights computed at each step of decoding, to get an idea of which parts of the input the model thinks are most relevant for generating the corresponding output. Some examples are shown in Figure 15.19. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n15.4.5 Seq2vec with attention (text classification) \nWe can also use attention with sequence classifiers. For example [Raj+18] apply an RNN classifier to the problem of predicting if a patient will die or not. The input is a set of electronic health records, which is a time series containing structured data, as well as unstructured text (clinical notes). Attention is useful for identifying “relevant” parts of the input, as illustrated in Figure 15.20. \n15.4.6 Seq+Seq2Vec with attention (text pair classification) \nSuppose we see the sentence “A person on a horse jumps over a log” (call this the premise) and then we later read “A person is outdoors on a horse” (call this the hypothesis). We may reasonably say that the premise entails the hypothesis, meaning that the hypothesis is more likely given the premise.3 Now suppose the hypothesis is “A person is at a diner ordering an omelette”. In this case, we would say that the premise contradicts the hypothesis, since the hypothesis is less likely given the premise. Finally, suppose the hypothesis is “A person is training his horse for a competition”. In this case, we see that the relationship between premise and hypothesis is neutral, since the hypothesis may or may not follow from the premise. The task of classifying a sentence pair into these three categories is known as textual entailment or “natural language inference”. A standard benchmark in this area is the Stanford Natural Language Inference or SNLI corpus [Bow+15]. This consists of 550,000 labeled sentence pairs. \nAn interesting solution to this classification problem was presented in [Par+16a]; at the time, it was the state of the art on the SNLI dataset. The overall approach is sketched in Figure 15.21. Let $mathbf { A } = ( pmb { a } _ { 1 } , dots , pmb { a } _ { m } )$ be the premise and $mathbf { B } = ( b _ { 1 } , ldots , b _ { n } )$ be the hypothesis, where $mathbf { boldsymbol { a } } _ { i } , mathbf { boldsymbol { b } } _ { j } in mathbb { R } ^ { E }$ are embedding vectors for the words. The model has 3 steps. First, each word in the premise, $mathbf { Delta } mathbf { a } _ { i }$ , attends to each word in the hypothesis, $b _ { j }$ , to compute an attention weight",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Seq2Seq with attention"
    },
    {
        "content": "15.4.5 Seq2vec with attention (text classification) \nWe can also use attention with sequence classifiers. For example [Raj+18] apply an RNN classifier to the problem of predicting if a patient will die or not. The input is a set of electronic health records, which is a time series containing structured data, as well as unstructured text (clinical notes). Attention is useful for identifying “relevant” parts of the input, as illustrated in Figure 15.20. \n15.4.6 Seq+Seq2Vec with attention (text pair classification) \nSuppose we see the sentence “A person on a horse jumps over a log” (call this the premise) and then we later read “A person is outdoors on a horse” (call this the hypothesis). We may reasonably say that the premise entails the hypothesis, meaning that the hypothesis is more likely given the premise.3 Now suppose the hypothesis is “A person is at a diner ordering an omelette”. In this case, we would say that the premise contradicts the hypothesis, since the hypothesis is less likely given the premise. Finally, suppose the hypothesis is “A person is training his horse for a competition”. In this case, we see that the relationship between premise and hypothesis is neutral, since the hypothesis may or may not follow from the premise. The task of classifying a sentence pair into these three categories is known as textual entailment or “natural language inference”. A standard benchmark in this area is the Stanford Natural Language Inference or SNLI corpus [Bow+15]. This consists of 550,000 labeled sentence pairs. \nAn interesting solution to this classification problem was presented in [Par+16a]; at the time, it was the state of the art on the SNLI dataset. The overall approach is sketched in Figure 15.21. Let $mathbf { A } = ( pmb { a } _ { 1 } , dots , pmb { a } _ { m } )$ be the premise and $mathbf { B } = ( b _ { 1 } , ldots , b _ { n } )$ be the hypothesis, where $mathbf { boldsymbol { a } } _ { i } , mathbf { boldsymbol { b } } _ { j } in mathbb { R } ^ { E }$ are embedding vectors for the words. The model has 3 steps. First, each word in the premise, $mathbf { Delta } mathbf { a } _ { i }$ , attends to each word in the hypothesis, $b _ { j }$ , to compute an attention weight",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Seq2vec with attention (text classification)"
    },
    {
        "content": "15.4.5 Seq2vec with attention (text classification) \nWe can also use attention with sequence classifiers. For example [Raj+18] apply an RNN classifier to the problem of predicting if a patient will die or not. The input is a set of electronic health records, which is a time series containing structured data, as well as unstructured text (clinical notes). Attention is useful for identifying “relevant” parts of the input, as illustrated in Figure 15.20. \n15.4.6 Seq+Seq2Vec with attention (text pair classification) \nSuppose we see the sentence “A person on a horse jumps over a log” (call this the premise) and then we later read “A person is outdoors on a horse” (call this the hypothesis). We may reasonably say that the premise entails the hypothesis, meaning that the hypothesis is more likely given the premise.3 Now suppose the hypothesis is “A person is at a diner ordering an omelette”. In this case, we would say that the premise contradicts the hypothesis, since the hypothesis is less likely given the premise. Finally, suppose the hypothesis is “A person is training his horse for a competition”. In this case, we see that the relationship between premise and hypothesis is neutral, since the hypothesis may or may not follow from the premise. The task of classifying a sentence pair into these three categories is known as textual entailment or “natural language inference”. A standard benchmark in this area is the Stanford Natural Language Inference or SNLI corpus [Bow+15]. This consists of 550,000 labeled sentence pairs. \nAn interesting solution to this classification problem was presented in [Par+16a]; at the time, it was the state of the art on the SNLI dataset. The overall approach is sketched in Figure 15.21. Let $mathbf { A } = ( pmb { a } _ { 1 } , dots , pmb { a } _ { m } )$ be the premise and $mathbf { B } = ( b _ { 1 } , ldots , b _ { n } )$ be the hypothesis, where $mathbf { boldsymbol { a } } _ { i } , mathbf { boldsymbol { b } } _ { j } in mathbb { R } ^ { E }$ are embedding vectors for the words. The model has 3 steps. First, each word in the premise, $mathbf { Delta } mathbf { a } _ { i }$ , attends to each word in the hypothesis, $b _ { j }$ , to compute an attention weight \n\nwhere $f : mathbb { R } ^ { E } to mathbb { R } ^ { D }$ is an MLP; we then compute a weighted average of the matching words in the hypothesis, \nNext, we compare $mathbf { mu } _ { mathbf { mu } } mathbf { mu } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathrm { ~ mu ~ } } mathbf { Lambda } _ { mathrm { ~ mu ~ } } mathbf { Lambda } _ { mathrm { ~ mu ~ } } mathrm { ~ bf ~ Lambda ~ } _ { mathrm { ~ mu ~ } } mathrm { ~ bf ~ Lambda ~ } _ { mathrm { ~ mu ~ } }$ with $beta _ { i }$ by mapping their concatenation to a hidden space using an MLP $g : mathbb { R } ^ { 2 E }  mathbb { R } ^ { H }$ : \nFinally, we aggregate over the comparisons to get an overall similarity of premise to hypothesis: \nWe can similarly compare the hypothesis to the premise using \nAt the end, we classify the output using another MLP $h : mathbb { R } ^ { 2 H }  mathbb { R } ^ { 3 }$ : \nSee entailment_attention_mlp_jax.ipynb for some sample code. \nWe can modify this model to learn other kinds of mappings from sentence pairs to output labels. For example, in the semantic textual similarity task, the goal is to predict how semantically related two input sentences are. A standard dataset for this is the STS Benchmark [Cer+17], where relatedness ranges from 0 (meaning unrelated) to 5 (meaning maximally related). \n15.4.7 Soft vs hard attention \nIf we force the attention heatmap to be sparse, so that each output can only attend to one input location instead of a weighted combination of all of them, the method is called hard attention. We compare these two approaches for an image captioning problem in Figure 15.22. Unfortunately, hard attention results in a nondifferentiable training objective, and requires methods such as reinforcement learning to fit the model. See [Xu+15] for the details. \nIt seems from the above examples that these attention heatmaps can “explain” why the model generates a given output. However, the interpretability of attention is controversial (see e.g., [JW19; WP19; SS19; Bru+19] for discussion). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Seq+Seq2Vec with attention (text pair classification)"
    },
    {
        "content": "where $f : mathbb { R } ^ { E } to mathbb { R } ^ { D }$ is an MLP; we then compute a weighted average of the matching words in the hypothesis, \nNext, we compare $mathbf { mu } _ { mathbf { mu } } mathbf { mu } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathbf { mu } } mathbf { Lambda } _ { mathrm { ~ mu ~ } } mathbf { Lambda } _ { mathrm { ~ mu ~ } } mathbf { Lambda } _ { mathrm { ~ mu ~ } } mathrm { ~ bf ~ Lambda ~ } _ { mathrm { ~ mu ~ } } mathrm { ~ bf ~ Lambda ~ } _ { mathrm { ~ mu ~ } }$ with $beta _ { i }$ by mapping their concatenation to a hidden space using an MLP $g : mathbb { R } ^ { 2 E }  mathbb { R } ^ { H }$ : \nFinally, we aggregate over the comparisons to get an overall similarity of premise to hypothesis: \nWe can similarly compare the hypothesis to the premise using \nAt the end, we classify the output using another MLP $h : mathbb { R } ^ { 2 H }  mathbb { R } ^ { 3 }$ : \nSee entailment_attention_mlp_jax.ipynb for some sample code. \nWe can modify this model to learn other kinds of mappings from sentence pairs to output labels. For example, in the semantic textual similarity task, the goal is to predict how semantically related two input sentences are. A standard dataset for this is the STS Benchmark [Cer+17], where relatedness ranges from 0 (meaning unrelated) to 5 (meaning maximally related). \n15.4.7 Soft vs hard attention \nIf we force the attention heatmap to be sparse, so that each output can only attend to one input location instead of a weighted combination of all of them, the method is called hard attention. We compare these two approaches for an image captioning problem in Figure 15.22. Unfortunately, hard attention results in a nondifferentiable training objective, and requires methods such as reinforcement learning to fit the model. See [Xu+15] for the details. \nIt seems from the above examples that these attention heatmaps can “explain” why the model generates a given output. However, the interpretability of attention is controversial (see e.g., [JW19; WP19; SS19; Bru+19] for discussion). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n15.5 Transformers \nThe transformer model [Vas+17] is a seq2seq model which uses attention in the encoder as well as the decoder, thus eliminating the need for RNNs, as we explain below. Transformers have been used for many (conditional) sequence generation tasks, such as machine translation [Vas+17], constituency parsing [Vas+17], music generation [Hua+18], protein sequence generation [Mad+20; Cho+20b], abstractive text summarization [Zha+19a], image generation [Par+18] (treating the image as a rasterized 1d sequence), etc. \nThe transformer is a rather complex model that uses several new kinds of building blocks or layers. We introduce these new blocks below, and then discuss how to put them all together.4 \n15.5.1 Self-attention \nIn Section 15.4.4 we showed how the decoder of an RNN could use attention to the input sequence in order to capture contexual embeddings of each input. However, rather than the decoder attending to the encoder, we can modify the model so the encoder attends to itself. This is called self attention [CDL16; Par+16b]. \nIn more detail, given a sequence of input tokens $pmb { x } _ { 1 } , ldots , pmb { x } _ { n }$ , where $pmb { x } _ { i } in mathbb { R } ^ { d }$ , self-attention can generate a sequence of outputs of the same size using \nwhere the query is ${ boldsymbol { x } } _ { i }$ , and the keys and values are all the (valid) inputs $pmb { x } _ { 1 } , ldots , pmb { x } _ { n }$ . \nTo use this in a decoder, we can set ${ pmb x } _ { i } = { pmb y } _ { i - 1 }$ , and $n = i - 1$ , so all the previously generated outputs are available. At training time, all the outputs are already known, so we can evaluate the above function in parallel, overcoming the sequential bottleneck of using RNNs. \nIn addition to improved speed, self-attention can give improved representations of context. As an example, consider translating the English sentences “The animal didn’t cross the street because it was too tired” and “The animal didn’t cross the street because it was too wide” into French. To generate a pronoun of the correct gender in French, we need to know what “it” refers to (this is called coreference resolution). In the first case, the word “it” refers to the animal. In the second case, the word “it” now refers to the street.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Attention",
        "subsubsection": "Soft vs hard attention"
    },
    {
        "content": "15.5 Transformers \nThe transformer model [Vas+17] is a seq2seq model which uses attention in the encoder as well as the decoder, thus eliminating the need for RNNs, as we explain below. Transformers have been used for many (conditional) sequence generation tasks, such as machine translation [Vas+17], constituency parsing [Vas+17], music generation [Hua+18], protein sequence generation [Mad+20; Cho+20b], abstractive text summarization [Zha+19a], image generation [Par+18] (treating the image as a rasterized 1d sequence), etc. \nThe transformer is a rather complex model that uses several new kinds of building blocks or layers. We introduce these new blocks below, and then discuss how to put them all together.4 \n15.5.1 Self-attention \nIn Section 15.4.4 we showed how the decoder of an RNN could use attention to the input sequence in order to capture contexual embeddings of each input. However, rather than the decoder attending to the encoder, we can modify the model so the encoder attends to itself. This is called self attention [CDL16; Par+16b]. \nIn more detail, given a sequence of input tokens $pmb { x } _ { 1 } , ldots , pmb { x } _ { n }$ , where $pmb { x } _ { i } in mathbb { R } ^ { d }$ , self-attention can generate a sequence of outputs of the same size using \nwhere the query is ${ boldsymbol { x } } _ { i }$ , and the keys and values are all the (valid) inputs $pmb { x } _ { 1 } , ldots , pmb { x } _ { n }$ . \nTo use this in a decoder, we can set ${ pmb x } _ { i } = { pmb y } _ { i - 1 }$ , and $n = i - 1$ , so all the previously generated outputs are available. At training time, all the outputs are already known, so we can evaluate the above function in parallel, overcoming the sequential bottleneck of using RNNs. \nIn addition to improved speed, self-attention can give improved representations of context. As an example, consider translating the English sentences “The animal didn’t cross the street because it was too tired” and “The animal didn’t cross the street because it was too wide” into French. To generate a pronoun of the correct gender in French, we need to know what “it” refers to (this is called coreference resolution). In the first case, the word “it” refers to the animal. In the second case, the word “it” now refers to the street. \n\nFigure 15.23 illustrates how self attention applied to the English sentence is able to resolve this ambiguity. In the first sentence, the representation for “it” depends on the earlier representations of “animal”, whereas in the latter, it depends on the earlier representations of “street”. \n15.5.2 Multi-headed attention \nIf we think of an attention matrix as like a kernel matrix (as discussed in Section 15.4.2), it is natural to want to use multiple attention matrices, to capture different notions of similarity. This is the basic idea behind multi-headed attention (MHA). In more detail, query a given $pmb q in mathbb { R } ^ { d _ { q } }$ , keys $boldsymbol { k } _ { j } in mathbb { R } ^ { d _ { k } }$ , and values $pmb { v } _ { j } in mathbb { R } ^ { d _ { v } }$ , we define the $i$ ’th attention head to be \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Self-attention"
    },
    {
        "content": "Figure 15.23 illustrates how self attention applied to the English sentence is able to resolve this ambiguity. In the first sentence, the representation for “it” depends on the earlier representations of “animal”, whereas in the latter, it depends on the earlier representations of “street”. \n15.5.2 Multi-headed attention \nIf we think of an attention matrix as like a kernel matrix (as discussed in Section 15.4.2), it is natural to want to use multiple attention matrices, to capture different notions of similarity. This is the basic idea behind multi-headed attention (MHA). In more detail, query a given $pmb q in mathbb { R } ^ { d _ { q } }$ , keys $boldsymbol { k } _ { j } in mathbb { R } ^ { d _ { k } }$ , and values $pmb { v } _ { j } in mathbb { R } ^ { d _ { v } }$ , we define the $i$ ’th attention head to be \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $mathbf { W } _ { i } ^ { ( q ) } in mathbb { R } ^ { p _ { q } times d _ { q } }$ , $mathbf { W } _ { i } ^ { ( k ) } in mathbb { R } ^ { p _ { k } times d _ { k } }$ , and $mathbf { W } _ { i } ^ { ( v ) } in mathbb { R } ^ { p _ { v } times d _ { v } }$ are projection matrices. We then stack the $h$ heads together, and project to $mathbb { R } ^ { p _ { o } }$ using \nwhere $boldsymbol { h } _ { i }$ is defined in Equation (15.55), and $mathbf { W } _ { o } in mathbb { R } ^ { p _ { o } times h p _ { v } }$ . If we set $p _ { q } h = p _ { k } h = p _ { v } h = p _ { o }$ , we can compute all the output heads in parallel. See multi_head_attention_jax.ipynb for some sample code. \n15.5.3 Positional encoding \nThe performance of “vanilla” self-attention can be low, since attention is permutation invariant, and hence ignores the input word ordering. To overcome this, we can concatenate the word embeddings with a positional embedding, so that the model knows what order the words occur in. \nOne way to do this is to represent each position by an integer. However, neural networks cannot natively handle integers. To overcome this, we can encode the integer in binary form. For example, if we assume the sequence length is $n = 3$ , we get the following sequence of $d = 3$ -dimensional bit vectors for each location: 000, 001, 010, 011, 100, 101, 110, 111. We see that the right most index toggles the fastest (has highest frequency), whereas the left most index (most significant bit) toggles the slowest. (We could of course change this, so that the left most bit toggles fastest.) We can represent this as a position matrix P ∈ Rn×d. \nWe can think of the above representation as using a set of basis functions (corresponding to powers of 2), where the coefficients are 0 or 1. We can obtain a more compact code by using a different set of basis functions, and real-valued weights. [Vas+17] propose to use a sinusoidal basis, as follows: \nwhere $C = 1 0 , 0 0 0$ corresponds to some maximum sequence length. For example, if $d = 4$ , the $i$ ’t row is \nFigure 15.25a shows the corresponding position matrix for $n = 6 0$ and $d = 3 2$ . In this case, the left-most columns toggle fastest. We see that each row has a real-valued “fingerprint” representing its location in the sequence. Figure 15.25b shows some of the basis functions (column vectors) for dimensions 6 to 9. \nThe advantage of this representation is two-fold. First, it can be computed for arbitrary length inputs (up to $T leq C$ ), unlike a learned mapping from integers to vectors. Second, the representation of one location is linearly predictable from any other, given knowledge of their relative distance. In \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 particular, we have $pmb { p } _ { t + phi } = f ( pmb { p } _ { t } )$ , where $f$ is a linear transformation. To see this, note that",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Multi-headed attention"
    },
    {
        "content": "where $mathbf { W } _ { i } ^ { ( q ) } in mathbb { R } ^ { p _ { q } times d _ { q } }$ , $mathbf { W } _ { i } ^ { ( k ) } in mathbb { R } ^ { p _ { k } times d _ { k } }$ , and $mathbf { W } _ { i } ^ { ( v ) } in mathbb { R } ^ { p _ { v } times d _ { v } }$ are projection matrices. We then stack the $h$ heads together, and project to $mathbb { R } ^ { p _ { o } }$ using \nwhere $boldsymbol { h } _ { i }$ is defined in Equation (15.55), and $mathbf { W } _ { o } in mathbb { R } ^ { p _ { o } times h p _ { v } }$ . If we set $p _ { q } h = p _ { k } h = p _ { v } h = p _ { o }$ , we can compute all the output heads in parallel. See multi_head_attention_jax.ipynb for some sample code. \n15.5.3 Positional encoding \nThe performance of “vanilla” self-attention can be low, since attention is permutation invariant, and hence ignores the input word ordering. To overcome this, we can concatenate the word embeddings with a positional embedding, so that the model knows what order the words occur in. \nOne way to do this is to represent each position by an integer. However, neural networks cannot natively handle integers. To overcome this, we can encode the integer in binary form. For example, if we assume the sequence length is $n = 3$ , we get the following sequence of $d = 3$ -dimensional bit vectors for each location: 000, 001, 010, 011, 100, 101, 110, 111. We see that the right most index toggles the fastest (has highest frequency), whereas the left most index (most significant bit) toggles the slowest. (We could of course change this, so that the left most bit toggles fastest.) We can represent this as a position matrix P ∈ Rn×d. \nWe can think of the above representation as using a set of basis functions (corresponding to powers of 2), where the coefficients are 0 or 1. We can obtain a more compact code by using a different set of basis functions, and real-valued weights. [Vas+17] propose to use a sinusoidal basis, as follows: \nwhere $C = 1 0 , 0 0 0$ corresponds to some maximum sequence length. For example, if $d = 4$ , the $i$ ’t row is \nFigure 15.25a shows the corresponding position matrix for $n = 6 0$ and $d = 3 2$ . In this case, the left-most columns toggle fastest. We see that each row has a real-valued “fingerprint” representing its location in the sequence. Figure 15.25b shows some of the basis functions (column vectors) for dimensions 6 to 9. \nThe advantage of this representation is two-fold. First, it can be computed for arbitrary length inputs (up to $T leq C$ ), unlike a learned mapping from integers to vectors. Second, the representation of one location is linearly predictable from any other, given knowledge of their relative distance. In \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 particular, we have $pmb { p } _ { t + phi } = f ( pmb { p } _ { t } )$ , where $f$ is a linear transformation. To see this, note that \n\nSo if $phi$ is small, then $p _ { t + phi } approx p _ { t }$ . This provides a useful form of inductive bias. \nOnce we have computed the positional emebddings $mathbf { P }$ , we need to combine them with the original word embeddings $mathbf { X }$ using the following: \n15.5.4 Putting it all together \nA transformer is a seq2seq model that uses self-attention for the encoder and decoder rather than an RNN. The encoder uses a series of encoder blocks, each of which uses multi-headed attention (Section 15.5.2), residual connections (Section 13.4.4), and layer normalization (Section 14.2.4.2). More precisely, the encoder block can be defined as follows: \ndef EncoderBlock(X): $Z  =$ LayerNorm(MultiHeadAttn( $sf { Q } = bf { X }$ , $mathtt { K } mathrm { = } mathtt { X }$ , V=X) + X) $tt { E } =$ LayerNorm(FeedForward(Z) + Z) return E \nThe overall encoder is defined by applying positional encoding to the embedding of the input sequence, following by $N$ copies of the encoder block, where $N$ controls the depth of the block:",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Positional encoding"
    },
    {
        "content": "So if $phi$ is small, then $p _ { t + phi } approx p _ { t }$ . This provides a useful form of inductive bias. \nOnce we have computed the positional emebddings $mathbf { P }$ , we need to combine them with the original word embeddings $mathbf { X }$ using the following: \n15.5.4 Putting it all together \nA transformer is a seq2seq model that uses self-attention for the encoder and decoder rather than an RNN. The encoder uses a series of encoder blocks, each of which uses multi-headed attention (Section 15.5.2), residual connections (Section 13.4.4), and layer normalization (Section 14.2.4.2). More precisely, the encoder block can be defined as follows: \ndef EncoderBlock(X): $Z  =$ LayerNorm(MultiHeadAttn( $sf { Q } = bf { X }$ , $mathtt { K } mathrm { = } mathtt { X }$ , V=X) + X) $tt { E } =$ LayerNorm(FeedForward(Z) + Z) return E \nThe overall encoder is defined by applying positional encoding to the embedding of the input sequence, following by $N$ copies of the encoder block, where $N$ controls the depth of the block: \ndef Encoder(X, N): E = POS(Embed(X)) for n in range(N): $tt { E } =$ EncoderBlock(E) return E \nSee the LHS of Figure 15.26 for an illustration. \nThe decoder has a somewhat more complex structure. It is given access to the encoder via another multi-head attention block. But it is also given access to previously generated outputs: these are shifted, and then combined with a positional embedding, and then fed into a masked (causal) multi-head attention model. Finally the output distribution over tokens at each location is computed in parallel. \nIn more detail, the decoder block is defined as follows: \ndef DecoderBlock(Y, E): $Z  =$ LayerNorm(MultiHeadAttn( $mathsf { Q } mathrm { = Y }$ , ${ sf K } = { sf Y }$ , V=Y) + Y) $z ^ { , } ~ =$ LayerNorm(MultiHeadAttn( $scriptstyle { mathsf { Q } } = { mathsf { Z } }$ , $mathtt { K } mathrm { = } mathtt { E }$ , V=E) + Z) $textsf { D } =$ LayerNorm(FeedForward(Z’) + Z’) return D \nThe overall decoder is defined by $N$ copies of the decoder block: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \ndef Decoder(Y, E, N): D = POS(Embed(Y)) for n in range(N): $textsf { D } =$ DecoderBlock(D,E) return D \nSee the RHS of Figure 15.26 for an illustration. \nDuring training time, all the inputs $mathbf { Y }$ to the decoder are known in advance, since they are derived from embedding the lagged target output sequence. During inference (test) time, we need to decode sequentially, and use masked attention, where we feed the generated output into the embedding layer, and add it to the set of keys/values that can be attended to. (We initialize by feeding in the start-of-sequence token.) See transformers_jax.ipynb for some sample code, and [Rus18; Ala18] for a detailed tutorial on this model. \n15.5.5 Comparing transformers, CNNs and RNNs \nIn Figure 15.27, we visually compare three different architectures for mapping a sequence ${ pmb x } _ { 1 : n }$ to another sequence $pmb { y } _ { 1 : n }$ : a 1d CNN, an RNN, and an attention-based model. Each model makes different tradeoffs in terms of speed and expressive power, where the latter can be quantified in terms of the maximum path length between any two inputs. See Table 15.1 for a summary. \nFor a 1d CNN with kernel size $k$ and $d$ feature channels, the time to compute the output is $O ( k n d ^ { 2 } )$ , which can be done in parallel. We need a stack of $n / k$ layers, or $log _ { k } ( n )$ if we use dilated convolution, to ensure all pairs can communicate. For example, in Figure 15.27, we see that $x _ { 1 }$ and $x _ { 5 }$ are initially \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Putting it all together"
    },
    {
        "content": "def Decoder(Y, E, N): D = POS(Embed(Y)) for n in range(N): $textsf { D } =$ DecoderBlock(D,E) return D \nSee the RHS of Figure 15.26 for an illustration. \nDuring training time, all the inputs $mathbf { Y }$ to the decoder are known in advance, since they are derived from embedding the lagged target output sequence. During inference (test) time, we need to decode sequentially, and use masked attention, where we feed the generated output into the embedding layer, and add it to the set of keys/values that can be attended to. (We initialize by feeding in the start-of-sequence token.) See transformers_jax.ipynb for some sample code, and [Rus18; Ala18] for a detailed tutorial on this model. \n15.5.5 Comparing transformers, CNNs and RNNs \nIn Figure 15.27, we visually compare three different architectures for mapping a sequence ${ pmb x } _ { 1 : n }$ to another sequence $pmb { y } _ { 1 : n }$ : a 1d CNN, an RNN, and an attention-based model. Each model makes different tradeoffs in terms of speed and expressive power, where the latter can be quantified in terms of the maximum path length between any two inputs. See Table 15.1 for a summary. \nFor a 1d CNN with kernel size $k$ and $d$ feature channels, the time to compute the output is $O ( k n d ^ { 2 } )$ , which can be done in parallel. We need a stack of $n / k$ layers, or $log _ { k } ( n )$ if we use dilated convolution, to ensure all pairs can communicate. For example, in Figure 15.27, we see that $x _ { 1 }$ and $x _ { 5 }$ are initially \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n5 apart, and then 3 apart in layer 1, and then connected in layer 2. \nFor an RNN, the computational complexity is $O ( n d ^ { 2 } )$ , for a hidden state of size $d$ , since we have to perform matrix-vector multiplication at each step. This is an inherently sequential operation. The maximum path length is $O ( n )$ . \nFinally, for self-attention models, every output is directly connected to every input, so the maximum path length is $O ( 1 )$ . However, the computational cost is $O ( n ^ { 2 } d )$ . For short sequences, we typically have $n ll d$ , so this is fine. For longer sequences, we discuss various fast versions of attention in Section 15.6. \n15.5.6 Transformers for images * \nCNNs (Chapter 14) are the most common model type for processing image data, since they have useful built-in inductive bias, such as locality (due to small kernels), equivariance (due to weight tying), and invariance (due to pooling). Suprisingly, it has been found that transformers can also do well at image classification [Rag+21], at least if trained on enough data. (They need a lot of data to overcome their lack of relevant inductive bias.) \nThe first model of this kind, known as ViT (vision transformer) [Dos+21], chops the input up into 16x16 patches, projects each patch into an embedding space, and then passes this set of embeddings ${ pmb x } _ { 1 : T }$ to a transformer, analogous to the way word embeddings are passed to a transformer. The input is also prepended with a special [CLASS] embedding, $scriptstyle { mathbf { x } } _ { 0 }$ . The output of the transformer is a set of encodings $e _ { 0 : T }$ ; the model maps $e _ { 0 }$ to the target class label $y$ , and is trained in a supervised way. See Figure 15.28 for an illustration. \nAfter supervised pretraining, the model is fine-tuned on various downstream classification tasks, an approach known as transfer learning (see Section 19.2 for more details). When trained on “small” datasets such as ImageNet (which has 1k classes and 1.3M images), they find that they cannot outperform a pretrained CNN ResNet model (Section 14.3.4) known as BiT (big transfer) [Kol+20]. However, when trained on larger datasets, such as ImageNet-21k (with 21k classes and 14M images), or the Google-internal JFT dataset (with 18k classes and 303M images), they find that ViT does better than BiT at transfer learning.6 ViT is also cheaper to train than ResNet at this scale. (However, training is still expensive: the large ViT model on ImageNet-21k takes 30 days on a Google Cloud TPUv3 with 8 cores!)",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Comparing transformers, CNNs and RNNs"
    },
    {
        "content": "5 apart, and then 3 apart in layer 1, and then connected in layer 2. \nFor an RNN, the computational complexity is $O ( n d ^ { 2 } )$ , for a hidden state of size $d$ , since we have to perform matrix-vector multiplication at each step. This is an inherently sequential operation. The maximum path length is $O ( n )$ . \nFinally, for self-attention models, every output is directly connected to every input, so the maximum path length is $O ( 1 )$ . However, the computational cost is $O ( n ^ { 2 } d )$ . For short sequences, we typically have $n ll d$ , so this is fine. For longer sequences, we discuss various fast versions of attention in Section 15.6. \n15.5.6 Transformers for images * \nCNNs (Chapter 14) are the most common model type for processing image data, since they have useful built-in inductive bias, such as locality (due to small kernels), equivariance (due to weight tying), and invariance (due to pooling). Suprisingly, it has been found that transformers can also do well at image classification [Rag+21], at least if trained on enough data. (They need a lot of data to overcome their lack of relevant inductive bias.) \nThe first model of this kind, known as ViT (vision transformer) [Dos+21], chops the input up into 16x16 patches, projects each patch into an embedding space, and then passes this set of embeddings ${ pmb x } _ { 1 : T }$ to a transformer, analogous to the way word embeddings are passed to a transformer. The input is also prepended with a special [CLASS] embedding, $scriptstyle { mathbf { x } } _ { 0 }$ . The output of the transformer is a set of encodings $e _ { 0 : T }$ ; the model maps $e _ { 0 }$ to the target class label $y$ , and is trained in a supervised way. See Figure 15.28 for an illustration. \nAfter supervised pretraining, the model is fine-tuned on various downstream classification tasks, an approach known as transfer learning (see Section 19.2 for more details). When trained on “small” datasets such as ImageNet (which has 1k classes and 1.3M images), they find that they cannot outperform a pretrained CNN ResNet model (Section 14.3.4) known as BiT (big transfer) [Kol+20]. However, when trained on larger datasets, such as ImageNet-21k (with 21k classes and 14M images), or the Google-internal JFT dataset (with 18k classes and 303M images), they find that ViT does better than BiT at transfer learning.6 ViT is also cheaper to train than ResNet at this scale. (However, training is still expensive: the large ViT model on ImageNet-21k takes 30 days on a Google Cloud TPUv3 with 8 cores!) \n15.5.7 Other transformer variants * \nMany extensions of transformers have been published in the last few years. For example, the Gshard paper [Lep+21] shows how to scale up transformers to even more parameters by replacing some of the feed forward dense layers with a mixture of experts (Section 13.6.2) regression module. This allows for sparse conditional computation, in which only a subset of the model capacity (chosen by the gating network) is used for any given input. \nAs another example, the conformer paper [Gul+20] showed how to add convolutional layers inside the transformer architecture, which was shown to be helpful for various speech recognition tasks. \n15.6 Efficient transformers * \nThis section is written by Krzysztof Choromanski. \nRegular transformers take $O ( N ^ { 2 } )$ time and space complexity, for a sequence of length $N$ , which makes them impractical to apply to long sequences. In the past few years, researchers have proposed several more efficient variants of transformers to bypass this difficulty. In this section, we give a brief survey of some of these methods (see Figure 15.29 for a summary). For more details, see e.g., [Tay+20b; Tay+20a; Lin+21]. \n15.6.1 Fixed non-learnable localized attention patterns \nThe simplest modification of the attention mechanism is to constrain it to a fixed non-learnable localized window, in other words restrict each token to attend only to a pre-selected set of other tokens. If for instance, each sequence is chunked into $K$ blocks, each of length $textstyle { frac { N } { K } }$ , and attention is conducted only within a block, then space/time complexity is reduced from $O ( N ^ { 2 } )$ to $textstyle { frac { N ^ { 2 } } { K } }$ . For \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Transformers for images *"
    },
    {
        "content": "15.5.7 Other transformer variants * \nMany extensions of transformers have been published in the last few years. For example, the Gshard paper [Lep+21] shows how to scale up transformers to even more parameters by replacing some of the feed forward dense layers with a mixture of experts (Section 13.6.2) regression module. This allows for sparse conditional computation, in which only a subset of the model capacity (chosen by the gating network) is used for any given input. \nAs another example, the conformer paper [Gul+20] showed how to add convolutional layers inside the transformer architecture, which was shown to be helpful for various speech recognition tasks. \n15.6 Efficient transformers * \nThis section is written by Krzysztof Choromanski. \nRegular transformers take $O ( N ^ { 2 } )$ time and space complexity, for a sequence of length $N$ , which makes them impractical to apply to long sequences. In the past few years, researchers have proposed several more efficient variants of transformers to bypass this difficulty. In this section, we give a brief survey of some of these methods (see Figure 15.29 for a summary). For more details, see e.g., [Tay+20b; Tay+20a; Lin+21]. \n15.6.1 Fixed non-learnable localized attention patterns \nThe simplest modification of the attention mechanism is to constrain it to a fixed non-learnable localized window, in other words restrict each token to attend only to a pre-selected set of other tokens. If for instance, each sequence is chunked into $K$ blocks, each of length $textstyle { frac { N } { K } }$ , and attention is conducted only within a block, then space/time complexity is reduced from $O ( N ^ { 2 } )$ to $textstyle { frac { N ^ { 2 } } { K } }$ . For \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Transformers",
        "subsubsection": "Other transformer variants *"
    },
    {
        "content": "15.5.7 Other transformer variants * \nMany extensions of transformers have been published in the last few years. For example, the Gshard paper [Lep+21] shows how to scale up transformers to even more parameters by replacing some of the feed forward dense layers with a mixture of experts (Section 13.6.2) regression module. This allows for sparse conditional computation, in which only a subset of the model capacity (chosen by the gating network) is used for any given input. \nAs another example, the conformer paper [Gul+20] showed how to add convolutional layers inside the transformer architecture, which was shown to be helpful for various speech recognition tasks. \n15.6 Efficient transformers * \nThis section is written by Krzysztof Choromanski. \nRegular transformers take $O ( N ^ { 2 } )$ time and space complexity, for a sequence of length $N$ , which makes them impractical to apply to long sequences. In the past few years, researchers have proposed several more efficient variants of transformers to bypass this difficulty. In this section, we give a brief survey of some of these methods (see Figure 15.29 for a summary). For more details, see e.g., [Tay+20b; Tay+20a; Lin+21]. \n15.6.1 Fixed non-learnable localized attention patterns \nThe simplest modification of the attention mechanism is to constrain it to a fixed non-learnable localized window, in other words restrict each token to attend only to a pre-selected set of other tokens. If for instance, each sequence is chunked into $K$ blocks, each of length $textstyle { frac { N } { K } }$ , and attention is conducted only within a block, then space/time complexity is reduced from $O ( N ^ { 2 } )$ to $textstyle { frac { N ^ { 2 } } { K } }$ . For \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n$K gg 1$ this constitutes substantial overall computational improvements. Such an approach is applied in particular in [Qiu+19b; Par+18]. The attention patterns do not need to be in the form of blocks. Other approaches involve strided / dilated windows, or hybrid patterns, where several fixed attention patterns are combined together [Chi+19b; BPC20]. \n15.6.2 Learnable sparse attention patterns \nA natural extension of the above approach is to allow the above compact patterns to be learned. The attention is still restricted to pairs of tokens within a single partition of some partitioning of the set of all the tokens, but now those partitionings are trained. In this class of methods we can distinguish two main approaches: based on hashing and clustering. In the hashing scenario all tokens are hashed and thus different partitions correspond to different hashing-buckets. This is the case for instance for the Reformer architecture [KKL20], where locality sensitive hashing (LSH) is applied. That leads to time complexity $O ( N M ^ { 2 } log ( M ) )$ of the attention module, where $M$ stands for the dimenionsality of tokens’ embeddings. \nHashing approaches require the set of queries to be identical to the set of keys. Furthermore, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the number of hashes needed for precise partitioning (which in the above expression is treated as a constant) can be a large constant. In the clustering approach, tokens are clustered using standard clustering algorithms such as K-means (Section 21.3); this is known as the “clustering transformer” [Roy+20]. As in the block-case, if $K$ equal-size clusters are used then space complexity of the attention module is reduced to $O ( frac { N ^ { 2 } } { K } )$ . In practice $K$ is often taken to be of order $K = Theta ( sqrt { N } )$ , yet imposing that the clusters be similar in size is in practice difficult.",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Efficient transformers *",
        "subsubsection": "Fixed non-learnable localized attention patterns"
    },
    {
        "content": "$K gg 1$ this constitutes substantial overall computational improvements. Such an approach is applied in particular in [Qiu+19b; Par+18]. The attention patterns do not need to be in the form of blocks. Other approaches involve strided / dilated windows, or hybrid patterns, where several fixed attention patterns are combined together [Chi+19b; BPC20]. \n15.6.2 Learnable sparse attention patterns \nA natural extension of the above approach is to allow the above compact patterns to be learned. The attention is still restricted to pairs of tokens within a single partition of some partitioning of the set of all the tokens, but now those partitionings are trained. In this class of methods we can distinguish two main approaches: based on hashing and clustering. In the hashing scenario all tokens are hashed and thus different partitions correspond to different hashing-buckets. This is the case for instance for the Reformer architecture [KKL20], where locality sensitive hashing (LSH) is applied. That leads to time complexity $O ( N M ^ { 2 } log ( M ) )$ of the attention module, where $M$ stands for the dimenionsality of tokens’ embeddings. \nHashing approaches require the set of queries to be identical to the set of keys. Furthermore, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the number of hashes needed for precise partitioning (which in the above expression is treated as a constant) can be a large constant. In the clustering approach, tokens are clustered using standard clustering algorithms such as K-means (Section 21.3); this is known as the “clustering transformer” [Roy+20]. As in the block-case, if $K$ equal-size clusters are used then space complexity of the attention module is reduced to $O ( frac { N ^ { 2 } } { K } )$ . In practice $K$ is often taken to be of order $K = Theta ( sqrt { N } )$ , yet imposing that the clusters be similar in size is in practice difficult. \n\n15.6.3 Memory and recurrence methods \nIn some approaches, a side memory module can access several tokens simultaneously. This method is often instantiated in the form of a global memory algorithm as used in [Lee+19; Zah+20]. \nAnother approach is to connect different local blocks via recurrence. A flagship example of this approach is the class of Transformer-XL methods [Dai+19]. \n15.6.4 Low-rank and kernel methods \nIn this section, we discuss methods that approximate attention using low rank matrices. In [She+18; Kat+20] they approximate the attention matrix A directly by a low rank matrix, so that \nwhere $pmb { phi } ( pmb { x } ) in mathbb { R } ^ { M }$ is some finite-dimensional vector with $M < D$ . One can leverage this structure to compute AV in $O ( N )$ time. Unfortunately, for softmax attention, the A is not low rank. \nIn Linformer [Wan+20a], they instead transform the keys and values via random Gaussian projections. They then apply the theory of the Johnson-Lindenstrauss Transform [AL13] to approximate softmax attention in this lower dimensional space. \nIn Performer [Cho+20a; Cho+20b], they show that the attention matrix can be computed using a (positive definite) kernel function. We define kernel functions in Section 17.1, but the basic idea is that ${ cal K } ( { pmb q } , { pmb k } ) geq 0$ is some measure of similarity between $pmb q in mathbb { R } ^ { D }$ and $pmb { k } in mathbb { R } ^ { D }$ . For example, the Gaussian kernel, also called the radial basis function kernel, has the form \nTo see how this can be used to compute an attention matrix, note that [Cho+20a] show the following: \nThe first term in the above expression is equal to $K _ { mathrm { g a u s s } } ( pmb q _ { i } D ^ { - 1 / 4 }$ , $k _ { j } D ^ { - 1 / 4 } )$ with $sigma = 1$ , and the other two terms are just independent scaling factors. \nSo far we have not gained anything computationally. However, we will show in Section 17.2.9.3 that the Gaussian kernel can be written as the expectation of a set of random features: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $pmb { eta } ( pmb { x } ) in mathbb { R } ^ { M }$ is a random feature vector derived from $_ { x }$ , either based on trigonometric functions Equation (17.60) or exponential functions Equation (17.61). (The latter has the advantage that all the features are positive, which gives much better results [Cho+20b].) Therefore for the regular softmax attention, $A _ { i , j }$ can be rewritten as",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Efficient transformers *",
        "subsubsection": "Learnable sparse attention patterns"
    },
    {
        "content": "15.6.3 Memory and recurrence methods \nIn some approaches, a side memory module can access several tokens simultaneously. This method is often instantiated in the form of a global memory algorithm as used in [Lee+19; Zah+20]. \nAnother approach is to connect different local blocks via recurrence. A flagship example of this approach is the class of Transformer-XL methods [Dai+19]. \n15.6.4 Low-rank and kernel methods \nIn this section, we discuss methods that approximate attention using low rank matrices. In [She+18; Kat+20] they approximate the attention matrix A directly by a low rank matrix, so that \nwhere $pmb { phi } ( pmb { x } ) in mathbb { R } ^ { M }$ is some finite-dimensional vector with $M < D$ . One can leverage this structure to compute AV in $O ( N )$ time. Unfortunately, for softmax attention, the A is not low rank. \nIn Linformer [Wan+20a], they instead transform the keys and values via random Gaussian projections. They then apply the theory of the Johnson-Lindenstrauss Transform [AL13] to approximate softmax attention in this lower dimensional space. \nIn Performer [Cho+20a; Cho+20b], they show that the attention matrix can be computed using a (positive definite) kernel function. We define kernel functions in Section 17.1, but the basic idea is that ${ cal K } ( { pmb q } , { pmb k } ) geq 0$ is some measure of similarity between $pmb q in mathbb { R } ^ { D }$ and $pmb { k } in mathbb { R } ^ { D }$ . For example, the Gaussian kernel, also called the radial basis function kernel, has the form \nTo see how this can be used to compute an attention matrix, note that [Cho+20a] show the following: \nThe first term in the above expression is equal to $K _ { mathrm { g a u s s } } ( pmb q _ { i } D ^ { - 1 / 4 }$ , $k _ { j } D ^ { - 1 / 4 } )$ with $sigma = 1$ , and the other two terms are just independent scaling factors. \nSo far we have not gained anything computationally. However, we will show in Section 17.2.9.3 that the Gaussian kernel can be written as the expectation of a set of random features: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $pmb { eta } ( pmb { x } ) in mathbb { R } ^ { M }$ is a random feature vector derived from $_ { x }$ , either based on trigonometric functions Equation (17.60) or exponential functions Equation (17.61). (The latter has the advantage that all the features are positive, which gives much better results [Cho+20b].) Therefore for the regular softmax attention, $A _ { i , j }$ can be rewritten as",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Efficient transformers *",
        "subsubsection": "Memory and recurrence methods"
    },
    {
        "content": "15.6.3 Memory and recurrence methods \nIn some approaches, a side memory module can access several tokens simultaneously. This method is often instantiated in the form of a global memory algorithm as used in [Lee+19; Zah+20]. \nAnother approach is to connect different local blocks via recurrence. A flagship example of this approach is the class of Transformer-XL methods [Dai+19]. \n15.6.4 Low-rank and kernel methods \nIn this section, we discuss methods that approximate attention using low rank matrices. In [She+18; Kat+20] they approximate the attention matrix A directly by a low rank matrix, so that \nwhere $pmb { phi } ( pmb { x } ) in mathbb { R } ^ { M }$ is some finite-dimensional vector with $M < D$ . One can leverage this structure to compute AV in $O ( N )$ time. Unfortunately, for softmax attention, the A is not low rank. \nIn Linformer [Wan+20a], they instead transform the keys and values via random Gaussian projections. They then apply the theory of the Johnson-Lindenstrauss Transform [AL13] to approximate softmax attention in this lower dimensional space. \nIn Performer [Cho+20a; Cho+20b], they show that the attention matrix can be computed using a (positive definite) kernel function. We define kernel functions in Section 17.1, but the basic idea is that ${ cal K } ( { pmb q } , { pmb k } ) geq 0$ is some measure of similarity between $pmb q in mathbb { R } ^ { D }$ and $pmb { k } in mathbb { R } ^ { D }$ . For example, the Gaussian kernel, also called the radial basis function kernel, has the form \nTo see how this can be used to compute an attention matrix, note that [Cho+20a] show the following: \nThe first term in the above expression is equal to $K _ { mathrm { g a u s s } } ( pmb q _ { i } D ^ { - 1 / 4 }$ , $k _ { j } D ^ { - 1 / 4 } )$ with $sigma = 1$ , and the other two terms are just independent scaling factors. \nSo far we have not gained anything computationally. However, we will show in Section 17.2.9.3 that the Gaussian kernel can be written as the expectation of a set of random features: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $pmb { eta } ( pmb { x } ) in mathbb { R } ^ { M }$ is a random feature vector derived from $_ { x }$ , either based on trigonometric functions Equation (17.60) or exponential functions Equation (17.61). (The latter has the advantage that all the features are positive, which gives much better results [Cho+20b].) Therefore for the regular softmax attention, $A _ { i , j }$ can be rewritten as \n\nwhere $phi$ is defined as: \nWe can write the full attention matrix as follows \nwhere $mathbf { Q } ^ { prime } , mathbf { K } ^ { prime } in mathbb { R } ^ { N times M }$ have rows encoding random feature maps corresponding to the queries and keys. (Note that we can get better performance if we ensure these random features are orthogonal, see [Cho+20a] for the details.) See Figure 15.30 for an illustration. \nWe can create an approximation to $mathbf { A }$ by using a single sample of the random features $phi ( { pmb q } _ { i } )$ and $phi ( k _ { j } )$ , and using a small value of $M$ , say $M = O ( D log ( D ) )$ . We can then approximate the entire attention operator in $O ( N )$ time using \nThis can be shown to be an unbiased approximation to the exact softmax attention operator. See Figure 15.31 for an illustration. (For details on how to generalize this to masked (causal) attention, see [Cho+20a].) \n15.7 Language models and unsupervised representation learning \nWe have discussed how RNNs and autoregressive (decoder-only) transformers can be used as language models, which are generative sequence models of the form $begin{array} { r } { p ( x _ { 1 } , . . . , x _ { T } ) = prod _ { t = 1 } ^ { T } p ( x _ { t } | pmb { x } _ { 1 : t - 1 } ) } end{array}$ , where \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 each $x _ { t }$ is a discrete token, such as a word or wordpiece. (See Section 1.5.4 for a discussion of text preprocessing methods.) The latent state of these models can then be used as a continuous vector representation of the text. That is, instead of using the one-hot vector $scriptstyle { mathbf { x } } _ { t }$ , or a learned embedding of it (such as those discussed in Section 20.5), we use the hidden state $mathbf { } h _ { t }$ , which depends on all the previous words in the sentence. These vectors can then be used as contextual word embeddings, for purposes such as text classification or seq2seq tasks (see e.g. [LKB20] for a review). The advantage of this approach is that we can pre-train the language model in an unsupervised way, on a large corpus of text, and then we can fine-tune the model in a supervised way on a small labeled task-specific dataset. (This general approach is called transfer learning, see Section 19.2 for details.)",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Efficient transformers *",
        "subsubsection": "Low-rank and kernel methods"
    },
    {
        "content": "If our primary goal is to compute useful representations for transfer learning, as opposed to generating text, we can replace the generative sequence model with non-causal models that can compute a representation of a sentence, but cannot generate it. These models have the advantage that now the hidden state $mathbf { } h _ { t }$ can depend on the past, $pmb { y } _ { 1 : t - 1 }$ , present ${ mathbf { } } _ { mathbf { } } mathbf { mathbf { } } _ { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } } mathbf { mathbf { } mathbf { } } mathbf { mathbf { } mathbf { } } mathbf { mathbf { } mathbf { } } mathbf { Xi } mathbf { mathbf { } } mathbf { Xi } mathbf { mathbf { } mathbf { } } mathbf { Xi } mathbf { mathbf { } Xi } mathbf { Xi } mathbf { Xi } mathbf { Xi } mathbf { Xi } mathbf { Xi } mathbf { Xi } mathbf { Xi } mathbf { Xi }$ , and future, $mathbf { mathscr { y } } _ { t + 1 : T }$ . This can sometimes result in better representations, since it takes into account more context. \nIn the sections below, we briefly discuss some unsupervised models for representation learning on text, using both causal and non-causal models. \n15.7.1 ELMo \nIn [Pet+18], they present a method called ELMo, which is short for “Embeddings from Language Model”. The basic idea is to fit two RNN language models, one left-to-right, and one right-to-left, and then to combine their hidden state representations to come up with an embedding for each word. Unlike a biRNN (Section 15.2.2), which needs an input-output pair, ELMo is trained in an unsupervised way, to minimize the negative log likelihood of the input sentence $pmb { x } _ { 1 : T }$ : \nwhere $pmb { theta } _ { e }$ are the shared parameters of the embedding layer, $pmb { theta } _ { s }$ are the shared parameters of the softmax output layer, and $theta ^ {  }$ and $mathbf { nabla } theta ^ {  }$ are the parameters of the two RNN models. (They use LSTM \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nU 0 Θ Softmax 中串电串 S'o S Θ Embedding \nRNNs, described in Section 15.2.7.2.) See Figure 15.32 for an illustration. \nAfter training, we define the contextual representation ${ pmb r } _ { t } = [ e _ { t } , h _ { t , 1 : L } ^ { right. } , h _ { t , 1 : L } ^ { left. } ]$ , where $L$ is the number of layers in the LSTM. We then learn a task-specific set of linear weights to map this to the final context-specific embedding of each token: $boldsymbol { r } _ { t } ^ { j } = boldsymbol { r } _ { t } ^ { intercal } boldsymbol { w } ^ { j }$ , where $j$ is the task id. If we are performing a syntactic task like part-of-speech (POS) tagging (i.e., labeling each word as a noun, verb, adjective, etc), then the task will learn to put more weight on lower layers. If we are performing a semantic task like word sense disambiguation (WSD), then the task will learn to put more weight on higher layers. In both cases, we only need a small amount of task-specific labeled data, since we are just learning a single weight vector, to map from $r _ { 1 : T }$ to the target labels $mathbf { pmb { y } } _ { 1 : T }$ . \n15.7.2 BERT \nIn this section, we describe the BERT model (Bidirectional Encoder Representations from Transformers) of [Dev+19]. Like ELMo, this is a non-causal model, that can be used to create representations of text, but not to generate text. In particular, it uses a transformer model to map a modified version of a sequence back to the unmodified form. The modified input at location $t$ omits all words except for the $t$ ’th, and the task is to predict the missing word. This is called the fill-in-the-blank or cloze task. \n15.7.2.1 Masked language model task \nMore precisely, the model is trained to minimize the negative log pseudo-likelihood: \nwhere $_ { m }$ is a random binary mask. For example, if we train the model on transcripts from cooking videos, we might create a training sentence of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Language models and unsupervised representation learning",
        "subsubsection": "ELMo"
    },
    {
        "content": "U 0 Θ Softmax 中串电串 S'o S Θ Embedding \nRNNs, described in Section 15.2.7.2.) See Figure 15.32 for an illustration. \nAfter training, we define the contextual representation ${ pmb r } _ { t } = [ e _ { t } , h _ { t , 1 : L } ^ { right. } , h _ { t , 1 : L } ^ { left. } ]$ , where $L$ is the number of layers in the LSTM. We then learn a task-specific set of linear weights to map this to the final context-specific embedding of each token: $boldsymbol { r } _ { t } ^ { j } = boldsymbol { r } _ { t } ^ { intercal } boldsymbol { w } ^ { j }$ , where $j$ is the task id. If we are performing a syntactic task like part-of-speech (POS) tagging (i.e., labeling each word as a noun, verb, adjective, etc), then the task will learn to put more weight on lower layers. If we are performing a semantic task like word sense disambiguation (WSD), then the task will learn to put more weight on higher layers. In both cases, we only need a small amount of task-specific labeled data, since we are just learning a single weight vector, to map from $r _ { 1 : T }$ to the target labels $mathbf { pmb { y } } _ { 1 : T }$ . \n15.7.2 BERT \nIn this section, we describe the BERT model (Bidirectional Encoder Representations from Transformers) of [Dev+19]. Like ELMo, this is a non-causal model, that can be used to create representations of text, but not to generate text. In particular, it uses a transformer model to map a modified version of a sequence back to the unmodified form. The modified input at location $t$ omits all words except for the $t$ ’th, and the task is to predict the missing word. This is called the fill-in-the-blank or cloze task. \n15.7.2.1 Masked language model task \nMore precisely, the model is trained to minimize the negative log pseudo-likelihood: \nwhere $_ { m }$ is a random binary mask. For example, if we train the model on transcripts from cooking videos, we might create a training sentence of the form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nLet’s make [MASK] chicken! [SEP] It [MASK] great with orange sauce. \nwhere [SEP] is a separator token inserted between two sentences. The desired target labels for the masked words are “some” and “tastes”. (This example is from [Sun+19a].) \nThe conditional probability is given by applying a softmax to the final layer hidden vector at location $i$ : \nwhere $hat { pmb x } = { pmb x } _ { - m }$ is the masked input sentence, and $e ( x )$ is the embedding for token $x$ . This is used to compute the loss at the masked locations; this is therefore called a masked language model. (This is similar to a denoising autoencoder, Section 20.3.2). See Figure 15.33a for an illustration of the model. \n15.7.2.2 Next sentence prediction task \nIn addition to the masked language model objective, the original BERT paper added an additional objective, in which the model is trained to classify if one sentence follows another. More precisely, the model is fed as input \nCLS $A _ { 1 }$ $A _ { 2 }$ ; . . . $A _ { m }$ ; SEP $B _ { 1 }  B _ { 2 } ; ldots ; B _ { n }$ SEP \nwhere SEP is a special separator token, and CLS is a special token marking the class. If sentence B follows A in the original text, we set the target label to $y = 1$ , but if B is a randomly chosen sentence, we set the target label to $y = 0$ . This is called the next sentence prediction task. This kind of pre-training can be useful for sentence-pair classification tasks, such as textual entailment or textual similarity, which we discussed in Section 15.4.6. (Note that this kind of pre-training is considered unsupervised, or self-supervised, since the target labels are automatically generated.) \nWhen performing next sentence prediction, the input to the model is specified using 3 different embeddings: one per token, one for each segment label (sentence A or B), and one per location \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license (using a learned positional embedding). These are then added. See Figure 15.34 for an illustration. BERT then uses a transformer encoder to learn a mapping from this input embedding sequence to an output embedding sequence, which gets decoded into word labels (for the masked locations) or a class label (for the CLS location). \n\n15.7.2.3 Fine-tuning BERT for NLP applications \nAfter pre-training BERT in an unsupervised way, we can use it for various downtream tasks by performing supervised fine-tuning. (See Section 19.2 for more background on such transfer learning methods.) Figure 15.35 illustrates how we can modify a BERT model to perform different tasks, by simply adding one or more new output heads to the final hidden layer. See bert_jax.ipynb for some sample code. \nIn Figure 15.35(a), we show how we can tackle single sentence classification (e.g., sentiment analysis): we simply take the feature vector associated with the dummy CLS token and feed it into an MLP. Since each output attends to all inputs, this hidden vector will summarize the entire sentence. The MLP then learns to map this to the desired label space. \nIn Figure 15.35(b), we show how we can tackle sentence-pair classification (e.g., textual entailment, as discussed in Section 15.4.6): we just feed in the two input sentences, formatted as in Equation (15.73), and then classify the CLS token. \nIn Figure 15.35(c), we show how we can tackle single sentence tagging, in which we associate a label or tag with each word, instead of just the entire sentence. A common application of this is part of speech tagging, in which we annotate each words a noun, verb, adjective, etc. Another application of this is noun phrase chunking, also called shallow parsing, in which we must annotate the span of each noun phrase. The span is encoded using the BIO notation, in which B is the beginning of an entity, I-x is for inside, and O is for outside any entity. For example, consider the following sentence: \nB I O O O B I O B I I \nBritish Airways rose after announcing its withdrawl from the UAI deal \nWe see that there are 3 noun phrases, “British Airways”, “its withdrawl” and “the UAI deal”. (We require that the B, I and O labels occur in order, so this a prior constraint that can be included in the model.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe can also associate types with each noun phrase, for example distinguishing person, location, organization, and other. Thus the label space becomes {B-Per, I-Per, B-Loc, I-Loc, B-Org, I-Org, Outside $}$ . This is called named entity recognition, and is a key step in information extraction. For example, consider the following sentence: \nBP IP O O O BL IL BP O O O O Mrs Green spoke today in New York. Green chairs the finance committee. \nFrom this, we infer that the first sentence has two named entities, namely “Mrs Green” (of type Person) and “New York” (of type Location). The second sentence mentions another person, “Green”, that most likely is the same as the first person, although this across-sentence entity resolution is not part of the basic NER task. \nFinally, in Figure 15.35(d), we show how we can tackle question answering. Here the first input sentence is the question, the second is the background text, and the output is required to specifying the start and end locations of the relevant part of the background that contains the answer (see Table 1.4). The start location $s$ and end location $e$ are computed by applying 2 different MLPs to a pooled version of the output encodings for the background text; the output of the MLPs is a softmax over all locations. At test time, we can extract the span $( i , j )$ which maximizes the sum of scores $s _ { i } + e _ { j }$ for $i leq j$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nBERT achieves state-of-the-art performance on many NLP tasks. Interestingly, [TDP19] shows that BERT implicitly rediscovers the standard NLP pipeline, in which different layers perform tasks such as part of speech (POS) tagging, parsing, named entity relationship (NER) detection, semantic role labeling (SRL), coreference resolution, etc. More details on NLP can be found in [JM20]. \n15.7.3 GPT \nIn [Rad+18], they propose a model called GPT, which is short for “Generative Pre-training Transformer”. This is a causal (generative) model, that uses a masked transformer as the decoder. See Figure 15.33b for an illustration. \nIn the original GPT paper, they jointly optimize on a large unlabeled dataset, and a small labeled dataset. In the classification setting, the loss is given by $mathcal { L } = mathcal { L } _ { mathrm { c l s } } + lambda mathcal { L } _ { mathrm { L M } }$ , where ${ mathcal { L } } _ { mathrm { c l s } } =$ $- sum _ { ( pmb { x } , y ) in mathcal { D } _ { L } } log p ( y | pmb { x } )$ is the classification loss on the labeled data, and $begin{array} { r } { mathcal { L } _ { mathrm { L M } } = - sum _ { mathbf { boldsymbol { x } } in mathcal { D } _ { U } } sum _ { t } p ( x _ { t } | mathbf { boldsymbol { x } } _ { 1 : t - 1 } ) } end{array}$ is the language modeling loss on the unlabeled data. \nIn [Rad+19], they propose GPT-2, which is a larger version of GPT, trained on a large web corpus called WebText. They also eliminate any task-specific training, and instead just train it as a language model. More recently, OpenAI released GPT-3 [Bro+20], which is an even larger version of GPT-2, but based on the same principles. An open-source version of the model is available at https://huggingface.co/EleutherAI, which was trained on an 800GB English-language web corpus called “The Pile” [Gao+20]. \n15.7.3.1 Applications of GPT \nGPT can generate text given an initial input prompt. The prompt can specify a task; if the generated response fulfills the task “out of the box”, we say the model is performing zero-shot task transfer (see Section 19.6 for details). \nFor example, to perform abstractive summarization of some input text $pmb { x } _ { 1 : T }$ (as opposed to extractive summarization, which just selects a subset of the input words), we sample from $p ( { pmb x } _ { T + 1 : T + 1 0 0 } | [ { pmb x } _ { 1 : T } ; mathrm { T L } ; mathrm { D R } ] ,$ , where TL;DR is a special token added to the end of the input text, which tells the system the user wants a summary. TL;DR stands for “too long; didn’t read” and frequently occurs in webtext followed by a human-created summary. By adding this token to the input, the user hopes to “trigger” the transformer decoder into a state in which it enters summarization mode. (A better way to tell the model what task to perform is to train it on input-output pairs, as discussed in Section 15.7.4.) \n15.7.4 T5 \nMany models are trained in an unsupervised way, and then fine-tuned on specific tasks. It is also possible to train a single model to perform multiple tasks, by telling the system what task to perform as part of the input sentence, and then training it as a seq2seq model, as illustrated in Figure 15.36. This is the approach used in T5 [Raf+20], which stands for “Text-to-text Transfer Transformer”. The model is a standard seq2seq transformer, that is pretrained on unsupervised $( { pmb x } ^ { prime } , { pmb x } ^ { prime prime } )$ pairs, where $mathbf { { x } ^ { prime } }$ is a masked version of $_ { x }$ and ${ pmb x } ^ { prime prime }$ are the missing tokens that need to be predicted, and then fine-tuned on multiple supervised $( { pmb x } , { pmb y } )$ pairs. \nThe unsupervised data comes from C4, or the “Colossal Clean Crawled Corpus”, a 750GB corpus of web text. This is used for pretraining using a BERT-like denoising objective. For example, the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \"translate English to German: That is good. \"cola sentence: The \"Das ist gut.\" course is jumping well. T5 \"not acceptable\" \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\" \"3.8\" \"summarize: state authorities \"six people hospitalized after dispatched emergency crews tuesday to a storm in attala county.\" survey the damage after an onslaught of severe weather in mississippi…\"",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Language models and unsupervised representation learning",
        "subsubsection": "BERT"
    },
    {
        "content": "BERT achieves state-of-the-art performance on many NLP tasks. Interestingly, [TDP19] shows that BERT implicitly rediscovers the standard NLP pipeline, in which different layers perform tasks such as part of speech (POS) tagging, parsing, named entity relationship (NER) detection, semantic role labeling (SRL), coreference resolution, etc. More details on NLP can be found in [JM20]. \n15.7.3 GPT \nIn [Rad+18], they propose a model called GPT, which is short for “Generative Pre-training Transformer”. This is a causal (generative) model, that uses a masked transformer as the decoder. See Figure 15.33b for an illustration. \nIn the original GPT paper, they jointly optimize on a large unlabeled dataset, and a small labeled dataset. In the classification setting, the loss is given by $mathcal { L } = mathcal { L } _ { mathrm { c l s } } + lambda mathcal { L } _ { mathrm { L M } }$ , where ${ mathcal { L } } _ { mathrm { c l s } } =$ $- sum _ { ( pmb { x } , y ) in mathcal { D } _ { L } } log p ( y | pmb { x } )$ is the classification loss on the labeled data, and $begin{array} { r } { mathcal { L } _ { mathrm { L M } } = - sum _ { mathbf { boldsymbol { x } } in mathcal { D } _ { U } } sum _ { t } p ( x _ { t } | mathbf { boldsymbol { x } } _ { 1 : t - 1 } ) } end{array}$ is the language modeling loss on the unlabeled data. \nIn [Rad+19], they propose GPT-2, which is a larger version of GPT, trained on a large web corpus called WebText. They also eliminate any task-specific training, and instead just train it as a language model. More recently, OpenAI released GPT-3 [Bro+20], which is an even larger version of GPT-2, but based on the same principles. An open-source version of the model is available at https://huggingface.co/EleutherAI, which was trained on an 800GB English-language web corpus called “The Pile” [Gao+20]. \n15.7.3.1 Applications of GPT \nGPT can generate text given an initial input prompt. The prompt can specify a task; if the generated response fulfills the task “out of the box”, we say the model is performing zero-shot task transfer (see Section 19.6 for details). \nFor example, to perform abstractive summarization of some input text $pmb { x } _ { 1 : T }$ (as opposed to extractive summarization, which just selects a subset of the input words), we sample from $p ( { pmb x } _ { T + 1 : T + 1 0 0 } | [ { pmb x } _ { 1 : T } ; mathrm { T L } ; mathrm { D R } ] ,$ , where TL;DR is a special token added to the end of the input text, which tells the system the user wants a summary. TL;DR stands for “too long; didn’t read” and frequently occurs in webtext followed by a human-created summary. By adding this token to the input, the user hopes to “trigger” the transformer decoder into a state in which it enters summarization mode. (A better way to tell the model what task to perform is to train it on input-output pairs, as discussed in Section 15.7.4.) \n15.7.4 T5 \nMany models are trained in an unsupervised way, and then fine-tuned on specific tasks. It is also possible to train a single model to perform multiple tasks, by telling the system what task to perform as part of the input sentence, and then training it as a seq2seq model, as illustrated in Figure 15.36. This is the approach used in T5 [Raf+20], which stands for “Text-to-text Transfer Transformer”. The model is a standard seq2seq transformer, that is pretrained on unsupervised $( { pmb x } ^ { prime } , { pmb x } ^ { prime prime } )$ pairs, where $mathbf { { x } ^ { prime } }$ is a masked version of $_ { x }$ and ${ pmb x } ^ { prime prime }$ are the missing tokens that need to be predicted, and then fine-tuned on multiple supervised $( { pmb x } , { pmb y } )$ pairs. \nThe unsupervised data comes from C4, or the “Colossal Clean Crawled Corpus”, a 750GB corpus of web text. This is used for pretraining using a BERT-like denoising objective. For example, the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \"translate English to German: That is good. \"cola sentence: The \"Das ist gut.\" course is jumping well. T5 \"not acceptable\" \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\" \"3.8\" \"summarize: state authorities \"six people hospitalized after dispatched emergency crews tuesday to a storm in attala county.\" survey the damage after an onslaught of severe weather in mississippi…\"",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Language models and unsupervised representation learning",
        "subsubsection": "GPT"
    },
    {
        "content": "BERT achieves state-of-the-art performance on many NLP tasks. Interestingly, [TDP19] shows that BERT implicitly rediscovers the standard NLP pipeline, in which different layers perform tasks such as part of speech (POS) tagging, parsing, named entity relationship (NER) detection, semantic role labeling (SRL), coreference resolution, etc. More details on NLP can be found in [JM20]. \n15.7.3 GPT \nIn [Rad+18], they propose a model called GPT, which is short for “Generative Pre-training Transformer”. This is a causal (generative) model, that uses a masked transformer as the decoder. See Figure 15.33b for an illustration. \nIn the original GPT paper, they jointly optimize on a large unlabeled dataset, and a small labeled dataset. In the classification setting, the loss is given by $mathcal { L } = mathcal { L } _ { mathrm { c l s } } + lambda mathcal { L } _ { mathrm { L M } }$ , where ${ mathcal { L } } _ { mathrm { c l s } } =$ $- sum _ { ( pmb { x } , y ) in mathcal { D } _ { L } } log p ( y | pmb { x } )$ is the classification loss on the labeled data, and $begin{array} { r } { mathcal { L } _ { mathrm { L M } } = - sum _ { mathbf { boldsymbol { x } } in mathcal { D } _ { U } } sum _ { t } p ( x _ { t } | mathbf { boldsymbol { x } } _ { 1 : t - 1 } ) } end{array}$ is the language modeling loss on the unlabeled data. \nIn [Rad+19], they propose GPT-2, which is a larger version of GPT, trained on a large web corpus called WebText. They also eliminate any task-specific training, and instead just train it as a language model. More recently, OpenAI released GPT-3 [Bro+20], which is an even larger version of GPT-2, but based on the same principles. An open-source version of the model is available at https://huggingface.co/EleutherAI, which was trained on an 800GB English-language web corpus called “The Pile” [Gao+20]. \n15.7.3.1 Applications of GPT \nGPT can generate text given an initial input prompt. The prompt can specify a task; if the generated response fulfills the task “out of the box”, we say the model is performing zero-shot task transfer (see Section 19.6 for details). \nFor example, to perform abstractive summarization of some input text $pmb { x } _ { 1 : T }$ (as opposed to extractive summarization, which just selects a subset of the input words), we sample from $p ( { pmb x } _ { T + 1 : T + 1 0 0 } | [ { pmb x } _ { 1 : T } ; mathrm { T L } ; mathrm { D R } ] ,$ , where TL;DR is a special token added to the end of the input text, which tells the system the user wants a summary. TL;DR stands for “too long; didn’t read” and frequently occurs in webtext followed by a human-created summary. By adding this token to the input, the user hopes to “trigger” the transformer decoder into a state in which it enters summarization mode. (A better way to tell the model what task to perform is to train it on input-output pairs, as discussed in Section 15.7.4.) \n15.7.4 T5 \nMany models are trained in an unsupervised way, and then fine-tuned on specific tasks. It is also possible to train a single model to perform multiple tasks, by telling the system what task to perform as part of the input sentence, and then training it as a seq2seq model, as illustrated in Figure 15.36. This is the approach used in T5 [Raf+20], which stands for “Text-to-text Transfer Transformer”. The model is a standard seq2seq transformer, that is pretrained on unsupervised $( { pmb x } ^ { prime } , { pmb x } ^ { prime prime } )$ pairs, where $mathbf { { x } ^ { prime } }$ is a masked version of $_ { x }$ and ${ pmb x } ^ { prime prime }$ are the missing tokens that need to be predicted, and then fine-tuned on multiple supervised $( { pmb x } , { pmb y } )$ pairs. \nThe unsupervised data comes from C4, or the “Colossal Clean Crawled Corpus”, a 750GB corpus of web text. This is used for pretraining using a BERT-like denoising objective. For example, the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \"translate English to German: That is good. \"cola sentence: The \"Das ist gut.\" course is jumping well. T5 \"not acceptable\" \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\" \"3.8\" \"summarize: state authorities \"six people hospitalized after dispatched emergency crews tuesday to a storm in attala county.\" survey the damage after an onslaught of severe weather in mississippi…\" \n\nsentence ${ bf mathcal { x } } =$ “Thank you for inviting me to your party last week” may get converted to the input $pmb { x } ^ { prime } =$ “Thank you $<$ <X $>$ me to your party $<$ <Y $>$ week” and the output (target) $pmb { x } ^ { prime prime } = mathbf { lambda } ^ { 6 } <$ <X $>$ for inviting $<$ <Y $>$ last $<$ <EOS $>$ ”, where $< X >$ and $< Y >$ are tokens that are unique to this example. The supervised datasets are manually created, and are taken from the literature. This approach is currently the state-of-the-art on many NLP tasks. \n15.7.5 Discussion \nGiant language models, such as BERT and GPT-3, have recently generated a lot of interest, and have even made their way into the mainstream media.7 However, there is some doubt about whether such systems “understand” language in any meaningful way, beyond just rearranging word patterns seen in their massive training sets. For example, [NK19] show that the ability of BERT to perform almost as well as humans on the Argument Reasoning Comprehension Task is “entirely accounted for by exploitation of spurious statistical cues in the dataset”. By slightly tweaking the dataset, performance can be reduced to chance levels. For other criticisms of such models, see e.g., [BK20; Mar20].",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Language models and unsupervised representation learning",
        "subsubsection": "T5"
    },
    {
        "content": "sentence ${ bf mathcal { x } } =$ “Thank you for inviting me to your party last week” may get converted to the input $pmb { x } ^ { prime } =$ “Thank you $<$ <X $>$ me to your party $<$ <Y $>$ week” and the output (target) $pmb { x } ^ { prime prime } = mathbf { lambda } ^ { 6 } <$ <X $>$ for inviting $<$ <Y $>$ last $<$ <EOS $>$ ”, where $< X >$ and $< Y >$ are tokens that are unique to this example. The supervised datasets are manually created, and are taken from the literature. This approach is currently the state-of-the-art on many NLP tasks. \n15.7.5 Discussion \nGiant language models, such as BERT and GPT-3, have recently generated a lot of interest, and have even made their way into the mainstream media.7 However, there is some doubt about whether such systems “understand” language in any meaningful way, beyond just rearranging word patterns seen in their massive training sets. For example, [NK19] show that the ability of BERT to perform almost as well as humans on the Argument Reasoning Comprehension Task is “entirely accounted for by exploitation of spurious statistical cues in the dataset”. By slightly tweaking the dataset, performance can be reduced to chance levels. For other criticisms of such models, see e.g., [BK20; Mar20]. \nPart IV \nNonparametric Models",
        "chapter": "III Deep Neural Networks",
        "section": "Neural Networks for Sequences",
        "subsection": "Language models and unsupervised representation learning",
        "subsubsection": "Discussion"
    },
    {
        "content": "16.1.1 Example \nWe illustrate the KNN classifier in 2d in Figure 16.1(a) for $K = 5$ . The test point is marked as an “x”. 3 of the 5 nearest neighbors have label 1, and 2 of the 5 have label 0. Hence we predict $p ( y = 1 | x , mathcal { D } ) = 3 / 5 = 0 . 6$ . \nIf we use $K = 1$ , we just return the label of the nearest neighbor, so the predictive distribution becomes a delta function. A KNN classifier with $K = 1$ induces a Voronoi tessellation of the points (see Figure 16.1(b)). This is a partition of space which associates a region $V ( pmb { x } _ { n } )$ with each point ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ in such a way that all points in $V ( pmb { x } _ { n } )$ are closer to ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ than to any other point. Within each cell, the predicted label is the label of the corresponding training point. Thus the training error will be 0 when $K = 1$ . However, such a model is usually overfitting the training set, as we show below. \nFigure 16.2 gives an example of KNN applied to a 2d dataset, in which we have three classes. We see how, with $K = 1$ , the method makes zero errors on the training set. As $K$ increases, the decision boundaries become smoother (since we are averaging over larger neighborhoods), so the training error increases, as we start to underfit. This is shown in Figure 16.2(d). The test error shows the usual U-shaped curve. \n16.1.2 The curse of dimensionality \nThe main statistical problem with KNN classifiers is that they do not work well with high dimensional inputs, due to the curse of dimensionality. \nThe basic problem is that the volume of space grows exponentially fast with dimension, so you might have to look quite far away in space to find your nearest neighbor. To make this more precise, consider this example from [HTF09, p22]. Suppose we apply a KNN classifier to data where the inputs are uniformly distributed in the $D$ -dimensional unit cube. Suppose we estimate the density of class labels around a test point $_ { x }$ by “growing” a hyper-cube around $_ { x }$ until it contains a desired fraction $p$ of the data points. The expected edge length of this cube will be $e _ { D } ( s ) triangleq p ^ { 1 / D }$ ; this function is plotted in Figure 16.3(b). If $D = 1 0$ , and we want to base our estimate on $1 0 %$ of the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "K nearest neighbor (KNN) classification",
        "subsubsection": "Example"
    },
    {
        "content": "16.1.1 Example \nWe illustrate the KNN classifier in 2d in Figure 16.1(a) for $K = 5$ . The test point is marked as an “x”. 3 of the 5 nearest neighbors have label 1, and 2 of the 5 have label 0. Hence we predict $p ( y = 1 | x , mathcal { D } ) = 3 / 5 = 0 . 6$ . \nIf we use $K = 1$ , we just return the label of the nearest neighbor, so the predictive distribution becomes a delta function. A KNN classifier with $K = 1$ induces a Voronoi tessellation of the points (see Figure 16.1(b)). This is a partition of space which associates a region $V ( pmb { x } _ { n } )$ with each point ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ in such a way that all points in $V ( pmb { x } _ { n } )$ are closer to ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ than to any other point. Within each cell, the predicted label is the label of the corresponding training point. Thus the training error will be 0 when $K = 1$ . However, such a model is usually overfitting the training set, as we show below. \nFigure 16.2 gives an example of KNN applied to a 2d dataset, in which we have three classes. We see how, with $K = 1$ , the method makes zero errors on the training set. As $K$ increases, the decision boundaries become smoother (since we are averaging over larger neighborhoods), so the training error increases, as we start to underfit. This is shown in Figure 16.2(d). The test error shows the usual U-shaped curve. \n16.1.2 The curse of dimensionality \nThe main statistical problem with KNN classifiers is that they do not work well with high dimensional inputs, due to the curse of dimensionality. \nThe basic problem is that the volume of space grows exponentially fast with dimension, so you might have to look quite far away in space to find your nearest neighbor. To make this more precise, consider this example from [HTF09, p22]. Suppose we apply a KNN classifier to data where the inputs are uniformly distributed in the $D$ -dimensional unit cube. Suppose we estimate the density of class labels around a test point $_ { x }$ by “growing” a hyper-cube around $_ { x }$ until it contains a desired fraction $p$ of the data points. The expected edge length of this cube will be $e _ { D } ( s ) triangleq p ^ { 1 / D }$ ; this function is plotted in Figure 16.3(b). If $D = 1 0$ , and we want to base our estimate on $1 0 %$ of the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license data, we have $e _ { 1 0 } ( 0 . 1 ) = 0 . 8$ , so we need to extend the cube 80% along each dimension around $_ { x }$ . Even if we only use $1 %$ of the data, we find $e _ { 1 0 } ( 0 . 0 1 ) = 0 . 6 3$ . Since the range of the data is only 0 to 1 along each dimension, we see that the method is no longer very local, despite the name “nearest neighbor”. The trouble with looking at neighbors that are so far away is that they may not be good predictors about the behavior of the function at a given point. \n\nThere are two main solutions to the curse: make some assumptions about the form of the function (i.e., use a parametric model), and/or use a metric that only cares about a subset of the dimensions (see Section 16.2). \n16.1.3 Reducing the speed and memory requirements \nKNN classifiers store all the training data. This is obviously very wasteful of space. Various heuristic pruning techniques have been proposed to remove points that do not affect the decision boundaries, see e.g., [WM00]. In Section 17.4, we discuss a more principled approach based on a sparsity promoting prior; the resulting method is called a sparse kernel machine, and only keeps a subset of the most useful exemplars. \nIn terms of running time, the challenge is to find the $K$ nearest neighbors in less than $O ( N )$ time, where $N$ is the size of the training set. Finding exact nearest neighbors is computationally intractable when the dimensionality of the space goes above about 10 dimensions, so most methods focus on finding the approximate nearest neighbors. There are two main classes of techniques, based on partitioning space into regions, or using hashing. \nFor partitioning methods, one can either use some kind of k-d tree, which divides space into axis-parallel regions, or some kind of clustering method, which uses anchor points. For hashing methods, locality sensitive hashing (LSH) [GIM99] is widely used, although more recent methods learn the hashing function from data (see e.g., [Wan+15]). See [LRU14] for a good introduction to hashing methods. \nAn open-source library called FAISS, for efficient exact and approximate nearest neighbor search (and K-means clustering) of dense vectors, is available at https://github.com/facebookresearch/ faiss, and described in [JDJ17]. \n16.1.4 Open set recognition \nAsk not what this is called, ask what this is like. — Moshe Bar.[Bar09] \nIn all of the classification problems we have considered so far, we have assumed that the set of classes $boldsymbol { mathcal { C } }$ is fixed. (This is an example of the closed world assumption, which assumes there is a fixed number of (types of) things.) However, many real world problems involve test samples that come from new categories. This is called open set recognition, as we discuss below. \n16.1.4.1 Online learning, OOD detection and open set recognition \nFor example, suppose we train a face recognition system to predict the identity of a person from a fixed set or gallery of face images. Let $mathcal { D } _ { t } = { ( boldsymbol { x } _ { n } , y _ { n } ) : boldsymbol { x } _ { n } in mathcal { X } , y _ { n } in mathcal { C } _ { t } , n = 1 : N _ { t } }$ be the labeled dataset at time $t$ , where $mathcal { X }$ is the set of (face) images, and $mathcal { C } _ { t } = { 1 , ldots , C _ { t } }$ is the set of people known to the system at time $t$ (where $C _ { t } leq t$ ). At test time, the system may encounter a new person that it has not seen before. Let $boldsymbol { x } _ { t + 1 }$ be this new image, and $y _ { t + 1 } = C _ { t + 1 }$ be its new label. The system \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 needs to recognize that the input is from a new category, and not accidentally classify it with a label from $scriptstyle { mathcal { C } } _ { t }$ . This is called novelty detection. In this case, the input is being generated from the distribution $p ( pmb { x } | y = C _ { t + 1 } )$ , where $C _ { t + 1 } notin mathcal { C } _ { t }$ is the new “class label”. Detecting that $boldsymbol { x } _ { t + 1 }$ is from a novel class may be hard if the appearance of this new image is similar to the appearance of any of the existing images in $mathcal { D } _ { t }$ .",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "K nearest neighbor (KNN) classification",
        "subsubsection": "The curse of dimensionality"
    },
    {
        "content": "There are two main solutions to the curse: make some assumptions about the form of the function (i.e., use a parametric model), and/or use a metric that only cares about a subset of the dimensions (see Section 16.2). \n16.1.3 Reducing the speed and memory requirements \nKNN classifiers store all the training data. This is obviously very wasteful of space. Various heuristic pruning techniques have been proposed to remove points that do not affect the decision boundaries, see e.g., [WM00]. In Section 17.4, we discuss a more principled approach based on a sparsity promoting prior; the resulting method is called a sparse kernel machine, and only keeps a subset of the most useful exemplars. \nIn terms of running time, the challenge is to find the $K$ nearest neighbors in less than $O ( N )$ time, where $N$ is the size of the training set. Finding exact nearest neighbors is computationally intractable when the dimensionality of the space goes above about 10 dimensions, so most methods focus on finding the approximate nearest neighbors. There are two main classes of techniques, based on partitioning space into regions, or using hashing. \nFor partitioning methods, one can either use some kind of k-d tree, which divides space into axis-parallel regions, or some kind of clustering method, which uses anchor points. For hashing methods, locality sensitive hashing (LSH) [GIM99] is widely used, although more recent methods learn the hashing function from data (see e.g., [Wan+15]). See [LRU14] for a good introduction to hashing methods. \nAn open-source library called FAISS, for efficient exact and approximate nearest neighbor search (and K-means clustering) of dense vectors, is available at https://github.com/facebookresearch/ faiss, and described in [JDJ17]. \n16.1.4 Open set recognition \nAsk not what this is called, ask what this is like. — Moshe Bar.[Bar09] \nIn all of the classification problems we have considered so far, we have assumed that the set of classes $boldsymbol { mathcal { C } }$ is fixed. (This is an example of the closed world assumption, which assumes there is a fixed number of (types of) things.) However, many real world problems involve test samples that come from new categories. This is called open set recognition, as we discuss below. \n16.1.4.1 Online learning, OOD detection and open set recognition \nFor example, suppose we train a face recognition system to predict the identity of a person from a fixed set or gallery of face images. Let $mathcal { D } _ { t } = { ( boldsymbol { x } _ { n } , y _ { n } ) : boldsymbol { x } _ { n } in mathcal { X } , y _ { n } in mathcal { C } _ { t } , n = 1 : N _ { t } }$ be the labeled dataset at time $t$ , where $mathcal { X }$ is the set of (face) images, and $mathcal { C } _ { t } = { 1 , ldots , C _ { t } }$ is the set of people known to the system at time $t$ (where $C _ { t } leq t$ ). At test time, the system may encounter a new person that it has not seen before. Let $boldsymbol { x } _ { t + 1 }$ be this new image, and $y _ { t + 1 } = C _ { t + 1 }$ be its new label. The system \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 needs to recognize that the input is from a new category, and not accidentally classify it with a label from $scriptstyle { mathcal { C } } _ { t }$ . This is called novelty detection. In this case, the input is being generated from the distribution $p ( pmb { x } | y = C _ { t + 1 } )$ , where $C _ { t + 1 } notin mathcal { C } _ { t }$ is the new “class label”. Detecting that $boldsymbol { x } _ { t + 1 }$ is from a novel class may be hard if the appearance of this new image is similar to the appearance of any of the existing images in $mathcal { D } _ { t }$ .",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "K nearest neighbor (KNN) classification",
        "subsubsection": "Reducing the speed and memory requirements"
    },
    {
        "content": "There are two main solutions to the curse: make some assumptions about the form of the function (i.e., use a parametric model), and/or use a metric that only cares about a subset of the dimensions (see Section 16.2). \n16.1.3 Reducing the speed and memory requirements \nKNN classifiers store all the training data. This is obviously very wasteful of space. Various heuristic pruning techniques have been proposed to remove points that do not affect the decision boundaries, see e.g., [WM00]. In Section 17.4, we discuss a more principled approach based on a sparsity promoting prior; the resulting method is called a sparse kernel machine, and only keeps a subset of the most useful exemplars. \nIn terms of running time, the challenge is to find the $K$ nearest neighbors in less than $O ( N )$ time, where $N$ is the size of the training set. Finding exact nearest neighbors is computationally intractable when the dimensionality of the space goes above about 10 dimensions, so most methods focus on finding the approximate nearest neighbors. There are two main classes of techniques, based on partitioning space into regions, or using hashing. \nFor partitioning methods, one can either use some kind of k-d tree, which divides space into axis-parallel regions, or some kind of clustering method, which uses anchor points. For hashing methods, locality sensitive hashing (LSH) [GIM99] is widely used, although more recent methods learn the hashing function from data (see e.g., [Wan+15]). See [LRU14] for a good introduction to hashing methods. \nAn open-source library called FAISS, for efficient exact and approximate nearest neighbor search (and K-means clustering) of dense vectors, is available at https://github.com/facebookresearch/ faiss, and described in [JDJ17]. \n16.1.4 Open set recognition \nAsk not what this is called, ask what this is like. — Moshe Bar.[Bar09] \nIn all of the classification problems we have considered so far, we have assumed that the set of classes $boldsymbol { mathcal { C } }$ is fixed. (This is an example of the closed world assumption, which assumes there is a fixed number of (types of) things.) However, many real world problems involve test samples that come from new categories. This is called open set recognition, as we discuss below. \n16.1.4.1 Online learning, OOD detection and open set recognition \nFor example, suppose we train a face recognition system to predict the identity of a person from a fixed set or gallery of face images. Let $mathcal { D } _ { t } = { ( boldsymbol { x } _ { n } , y _ { n } ) : boldsymbol { x } _ { n } in mathcal { X } , y _ { n } in mathcal { C } _ { t } , n = 1 : N _ { t } }$ be the labeled dataset at time $t$ , where $mathcal { X }$ is the set of (face) images, and $mathcal { C } _ { t } = { 1 , ldots , C _ { t } }$ is the set of people known to the system at time $t$ (where $C _ { t } leq t$ ). At test time, the system may encounter a new person that it has not seen before. Let $boldsymbol { x } _ { t + 1 }$ be this new image, and $y _ { t + 1 } = C _ { t + 1 }$ be its new label. The system \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 needs to recognize that the input is from a new category, and not accidentally classify it with a label from $scriptstyle { mathcal { C } } _ { t }$ . This is called novelty detection. In this case, the input is being generated from the distribution $p ( pmb { x } | y = C _ { t + 1 } )$ , where $C _ { t + 1 } notin mathcal { C } _ { t }$ is the new “class label”. Detecting that $boldsymbol { x } _ { t + 1 }$ is from a novel class may be hard if the appearance of this new image is similar to the appearance of any of the existing images in $mathcal { D } _ { t }$ . \n\nIf the system is successful at detecting that $boldsymbol { x } _ { t + 1 }$ is novel, then it may ask for the id of this new instance, call it $C _ { t + 1 }$ . It can then add the labeled pair $( pmb { x } _ { t + 1 } , C _ { t + 1 } )$ to the dataset to create $mathcal { D } _ { t + 1 }$ , and can grow the set of unique classes by adding $C _ { t + 1 }$ to $mathit { check { C } } _ { t }$ (c.f., [JK13]). This is called incremental learning, online learning, life-long learning, or continual learning. At future time points, the system may encounter an image sampled from $p ( { pmb x } | y = c )$ , where $c$ is an existing class, or where $c$ is a new class, or the image may be sampled from some entirely different kind of distribution $p ^ { prime } ( { pmb x } )$ unrelated to faces (e.g., someone uploads a photo of their dog). (Detecting this latter kind of event is called out-of-distribution or OOD detection.) \nIn this online setting, we often only get a few (sometimes just one) example of each class. Prediction in this setting is known as few-shot classification, and is discussed in more detail in Section 19.6. KNN classifiers are well-suited to this task. For example, we can just store all the instances of each class in a gallery of examples, as we explained above. At time $t + 1$ , when we get input $pmb { x } _ { t + 1 }$ , rather than predicting a label for $pmb { x } _ { t + 1 }$ by comparing it to some parametric model for each class, we just find the example in the gallery that is nearest (most similar) to $pmb { x } _ { t + 1 }$ , call it $mathbf { { x } ^ { prime } }$ . We then need to determine if $mathbf { { x } ^ { prime } }$ and are sufficiently similar to constitute a match. (In the context of person $boldsymbol { x } _ { t + 1 }$ classification, this is known as person re-identification or face verification, see e.g., [WSH16]).) If there is no match, we can declare the input to be novel or OOD. \nThe key ingredient for all of the above problems is the (dis)similarity metric between inputs. We discuss ways to learn this in Section 16.2. \n16.1.4.2 Other open world problems \nThe problem of open-set recognition, and incremental learning, are just examples of problems that require the open world assumption c.f., [Rus15]. There are many other examples of such problems. For example, consider the problem of entity resolution, called entity linking. In this problem, we need to determine if different strings (e.g., “John Smith” and “Jon Smith”) refer to the same entity or not. See e.g. [SHF15] for details. \nAnother important application is in multi-object tracking. For example, when a radar system detects a new “blip”, is it due to an existing missile that is being tracked, or is it a new objective that has entered the airspace? An elegant mathematical framework for dealing with such problems, known as random finite sets, is described in [Mah07; Mah13; Vo+15]. \n16.2 Learning distance metrics \nBeing able to compute the “semantic distance” between a pair of points, $d ( { pmb x } , { pmb x } ^ { prime } ) in mathbb { R } ^ { + }$ for $pmb { x } , pmb { x } ^ { prime } in mathcal { X }$ , or equivalently their similarity $s ( pmb { x } , pmb { x } ^ { prime } ) in mathbb { R } ^ { + }$ , is of crucial importance to tasks such as nearest neighbor classification (Section 16.1), self-supervised learning (Section 19.2.4.4), similarity-based clustering (Section 21.5), content-based retrieval, visual tracking, etc. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "K nearest neighbor (KNN) classification",
        "subsubsection": "Open set recognition"
    },
    {
        "content": "When the input space is $chi = mathbb { R } ^ { D }$ , the most common distance metric is the Mahalanobis distance \nWe discuss some methods to learn the matrix $mathbf { M }$ in Section 16.2.1. For high dimensional inputs, or structured inputs, it is better to first learn an embedding $e = f ( { pmb x } )$ , and then to compute distances in embedding space. When $f$ is a DNN, this is called deep metric learning; we discuss this in Section 16.2.2. \n16.2.1 Linear and convex methods \nIn this section, we discuss some methods that try to learn the Mahalanobis distance matrix M, either directly (as a convex problem), or indirectly via a linear projection. For other approaches to metric learning, see e.g., [Kul13; Kim19] for more details. \n16.2.1.1 Large margin nearest neighbors \nIn [WS09], they propose to learn the Mahalanobis matrix M so that the resulting distance metric works well when used by a nearest neighbor classifier. The resulting method is called large margin nearest neighbor or LMNN. \nThis works as follows. For each example data point $i$ , let $N _ { i }$ be a set of target neighbors; these are usually chosen to be the set of $K$ points with the same class label that are closest in Euclidean distance. We now optimize $mathbf { M }$ so that we minimize the distance between each point $i$ and all of its target neighbors $j in N _ { i }$ : \nWe also want to ensure that examples with incorrect labels are far away. To do this, we ensure that each example $i$ is closer (by some margin $m geq 0$ ) to its target neighbors $j$ than to other points $it l$ with different labels (so-called impostors). We can do this by minimizing \nwhere $[ z ] _ { + } = operatorname* { m a x } ( z , 0 )$ is the hinge loss function (Section 4.3.2). The overall objective is ${ mathcal { L } } ( mathbf { M } ) =$ $( 1 - lambda ) mathcal { L } _ { mathrm { p u l l } } ( mathbf { M } ) + lambda mathcal { L } _ { mathrm { p u s h } } ( mathbf { M } )$ , where $0 < lambda < 1$ . This is a convex function defined over a convex set, which can be minimized using semidefinite programming. Alternatively, we can parameterize the problem using $mathbf { M } = mathbf { W } ^ { parallel } mathbf { W }$ , and then minimize wrt $mathbf { W }$ using unconstrained gradient methods. This is no longer convex, but allows us to use a low-dimensional mapping $mathbf { W }$ . \nFor large datasets, we need to tackle the $O ( N ^ { 3 } )$ cost of computing Equation (16.5). We discuss some speedup tricks in Section 16.2.5. \n16.2.1.2 Neighborhood components analysis \nAnother way to learn a linear mapping W such that $mathbf { M } = mathbf { W } ^ { sf I } mathbf { W }$ is known as neighborhood components analysis or NCA [Gol+05]. This defines the probability that sample ${ boldsymbol { x } } _ { i }$ has $boldsymbol { x } _ { j }$ as its \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nnearest neighbor using the linear softmax function \n(This is a supervised version of stochastic neighborhood embeddings discussed in Section 20.4.10.1.) W is given by J(W) = PiN=1 Pj̸=i:yj= The expected number of correctly classified examples according for a 1NN classifier using distance . Let $mathcal { L } ( mathbf { W } ) = 1 - J ( mathbf { W } ) / N$ be the leave one out error. We can minimize $mathcal { L }$ wrt $mathbf { W }$ using gradient methods. \n16.2.1.3 Latent coincidence analysis \nYet another way to learn a linear mapping W such that $mathbf { M } = mathbf { W } ^ { parallel } mathbf { W }$ is known as latent coincidence analysis or LCA [DS12]. This defines a conditional latent variable model for mapping a pair of inputs, $_ { x }$ and $mathbf { { x } ^ { prime } }$ , to a label $y in { 0 , 1 }$ , which specifies if the inputs are similar (e.g., have same class label) or dissimilar. Each input $pmb { x } in mathbb { R } ^ { D }$ is mapped to a low dimensional latent point $z in mathbb { R } ^ { L }$ using a stochastic mapping $p ( z | mathbf { boldsymbol { x } } ) = mathcal { N } ( z | mathbf { W } mathbf { boldsymbol { x } } , sigma ^ { 2 } mathbf { I } )$ , and $p ( z ^ { prime } | x ^ { prime } ) = mathcal { N } ( z ^ { prime } | mathbf { W } x ^ { prime } , sigma ^ { 2 } mathbf { I } )$ . (Compare this to factor analysis, discussed in Section 20.2.) We then define the probability that the two inputs are similar using $begin{array} { r } { p ( y = 1 | z , z ^ { prime } ) = exp ( - frac { 1 } { 2 kappa ^ { 2 } } | | z - z ^ { prime } | | ) } end{array}$ . See Figure 16.4 for an illustration of the modeling assumptions. \nWe can maximize the log marginal likelihood $begin{array} { r } { ell ( mathbf { W } , sigma ^ { 2 } , kappa ^ { 2 } ) = sum _ { n } log p ( y _ { n } | pmb { x } _ { n } , pmb { x } _ { n } ^ { prime } ) } end{array}$ using the EM algorithm (Section 8.7.2). (We can set $kappa = 1$ WLOG, since it just changes the scale of W.) More precisely, in the E step, we compute the posterior $p ( z , z ^ { prime } | mathbf { x } , mathbf { x } ^ { prime } , y )$ (which can be done in closed form), and in the M step, we solve a weighted least squares problem (c.f., Section 13.6.2). EM will monotonically increase the objective, and does not need step size adjustment, unlike the gradient based methods used in NCA (Section 16.2.1.2). (It is also possible to use variational Bayes (Section 4.6.8.3) to fit this model, as well as various sparse and nonlinear extensions, as discussed in [ZMY19].) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n16.2.2 Deep metric learning \nWhen measuring the distance between high-dimensional or structured inputs, it is very useful to first learn an embedding to a lower dimensional “semantic” space, where distances are more meaningful, and less subject to the curse of dimensionality (Section 16.1.2). Let $pmb { e } = f ( pmb { x } ; pmb { theta } ) in mathbb { R } ^ { L }$ be an embedding of the input that preserves the “relevant” semantic aspects of the input, and let $hat { pmb { e } } = pmb { e } / | | pmb { e } | | _ { 2 }$ be the $ell _ { 2 }$ -normalized version. This ensures that all points lie on a hyper-sphere. We can then measure the distance between two points using the normalized Euclidean distance \nwhere smaller values means more similar, or the cosine similarity \nwhere larger values means more similar. (Cosine similarity measures the angle between the two vectors, as illustrated in Figure 20.43.) These quantities are related via \nThis overall approach is called deep metric learning or DML. \nThe basic idea in DML is to learn the embedding function such that similar examples are closer than dissimilar examples. More precisely, we assume we have a labeled dataset, $mathcal { D } = { ( x _ { i } , y _ { i } ) : i = 1 : N }$ , from which we can derive a set of similar pairs, $S = { ( i , j ) : y _ { i } = y _ { j } }$ . If $( i , j ) in S$ but $( i , k ) notin S$ , then we assume that ${ bf { x } } _ { i }$ and $boldsymbol { mathscr { x } } _ { j }$ should be close in embedding space, whereas ${ boldsymbol { x } } _ { i }$ and ${ boldsymbol { x } } _ { k }$ should be far. We discuss various ways to enforce this property below. Note that these methods also work when we do not have class labels, provided we have some other way of defining similar pairs. For example, in Section 19.2.4.3, we discuss self-supervised approaches to representation learning, that automatically create semantically similar pairs, and learn embeddings to force these pairs to be closer than unrelated pairs. \nBefore discussing DML in more detail, it is worth mentioning that many recent approaches to DML are not as good as they claim to be, as pointed out in [MBL20; Rot+20]. (The claims in some of these papers are often invalid due to improper experimental comparisons, a common flaw in contemporary ML research, as discussed in e.g., [BLV19; LS19b].) We therefore focus on (slightly) older and simpler methods, that tend to be more robust. \n16.2.3 Classification losses \nSuppose we have labeled data with $C$ classes. Then we can fit a classification model in $O ( N C )$ time, and then reuse the hidden features as an embedding function. (It is common to use the second-to-last layer, since it generalizes better to new classes than the final layer.) This approach is simple and scalable. However, it only learns to embed examples on the correct side of a decision boundary, which does not necessarily result in similar examples being placed close together and dissimilar examples being placed far apart. In addition, this method cannot be used if we do not have labeled training data. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Learning distance metrics",
        "subsubsection": "Linear and convex methods"
    },
    {
        "content": "16.2.2 Deep metric learning \nWhen measuring the distance between high-dimensional or structured inputs, it is very useful to first learn an embedding to a lower dimensional “semantic” space, where distances are more meaningful, and less subject to the curse of dimensionality (Section 16.1.2). Let $pmb { e } = f ( pmb { x } ; pmb { theta } ) in mathbb { R } ^ { L }$ be an embedding of the input that preserves the “relevant” semantic aspects of the input, and let $hat { pmb { e } } = pmb { e } / | | pmb { e } | | _ { 2 }$ be the $ell _ { 2 }$ -normalized version. This ensures that all points lie on a hyper-sphere. We can then measure the distance between two points using the normalized Euclidean distance \nwhere smaller values means more similar, or the cosine similarity \nwhere larger values means more similar. (Cosine similarity measures the angle between the two vectors, as illustrated in Figure 20.43.) These quantities are related via \nThis overall approach is called deep metric learning or DML. \nThe basic idea in DML is to learn the embedding function such that similar examples are closer than dissimilar examples. More precisely, we assume we have a labeled dataset, $mathcal { D } = { ( x _ { i } , y _ { i } ) : i = 1 : N }$ , from which we can derive a set of similar pairs, $S = { ( i , j ) : y _ { i } = y _ { j } }$ . If $( i , j ) in S$ but $( i , k ) notin S$ , then we assume that ${ bf { x } } _ { i }$ and $boldsymbol { mathscr { x } } _ { j }$ should be close in embedding space, whereas ${ boldsymbol { x } } _ { i }$ and ${ boldsymbol { x } } _ { k }$ should be far. We discuss various ways to enforce this property below. Note that these methods also work when we do not have class labels, provided we have some other way of defining similar pairs. For example, in Section 19.2.4.3, we discuss self-supervised approaches to representation learning, that automatically create semantically similar pairs, and learn embeddings to force these pairs to be closer than unrelated pairs. \nBefore discussing DML in more detail, it is worth mentioning that many recent approaches to DML are not as good as they claim to be, as pointed out in [MBL20; Rot+20]. (The claims in some of these papers are often invalid due to improper experimental comparisons, a common flaw in contemporary ML research, as discussed in e.g., [BLV19; LS19b].) We therefore focus on (slightly) older and simpler methods, that tend to be more robust. \n16.2.3 Classification losses \nSuppose we have labeled data with $C$ classes. Then we can fit a classification model in $O ( N C )$ time, and then reuse the hidden features as an embedding function. (It is common to use the second-to-last layer, since it generalizes better to new classes than the final layer.) This approach is simple and scalable. However, it only learns to embed examples on the correct side of a decision boundary, which does not necessarily result in similar examples being placed close together and dissimilar examples being placed far apart. In addition, this method cannot be used if we do not have labeled training data. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Learning distance metrics",
        "subsubsection": "Deep metric learning"
    },
    {
        "content": "16.2.2 Deep metric learning \nWhen measuring the distance between high-dimensional or structured inputs, it is very useful to first learn an embedding to a lower dimensional “semantic” space, where distances are more meaningful, and less subject to the curse of dimensionality (Section 16.1.2). Let $pmb { e } = f ( pmb { x } ; pmb { theta } ) in mathbb { R } ^ { L }$ be an embedding of the input that preserves the “relevant” semantic aspects of the input, and let $hat { pmb { e } } = pmb { e } / | | pmb { e } | | _ { 2 }$ be the $ell _ { 2 }$ -normalized version. This ensures that all points lie on a hyper-sphere. We can then measure the distance between two points using the normalized Euclidean distance \nwhere smaller values means more similar, or the cosine similarity \nwhere larger values means more similar. (Cosine similarity measures the angle between the two vectors, as illustrated in Figure 20.43.) These quantities are related via \nThis overall approach is called deep metric learning or DML. \nThe basic idea in DML is to learn the embedding function such that similar examples are closer than dissimilar examples. More precisely, we assume we have a labeled dataset, $mathcal { D } = { ( x _ { i } , y _ { i } ) : i = 1 : N }$ , from which we can derive a set of similar pairs, $S = { ( i , j ) : y _ { i } = y _ { j } }$ . If $( i , j ) in S$ but $( i , k ) notin S$ , then we assume that ${ bf { x } } _ { i }$ and $boldsymbol { mathscr { x } } _ { j }$ should be close in embedding space, whereas ${ boldsymbol { x } } _ { i }$ and ${ boldsymbol { x } } _ { k }$ should be far. We discuss various ways to enforce this property below. Note that these methods also work when we do not have class labels, provided we have some other way of defining similar pairs. For example, in Section 19.2.4.3, we discuss self-supervised approaches to representation learning, that automatically create semantically similar pairs, and learn embeddings to force these pairs to be closer than unrelated pairs. \nBefore discussing DML in more detail, it is worth mentioning that many recent approaches to DML are not as good as they claim to be, as pointed out in [MBL20; Rot+20]. (The claims in some of these papers are often invalid due to improper experimental comparisons, a common flaw in contemporary ML research, as discussed in e.g., [BLV19; LS19b].) We therefore focus on (slightly) older and simpler methods, that tend to be more robust. \n16.2.3 Classification losses \nSuppose we have labeled data with $C$ classes. Then we can fit a classification model in $O ( N C )$ time, and then reuse the hidden features as an embedding function. (It is common to use the second-to-last layer, since it generalizes better to new classes than the final layer.) This approach is simple and scalable. However, it only learns to embed examples on the correct side of a decision boundary, which does not necessarily result in similar examples being placed close together and dissimilar examples being placed far apart. In addition, this method cannot be used if we do not have labeled training data. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n16.2.4 Ranking losses \nIn this section, we consider minimizing ranking loss, to ensure that similar examples are closer than dissimilar examples. Most of these methods do not need class labels (although we sometimes assume that labels exist as a notationally simple way to define similarity). \n16.2.4.1 Pairwise (contrastive) loss and Siamese networks \nOne of the earliest approaches to representation learning from similar/dissimilar pairs was based on minimizing the following contrastive loss [CHL05]: \nwhere $[ z ] _ { + } = operatorname* { m a x } ( 0 , z )$ is the hinge loss and $m > 0$ is a margin parameter. Intuitively, we want to force positive pairs (with the same label) to be close, and negative pairs (with different labels) to be further apart than some minimal safety margin. We minimize this loss over all pairs of data. Naively this takes $O ( N ^ { 2 } )$ time; see Section 16.2.5 for some speedups. \nNote that we use the same feature extractor $f ( cdot ; theta )$ for both inputs, ${ bf { sigma } } _ { { bf { mathcal { X } } } _ { i } }$ and $boldsymbol { x } _ { j }$ . when computing the distance, as illustrated in Figure 16.5a. The resulting network is therefore called a Siamese network (named after Siamese twins). \n16.2.4.2 Triplet loss \nOne disadvantage of pairwise losses is that the optimization of the positive pairs is independent of the negative pairs, which can make their magnitudes incomparable. A solution to this is to use the triplet loss [SKP15]. This is defined as follows. For each example $i$ (known as an anchor), we find a similar (positive) example $boldsymbol { x } _ { i } ^ { + }$ and a dissimilar (negative) example $pmb { x } _ { i } ^ { - }$ . We then minimize the following loss, averaged overall all triples: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Learning distance metrics",
        "subsubsection": "Classification losses"
    },
    {
        "content": "16.2.4 Ranking losses \nIn this section, we consider minimizing ranking loss, to ensure that similar examples are closer than dissimilar examples. Most of these methods do not need class labels (although we sometimes assume that labels exist as a notationally simple way to define similarity). \n16.2.4.1 Pairwise (contrastive) loss and Siamese networks \nOne of the earliest approaches to representation learning from similar/dissimilar pairs was based on minimizing the following contrastive loss [CHL05]: \nwhere $[ z ] _ { + } = operatorname* { m a x } ( 0 , z )$ is the hinge loss and $m > 0$ is a margin parameter. Intuitively, we want to force positive pairs (with the same label) to be close, and negative pairs (with different labels) to be further apart than some minimal safety margin. We minimize this loss over all pairs of data. Naively this takes $O ( N ^ { 2 } )$ time; see Section 16.2.5 for some speedups. \nNote that we use the same feature extractor $f ( cdot ; theta )$ for both inputs, ${ bf { sigma } } _ { { bf { mathcal { X } } } _ { i } }$ and $boldsymbol { x } _ { j }$ . when computing the distance, as illustrated in Figure 16.5a. The resulting network is therefore called a Siamese network (named after Siamese twins). \n16.2.4.2 Triplet loss \nOne disadvantage of pairwise losses is that the optimization of the positive pairs is independent of the negative pairs, which can make their magnitudes incomparable. A solution to this is to use the triplet loss [SKP15]. This is defined as follows. For each example $i$ (known as an anchor), we find a similar (positive) example $boldsymbol { x } _ { i } ^ { + }$ and a dissimilar (negative) example $pmb { x } _ { i } ^ { - }$ . We then minimize the following loss, averaged overall all triples: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIntuitively this says we want the distance from the anchor to the positive to be less (by some safety margin $m$ ) than the distance from the anchor to the negative. We can compute the triplet loss using a triplet network as shown in Figure 16.5b. \nNaively minimizing triplet loss takes $O ( N ^ { 3 } )$ time. In practice we compute the loss on a minibatch (chosen so that there is at least one similar and one dissimilar example for the anchor point, often taken to be the first entry in the minibatch). Nevertheless the method can be slow. We discuss some speedups in Section 16.2.5. \n16.2.4.3 N-pairs loss \nOne problem with the triplet loss is that each anchor is only compared to one negative example at a time. This might not provide a strong enough learning signal. One solution to this is to create a multi-class classification problem in which we create a set of $N - 1$ negatives and 1 positive for every anchor. This is called the N-pairs loss [Soh16]. More precisely, we define the following loss for each set: \nNote that the N-pairs loss is the same as the InfoNCE loss used in the CPC paper [OLV18]. In [Che+20a], they propose a version where they scale the similarities by a temperature term; they call this the NT-Xent (normalized temperature-scaled cross-entropy) loss. We can view the temperature parameter as scaling the radius of the hypersphere on which the data lives. \nWhen $N = 2$ , the loss reduces to the logistic loss \nCompare this to the margin loss used by triplet learning (when $m = 1$ ): \nSee Figure 4.2 for a comparison of these two functions. \n16.2.5 Speeding up ranking loss optimization \nThe main disadvantage of ranking loss is the $O ( N ^ { 2 } )$ or $O ( N ^ { 3 } )$ cost of computing the loss function, due to the need to compare all pairs or triples of examples. In this section, we discuss various speedup tricks. \n16.2.5.1 Mining techniques \nA key insight is that we don’t need to consider all negative examples for each anchor, since most will be uninformative (i.e., will incur zero loss). Instead we can focus attention on negative examples which are closer to the anchor than its nearest positive example. These are called hard negatives, and are particularly useful for speeding up triplet loss. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Learning distance metrics",
        "subsubsection": "Ranking losses"
    },
    {
        "content": "Intuitively this says we want the distance from the anchor to the positive to be less (by some safety margin $m$ ) than the distance from the anchor to the negative. We can compute the triplet loss using a triplet network as shown in Figure 16.5b. \nNaively minimizing triplet loss takes $O ( N ^ { 3 } )$ time. In practice we compute the loss on a minibatch (chosen so that there is at least one similar and one dissimilar example for the anchor point, often taken to be the first entry in the minibatch). Nevertheless the method can be slow. We discuss some speedups in Section 16.2.5. \n16.2.4.3 N-pairs loss \nOne problem with the triplet loss is that each anchor is only compared to one negative example at a time. This might not provide a strong enough learning signal. One solution to this is to create a multi-class classification problem in which we create a set of $N - 1$ negatives and 1 positive for every anchor. This is called the N-pairs loss [Soh16]. More precisely, we define the following loss for each set: \nNote that the N-pairs loss is the same as the InfoNCE loss used in the CPC paper [OLV18]. In [Che+20a], they propose a version where they scale the similarities by a temperature term; they call this the NT-Xent (normalized temperature-scaled cross-entropy) loss. We can view the temperature parameter as scaling the radius of the hypersphere on which the data lives. \nWhen $N = 2$ , the loss reduces to the logistic loss \nCompare this to the margin loss used by triplet learning (when $m = 1$ ): \nSee Figure 4.2 for a comparison of these two functions. \n16.2.5 Speeding up ranking loss optimization \nThe main disadvantage of ranking loss is the $O ( N ^ { 2 } )$ or $O ( N ^ { 3 } )$ cost of computing the loss function, due to the need to compare all pairs or triples of examples. In this section, we discuss various speedup tricks. \n16.2.5.1 Mining techniques \nA key insight is that we don’t need to consider all negative examples for each anchor, since most will be uninformative (i.e., will incur zero loss). Instead we can focus attention on negative examples which are closer to the anchor than its nearest positive example. These are called hard negatives, and are particularly useful for speeding up triplet loss. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nMore precisely, if $a$ is an anchor and $p$ is its nearest positive example, we say that $n$ is a hard negative (for $u$ ) if $d ( pmb { x } _ { a } , pmb { x } _ { n } ) < d ( pmb { x } _ { a } , pmb { x } _ { p } )$ and $y _ { n } neq y _ { a }$ . Sometimes an anchor may not have any hard negatives. We can therefore increase the pool of candidates by considering semi-hard negatives, for which \nwhere $m > 0$ is a margin parameter. See Figure 16.6a for an illustration. This is the technique used by Google’s FaceNet model [SKP15], which learns an embedding function for faces, so it can cluster similar looking faces together, to which the user can attach a name. \nIn practice, the hard negatives are usually chosen from within the minibatch. This therefore requires large batch sizes to ensure sufficient diversity. Alternatively, we can have a separate process that continually updates the set of candidate hard negatives, as the distance measure evolves during training. \n16.2.5.2 Proxy methods\nTriplet loss minimization is expensive even with hard negative mining (Section 16.2.5.1). Ideally we can find a method that is $O ( N )$ time, just like classification loss. \nOne such method, proposed in [MA+17], measures the distance between each anchor and a set of $P$ proxies that represent each class, rather than directly measuring distance between examples. These proxies need to be updated online as the distance metric evolves during learning. The overall procedure takes $O ( N P ^ { 2 } )$ time, where $P sim C$ . \nMore recently, [Qia+19] proposed to represent each class with multiple prototypes, while still achieving linear time complexity, using a soft triple loss. \n16.2.5.3 Optimizing an upper bound \n[Do+19] proposed a simple and fast method for optimizing the triplet loss. The key idea is to define one fixed proxy or centroid per class, and then to use distance to the proxy as an upper bound on \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nthe triplet loss. \nMore precisely, consider a simplified form of the triplet loss, without the margin term: \nwhere $hat { pmb { e } } _ { i } = hat { pmb { e } } _ { pmb { theta } } ( { pmb { x } } _ { i } )$ , etc. Using the triangle inequality we have \nHence \nWe can use this to derive a tractable upper bound on the triplet loss as follows: \nwhere $begin{array} { r } { C ^ { prime } = 3 ( C - 1 ) ( frac { N } { C } - 1 ) frac { N } { C } } end{array}$ is a constant. It is clear that $mathcal { L } _ { u }$ can be computed in $O ( N C )$ time See Figure 16.6b for an illustration. \nIn [Do+19], they show that $begin{array} { r } { 0 leq mathcal { L } _ { t } - mathcal { L } _ { u } leq frac { N ^ { 3 } } { C ^ { 2 } } K } end{array}$ , where $K$ is some constant that depends on the spread of the centroids. To ensure the bound is tight, the centroids should be as far from each other as possible, and the distances between them should be as similar as possible. An easy way to ensure is to define the $mathbf { c } _ { m }$ vectors to be one-hot vectors, one per class. These vectors already have unit norm, and are orthogonal to each other. The distance between each pair of centroids is $sqrt { 2 }$ , which ensures the upper bound is fairly tight. \nThe downside of this approach is that it assumes the embedding layer is $L = C$ dimensional. There are two solutions to this. First, after training, we can add a linear projection layer to map from $C$ to $L neq C$ , or we can take the second-to-last layer of the embedding network. The second approach is to sample a large number of points on the $L$ -dimensional unit hyper-sphere (which we can do by sampling from the standard normal, and then normalizing [Mar72]), and then running K-means clustering (Section 21.3) with $K = C$ . In the experiments reported in [Do+19], these two approaches give similar results. \nInterestingly, in [Rot+20], they show that increasing $pi _ { mathrm { i n t r a } } / pi _ { mathrm { i n t e r } }$ results in improved downstream performance on various retrieval tasks, where \nis the average intra-class distance, and \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 is the average inter-class distance, where $begin{array} { r } { pmb { mu } _ { c } = frac { 1 } { Z _ { c } } sum _ { i : y _ { i } = c } hat { pmb { e } } _ { i } } end{array}$ is the mean embedding for examples from class $c$ . This suggests that we should not only keep the centroids far apart (in order to maximize the numerator), but we should also prevent examples from getting too close to their centroids (in order to minimize the denominator); this latter term is not captured in the method of [Do+19]. \n\n16.2.6 Other training tricks for DML \nBesides the speedup tricks in Section 16.2.5, there are a lot of other details that are important to get right in order to ensure good DML performance. Many of these details are discussed in [MBL20; Rot+20]. Here we just briefly mention a few. \nOne important issue is how the minibatches are created. In classification problems (at least with balanced classes), selecting examples at random from the training set is usually sufficient. However, for DML, we need to ensure that each example has some other examples in the minibatch that are similar to it, as well as some others that are dissimilar to it. One approach is to use hard mining techniques (Section 16.2.5.1). Another idea is to use coreset methods applied to previously learned embeddings to select a diverse minibatch at each step [Sin+20]. However, [Rot+20] show that the following simple strategy also works well for creating each batch: pick $B / n$ classes, and then pick $N _ { c }$ examples randomly from each class, where $B$ is the batch size, and $N _ { c } = 2$ is a tuning parameter. \nAnother important issue is avoiding overfitting. Since most datasets used in the DML literature are small, it is standard to use an image classifier, such as GoogLeNet (Section 14.3.3) or ResNet (Section 14.3.4), which has been pre-trained on ImageNet, and then to fine-tune the model using the DML loss. (See Section 19.2 for more details on this kind of transfer learning.) In addition, it is standard to use data augmentation (see Section 19.1). (Indeed, with some self-supervised learning methods, data aug is the only way to create similar pairs.) \nIn [ZLZ20], they propose to add a spherical embedding constraint (SEC), which is an additional batchwise regularization term, which encourages all the examples to have the same norm. That is, the regularizer is just the empirical variance of the norms of the (unnormalized) embeddings in that batch. See Figure 16.7 for an illustration. This regularizer can be added to any of the existing DML losses to modestly improve training speed and stability, as well as final performance, analogously to how batchnorm (Section 14.2.4.1) is used. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Learning distance metrics",
        "subsubsection": "Speeding up ranking loss optimization"
    },
    {
        "content": "16.2.6 Other training tricks for DML \nBesides the speedup tricks in Section 16.2.5, there are a lot of other details that are important to get right in order to ensure good DML performance. Many of these details are discussed in [MBL20; Rot+20]. Here we just briefly mention a few. \nOne important issue is how the minibatches are created. In classification problems (at least with balanced classes), selecting examples at random from the training set is usually sufficient. However, for DML, we need to ensure that each example has some other examples in the minibatch that are similar to it, as well as some others that are dissimilar to it. One approach is to use hard mining techniques (Section 16.2.5.1). Another idea is to use coreset methods applied to previously learned embeddings to select a diverse minibatch at each step [Sin+20]. However, [Rot+20] show that the following simple strategy also works well for creating each batch: pick $B / n$ classes, and then pick $N _ { c }$ examples randomly from each class, where $B$ is the batch size, and $N _ { c } = 2$ is a tuning parameter. \nAnother important issue is avoiding overfitting. Since most datasets used in the DML literature are small, it is standard to use an image classifier, such as GoogLeNet (Section 14.3.3) or ResNet (Section 14.3.4), which has been pre-trained on ImageNet, and then to fine-tune the model using the DML loss. (See Section 19.2 for more details on this kind of transfer learning.) In addition, it is standard to use data augmentation (see Section 19.1). (Indeed, with some self-supervised learning methods, data aug is the only way to create similar pairs.) \nIn [ZLZ20], they propose to add a spherical embedding constraint (SEC), which is an additional batchwise regularization term, which encourages all the examples to have the same norm. That is, the regularizer is just the empirical variance of the norms of the (unnormalized) embeddings in that batch. See Figure 16.7 for an illustration. This regularizer can be added to any of the existing DML losses to modestly improve training speed and stability, as well as final performance, analogously to how batchnorm (Section 14.2.4.1) is used. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n16.3 Kernel density estimation (KDE) \nIn this section, we consider a form of non-parametric density estimation known as kernel density estimation or KDE. This is a form of generative model, since it defines a probability distribution $p ( { pmb x } )$ that can be evaluated pointwise, and which can be sampled from to generate new data. \n16.3.1 Density kernels \nBefore explaining KDE, we must define what we mean by a “kernel”. This term has several different meanings in machine learning and statistics.1 In this section, we use a specific kind of kernel which we refer to as a density kernel. This is a function $K : mathbb { R }  mathbb { R } _ { + }$ such that $textstyle int K ( x ) d x = 1$ and $begin{array} { r } { { cal { K } } ( - x ) = { cal { K } } ( x ) } end{array}$ . This latter symmetry property implies the $textstyle int x K ( x ) d x = 0$ , and hence \nA simple example of such a kernel is the boxcar kernel, which is the uniform distribution within the unit interval around the origin: \nAnother example is the Gaussian kernel: \nWe can control the width of the kernel by introducing a bandwidth parameter $h$ : \nWe can generalize to vector valued inputs by defining a radial basis function or RBF kernel: \nIn the case of the Gaussian kernel, this becomes \nAlthough Gaussian kernels are popular, they have unbounded support. Some alternative kernels, which have compact support (which can be computationally faster), are listed in Table 16.1. See Figure 16.8 for a plot of these kernel functions.",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Learning distance metrics",
        "subsubsection": "Other training tricks for DML"
    },
    {
        "content": "16.3 Kernel density estimation (KDE) \nIn this section, we consider a form of non-parametric density estimation known as kernel density estimation or KDE. This is a form of generative model, since it defines a probability distribution $p ( { pmb x } )$ that can be evaluated pointwise, and which can be sampled from to generate new data. \n16.3.1 Density kernels \nBefore explaining KDE, we must define what we mean by a “kernel”. This term has several different meanings in machine learning and statistics.1 In this section, we use a specific kind of kernel which we refer to as a density kernel. This is a function $K : mathbb { R }  mathbb { R } _ { + }$ such that $textstyle int K ( x ) d x = 1$ and $begin{array} { r } { { cal { K } } ( - x ) = { cal { K } } ( x ) } end{array}$ . This latter symmetry property implies the $textstyle int x K ( x ) d x = 0$ , and hence \nA simple example of such a kernel is the boxcar kernel, which is the uniform distribution within the unit interval around the origin: \nAnother example is the Gaussian kernel: \nWe can control the width of the kernel by introducing a bandwidth parameter $h$ : \nWe can generalize to vector valued inputs by defining a radial basis function or RBF kernel: \nIn the case of the Gaussian kernel, this becomes \nAlthough Gaussian kernels are popular, they have unbounded support. Some alternative kernels, which have compact support (which can be computationally faster), are listed in Table 16.1. See Figure 16.8 for a plot of these kernel functions. \nTable 16.1: List of some popular normalized kernels in 1d. Compact=1 means the function is non-zero for a finite range of inputs. Smooth=1 means the function is differentiable over the range of its support. Boundaries=1 means the function is also differentiable at the boundaries of its support. \n16.3.2 Parzen window density estimator \nTo explain how to use kernels to define a nonparametric density estimate, recall the form of the Gaussian mixture model from Section 3.5.1. If we assume a fixed spherical Gaussian covariance and uniform mixture weights, we get \nOne problem with this model is that it requires specifying the number $K$ of clusters, as well as their locations $pmb { mu } _ { k }$ . An alternative to estimating these parameters is to allocate one cluster center per data point. In this case, the model becomes \nWe can generalize Equation (16.32) by writing \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $boldsymbol { mathcal { K } } _ { h }$ is a density kernel. This is called a Parzen window density estimator, or kernel density estimator (KDE).",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Kernel density estimation (KDE)",
        "subsubsection": "Density kernels"
    },
    {
        "content": "Table 16.1: List of some popular normalized kernels in 1d. Compact=1 means the function is non-zero for a finite range of inputs. Smooth=1 means the function is differentiable over the range of its support. Boundaries=1 means the function is also differentiable at the boundaries of its support. \n16.3.2 Parzen window density estimator \nTo explain how to use kernels to define a nonparametric density estimate, recall the form of the Gaussian mixture model from Section 3.5.1. If we assume a fixed spherical Gaussian covariance and uniform mixture weights, we get \nOne problem with this model is that it requires specifying the number $K$ of clusters, as well as their locations $pmb { mu } _ { k }$ . An alternative to estimating these parameters is to allocate one cluster center per data point. In this case, the model becomes \nWe can generalize Equation (16.32) by writing \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $boldsymbol { mathcal { K } } _ { h }$ is a density kernel. This is called a Parzen window density estimator, or kernel density estimator (KDE). \n\nThe advantage over a parametric model is that no model fitting is required (except for choosing $h$ , discussed in Section 16.3.3), and there is no need to pick the number of cluster centers. The disadvantage is that the model takes a lot of memory (you need to store all the data) and a lot of time to evaluate. \nFigure 16.9 illustrates KDE in 1d for two kinds of kernel. On the top, we use a boxcar kernel; the resulting model just counts how many data points land within an interval of size $h$ around each $x _ { n }$ to get a piecewise constant density. On the bottom, we use a Gaussian kernel, which results in a smoother density. \n16.3.3 How to choose the bandwidth parameter \nWe see from Figure 16.9 that the bandwidth parameter $h$ has a large effect on the learned distribution.   \nWe can view this as controlling the complexity of the model. \nIn the case of 1d data, where the “true” data generating distribution is assumed to be a Gaussian, one can show [BA97a] that the optimal bandwidth for a Gaussian kernel (from the point of view of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 minimizing frequentist risk) is given by h = σ \u0000 4N \u00011/5. We can compute a robust approximation to the standard deviation by first computing the median absolute deviation, median( $| { pmb x } - mathrm { m e d i a n } ( { pmb x } ) | )$ , and then using $hat { sigma } = 1 . 4 8 2 6$ MAD. If we have $D$ dimensions, we can estimate $h _ { d }$ separately for each dimension, and then set $textstyle h = ( prod _ { d = 1 } ^ { D } h _ { d } ) ^ { 1 / D }$ .",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Kernel density estimation (KDE)",
        "subsubsection": "Parzen window density estimator"
    },
    {
        "content": "The advantage over a parametric model is that no model fitting is required (except for choosing $h$ , discussed in Section 16.3.3), and there is no need to pick the number of cluster centers. The disadvantage is that the model takes a lot of memory (you need to store all the data) and a lot of time to evaluate. \nFigure 16.9 illustrates KDE in 1d for two kinds of kernel. On the top, we use a boxcar kernel; the resulting model just counts how many data points land within an interval of size $h$ around each $x _ { n }$ to get a piecewise constant density. On the bottom, we use a Gaussian kernel, which results in a smoother density. \n16.3.3 How to choose the bandwidth parameter \nWe see from Figure 16.9 that the bandwidth parameter $h$ has a large effect on the learned distribution.   \nWe can view this as controlling the complexity of the model. \nIn the case of 1d data, where the “true” data generating distribution is assumed to be a Gaussian, one can show [BA97a] that the optimal bandwidth for a Gaussian kernel (from the point of view of \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 minimizing frequentist risk) is given by h = σ \u0000 4N \u00011/5. We can compute a robust approximation to the standard deviation by first computing the median absolute deviation, median( $| { pmb x } - mathrm { m e d i a n } ( { pmb x } ) | )$ , and then using $hat { sigma } = 1 . 4 8 2 6$ MAD. If we have $D$ dimensions, we can estimate $h _ { d }$ separately for each dimension, and then set $textstyle h = ( prod _ { d = 1 } ^ { D } h _ { d } ) ^ { 1 / D }$ . \n\n16.3.4 From KDE to KNN classification \nIn Section 16.1, we discussed the K nearest neighbor classifier as a heuristic approach to classification. Interestingly, we can derive it as a generative classifier in which the class conditional densities $p ( { pmb x } | y = c )$ are modeled using KDE. Rather than using a fixed bandwidth and counting how many data points fall within the hyper-cube centered on a datapoint, we will allow the bandwidth or volume to be different for each data point. Specifically, we will “grow” a volume around $_ { x }$ until we encounter $K$ data points, regardless of their class label. This is called a balloon kernel density estimator [TS92]. Let the resulting volume have size $V ( { pmb x } )$ (this was previously $h ^ { D }$ ), and let there be $N _ { c } ( { pmb x } )$ examples from class $c$ in this volume. Then we can estimate the class conditional density as follows: \nwhere $N _ { c }$ is the total number of examples in class $c$ in the whole data set. If we take the class prior to be $p ( y = c ) = N _ { c } / N$ , then the class posterior is given by \nwhere we used the fact that $begin{array} { r } { sum _ { c } N _ { c } ( pmb { x } ) = K } end{array}$ , since we choose a total of $K$ points (regardless of class) around every point. This matches 16.1. \n16.3.5 Kernel regression \nJust as KDE can be used for generative classifiers (see Section 16.1), it can also be used for generative models for regression, as we discuss below. \n16.3.5.1 Nadaraya-Watson estimator for the mean \nIn regression, our goal is to compute the conditional expectation \nIf we use an MVN for $p ( boldsymbol { y } , pmb { x } | mathcal { D } )$ , we derive a result which is equivalent to linear regression, as we showed in Section 11.2.3.5. However, the assumption that $p ( boldsymbol { y } , pmb { x } | mathcal { D } )$ is Gaussian is rather limiting. We can use KDE to more accurately approximate the joint density $p ( boldsymbol { x } , boldsymbol { y } | mathcal { D } )$ as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Kernel density estimation (KDE)",
        "subsubsection": "How to choose the bandwidth parameter"
    },
    {
        "content": "16.3.4 From KDE to KNN classification \nIn Section 16.1, we discussed the K nearest neighbor classifier as a heuristic approach to classification. Interestingly, we can derive it as a generative classifier in which the class conditional densities $p ( { pmb x } | y = c )$ are modeled using KDE. Rather than using a fixed bandwidth and counting how many data points fall within the hyper-cube centered on a datapoint, we will allow the bandwidth or volume to be different for each data point. Specifically, we will “grow” a volume around $_ { x }$ until we encounter $K$ data points, regardless of their class label. This is called a balloon kernel density estimator [TS92]. Let the resulting volume have size $V ( { pmb x } )$ (this was previously $h ^ { D }$ ), and let there be $N _ { c } ( { pmb x } )$ examples from class $c$ in this volume. Then we can estimate the class conditional density as follows: \nwhere $N _ { c }$ is the total number of examples in class $c$ in the whole data set. If we take the class prior to be $p ( y = c ) = N _ { c } / N$ , then the class posterior is given by \nwhere we used the fact that $begin{array} { r } { sum _ { c } N _ { c } ( pmb { x } ) = K } end{array}$ , since we choose a total of $K$ points (regardless of class) around every point. This matches 16.1. \n16.3.5 Kernel regression \nJust as KDE can be used for generative classifiers (see Section 16.1), it can also be used for generative models for regression, as we discuss below. \n16.3.5.1 Nadaraya-Watson estimator for the mean \nIn regression, our goal is to compute the conditional expectation \nIf we use an MVN for $p ( boldsymbol { y } , pmb { x } | mathcal { D } )$ , we derive a result which is equivalent to linear regression, as we showed in Section 11.2.3.5. However, the assumption that $p ( boldsymbol { y } , pmb { x } | mathcal { D } )$ is Gaussian is rather limiting. We can use KDE to more accurately approximate the joint density $p ( boldsymbol { x } , boldsymbol { y } | mathcal { D } )$ as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Kernel density estimation (KDE)",
        "subsubsection": "From KDE to KNN classification"
    },
    {
        "content": "16.3.4 From KDE to KNN classification \nIn Section 16.1, we discussed the K nearest neighbor classifier as a heuristic approach to classification. Interestingly, we can derive it as a generative classifier in which the class conditional densities $p ( { pmb x } | y = c )$ are modeled using KDE. Rather than using a fixed bandwidth and counting how many data points fall within the hyper-cube centered on a datapoint, we will allow the bandwidth or volume to be different for each data point. Specifically, we will “grow” a volume around $_ { x }$ until we encounter $K$ data points, regardless of their class label. This is called a balloon kernel density estimator [TS92]. Let the resulting volume have size $V ( { pmb x } )$ (this was previously $h ^ { D }$ ), and let there be $N _ { c } ( { pmb x } )$ examples from class $c$ in this volume. Then we can estimate the class conditional density as follows: \nwhere $N _ { c }$ is the total number of examples in class $c$ in the whole data set. If we take the class prior to be $p ( y = c ) = N _ { c } / N$ , then the class posterior is given by \nwhere we used the fact that $begin{array} { r } { sum _ { c } N _ { c } ( pmb { x } ) = K } end{array}$ , since we choose a total of $K$ points (regardless of class) around every point. This matches 16.1. \n16.3.5 Kernel regression \nJust as KDE can be used for generative classifiers (see Section 16.1), it can also be used for generative models for regression, as we discuss below. \n16.3.5.1 Nadaraya-Watson estimator for the mean \nIn regression, our goal is to compute the conditional expectation \nIf we use an MVN for $p ( boldsymbol { y } , pmb { x } | mathcal { D } )$ , we derive a result which is equivalent to linear regression, as we showed in Section 11.2.3.5. However, the assumption that $p ( boldsymbol { y } , pmb { x } | mathcal { D } )$ is Gaussian is rather limiting. We can use KDE to more accurately approximate the joint density $p ( boldsymbol { x } , boldsymbol { y } | mathcal { D } )$ as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHence \nWe can simplify the numerator using the fact that $begin{array} { r } { int y K _ { h } ( y - y _ { n } ) d y = y _ { n } } end{array}$ (from Equation (16.25)). We can simplify the denominator using the fact that density kernels integrate to one, i.e., $begin{array} { r l } { int mathcal { K } _ { h } ( y - y _ { n } ) d y = } end{array}$ 1. Thus \nWe see that the prediction is just a weighted sum of the outputs at the training points, where the weights depend on how similar $_ { x }$ is to the stored training points. This method is called kernel regression, kernel smoothing, or the Nadaraya-Watson (N-W) model. See Figure 16.10 for an example, where we use a Gaussian kernel. \nIn Section 17.2.3, we discuss the connection between kernel regression and Gaussian process regression. \n16.3.5.2 Estimator for the variance \nSometimes it is useful to compute the predictive variance, as well as the predictive mean. We can do this by noting that \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $begin{array} { r } { mu ( pmb { x } ) = mathbb { E } left[ y | pmb { x } , mathcal { D } right] } end{array}$ is the N-W estimate. If we use a Gaussian kernel with variance $sigma ^ { 2 }$ , we can compute $mathbb { E } leftlfloor y ^ { 2 } vert x , mathcal { D } rightrfloor$ as follows: \nwhere we used the fact that \nCombining Equation (16.43) with Equation (16.41) gives \nThis matches Eqn. 8 of [BA10] (modulo the initial $sigma ^ { 2 }$ term). \n16.3.5.3 Locally weighted regression \nWe can drop the normalization term from Equation (16.39) to get \nThis is just a weighted sum of the observed responses, where the weights depend on how similar the test input $_ { x }$ is to the training points ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . \nRather than just interpolating the stored responses $y _ { n }$ , we can fit a locally linear model around each training point: \nwhere $phi ( pmb { x } ) = [ 1 , pmb { x } ]$ . This is called locally linear regression (LRR) or locally-weighted scatterplot smoothing, and is commonly known by the acronym LOWESS or LOESS [CD88]. This is often used when annotating scatter plots with local trend lines. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n17 Kernel Methods * \nIn this chapter, we consider nonparametric methods for regression and classification. Such methods do not assume a fixed parametric form for the prediction function, but instead try to estimate the function itself (rather than the parameters) directly from data. The key idea is that we observe the function value at a fixed set of $N$ points, namely $y _ { n } = f ( pmb { x } _ { n } )$ for $n = 1 : N$ , where $f$ is the unknown function, so to predict the function value at a new point, say $x _ { * }$ , we just have to compare how “similar” ${ pmb x } _ { ast }$ is to each of the $N$ training points, ${ { pmb x } _ { n } }$ , and then we can predict that $f ( { pmb x } _ { * } )$ is some weighted combination of the ${ f ( { pmb x } _ { n } ) }$ values. Thus we may need to “remember” the entire training set, $mathcal { D } = { ( { pmb x } _ { n } , y _ { n } ) }$ , in order to make predictions at test time — we cannot “compress” $mathcal { D }$ into a fixed-sized parameter vector. \nThe weights that are used for prediction are determined by the similarity between ${ pmb x } _ { ast }$ and each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , which is computed using a special kind of function known as kernel function, $K ( pmb { x } _ { n } , pmb { x } _ { ast } ) geq 0$ , which we explain in Section 17.1. This approach is similar to RBF networks (Section 13.6.1), except we use the datapoints ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ themselves as the “anchors”, rather than learning centroid ${ pmb mu } _ { n }$ . \nIn Section 17.2, we discuss an approach called Gaussian processes, which allows us to use the kernel to define a prior over functions, which we can update given data to get a posterior over functions. Alternatively we can use the same kernel with a method called Support Vector Machines to compute a MAP estimate of the function, as we explain in Section 17.3. \n17.1 Mercer kernels \nThe key to nonparametric methods is that we need a way to encode prior knowledge about the similarity of two input vectors. If we know that ${ boldsymbol { x } } _ { i }$ is similar to $boldsymbol { x } _ { j }$ , then we can encourage the model to make the predicted output at both locations (i.e., $f ( pmb { x } _ { i } )$ and $f ( pmb { x } _ { j } )$ ) to be similar. \nTo define similarity, we introduce the notion of a kernel function. The word “kernel” has many different meanings in mathematics, including density kernels (Section 16.3.1), transition kernels of a Markov chain (Section 3.6.1.2), and convolutional kernels (Section 14.1). Here we consider a Mercer kernel, also called a positive definite kernel. This is any symmetric function $K : mathcal { X } times mathcal { X }  mathbb { R } ^ { + }$ such that \nfor any set of $N$ (unique) points $pmb { x } _ { i } in mathcal { X }$ , and any choice of numbers $c _ { i } in mathbb { R }$ . (We assume $mathscr { K } ( pmb { x } _ { i } , pmb { x } _ { j } ) > 0$ , so that we can only achieve equality in the above equation if $c _ { i } = 0$ for all $i$ .)",
        "chapter": "IV Nonparametric Models",
        "section": "Exemplar-based Methods",
        "subsection": "Kernel density estimation (KDE)",
        "subsubsection": "Kernel regression"
    },
    {
        "content": "Another way to understand this condition is the following. Given a set of $N$ datapoints, let us define the Gram matrix as the following $N times N$ similarity matrix: \nWe say that $mathcal { K }$ is a Mercer kernel iff the Gram matrix is positive definite for any set of (distinct) inputs ${ { pmb x } _ { i } } _ { i = 1 } ^ { N }$ . \nThe most widely used kernel for real-valued inputs is the squared exponential kernel (SE kernel), also called the exponentiated quadratic, Gaussian kernel RBF kernel. It is defined by \nHere $ell$ corresponds to the length scale of the kernel, i.e., the distance over which we expect differences to matter. This is known as the bandwidth parameter. The RBF kernel measures similarity between two vectors in $mathbb { R } ^ { D }$ using (scaled) Euclidean distance. In Section 17.1.2, we will discuss several other kinds of kernel. \nIn Section 17.2, we show how to use kernels to define priors and posteriors over functions. The basic idea is this: if $kappa ( pmb { x } , pmb { x } ^ { prime } )$ is large, meaning the inputs are similar, then we expect the output of the function to be similar as well, so $f ( pmb { x } ) approx f ( pmb { x } ^ { prime } )$ . More precisely, information we learn about $f ( { pmb x } )$ will help us predict $f ( pmb { x } ^ { prime } )$ for all $mathbf { { x } ^ { prime } }$ which are correlated with $_ { x }$ , and hence for which $boldsymbol { kappa } ( boldsymbol { mathbf { mathit { x } } } , boldsymbol { mathbf { mathit { x } } } ^ { prime } )$ is large. \nIn Section 17.3, we show how to use kernels to generalize from Euclidean distance to a more general notion of distance, so that we can use geometric methods such as linear discriminant analysis in an implicit feature space instead of input space. \n17.1.1 Mercer’s theorem \nRecall from Section 7.4 that any positive definite matrix $mathbf { K }$ can be represented using an eigendecomposition of the form $mathbf { K } = mathbf { U } ^ { top } mathbf { A } mathbf { U }$ , where $pmb { Lambda }$ is a diagonal matrix of eigenvalues $lambda _ { i } > 0$ , and $mathbf { U }$ is a matrix containing the eigenvectors. Now consider element $( i , j )$ of $mathbf { K }$ : \nwhere $mathbf { U } _ { : i }$ is the $i$ ’th column of $mathbf { U }$ . If we define $phi ( mathbf { x } _ { i } ) = Lambda ^ { frac { 1 } { 2 } } mathbf { U } _ { : i }$ , then we can write \nThus we see that the entries in the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly defined by the eigenvectors of the kernel matrix. This idea can be generalized to apply to kernel functions, not just kernel matrices; this result is known as Mercer’s theorem. \nFor example, consider the quadratic kernel $K ( { pmb x } , { pmb x } ^ { prime } ) = langle { pmb x } , { pmb x } ^ { prime } rangle ^ { 2 }$ . In 2d, we have \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe can write this as ${ mathcal K } ( { pmb x } , { pmb x } ^ { prime } ) = phi ( { pmb x } ) ^ { prime } phi ( { pmb x } )$ if we define $phi ( x _ { 1 } , x _ { 2 } ) = [ x _ { 1 } ^ { 2 } , sqrt { 2 } x _ { 1 } x _ { 2 } , x _ { 2 } ^ { 2 } ] in mathbb { R } ^ { 3 }$ . So we embed the 2d inputs $_ { x }$ into a 3d feature space $phi ( { pmb x } )$ . \nNow consider the RBF kernel. In this case, the corresponding feature representation is infinite dimensional (see Section 17.2.9.3 for details). However, by working with kernel functions, we can avoid having to deal with infinite dimensional vectors. \n17.1.2 Some popular Mercer kernels \nIn the sections below, we describe some popular Mercer kernels. More details can be found at [Wil14] and https://www.cs.toronto.edu/~duvenaud/cookbook/. \n17.1.2.1 Stationary kernels for real-valued vectors \nFor real-valued inputs, $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , it is common to use stationary kernels, which are functions of the form $begin{array} { r } { K ( pmb { x } , pmb { x } ^ { prime } ) = K ( | | pmb { x } - pmb { x } ^ { prime } | | ) } end{array}$ ; thus the value only depends on the elementwise difference between the inputs. The RBF kernel is a stationary kernel. We give some other examples below. \nARD kernel \nWe can generalize the RBF kernel by replacing Euclidean distance with Mahalanobis distance, as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Mercer kernels",
        "subsubsection": "Mercer's theorem"
    },
    {
        "content": "We can write this as ${ mathcal K } ( { pmb x } , { pmb x } ^ { prime } ) = phi ( { pmb x } ) ^ { prime } phi ( { pmb x } )$ if we define $phi ( x _ { 1 } , x _ { 2 } ) = [ x _ { 1 } ^ { 2 } , sqrt { 2 } x _ { 1 } x _ { 2 } , x _ { 2 } ^ { 2 } ] in mathbb { R } ^ { 3 }$ . So we embed the 2d inputs $_ { x }$ into a 3d feature space $phi ( { pmb x } )$ . \nNow consider the RBF kernel. In this case, the corresponding feature representation is infinite dimensional (see Section 17.2.9.3 for details). However, by working with kernel functions, we can avoid having to deal with infinite dimensional vectors. \n17.1.2 Some popular Mercer kernels \nIn the sections below, we describe some popular Mercer kernels. More details can be found at [Wil14] and https://www.cs.toronto.edu/~duvenaud/cookbook/. \n17.1.2.1 Stationary kernels for real-valued vectors \nFor real-valued inputs, $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , it is common to use stationary kernels, which are functions of the form $begin{array} { r } { K ( pmb { x } , pmb { x } ^ { prime } ) = K ( | | pmb { x } - pmb { x } ^ { prime } | | ) } end{array}$ ; thus the value only depends on the elementwise difference between the inputs. The RBF kernel is a stationary kernel. We give some other examples below. \nARD kernel \nWe can generalize the RBF kernel by replacing Euclidean distance with Mahalanobis distance, as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $pmb { r } = pmb { x } - pmb { x } ^ { prime }$ . If $pmb { Sigma }$ is diagonal, this can be written as \nwhere \nWe can interpret $sigma ^ { 2 }$ as the overall variance, and $ell _ { d }$ as defining the characteristic length scale of dimension $d$ . If $d$ is an irrelevant input dimension, we can set $ell _ { d } = infty$ , so the corresponding dimension will be ignored. This is known as automatic relevancy determination or ARD (Section 11.7.7). Hence the corresponding kernel is called the ARD kernel. See Figure 17.1 for an illustration of some 2d functions sampled from a GP using this prior. \nMatern kernels \nThe SE kernel gives rise to functions that are infinitely differentiable, and therefore are very smooth. For many applications, it is better to use the Matern kernel, which gives rise to “rougher” functions, which can better model local “wiggles” without having to make the overall length scale very small. The Matern kernel has the following form: \nwhere $K _ { nu }$ is a modified Bessel function and $ell$ is the length scale. Functions sampled from this GP are $k$ -times differentiable iff $nu > k$ . As $nu to infty$ , this approaches the SE kernel. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nFor values $nu in { { textstyle { frac { 1 } { 2 } } } , { frac { 3 } { 2 } } , { frac { 5 } { 2 } } }$ , the function simplifies as follows: \nThe value $begin{array} { r } { nu = frac { 1 } { 2 } } end{array}$ corresponds to the Ornstein-Uhlenbeck process, which describes the velocity of a particle undergoing Brownian motion. The corresponding function is continuous but not differentiable, and hence is very “jagged”. See Figure 17.2b for an illustration. \nPeriodic kernels \nThe periodic kernel captures repeating structure, and has the form \nwhere $p$ is the period. See Figure 17.3a for an illustration. \nA related kernel is the cosine kernel: \nSee Figure 17.3b for an illustration. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n17.1.2.2 Making new kernels from old \nGiven two valid kernels ${ cal { K } } _ { 1 } ( { pmb x } , { pmb x } ^ { prime } )$ and ${  K } _ { 2 } ( { pmb x } , { pmb x } ^ { prime } )$ , we can create a new kernel using any of the following methods: \n${  K } ( { pmb x } , { pmb x } ^ { prime } ) = q ( {  K } _ { 1 } ( { pmb x } , { pmb x } ^ { prime } ) )$ for any function polynomial $q$ with nonneg. coef. \n$K ( { pmb x } , { pmb x } ^ { prime } ) = { pmb x } ^ { top } { pmb mathrm { A } } { pmb x } ^ { prime }$ , for any psd matrix A \nFor example, suppose we start with the linear kernel $K ( { pmb x } , { pmb x } ^ { prime } ) = { pmb x } ^ { top } { pmb x } ^ { prime }$ . We know this is a valid Mercer kernel, since the corresponding Gram matrix is just the (scaled) covariance matrix of the data. From the above rules, we can see that the polynomial kernel $begin{array} { r } { mathcal { K } ( pmb { x } , pmb { x } ^ { prime } ) = ( pmb { x } ^ { prime } pmb { x } ^ { prime } ) ^ { M } } end{array}$ is a valid Mercer kernel. This contains all monomials of order $M$ . For example, if $M = 2$ and the inputs are 2d, we have \nWe can generalize this to contain all terms up to degree $M$ by using the kernel $mathcal { K } ( pmb { x } , pmb { x } ^ { prime } ) = ( pmb { x } ^ { top } pmb { x } ^ { prime } + c ) ^ { M }$ . For example, if $M = 2$ and the inputs are 2d, we have \nWe can also use the above rules to establish that the Gaussian kernel is a valid kernel. To see this, note that \nan \nis a valid kernel. \n17.1.2.3 Combining kernels by addition and multiplication \nWe can also combine kernels using addition or multiplication: \nMultiplying two positive-definite kernels together always results in another positive definite kernel. This is a way to get a conjunction of the individual properties of each kernel, as illustrated in Figure 17.4. \nIn addition, adding two positive-definite kernels together always results in another positive definite kernel. This is a way to get a disjunction of the individual properties of each kernel, as illustrated in Figure 17.5. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n17.1.2.4 Kernels for structured inputs \nKernels are particularly useful when the inputs are structured objects, such as strings and graphs, since it is often hard to “featurize” variable-sized inputs. For example, we can define a string kernel which compares strings in terms of the number of n-grams they have in common [Lod+02; BC17]. \nWe can also define kernels on graphs [KJM19]. For example, the random walk kernel conceptually performs random walks on two graphs simultaneously, and then counts the number of paths that were produced by both walks. This can be computed efficiently as discussed in [Vis+10]. For more details on graph kernels, see [KJM19]. \nFor a review of kernels on structured objects, see e.g., [Gär03]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFigure 17.6: A Gaussian process for $boldsymbol { mathcal { Z } }$ training points, ${ bf { x } } _ { 1 }$ and ${ bf { sigma } } ^ { x _ { 2 } }$ , and $^ { 1 }$ testing point, ${ pmb x } _ { ast }$ , represented as a graphical model representing $p ( { pmb y } , { pmb f } _ { X } | { bf X } ) = mathcal { N } ( { pmb f } _ { X } | m ( { bf X } ) , mathcal { K } ( { bf X } ) ) prod _ { i } p ( { pmb y }$ i|fi). The hidden nodes $f _ { i } = f ( pmb { x } _ { i } )$ represent the value of the function at each of the data points. Th ese hidden nodes are fully interconnected by undirected edges, forming a Gaussian graphical model; the edge strengths represent the covariance terms $Sigma _ { i j } = { cal K } ( { pmb x } _ { i } , { pmb x } _ { j } )$ . If the test point $^ { mathbf { nabla } _ { mathbf { x } } }$ is similar to the training points $pmb { x } _ { 1 }$ and $scriptstyle { pmb x } _ { 2 }$ , then the value of the hidden function $f _ { * }$ will be similar to $f _ { 1 }$ and $f _ { 2 }$ , and hence the predicted output $y _ { * }$ will be similar to the training values $_ { y _ { 1 } }$ and $_ { y _ { 2 } }$ . \n17.2 Gaussian processes \nIn this section, we discuss Gaussian processes, which is a way to define distributions over functions of the form $f : mathcal { X } to mathbb { R }$ , where $mathcal { X }$ is any domain. The key assumption is that the function values at a set of $M > 0$ inputs, $pmb { f } = [ f ( pmb { x } _ { 1 } ) , dots , f ( pmb { x } _ { M } ) ]$ , is jointly Gaussian, with mean ${ pmb mu } = m ( { pmb x } _ { 1 } ) , dots , m ( { pmb x } _ { M } ) )$ and covariance $pmb { Sigma } _ { i j } = mathcal { K } ( pmb { x } _ { i } , pmb { x } _ { j } )$ , where $m$ is a mean function and $mathcal { K }$ is a positive definite (Mercer) kernel. Since we assume this holds for any $M  >  0$ , this includes the case where $M = N + 1$ , containing $N$ training points ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and 1 test point $x _ { * }$ . Thus we can infer $f ( { pmb x } _ { * } )$ from knowledge of $f ( pmb { x } _ { 1 } ) , dotsc , f ( pmb { x } _ { n } )$ by manipulating the joint Gaussian distribution $p ( f ( pmb { x } _ { 1 } ) , ldots , f ( pmb { x } _ { N } ) , f ( pmb { x } _ { * } ) )$ , as we explain below. We can also extend this to work with the case where we observe noisy functions of $f ( { pmb x } _ { n } )$ , such as in regression or classification problems. \n17.2.1 Noise-free observations \nSuppose we observe a training set $mathcal { D } = { ( boldsymbol { x } _ { n } , y _ { n } ) : n = 1 : N }$ , where $y _ { n } = f ( pmb { x } _ { n } )$ is the noise-free observation of the function evaluated at ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . If we ask the GP to predict $f ( { pmb x } )$ for a value of $_ { x }$ that it has already seen, we want the GP to return the answer $f ( { pmb x } )$ with no uncertainty. In other words, it should act as an interpolator of the training data. \nNow we consider the case of predicting the outputs for new inputs that may not be in $mathcal { D }$ . Specifically, given a test set ${ mathbf { X } } _ { * }$ of size $N _ { * } times D$ , we want to predict the function outputs $pmb { f } _ { * } = [ f ( pmb { x } _ { 1 } ) , dots , f ( pmb { x } _ { N _ { * } } ) ]$ . By definition of the GP, the joint distribution $p ( { f } _ { X } , { f } _ { * } | mathbf { X } , mathbf { X } _ { * } )$ has the following form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Mercer kernels",
        "subsubsection": "Some popular Mercer kernels"
    },
    {
        "content": "Figure 17.6: A Gaussian process for $boldsymbol { mathcal { Z } }$ training points, ${ bf { x } } _ { 1 }$ and ${ bf { sigma } } ^ { x _ { 2 } }$ , and $^ { 1 }$ testing point, ${ pmb x } _ { ast }$ , represented as a graphical model representing $p ( { pmb y } , { pmb f } _ { X } | { bf X } ) = mathcal { N } ( { pmb f } _ { X } | m ( { bf X } ) , mathcal { K } ( { bf X } ) ) prod _ { i } p ( { pmb y }$ i|fi). The hidden nodes $f _ { i } = f ( pmb { x } _ { i } )$ represent the value of the function at each of the data points. Th ese hidden nodes are fully interconnected by undirected edges, forming a Gaussian graphical model; the edge strengths represent the covariance terms $Sigma _ { i j } = { cal K } ( { pmb x } _ { i } , { pmb x } _ { j } )$ . If the test point $^ { mathbf { nabla } _ { mathbf { x } } }$ is similar to the training points $pmb { x } _ { 1 }$ and $scriptstyle { pmb x } _ { 2 }$ , then the value of the hidden function $f _ { * }$ will be similar to $f _ { 1 }$ and $f _ { 2 }$ , and hence the predicted output $y _ { * }$ will be similar to the training values $_ { y _ { 1 } }$ and $_ { y _ { 2 } }$ . \n17.2 Gaussian processes \nIn this section, we discuss Gaussian processes, which is a way to define distributions over functions of the form $f : mathcal { X } to mathbb { R }$ , where $mathcal { X }$ is any domain. The key assumption is that the function values at a set of $M > 0$ inputs, $pmb { f } = [ f ( pmb { x } _ { 1 } ) , dots , f ( pmb { x } _ { M } ) ]$ , is jointly Gaussian, with mean ${ pmb mu } = m ( { pmb x } _ { 1 } ) , dots , m ( { pmb x } _ { M } ) )$ and covariance $pmb { Sigma } _ { i j } = mathcal { K } ( pmb { x } _ { i } , pmb { x } _ { j } )$ , where $m$ is a mean function and $mathcal { K }$ is a positive definite (Mercer) kernel. Since we assume this holds for any $M  >  0$ , this includes the case where $M = N + 1$ , containing $N$ training points ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and 1 test point $x _ { * }$ . Thus we can infer $f ( { pmb x } _ { * } )$ from knowledge of $f ( pmb { x } _ { 1 } ) , dotsc , f ( pmb { x } _ { n } )$ by manipulating the joint Gaussian distribution $p ( f ( pmb { x } _ { 1 } ) , ldots , f ( pmb { x } _ { N } ) , f ( pmb { x } _ { * } ) )$ , as we explain below. We can also extend this to work with the case where we observe noisy functions of $f ( { pmb x } _ { n } )$ , such as in regression or classification problems. \n17.2.1 Noise-free observations \nSuppose we observe a training set $mathcal { D } = { ( boldsymbol { x } _ { n } , y _ { n } ) : n = 1 : N }$ , where $y _ { n } = f ( pmb { x } _ { n } )$ is the noise-free observation of the function evaluated at ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . If we ask the GP to predict $f ( { pmb x } )$ for a value of $_ { x }$ that it has already seen, we want the GP to return the answer $f ( { pmb x } )$ with no uncertainty. In other words, it should act as an interpolator of the training data. \nNow we consider the case of predicting the outputs for new inputs that may not be in $mathcal { D }$ . Specifically, given a test set ${ mathbf { X } } _ { * }$ of size $N _ { * } times D$ , we want to predict the function outputs $pmb { f } _ { * } = [ f ( pmb { x } _ { 1 } ) , dots , f ( pmb { x } _ { N _ { * } } ) ]$ . By definition of the GP, the joint distribution $p ( { f } _ { X } , { f } _ { * } | mathbf { X } , mathbf { X } _ { * } )$ has the following form \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere ${ pmb mu } _ { X } = [ m ( { pmb x } _ { 1 } ) , dots , m ( { pmb x } _ { N _ { D } } ) ]$ , $pmb { mu } _ { * } = [ m ( pmb { x } _ { 1 } ^ { * } ) , dots , m ( pmb { x } _ { N _ { * } } ^ { * } ) ]$ , $mathbf { K } _ { X , X } = mathcal { K } ( mathbf { X } , mathbf { X } )$ is $N _ { mathcal { D } } times N _ { mathcal { D } }$ , ${ bf K } _ { X , * } = mathcal { K } ( { bf X } , { bf X } _ { * } )$ is $N _ { cal D } times N _ { ast }$ , and $mathbf { K } _ { * , * } = mathcal { K } ( mathbf { X } _ { * } , mathbf { X } _ { * } )$ is $N _ { * } times N _ { * }$ . See Figure 17.6 for an illustration. By the standard rules for conditioning Gaussians (Section 3.2.3), the posterior has the following form \nThis process is illustrated in Figure 17.7. On the left we show some samples from the prior, $p ( f )$ , where we use an RBF kernel (Section 17.1) and a zero mean function. On the right, we show samples from the posterior, $p ( f | mathcal { D } )$ . We see that the model perfectly interpolates the training data, and that the predictive uncertainty increases as we move further away from the observed data. \n17.2.2 Noisy observations \nNow let us consider the case where what we observe is a noisy version of the underlying function, $y _ { n } = f ( pmb { x } _ { n } ) + epsilon _ { n }$ , where $epsilon _ { n } sim mathcal { N } ( 0 , sigma _ { y } ^ { 2 } )$ . In this case, the model is not required to interpolate the data, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Noise-free observations"
    },
    {
        "content": "where ${ pmb mu } _ { X } = [ m ( { pmb x } _ { 1 } ) , dots , m ( { pmb x } _ { N _ { D } } ) ]$ , $pmb { mu } _ { * } = [ m ( pmb { x } _ { 1 } ^ { * } ) , dots , m ( pmb { x } _ { N _ { * } } ^ { * } ) ]$ , $mathbf { K } _ { X , X } = mathcal { K } ( mathbf { X } , mathbf { X } )$ is $N _ { mathcal { D } } times N _ { mathcal { D } }$ , ${ bf K } _ { X , * } = mathcal { K } ( { bf X } , { bf X } _ { * } )$ is $N _ { cal D } times N _ { ast }$ , and $mathbf { K } _ { * , * } = mathcal { K } ( mathbf { X } _ { * } , mathbf { X } _ { * } )$ is $N _ { * } times N _ { * }$ . See Figure 17.6 for an illustration. By the standard rules for conditioning Gaussians (Section 3.2.3), the posterior has the following form \nThis process is illustrated in Figure 17.7. On the left we show some samples from the prior, $p ( f )$ , where we use an RBF kernel (Section 17.1) and a zero mean function. On the right, we show samples from the posterior, $p ( f | mathcal { D } )$ . We see that the model perfectly interpolates the training data, and that the predictive uncertainty increases as we move further away from the observed data. \n17.2.2 Noisy observations \nNow let us consider the case where what we observe is a noisy version of the underlying function, $y _ { n } = f ( pmb { x } _ { n } ) + epsilon _ { n }$ , where $epsilon _ { n } sim mathcal { N } ( 0 , sigma _ { y } ^ { 2 } )$ . In this case, the model is not required to interpolate the data, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nbut it must come “close” to the observed data. The covariance of the observed noisy responses is \nwhere $delta _ { i j } = mathbb { I } left( i = j right)$ . In other words \nThe joint density of the observed data and the latent, noise-free function on the test points is given by \nHence the posterior predictive density at a set of test points ${ bf X } _ { * }$ is \nIn the case of a single test input, this simplifies as follows \nwhere $pmb { k } _ { * } = [ mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { 1 } ) , dots , mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { N } ) ]$ and $k _ { * * } = mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { * } )$ . If the mean function is zero, we can write the posterior mean as follows: \nThis is identical to the predictions from kernel ridge regression in Equation (17.108). \n17.2.3 Comparison to kernel regression \nIn Section 16.3.5, we discussed kernel regression, which is a generative approach to regression in which we approximate $p ( boldsymbol { y } , pmb { x } )$ using kernel density estimation. In particular, Equation (16.39) gives us \nThis is very similar to Equation (17.38). However, there are a few important differences. Firstly, in a GP, we use a positive definite (Mercer) kernel instead of a density kernel; Mercer kernels can be defined on structured objects, such as strings and graphs, which is harder to do for density kernels. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Noisy observations"
    },
    {
        "content": "but it must come “close” to the observed data. The covariance of the observed noisy responses is \nwhere $delta _ { i j } = mathbb { I } left( i = j right)$ . In other words \nThe joint density of the observed data and the latent, noise-free function on the test points is given by \nHence the posterior predictive density at a set of test points ${ bf X } _ { * }$ is \nIn the case of a single test input, this simplifies as follows \nwhere $pmb { k } _ { * } = [ mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { 1 } ) , dots , mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { N } ) ]$ and $k _ { * * } = mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { * } )$ . If the mean function is zero, we can write the posterior mean as follows: \nThis is identical to the predictions from kernel ridge regression in Equation (17.108). \n17.2.3 Comparison to kernel regression \nIn Section 16.3.5, we discussed kernel regression, which is a generative approach to regression in which we approximate $p ( boldsymbol { y } , pmb { x } )$ using kernel density estimation. In particular, Equation (16.39) gives us \nThis is very similar to Equation (17.38). However, there are a few important differences. Firstly, in a GP, we use a positive definite (Mercer) kernel instead of a density kernel; Mercer kernels can be defined on structured objects, such as strings and graphs, which is harder to do for density kernels. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nSecond, a GP is an interpolator (at least when $sigma ^ { 2 } = 0$ ), so $mathbb { E } left[ y | { pmb x } _ { n } , { pmb D } right] = y _ { n }$ . By contrast, kernel regression is not an interpolator (although it can be made into one by iteratively fitting the residuals, as in [KJ16]). Third, a GP is a Bayesian method, which means we can estimate hyperparameters (of the kernel) by maximizing the marginal likelihood; by contrast, in kernel regression we must use cross-validation to estimate the kernel parameters, such as the bandwidth. Fourth, computing the weights $w _ { n }$ for kernel regression takes $O ( N )$ time, where $N = | mathcal { D } |$ , whereas computing the weights $alpha _ { n }$ for GP regression takes $O ( N ^ { 3 } )$ time (although there are approximation methods that can reduce this to $O ( N M ^ { 2 } )$ , as we discuss in Section 17.2.9). \n17.2.4 Weight space vs function space \nIn this section, we show how Bayesian linear regression is a special case of a GP. \nConsider the linear regression model $y = f ( pmb { x } ) + epsilon$ , where $f ( pmb { x } ) = pmb { w } ^ { 1 } phi ( pmb { x } )$ and $epsilon sim mathcal { N } ( 0 , sigma _ { y } ^ { 2 } )$ . If we use a Gaussian prior $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , pmb { Sigma } _ { w } )$ , then the posterior is as follows (see Section 11.7.2 for the derivation): \nwhere $Phi$ is the $N times D$ design matrix, and \nThe posterior predictive distribution for $f _ { * } = f ( pmb { x } _ { * } )$ is therefore \nwhere $phi _ { * } = phi ( pmb { x } _ { * } )$ . This views the problem of inference and prediction in weight space. \nWe now show that this is equivalent to the predictions made by a GP using a kernel of the form $begin{array} { r } { mathcal { K } ( pmb { x } , pmb { x } ^ { prime } ) = phi ( pmb { x } ) ^ { 1 } pmb { Sigma } _ { w } phi ( pmb { x } ^ { prime } ) } end{array}$ . To see this, let $mathbf { K } = Phi pmb { Sigma } _ { w } pmb { Phi } ^ { mathsf { I } }$ , $pmb { k } _ { * } = pmb { Phi } pmb { Sigma } _ { w } pmb { phi } _ { * }$ , and $k _ { * * } = phi _ { * } ^ { 1 } pmb { Sigma } _ { w } phi _ { * }$ . Using this notation, and the matrix inversion lemma, we can rewrite Equation (17.43) as follows \nwhich matches the results in Equation (17.37), assuming $m ( { pmb x } ) = 0$ . (Non-zero mean can be captured by adding a constant feature with value $1$ to $phi ( { pmb x } )$ .) \nThus we can derive a GP from Bayesian linear regression. Note, however, that linear regression assumes $phi ( { pmb x } )$ is a finite length vector, whereas a GP allows us to work directly in terms of kernels, which may correspond to infinite length feature vectors (see Section 17.1.1). That is, a GP works in function space. \n17.2.5 Numerical issues \nIn this section, we discuss computational and numerical issues which arise when implementing the above equations. For notational simplicity, we assume the prior mean is zero, $m ( { pmb x } ) = 0$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Comparison to kernel regression"
    },
    {
        "content": "Second, a GP is an interpolator (at least when $sigma ^ { 2 } = 0$ ), so $mathbb { E } left[ y | { pmb x } _ { n } , { pmb D } right] = y _ { n }$ . By contrast, kernel regression is not an interpolator (although it can be made into one by iteratively fitting the residuals, as in [KJ16]). Third, a GP is a Bayesian method, which means we can estimate hyperparameters (of the kernel) by maximizing the marginal likelihood; by contrast, in kernel regression we must use cross-validation to estimate the kernel parameters, such as the bandwidth. Fourth, computing the weights $w _ { n }$ for kernel regression takes $O ( N )$ time, where $N = | mathcal { D } |$ , whereas computing the weights $alpha _ { n }$ for GP regression takes $O ( N ^ { 3 } )$ time (although there are approximation methods that can reduce this to $O ( N M ^ { 2 } )$ , as we discuss in Section 17.2.9). \n17.2.4 Weight space vs function space \nIn this section, we show how Bayesian linear regression is a special case of a GP. \nConsider the linear regression model $y = f ( pmb { x } ) + epsilon$ , where $f ( pmb { x } ) = pmb { w } ^ { 1 } phi ( pmb { x } )$ and $epsilon sim mathcal { N } ( 0 , sigma _ { y } ^ { 2 } )$ . If we use a Gaussian prior $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , pmb { Sigma } _ { w } )$ , then the posterior is as follows (see Section 11.7.2 for the derivation): \nwhere $Phi$ is the $N times D$ design matrix, and \nThe posterior predictive distribution for $f _ { * } = f ( pmb { x } _ { * } )$ is therefore \nwhere $phi _ { * } = phi ( pmb { x } _ { * } )$ . This views the problem of inference and prediction in weight space. \nWe now show that this is equivalent to the predictions made by a GP using a kernel of the form $begin{array} { r } { mathcal { K } ( pmb { x } , pmb { x } ^ { prime } ) = phi ( pmb { x } ) ^ { 1 } pmb { Sigma } _ { w } phi ( pmb { x } ^ { prime } ) } end{array}$ . To see this, let $mathbf { K } = Phi pmb { Sigma } _ { w } pmb { Phi } ^ { mathsf { I } }$ , $pmb { k } _ { * } = pmb { Phi } pmb { Sigma } _ { w } pmb { phi } _ { * }$ , and $k _ { * * } = phi _ { * } ^ { 1 } pmb { Sigma } _ { w } phi _ { * }$ . Using this notation, and the matrix inversion lemma, we can rewrite Equation (17.43) as follows \nwhich matches the results in Equation (17.37), assuming $m ( { pmb x } ) = 0$ . (Non-zero mean can be captured by adding a constant feature with value $1$ to $phi ( { pmb x } )$ .) \nThus we can derive a GP from Bayesian linear regression. Note, however, that linear regression assumes $phi ( { pmb x } )$ is a finite length vector, whereas a GP allows us to work directly in terms of kernels, which may correspond to infinite length feature vectors (see Section 17.1.1). That is, a GP works in function space. \n17.2.5 Numerical issues \nIn this section, we discuss computational and numerical issues which arise when implementing the above equations. For notational simplicity, we assume the prior mean is zero, $m ( { pmb x } ) = 0$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Weight space vs function space"
    },
    {
        "content": "Second, a GP is an interpolator (at least when $sigma ^ { 2 } = 0$ ), so $mathbb { E } left[ y | { pmb x } _ { n } , { pmb D } right] = y _ { n }$ . By contrast, kernel regression is not an interpolator (although it can be made into one by iteratively fitting the residuals, as in [KJ16]). Third, a GP is a Bayesian method, which means we can estimate hyperparameters (of the kernel) by maximizing the marginal likelihood; by contrast, in kernel regression we must use cross-validation to estimate the kernel parameters, such as the bandwidth. Fourth, computing the weights $w _ { n }$ for kernel regression takes $O ( N )$ time, where $N = | mathcal { D } |$ , whereas computing the weights $alpha _ { n }$ for GP regression takes $O ( N ^ { 3 } )$ time (although there are approximation methods that can reduce this to $O ( N M ^ { 2 } )$ , as we discuss in Section 17.2.9). \n17.2.4 Weight space vs function space \nIn this section, we show how Bayesian linear regression is a special case of a GP. \nConsider the linear regression model $y = f ( pmb { x } ) + epsilon$ , where $f ( pmb { x } ) = pmb { w } ^ { 1 } phi ( pmb { x } )$ and $epsilon sim mathcal { N } ( 0 , sigma _ { y } ^ { 2 } )$ . If we use a Gaussian prior $p ( pmb { w } ) = mathcal { N } ( pmb { w } | mathbf { 0 } , pmb { Sigma } _ { w } )$ , then the posterior is as follows (see Section 11.7.2 for the derivation): \nwhere $Phi$ is the $N times D$ design matrix, and \nThe posterior predictive distribution for $f _ { * } = f ( pmb { x } _ { * } )$ is therefore \nwhere $phi _ { * } = phi ( pmb { x } _ { * } )$ . This views the problem of inference and prediction in weight space. \nWe now show that this is equivalent to the predictions made by a GP using a kernel of the form $begin{array} { r } { mathcal { K } ( pmb { x } , pmb { x } ^ { prime } ) = phi ( pmb { x } ) ^ { 1 } pmb { Sigma } _ { w } phi ( pmb { x } ^ { prime } ) } end{array}$ . To see this, let $mathbf { K } = Phi pmb { Sigma } _ { w } pmb { Phi } ^ { mathsf { I } }$ , $pmb { k } _ { * } = pmb { Phi } pmb { Sigma } _ { w } pmb { phi } _ { * }$ , and $k _ { * * } = phi _ { * } ^ { 1 } pmb { Sigma } _ { w } phi _ { * }$ . Using this notation, and the matrix inversion lemma, we can rewrite Equation (17.43) as follows \nwhich matches the results in Equation (17.37), assuming $m ( { pmb x } ) = 0$ . (Non-zero mean can be captured by adding a constant feature with value $1$ to $phi ( { pmb x } )$ .) \nThus we can derive a GP from Bayesian linear regression. Note, however, that linear regression assumes $phi ( { pmb x } )$ is a finite length vector, whereas a GP allows us to work directly in terms of kernels, which may correspond to infinite length feature vectors (see Section 17.1.1). That is, a GP works in function space. \n17.2.5 Numerical issues \nIn this section, we discuss computational and numerical issues which arise when implementing the above equations. For notational simplicity, we assume the prior mean is zero, $m ( { pmb x } ) = 0$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe posterior predictive mean is given by $mu _ { * } = k _ { * } ^ { mathrm { I } } mathbf { K } _ { sigma } ^ { - 1 } pmb { y }$ . For reasons of numerical stability, it is unwise to directly invert ${ bf K } _ { sigma }$ . A more robust alternative is to compute a Cholesky decomposition, ${ bf K } _ { sigma } = { bf L L } ^ { sf I }$ , which takes $O ( N ^ { 3 } )$ time. Then we compute ${ pmb { alpha } } = { bf L } ^ { 1 } setminus ( { bf L } setminus { pmb { y } } )$ , where we have used the backslash operator to represent backsubstitution (Section 7.7.1). Given this, we can compute the posterior mean for each test case in $O ( N )$ time using \nWe can compute the variance in $O ( N ^ { 2 } )$ time for each test case using \nwhere ${ pmb v } = { bf L } backslash { pmb k } _ { * }$ . \nFinally, the log marginal likelihood (needed for kernel learning, Section 17.2.6) can be computed using \n17.2.6 Estimating the kernel \nMost kernels have some free parameters, which can have a large effect on the predictions from the model. For example, suppose we are performing 1d regression using a GP with an RBF kernel of the form \nHere $ell$ is the horizontal scale over which the function changes, $sigma _ { f } ^ { 2 }$ controls the vertical scale of the function. We assume observation noise with variance $sigma _ { y } ^ { 2 }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Numerical issues"
    },
    {
        "content": "The posterior predictive mean is given by $mu _ { * } = k _ { * } ^ { mathrm { I } } mathbf { K } _ { sigma } ^ { - 1 } pmb { y }$ . For reasons of numerical stability, it is unwise to directly invert ${ bf K } _ { sigma }$ . A more robust alternative is to compute a Cholesky decomposition, ${ bf K } _ { sigma } = { bf L L } ^ { sf I }$ , which takes $O ( N ^ { 3 } )$ time. Then we compute ${ pmb { alpha } } = { bf L } ^ { 1 } setminus ( { bf L } setminus { pmb { y } } )$ , where we have used the backslash operator to represent backsubstitution (Section 7.7.1). Given this, we can compute the posterior mean for each test case in $O ( N )$ time using \nWe can compute the variance in $O ( N ^ { 2 } )$ time for each test case using \nwhere ${ pmb v } = { bf L } backslash { pmb k } _ { * }$ . \nFinally, the log marginal likelihood (needed for kernel learning, Section 17.2.6) can be computed using \n17.2.6 Estimating the kernel \nMost kernels have some free parameters, which can have a large effect on the predictions from the model. For example, suppose we are performing 1d regression using a GP with an RBF kernel of the form \nHere $ell$ is the horizontal scale over which the function changes, $sigma _ { f } ^ { 2 }$ controls the vertical scale of the function. We assume observation noise with variance $sigma _ { y } ^ { 2 }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe sampled 20 observations from an MVN with a covariance given by $pmb { Sigma } = mathcal { K } ( x _ { i } , x _ { j } )$ for a grid of points ${ x _ { i } }$ , and added observation noise of value $sigma _ { y }$ . We then fit this data using a GP with the same kernel, but with a range of hyperparmameters. Figure 17.8 illustrates the effects of changing these parameters. In Figure 17.8(a), we use $( ell , sigma _ { f } , sigma _ { y } ) = ( 1 , 1 , 0 . 1 )$ , and the result is a good fit. In Figure 17.8(b), we increase the length scale to $ell = 3$ ; now the function looks overly smooth. \n17.2.6.1 Empirical Bayes \nTo estimate the kernel parameters $pmb theta$ (sometimes called hyperparameters), we could use exhaustive search over a discrete grid of values, with validation loss as an objective, but this can be quite slow. (This is the approach used by nonprobabilistic methods, such as SVMs (Section 17.3) to tune kernels.) Here we consider an empirical Bayes approach (Section 4.6.5.3), which will allow us to use gradient-based optimization methods, which are much faster. In particular, we will maximize the marginal likelihood \n(The reason it is called the marginal likelihood, rather than just likelihood, is because we have marginalized out the latent Gaussian vector $f$ .) \nFor notational simplicity, we assume the mean function is $0$ . Since $p ( f | mathbf { X } ) = { mathcal { N } } ( f | mathbf { 0 } , mathbf { K } )$ , and $begin{array} { r } { p ( { pmb y } | { pmb f } ) = prod _ { n = 1 } ^ { N } mathcal { N } ( y _ { n } | f _ { n } , sigma _ { y } ^ { 2 } ) } end{array}$ , the marginal likelihood is given by \nwhere the dependence of $mathbf { K } _ { sigma } = mathbf { K } _ { X , X } + sigma _ { 2 } ^ { 2 } mathbf { I } _ { N }$ on $pmb theta$ is implicit. The first term is a data fit term, the second term is a model complexity term, and the third term is just a constant. To understand the tradeoff between the first two terms, consider a SE kernel in 1D, as we vary the length scale $ell$ and hold $sigma _ { y } ^ { 2 }$ fixed. For short length scales, the fit will be good, so ${ pmb y } ^ { mathrm { | } } { bf K } _ { sigma } ^ { - 1 } { pmb y }$ will be small. However, the model complexity will be high: $mathbf { K }$ will be almost diagonal, (as in Figure 13.22, top right), since most points will not be considered “near” any others, so the $mathrm { l o g } left| { bf K } _ { sigma } right|$ term will be large. For long length scales, the fit will be poor but the model complexity will be low: $mathbf { K }$ will be almost all 1’s, (as in Figure 13.22, bottom right), so $log | mathbf { K } _ { sigma } |$ will be small. \nWe now discuss how to maximize the marginal likelihood. One can show that \nwhere $pmb { alpha } = mathbf { K } _ { sigma } ^ { - 1 } pmb { y }$ . It takes $O ( N ^ { 3 } )$ time to compute ${ bf K } _ { sigma } ^ { - 1 }$ , and then $O ( N ^ { 2 } )$ time per hyper-parameter to compute the gradient. \nThe form of $frac { partial mathbf { K } _ { sigma } } { partial theta _ { j } }$ depends on the form of the kernel, and which parameter we are taking derivatives with respect to. Often we have constraints on the hyper-parameters, such as $sigma _ { y } ^ { 2 } geq 0$ . In this case, we can define $theta = log ( sigma _ { y } ^ { 2 } )$ , and then use the chain rule. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nGiven an expression for the log marginal likelihood and its derivative, we can estimate the kernel parameters using any standard gradient-based optimizer. However, since the objective is not convex, local minima can be a problem, as we illustrate below, so we may need to use multiple restarts. \nAs an example, consider the RBF in Equation (17.50) with $sigma _ { f } ^ { 2 } = 1$ . In Figure 17.9(a), we plot $log p ( pmb { y } | mathbf { X } , ell , sigma _ { y } ^ { 2 } )$ (where $mathbf { X }$ and $pmb { y }$ are the 7 data points shown in panels b and c) as we vary $ell$ and $sigma _ { y } ^ { 2 }$ . The two local optima are indicated by $^ +$ . The bottom left optimum corresponds to a low-noise, short-length scale solution (shown in panel b). The top right optimum corresponds to a high-noise, long-length scale solution (shown in panel c). With only 7 data points, there is not enough evidence to confidently decide which is more reasonable, although the more complex model (panel b) has a marginal likelihood that is about $6 0 %$ higher than the simpler model (panel c). With more data, the more complex model would become even more preferred. \nFigure 17.9 illustrates some other interesting (and typical) features. The region where $sigma _ { y } ^ { 2 } approx 1$ (top of panel a) corresponds to the case where the noise is very high; in this regime, the marginal likelihood is insensitive to the length scale (indicated by the horizontal contours), since all the data is explained as noise. The region where $ell approx 0 . 5$ (left hand side of panel a) corresponds to the case where the length scale is very short; in this regime, the marginal likelihood is insensitive to the noise level (indicated by the vertical contours), since the data is perfectly interpolated. Neither of these regions would be chosen by a good optimizer. \n17.2.6.2 Bayesian inference \nWhen we have a small number of datapoints (e.g., when using GPs for Bayesian optimization), using a point estimate of the kernel parameters can give poor results [Bul11; WF14]. In such cases, we may wish to approximate the posterior over the kernel parameters. Several methods can be used. For example, [MA10] shows how to use slice sampling, [Hen+15] shows how to use Hamiltonian Monte Carlo, and [BBV11] shows how to use sequential Monte Carlo. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n17.2.7 GPs for classification \nSo far, we have focused on GPs for regression using Gaussian likelihoods. In this case, the posterior is also a GP, and all computation can be performed analytically. However, if the likelihood is non-Gaussian, such as the Bernoulli likelihood for binary classification, we can no longer compute the posterior exactly. \nThere are various approximations we can make. some of which we discuss in the sequel to this book, [Mur23]. In this section, we use the Hamiltonian Monte Carlo method (Section 4.6.8.4), both for the latent Gaussian function $f$ as well as the kernel hyperparameters $pmb theta$ . The basic idea is to specify the negative log joint \nWe then use autograd to compute $nabla _ { f } mathcal { E } ( f , theta )$ and $nabla _ { boldsymbol { theta } } mathcal { E } ( boldsymbol { f } , boldsymbol { theta } )$ , and use these gradients as inputs to a Gaussian proposal distribution. \nLet us consider a 1d example from [Mar18]. This is similar to the Bayesian logistic regression example from Figure 4.20, where the goal is to classify iris flowers as being Setosa or Versicolor, $y _ { n } in { 0 , 1 }$ , given information about the sepal length, $x _ { n }$ . We will use an SE kernel with length scale $ell$ . We put a $mathrm { G a ( 2 , 0 . 5 ) }$ prior on $ell$ . \nFigure 17.10a shows the results using the SE kernel. This is similar to the results of linear logistic regression (see Figure 4.20), except that at the edges (away from the data), the probability curves towards 0.5. This is because the prior mean function is $m ( x ) = 0$ , and $sigma ( 0 ) = 0 . 5$ . We can eliminate this artefact by using a more flexible kernel, which encodes the prior knowledge that we expect the output to be monotonically increasing or decreasing in the input. We can do this using a linear kernel, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Estimating the kernel"
    },
    {
        "content": "17.2.7 GPs for classification \nSo far, we have focused on GPs for regression using Gaussian likelihoods. In this case, the posterior is also a GP, and all computation can be performed analytically. However, if the likelihood is non-Gaussian, such as the Bernoulli likelihood for binary classification, we can no longer compute the posterior exactly. \nThere are various approximations we can make. some of which we discuss in the sequel to this book, [Mur23]. In this section, we use the Hamiltonian Monte Carlo method (Section 4.6.8.4), both for the latent Gaussian function $f$ as well as the kernel hyperparameters $pmb theta$ . The basic idea is to specify the negative log joint \nWe then use autograd to compute $nabla _ { f } mathcal { E } ( f , theta )$ and $nabla _ { boldsymbol { theta } } mathcal { E } ( boldsymbol { f } , boldsymbol { theta } )$ , and use these gradients as inputs to a Gaussian proposal distribution. \nLet us consider a 1d example from [Mar18]. This is similar to the Bayesian logistic regression example from Figure 4.20, where the goal is to classify iris flowers as being Setosa or Versicolor, $y _ { n } in { 0 , 1 }$ , given information about the sepal length, $x _ { n }$ . We will use an SE kernel with length scale $ell$ . We put a $mathrm { G a ( 2 , 0 . 5 ) }$ prior on $ell$ . \nFigure 17.10a shows the results using the SE kernel. This is similar to the results of linear logistic regression (see Figure 4.20), except that at the edges (away from the data), the probability curves towards 0.5. This is because the prior mean function is $m ( x ) = 0$ , and $sigma ( 0 ) = 0 . 5$ . We can eliminate this artefact by using a more flexible kernel, which encodes the prior knowledge that we expect the output to be monotonically increasing or decreasing in the input. We can do this using a linear kernel, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can scale and add this to the SE kernel to get \nThe results are shown in Figure 17.10b, and look more reasonable. \nOne might wonder why we bothered to use a GP, when the results are no better than a simple linear logistic regression model. The reason is that the GP is much more flexible, and makes fewer a priori assumptions, beyond smoothness. For example, suppose the data looked like Figure 17.11a. In this case, a linear logistic regression model could not fit the data. We could in principle use a neural network, but it may not work well since we only have 60 data points. However, GPs are well designed to handle the small sample setting. In Figure 17.11b, we show the results of fitting a GP with an SE kernel to this data. The results look reasonable. \n17.2.8 Connections with deep learning \nIt turns out that there are many interesting connections and similarities between GPs and deep neural networks. For example, one can show that a neural network with a single, infinitely wide layer of RBF units is equivalent to a GP with an RBF kernel. (This follows from the fact that the RBF kernel can be expressed as the inner product of an infinite number of features.) In fact, many kinds of DNNs (in the infinite limit) can be converted to an equivalent GP using a specific kind of kernel known as the neural tangent kernel [JGH18]. See the sequel to this book, [Mur23], for details. \n17.2.9 Scaling GPs to large datasets \nThe main disadvantage of GPs (and other kernel methods, such as SVMs, which we discuss in Section 17.3) is that inverting the $N times N$ kernel matrix takes $O ( N ^ { 3 } )$ time, making the method too \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "GPs for classification"
    },
    {
        "content": "We can scale and add this to the SE kernel to get \nThe results are shown in Figure 17.10b, and look more reasonable. \nOne might wonder why we bothered to use a GP, when the results are no better than a simple linear logistic regression model. The reason is that the GP is much more flexible, and makes fewer a priori assumptions, beyond smoothness. For example, suppose the data looked like Figure 17.11a. In this case, a linear logistic regression model could not fit the data. We could in principle use a neural network, but it may not work well since we only have 60 data points. However, GPs are well designed to handle the small sample setting. In Figure 17.11b, we show the results of fitting a GP with an SE kernel to this data. The results look reasonable. \n17.2.8 Connections with deep learning \nIt turns out that there are many interesting connections and similarities between GPs and deep neural networks. For example, one can show that a neural network with a single, infinitely wide layer of RBF units is equivalent to a GP with an RBF kernel. (This follows from the fact that the RBF kernel can be expressed as the inner product of an infinite number of features.) In fact, many kinds of DNNs (in the infinite limit) can be converted to an equivalent GP using a specific kind of kernel known as the neural tangent kernel [JGH18]. See the sequel to this book, [Mur23], for details. \n17.2.9 Scaling GPs to large datasets \nThe main disadvantage of GPs (and other kernel methods, such as SVMs, which we discuss in Section 17.3) is that inverting the $N times N$ kernel matrix takes $O ( N ^ { 3 } )$ time, making the method too \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Connections with deep learning"
    },
    {
        "content": "We can scale and add this to the SE kernel to get \nThe results are shown in Figure 17.10b, and look more reasonable. \nOne might wonder why we bothered to use a GP, when the results are no better than a simple linear logistic regression model. The reason is that the GP is much more flexible, and makes fewer a priori assumptions, beyond smoothness. For example, suppose the data looked like Figure 17.11a. In this case, a linear logistic regression model could not fit the data. We could in principle use a neural network, but it may not work well since we only have 60 data points. However, GPs are well designed to handle the small sample setting. In Figure 17.11b, we show the results of fitting a GP with an SE kernel to this data. The results look reasonable. \n17.2.8 Connections with deep learning \nIt turns out that there are many interesting connections and similarities between GPs and deep neural networks. For example, one can show that a neural network with a single, infinitely wide layer of RBF units is equivalent to a GP with an RBF kernel. (This follows from the fact that the RBF kernel can be expressed as the inner product of an infinite number of features.) In fact, many kinds of DNNs (in the infinite limit) can be converted to an equivalent GP using a specific kind of kernel known as the neural tangent kernel [JGH18]. See the sequel to this book, [Mur23], for details. \n17.2.9 Scaling GPs to large datasets \nThe main disadvantage of GPs (and other kernel methods, such as SVMs, which we discuss in Section 17.3) is that inverting the $N times N$ kernel matrix takes $O ( N ^ { 3 } )$ time, making the method too \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nslow for big datasets. Many different approximate schemes have been proposed to speedup GPs (see e.g., [Liu+18a] for a review). In this section, we briefly mention some of them. For more details, see the sequel to this book, [Mur23]. \n17.2.9.1 Sparse (inducing-point) approximations \nA simple approach to speeding up GP inference is to use less data. A better approach is to try to “summarize” the $N$ training points $mathbf { X }$ into $M ll N$ inducing points or pseudo inputs $mathbf { Z }$ . This lets us replace $p ( f | f _ { X } )$ with $p ( f | f _ { Z } )$ , where ${ pmb f } _ { X } = { f ( { pmb x } ) : { pmb x } in { pmb Z } }$ is the vector of observed function values at the training points, and ${ pmb f } _ { Z } = { f ( { pmb x } ) : { pmb x } in { pmb Z } }$ is the vector of estimated function values at the inducing points. By optimizing $( mathbf { Z } , f _ { Z } )$ we can learn to “compress” the training data $( mathbf { X } , f _ { X } )$ into a “bottleneck” $( mathbf { Z } , f _ { Z } )$ , thus speeding up computation from $O ( N ^ { 3 } )$ to $O ( M ^ { 3 } )$ . This is called a sparse GP. This whole process can be made rigorous using the framework of variational inference. For details, see the sequel to this book, [Mur23]. \n17.2.9.2 Exploiting parallelization and kernel matrix structure \nIt takes $O ( N ^ { 3 } )$ time to compute the Cholesky decomposition of $mathbf { K } _ { X , X }$ , which is needed to solve the linear system $mathbf { K } _ { sigma } mathbf { alpha } alpha = mathbf { mathscr { y } }$ and to compute $| mathbf { K } _ { X , X } |$ , where ${ mathbf K } _ { sigma } = { mathbf K } _ { X , X } + sigma ^ { 2 } { mathbf I } _ { N }$ . An alternative to Cholesky decomposition is to use linear algebra methods, often called Krylov subspace methods, which are based just on matrix vector multiplication or MVM. These approaches are often much faster, since they can naturally exploit structure in the kernel matrix. Moreover, even if the kernel matrix does not have special structure, matrix multiplies are trivial to parallelize, and can thus be greatly accelerated by GPUs, unlike Cholesky based methods which are largely sequential. This is the basis of the popular GPyTorch package [Gar+18]. For more details, see the sequel to this book, [Mur23]. \n17.2.9.3 Random feature approximation \nAlthough the power of kernels resides in the ability to avoid working with featurized representations of the inputs, such kernelized methods take $O ( N ^ { 3 } )$ time, in order to invert the Gram matrix $mathbf { K }$ . This can make it difficult to use such methods on large scale data. Fortunately, we can approximate the feature map for many (shift invariant) kernels using a randomly chosen finite set of $M$ basis functions, thus reducing the cost to $O ( N M + M ^ { 3 } )$ . We briefly discuss this idea below. For more details, see e.g., [Liu+20]. \nRandom features for RBF kernel \nWe will focus on the case of the Gaussian RBF kernel. One can show that \nwhere the (real-valued) feature vector is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $T = M / 2$ , and $pmb { Omega } in mathbb { R } ^ { T times D }$ is a random Gaussian matrix, where the entries are sampled iid from $mathcal { N } ( 0 , 1 / sigma ^ { 2 } )$ , where $sigma$ is the kernel bandwidth. The bias of the approximation decreases as we increase $M$ . In practice, we use a finite $M$ , and compute a single sample Monte Carlo approximation to the expectation by drawing a single random matrix. The features in Equation (17.60) are called random Fourier features (RFF) [RR08] or “weighted sums of random kitchen sinks” [RR09]. \n\nWe can also use positive random features, rather than trigonometric random features, which can be preferable in some applications, such as models which use attention (see Section 15.6.4). In particular, we can use \nwhere $omega _ { m }$ are sampled as before. For details, see [Cho+20b]. \nRegardless of whether we use trigonometric or positive features, we can obtain a lower variance estimate by ensuring that the rows of $mathbf { Z }$ are random but orthogonal; these are called orthogonal random features. Such sampling can be conducted efficiently via Gram-Schmidt orthogonalization of the unstructured Gaussian matrices [Yu+16], or several approximations that are even faster (see [CRW17; Cho+19]). \nFastfood approximation \nUnfortunately, storing the random matrix $Omega$ takes $O ( D M )$ space, and computing $Omega x$ takes $O ( D M )$ time, where $D$ is the input dimensionality, and $M$ is the number of random features. This can be prohibitive if $M gg D$ , which it may need to be in order to get any benefits over using the original set of features. Fortunately, we can use the fast Hadamard transform to reduce the memory from $O ( M D )$ to $O ( M )$ , and reduce the time from $O ( M D )$ to $O ( M log D )$ . This approach has been called fastfood [LSS13], a reference to the original term “kitchen sinks”. \nExtreme learning machines \nWe can use the random features approximation to the kernel to convert a GP into a linear model of the form \nwhere $h ( a ) = sqrt { 1 / M } [ sin ( a ) , cos ( a ) ]$ for RBF kernels. This is equivalent to a one-layer MLP with random (and fixed) input-to-hidden weights. When $M > N$ , this corresponds to an over-parameterized model, which can perfectly interpolate the training data. \nIn [Cur+17], they apply this method to fit a logistic regression model of the form $f ( { pmb x } ; { pmb theta } ) =$ $mathbf { W } ^ { top } h ( widehat { mathbf { Z } } mathbf { x } ) + b$ using SGD; they call the resulting method “McKernel”. We can also optimize $mathbf { Z }$ as well as $mathbf { W }$ , as discussed in [Alb+17], although now the problem is no longer convex. \nAlternatively, we can use $M < N$ , but stack many such random nonlinear layers together, and just optimize the output weights. This has been called an extreme learning machine or ELM (see e.g., [Hua14]), although this work is controversial.1 \n17.3 Support vector machines (SVMs) \nIn this section, we discuss a form of (non-probabilistic) predictors for classification and regression problems which have the form \nBy adding suitable constraints, we can ensure that many of the $alpha _ { i }$ coefficients are $0$ , so that predictions at test time only depend on a subset of the training points. The surving points are called “support vectors”, and the resulting model is called a support vector machine or SVM. We give a brief summary below. More details, can be found in e.g., [VGS97; SS01], \n17.3.1 Large margin classifiers \nConsider a binary classifier of the form $h ( { pmb x } ) = mathrm { s i g n } ( f ( { pmb x } ) )$ , where the decision boundary is given by the following linear function: \n(In the SVM literature, it is common to assume the class labels are $- 1$ and $+ 1$ , rather than 0 and $1$ . To avoid confusion, we denote such target labels by $tilde { y }$ rather than $y$ .) There may be many lines that separate the data. However, intuitively we would like to pick the one that has maximum margin, which is the distance of the closest point to the decision boundary, since this will give us the most robust solution. This idea is illustrated in Figure 17.12: the solution on the left has larger margin than the one on the right, and intuitively is is better, since it will be less sensitive to perturbations of the data. \nHow can we compute such a large margin classifier? First we need to derive an expression for the distance of a point to the decision boundary. Referring to Figure 17.13(a), we see that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Gaussian processes",
        "subsubsection": "Scaling GPs to large datasets"
    },
    {
        "content": "17.3 Support vector machines (SVMs) \nIn this section, we discuss a form of (non-probabilistic) predictors for classification and regression problems which have the form \nBy adding suitable constraints, we can ensure that many of the $alpha _ { i }$ coefficients are $0$ , so that predictions at test time only depend on a subset of the training points. The surving points are called “support vectors”, and the resulting model is called a support vector machine or SVM. We give a brief summary below. More details, can be found in e.g., [VGS97; SS01], \n17.3.1 Large margin classifiers \nConsider a binary classifier of the form $h ( { pmb x } ) = mathrm { s i g n } ( f ( { pmb x } ) )$ , where the decision boundary is given by the following linear function: \n(In the SVM literature, it is common to assume the class labels are $- 1$ and $+ 1$ , rather than 0 and $1$ . To avoid confusion, we denote such target labels by $tilde { y }$ rather than $y$ .) There may be many lines that separate the data. However, intuitively we would like to pick the one that has maximum margin, which is the distance of the closest point to the decision boundary, since this will give us the most robust solution. This idea is illustrated in Figure 17.12: the solution on the left has larger margin than the one on the right, and intuitively is is better, since it will be less sensitive to perturbations of the data. \nHow can we compute such a large margin classifier? First we need to derive an expression for the distance of a point to the decision boundary. Referring to Figure 17.13(a), we see that \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere $r$ is the distance of $_ { x }$ from the decision boundary whose normal vector is $mathbf { boldsymbol { w } }$ , and ${ pmb x } _ { bot }$ is the orthogonal projection of $_ { x }$ onto this boundary. \nWe would like to maximize $r$ , so we need to express it as a function of $mathbf { boldsymbol { w } }$ . First, note that \nSince $0 = f ( pmb { x } _ { bot } ) = pmb { w } ^ { top } pmb { x } _ { bot } + w _ { 0 }$ , we have $f ( pmb { x } ) = r | | pmb { w } | |$ and hence $begin{array} { r } { r = frac { f ( pmb { x } ) } { | | pmb { w } | | } } end{array}$ \nSince we want to ensure each point is on the correct side of the boundary, we also require $f ( { pmb x } _ { n } ) { widetilde y } _ { n } > 0$ . We want to maximize the distance of the closest point, so our final objective becomes \nNote that by rescaling the parameters using ${ pmb w }  k { pmb w }$ and $w _ { 0 } to k w _ { 0 }$ , we do not change the distance of any point to the boundary, since the $k$ factor cancels out when we divide by $| | pmb { w } | |$ . Therefore let \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nus define the scale factor such that $tilde { y } _ { n } f _ { n } = 1$ for the point that is closest to the decision boundary. Hence we require $tilde { y } _ { n } f _ { n } ge 1$ for all $n$ . Finally, note that maximizing $1 / | | w | |$ is equivalent to minimizing $| | boldsymbol { w } | | ^ { 2 }$ . Thus we get the new objective \n(The factor of $textstyle { frac { 1 } { 2 } }$ is added for convenience and doesn’t affect the optimal parameters.) The constraint says that we want all points to be on the correct side of the decision boundary with a margin of at least 1. \nNote that it is important to scale the input variables before using an SVM, otherwise the margin measures distance of a point to the boundary using all input dimensions equally. See Figure 17.14 for an illustration. \n17.3.2 The dual problem \nThe objective in Equation (17.68) is a standard quadratic programming problem (Section 8.5.4), since we have a quadratic objective subject to linear constraints. This has $N + D + 1$ variables subject to $N$ constraints, and is known as a primal problem. \nIn convex optimization, for every primal problem we can derive a dual problem. Let ${ pmb { alpha } } in mathbb { R } ^ { N }$ be the dual variables, corresponding to Lagrange multipliers that enforce the $N$ inequality constraints. The generalized Lagrangian is given below (see Section 8.5.2 for relevant background information on constrained optimization): \nTo optimize this, we must find a stationary point that satisfies \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "Large margin classifiers"
    },
    {
        "content": "us define the scale factor such that $tilde { y } _ { n } f _ { n } = 1$ for the point that is closest to the decision boundary. Hence we require $tilde { y } _ { n } f _ { n } ge 1$ for all $n$ . Finally, note that maximizing $1 / | | w | |$ is equivalent to minimizing $| | boldsymbol { w } | | ^ { 2 }$ . Thus we get the new objective \n(The factor of $textstyle { frac { 1 } { 2 } }$ is added for convenience and doesn’t affect the optimal parameters.) The constraint says that we want all points to be on the correct side of the decision boundary with a margin of at least 1. \nNote that it is important to scale the input variables before using an SVM, otherwise the margin measures distance of a point to the boundary using all input dimensions equally. See Figure 17.14 for an illustration. \n17.3.2 The dual problem \nThe objective in Equation (17.68) is a standard quadratic programming problem (Section 8.5.4), since we have a quadratic objective subject to linear constraints. This has $N + D + 1$ variables subject to $N$ constraints, and is known as a primal problem. \nIn convex optimization, for every primal problem we can derive a dual problem. Let ${ pmb { alpha } } in mathbb { R } ^ { N }$ be the dual variables, corresponding to Lagrange multipliers that enforce the $N$ inequality constraints. The generalized Lagrangian is given below (see Section 8.5.2 for relevant background information on constrained optimization): \nTo optimize this, we must find a stationary point that satisfies \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can do this by computing the partial derivatives wrt $mathbf { boldsymbol { w } }$ and $w _ { 0 }$ and setting to zero. We have \nand hence \nPlugging these into the Lagrangian yields the following \nconstraints that This is called the dual form of the objective. We want to maximize this wrt $begin{array} { r } { sum _ { n = 1 } ^ { N } alpha _ { n } tilde { y } _ { n } = 0 } end{array}$ and $0 leq alpha _ { n }$ for $n = 1 : N$ . $alpha$ subject to the \nThe above ob ective is a quadratic problem in $N$ variables. Standard QP solvers take $O ( N ^ { 3 } )$ time. However, specialized algorithms, which avoid the use of generic QP solvers, have been developed for this problem, such as the sequential minimal optimization or SMO algorithm [Pla98], which takes $O ( N )$ to $O ( N ^ { 2 } )$ time. \nSince this is a convex objective, the solution must satisfy the KKT conditions (Section 8.5.2), which tell us that the following properties hold: \nHence either $alpha _ { n } = 0$ (in which case example $n$ is ignored when computing $hat { textbf { textit { w } } }$ ) or the constraint $tilde { y } _ { n } ( hat { w } ^ { top } x _ { n } + hat { w } _ { 0 } ) = 1$ is active. This latter condition means that example $n$ lies on the decision boundary; these points are known as the support vectors, as shown in Figure 17.13(b). We denote the set of support vectors by $boldsymbol { S }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo perform prediction, we use \nTo solve for $hat { w } _ { 0 }$ we can use the fact that for any support vector, we have $tilde { y } _ { n } f ( pmb { x } ; hat { pmb { w } } , hat { w } _ { 0 } ) = 1$ . Multiplying both sides by $tilde { y } _ { n }$ , and exploiting the fact that $tilde { y } _ { n } ^ { 2 } = 1$ , we get $hat { w } _ { 0 } = tilde { y } _ { n } - hat { w } ^ { prime } x _ { n }$ . In practice we get better results by averaging over all the support vectors to get \n17.3.3 Soft margin classifiers \nIf the data is not linearly separable, there will be no feasible solution in which $tilde { y } _ { n } f _ { n } ge 1$ for all $n$ . We therefore introduce slack variables $xi _ { n } geq 0$ and replace the hard constraints that $tilde { y } _ { n } f _ { n } ge 0$ with the soft margin constraints that $tilde { y } _ { n } f _ { n } geq 1 - xi _ { n }$ . The new objective becomes \nwhere $C geq 0$ is a hyper parameter controlling how many points we allow to violate the margin constraint. (If $C = infty$ , we recover the unregularized, hard-margin classifier.) \nThe corresponding Lagrangian for the soft margin classifier becomes \nwhere $alpha _ { n } geq 0$ and $mu _ { n } geq 0$ are the Lagrange multipliers. Optimizing out $mathbf { boldsymbol { w } }$ , $w _ { 0 }$ and $xi$ gives the dual form \nThis is identical to the hard margin case; however, the constraints are different. In particular, the KKT conditions imply \nIf $alpha _ { n } = 0$ , the point is ignored. If $0 < alpha _ { n } < C$ then $xi _ { n } = 0$ , so the point lies on the margin. If $alpha _ { n } = C$ , the point can lie inside the margin, and can either be correctly classified if $xi _ { n } leq 1$ , or misclassified if $xi _ { n } > 1$ . See Figure 17.13(b) for an illustration. Hence $textstyle sum _ { n } xi _ { n }$ is an upper bound on the number of misclassified points. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "The dual problem"
    },
    {
        "content": "To perform prediction, we use \nTo solve for $hat { w } _ { 0 }$ we can use the fact that for any support vector, we have $tilde { y } _ { n } f ( pmb { x } ; hat { pmb { w } } , hat { w } _ { 0 } ) = 1$ . Multiplying both sides by $tilde { y } _ { n }$ , and exploiting the fact that $tilde { y } _ { n } ^ { 2 } = 1$ , we get $hat { w } _ { 0 } = tilde { y } _ { n } - hat { w } ^ { prime } x _ { n }$ . In practice we get better results by averaging over all the support vectors to get \n17.3.3 Soft margin classifiers \nIf the data is not linearly separable, there will be no feasible solution in which $tilde { y } _ { n } f _ { n } ge 1$ for all $n$ . We therefore introduce slack variables $xi _ { n } geq 0$ and replace the hard constraints that $tilde { y } _ { n } f _ { n } ge 0$ with the soft margin constraints that $tilde { y } _ { n } f _ { n } geq 1 - xi _ { n }$ . The new objective becomes \nwhere $C geq 0$ is a hyper parameter controlling how many points we allow to violate the margin constraint. (If $C = infty$ , we recover the unregularized, hard-margin classifier.) \nThe corresponding Lagrangian for the soft margin classifier becomes \nwhere $alpha _ { n } geq 0$ and $mu _ { n } geq 0$ are the Lagrange multipliers. Optimizing out $mathbf { boldsymbol { w } }$ , $w _ { 0 }$ and $xi$ gives the dual form \nThis is identical to the hard margin case; however, the constraints are different. In particular, the KKT conditions imply \nIf $alpha _ { n } = 0$ , the point is ignored. If $0 < alpha _ { n } < C$ then $xi _ { n } = 0$ , so the point lies on the margin. If $alpha _ { n } = C$ , the point can lie inside the margin, and can either be correctly classified if $xi _ { n } leq 1$ , or misclassified if $xi _ { n } > 1$ . See Figure 17.13(b) for an illustration. Hence $textstyle sum _ { n } xi _ { n }$ is an upper bound on the number of misclassified points. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nAs before, the bias term can be computed using \nwhere $mathcal { M }$ is the set of points having $0 < alpha _ { n } < C$ . \nThere is an alternative formulation of the soft margin SVM known as the $nu$ -SVM classifier [Sch+00]. This involves maximizing \nsubject to the constraints that \nThis has the advantage that the parameter $nu$ , which replaces $C$ , can be interpreted as an upper bound on the fraction of margin errors (points for which $xi _ { n } > 0$ ), as well as a lower bound on the number of support vectors. \n17.3.4 The kernel trick \nSo far we have converted the large margin binary classification problem into a dual problem in $N$ unknowns $( alpha )$ which (in general) takes $O ( N ^ { 3 } )$ time to solve, which can be slow. However, the principal benefit of the dual problem is that we can replace all inner product operations ${ pmb x } ^ { 1 } { pmb x } ^ { prime }$ with a call to a positive definite (Mercer) kernel function, $kappa ( pmb { x } , pmb { x } ^ { prime } )$ . This is called the kernel trick. \nIn particular, we can rewrite the prediction function in Equation (17.81) as follows: \nWe also need to kernelize the bias term. This can be done by kernelizing Equation (17.82) as follows: \nThe kernel trick allows us to avoid having to deal with an explicit feature representation of our data, and allows us to easily apply classifiers to structured objects, such as strings and graphs. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "Soft margin classifiers"
    },
    {
        "content": "As before, the bias term can be computed using \nwhere $mathcal { M }$ is the set of points having $0 < alpha _ { n } < C$ . \nThere is an alternative formulation of the soft margin SVM known as the $nu$ -SVM classifier [Sch+00]. This involves maximizing \nsubject to the constraints that \nThis has the advantage that the parameter $nu$ , which replaces $C$ , can be interpreted as an upper bound on the fraction of margin errors (points for which $xi _ { n } > 0$ ), as well as a lower bound on the number of support vectors. \n17.3.4 The kernel trick \nSo far we have converted the large margin binary classification problem into a dual problem in $N$ unknowns $( alpha )$ which (in general) takes $O ( N ^ { 3 } )$ time to solve, which can be slow. However, the principal benefit of the dual problem is that we can replace all inner product operations ${ pmb x } ^ { 1 } { pmb x } ^ { prime }$ with a call to a positive definite (Mercer) kernel function, $kappa ( pmb { x } , pmb { x } ^ { prime } )$ . This is called the kernel trick. \nIn particular, we can rewrite the prediction function in Equation (17.81) as follows: \nWe also need to kernelize the bias term. This can be done by kernelizing Equation (17.82) as follows: \nThe kernel trick allows us to avoid having to deal with an explicit feature representation of our data, and allows us to easily apply classifiers to structured objects, such as strings and graphs. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n17.3.5 Converting SVM outputs into probabilities \nAn SVM classifier produces a hard-labeling, ${ hat { y } } ( pmb { x } ) = operatorname { s i g n } ( f ( pmb { x } ) )$ . However, we often want a measure .ceWine ocaurn tphredn ctoinovne. tOtnhe hoeutrpiust cofaapnprSoVacMh itso taopirnotebrapbrileit $f ( { pmb x } )$ nags the log-odds ratio, $log { frac { p ( y = 1 | mathbf { x } ) } { p ( y = 0 | mathbf { x } ) } }$ \nwhere $a$ , $b$ can be estimated by maximum likelihood on a separate validation set. (Using the training set to estimate $a$ and $b$ leads to severe overfitting.) This technique was first proposed in [Pla00], and is known as Platt scaling. \nHowever, the resulting probabilities are not particularly well calibrated, since there is nothing in the SVM training procedure that justifies interpreting $f ( { pmb x } )$ as a log-odds ratio. To illustrate this, consider an example from [Tip01]. Suppose we have 1d data where $p ( x | y = 0 ) = mathrm { U n i f } ( 0 , 1 )$ and $p ( x | y = 1 ) = mathrm { U n i f } ( 0 . 5 , 1 . 5 )$ . Since the class-conditional distributions overlap in the [0.5, 1] range, the log-odds of class 1 over class 0 should be zero in this region, and infinite outside this region. We sampled 1000 points from the model, and then fit a probabilistic kernel classifier (an RVM, described in Section 17.4.1) and an SVM with a Gaussian kernel of width 0.1. Both models can perfectly capture the decision boundary, and achieve a generalization error of $2 5 %$ , which is Bayes optimal in this problem. The probabilistic output from the RVM is a good approximation to the true log-odds, but this is not the case for the SVM, as shown in Figure 17.15. \n17.3.6 Connection with logistic regression \nWe have seen that data points that are on the correct side of the decision boundary have $xi _ { n } = 0$ ; for the others, we have $xi _ { n } = 1 - tilde { y } _ { n } f ( { pmb x } _ { n } )$ . Therefore we can rewrite the objective in Equation (17.83) as follows: \nwhere $lambda = ( 2 C ) ^ { - 1 }$ and $ell _ { mathrm { h i n g e } } ( y , eta )$ is the hinge loss function defined by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "The kernel trick"
    },
    {
        "content": "17.3.5 Converting SVM outputs into probabilities \nAn SVM classifier produces a hard-labeling, ${ hat { y } } ( pmb { x } ) = operatorname { s i g n } ( f ( pmb { x } ) )$ . However, we often want a measure .ceWine ocaurn tphredn ctoinovne. tOtnhe hoeutrpiust cofaapnprSoVacMh itso taopirnotebrapbrileit $f ( { pmb x } )$ nags the log-odds ratio, $log { frac { p ( y = 1 | mathbf { x } ) } { p ( y = 0 | mathbf { x } ) } }$ \nwhere $a$ , $b$ can be estimated by maximum likelihood on a separate validation set. (Using the training set to estimate $a$ and $b$ leads to severe overfitting.) This technique was first proposed in [Pla00], and is known as Platt scaling. \nHowever, the resulting probabilities are not particularly well calibrated, since there is nothing in the SVM training procedure that justifies interpreting $f ( { pmb x } )$ as a log-odds ratio. To illustrate this, consider an example from [Tip01]. Suppose we have 1d data where $p ( x | y = 0 ) = mathrm { U n i f } ( 0 , 1 )$ and $p ( x | y = 1 ) = mathrm { U n i f } ( 0 . 5 , 1 . 5 )$ . Since the class-conditional distributions overlap in the [0.5, 1] range, the log-odds of class 1 over class 0 should be zero in this region, and infinite outside this region. We sampled 1000 points from the model, and then fit a probabilistic kernel classifier (an RVM, described in Section 17.4.1) and an SVM with a Gaussian kernel of width 0.1. Both models can perfectly capture the decision boundary, and achieve a generalization error of $2 5 %$ , which is Bayes optimal in this problem. The probabilistic output from the RVM is a good approximation to the true log-odds, but this is not the case for the SVM, as shown in Figure 17.15. \n17.3.6 Connection with logistic regression \nWe have seen that data points that are on the correct side of the decision boundary have $xi _ { n } = 0$ ; for the others, we have $xi _ { n } = 1 - tilde { y } _ { n } f ( { pmb x } _ { n } )$ . Therefore we can rewrite the objective in Equation (17.83) as follows: \nwhere $lambda = ( 2 C ) ^ { - 1 }$ and $ell _ { mathrm { h i n g e } } ( y , eta )$ is the hinge loss function defined by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "Converting SVM outputs into probabilities"
    },
    {
        "content": "17.3.5 Converting SVM outputs into probabilities \nAn SVM classifier produces a hard-labeling, ${ hat { y } } ( pmb { x } ) = operatorname { s i g n } ( f ( pmb { x } ) )$ . However, we often want a measure .ceWine ocaurn tphredn ctoinovne. tOtnhe hoeutrpiust cofaapnprSoVacMh itso taopirnotebrapbrileit $f ( { pmb x } )$ nags the log-odds ratio, $log { frac { p ( y = 1 | mathbf { x } ) } { p ( y = 0 | mathbf { x } ) } }$ \nwhere $a$ , $b$ can be estimated by maximum likelihood on a separate validation set. (Using the training set to estimate $a$ and $b$ leads to severe overfitting.) This technique was first proposed in [Pla00], and is known as Platt scaling. \nHowever, the resulting probabilities are not particularly well calibrated, since there is nothing in the SVM training procedure that justifies interpreting $f ( { pmb x } )$ as a log-odds ratio. To illustrate this, consider an example from [Tip01]. Suppose we have 1d data where $p ( x | y = 0 ) = mathrm { U n i f } ( 0 , 1 )$ and $p ( x | y = 1 ) = mathrm { U n i f } ( 0 . 5 , 1 . 5 )$ . Since the class-conditional distributions overlap in the [0.5, 1] range, the log-odds of class 1 over class 0 should be zero in this region, and infinite outside this region. We sampled 1000 points from the model, and then fit a probabilistic kernel classifier (an RVM, described in Section 17.4.1) and an SVM with a Gaussian kernel of width 0.1. Both models can perfectly capture the decision boundary, and achieve a generalization error of $2 5 %$ , which is Bayes optimal in this problem. The probabilistic output from the RVM is a good approximation to the true log-odds, but this is not the case for the SVM, as shown in Figure 17.15. \n17.3.6 Connection with logistic regression \nWe have seen that data points that are on the correct side of the decision boundary have $xi _ { n } = 0$ ; for the others, we have $xi _ { n } = 1 - tilde { y } _ { n } f ( { pmb x } _ { n } )$ . Therefore we can rewrite the objective in Equation (17.83) as follows: \nwhere $lambda = ( 2 C ) ^ { - 1 }$ and $ell _ { mathrm { h i n g e } } ( y , eta )$ is the hinge loss function defined by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nAs we see from Figure 4.2, this is a convex, piecewise differentiable upper bound to the 0-1 loss, that has the shape of a partially open door hinge. \nBy contrast, (penalized) logistic regression optimizes \nwhere the log loss is given by \nThis is also plotted in Figure 4.2. We see that it is similar to the hinge loss, but with two important differences. First the hinge loss is piecewise linear, so we cannot use regular gradient methods to optimize it. (We can, however, compute the subgradient at $tilde { y } eta = 1$ .) Second, the hinge loss has a region where it is strictly 0; this results in sparse estimates. \nWe see that both functions are convex upper bounds on the 0-1 loss, which is given by \nThese upper bounds are easier to optimize and can be viewed as surrogates for the 0-1 loss. See Section 4.3.2 for details. \n17.3.7 Multi-class classification with SVMs \nSVMs are inherently a binary classifier. One way to convert them to a multi-class classification model is to train $C$ binary classifiers, where the data from class $c$ is treated as positive, and the data from all the other classes is treated as negative. We then use the rule ${ hat { y } } ( pmb { x } ) = arg operatorname* { m a x } _ { c } f _ { c } ( pmb { x } )$ to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 predict the final label, where $begin{array} { r } { f _ { c } ( pmb { x } ) = log frac { p ( c = 1 | pmb { x } ) } { p ( c = 0 | pmb { x } ) } } end{array}$ is the score given by classifier $c$ . This is known as the one-versus-the-rest approach (also called one-vs-all).",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "Connection with logistic regression"
    },
    {
        "content": "As we see from Figure 4.2, this is a convex, piecewise differentiable upper bound to the 0-1 loss, that has the shape of a partially open door hinge. \nBy contrast, (penalized) logistic regression optimizes \nwhere the log loss is given by \nThis is also plotted in Figure 4.2. We see that it is similar to the hinge loss, but with two important differences. First the hinge loss is piecewise linear, so we cannot use regular gradient methods to optimize it. (We can, however, compute the subgradient at $tilde { y } eta = 1$ .) Second, the hinge loss has a region where it is strictly 0; this results in sparse estimates. \nWe see that both functions are convex upper bounds on the 0-1 loss, which is given by \nThese upper bounds are easier to optimize and can be viewed as surrogates for the 0-1 loss. See Section 4.3.2 for details. \n17.3.7 Multi-class classification with SVMs \nSVMs are inherently a binary classifier. One way to convert them to a multi-class classification model is to train $C$ binary classifiers, where the data from class $c$ is treated as positive, and the data from all the other classes is treated as negative. We then use the rule ${ hat { y } } ( pmb { x } ) = arg operatorname* { m a x } _ { c } f _ { c } ( pmb { x } )$ to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 predict the final label, where $begin{array} { r } { f _ { c } ( pmb { x } ) = log frac { p ( c = 1 | pmb { x } ) } { p ( c = 0 | pmb { x } ) } } end{array}$ is the score given by classifier $c$ . This is known as the one-versus-the-rest approach (also called one-vs-all). \n\nUnfortunately, this approach has several problems. First, it can result in regions of input space which are ambiguously labeled. For example, the green region at the top of Figure 17.16(a) is predicted to be both class 2 and class 1. A second problem is that the magnitude of the $f _ { c }$ ’s scores are not calibrated with each other, so it is hard to compare them. Finally, each binary subproblem is likely to suffer from the class imbalance problem (Section 10.3.8.2). For example, suppose we have 10 equally represented classes. When training $f _ { 1 }$ , we will have 10% positive examples and 90% negative examples, which can hurt performance. \nAnother approach is to use the one-versus-one or OVO approach, also called all pairs, in which we train $C ( C - 1 ) / 2$ classifiers to discriminate all pairs $f _ { c , c ^ { prime } }$ . We then classify a point into the class which has the highest number of votes. However, this can also result in ambiguities, as shown in Figure 17.16(b). Also, this requires fitting $O ( C ^ { 2 } )$ models. \n17.3.8 How to choose the regularizer $C$ \nSVMs require that you specify the kernel function and the parameter $C$ . Typically $C$ is chosen by cross-validation. Note, however, that $C$ interacts quite strongly with the kernel parameters. For example, suppose we are using an RBF kernel with precision $textstyle gamma = { frac { 1 } { 2 sigma ^ { 2 } } }$ . If $gamma$ is large, corresponding to narrow kernels, we may need heavy regularization, and hence small $C$ . If is small, a larger value of $gamma$ $C$ should be used. So we see that $gamma$ and $C$ are tightly coupled, as illustrated in Figure 17.17. \nThe authors of libsvm [HCL09] recommend using CV over a 2d grid with values $C in { 2 ^ { - 5 } , 2 ^ { - 3 } , dots , 2 ^ { 1 5 } }$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license and $gamma in { 2 ^ { - 1 5 } , 2 ^ { - 1 3 } , . . . , 2 ^ { 3 } }$ . See Figure 17.18 which shows the CV estimate of the 0-1 risk as a function of $C$ and $gamma$ .",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "Multi-class classification with SVMs"
    },
    {
        "content": "Unfortunately, this approach has several problems. First, it can result in regions of input space which are ambiguously labeled. For example, the green region at the top of Figure 17.16(a) is predicted to be both class 2 and class 1. A second problem is that the magnitude of the $f _ { c }$ ’s scores are not calibrated with each other, so it is hard to compare them. Finally, each binary subproblem is likely to suffer from the class imbalance problem (Section 10.3.8.2). For example, suppose we have 10 equally represented classes. When training $f _ { 1 }$ , we will have 10% positive examples and 90% negative examples, which can hurt performance. \nAnother approach is to use the one-versus-one or OVO approach, also called all pairs, in which we train $C ( C - 1 ) / 2$ classifiers to discriminate all pairs $f _ { c , c ^ { prime } }$ . We then classify a point into the class which has the highest number of votes. However, this can also result in ambiguities, as shown in Figure 17.16(b). Also, this requires fitting $O ( C ^ { 2 } )$ models. \n17.3.8 How to choose the regularizer $C$ \nSVMs require that you specify the kernel function and the parameter $C$ . Typically $C$ is chosen by cross-validation. Note, however, that $C$ interacts quite strongly with the kernel parameters. For example, suppose we are using an RBF kernel with precision $textstyle gamma = { frac { 1 } { 2 sigma ^ { 2 } } }$ . If $gamma$ is large, corresponding to narrow kernels, we may need heavy regularization, and hence small $C$ . If is small, a larger value of $gamma$ $C$ should be used. So we see that $gamma$ and $C$ are tightly coupled, as illustrated in Figure 17.17. \nThe authors of libsvm [HCL09] recommend using CV over a 2d grid with values $C in { 2 ^ { - 5 } , 2 ^ { - 3 } , dots , 2 ^ { 1 5 } }$ \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license and $gamma in { 2 ^ { - 1 5 } , 2 ^ { - 1 3 } , . . . , 2 ^ { 3 } }$ . See Figure 17.18 which shows the CV estimate of the 0-1 risk as a function of $C$ and $gamma$ . \n\nTo choose $C$ efficiently, one can develop a path following algorithm in the spirit of lars (Section 11.4.4). The basic idea is to start with $C$ small, so that the margin is wide, and hence all points are inside of it and have $alpha _ { i } = 1$ . By slowly increasing $C$ , a small set of points will move from inside the margin to outside, and their $alpha _ { i }$ values will change from 1 to $0$ , as they cease to be support vectors. When $C$ is maximal, the margin becomes empty, and no support vectors remain. See [Has+04] for the details. \n17.3.9 Kernel ridge regression \nRecall the equation for ridge regression from Equation (11.55): \nUsing the matrix inversion lemma (Section 7.3.3), we can rewrite the ridge estimate as follows \nLet us define the following dual variables: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "How to choose the regularizer C"
    },
    {
        "content": "To choose $C$ efficiently, one can develop a path following algorithm in the spirit of lars (Section 11.4.4). The basic idea is to start with $C$ small, so that the margin is wide, and hence all points are inside of it and have $alpha _ { i } = 1$ . By slowly increasing $C$ , a small set of points will move from inside the margin to outside, and their $alpha _ { i }$ values will change from 1 to $0$ , as they cease to be support vectors. When $C$ is maximal, the margin becomes empty, and no support vectors remain. See [Has+04] for the details. \n17.3.9 Kernel ridge regression \nRecall the equation for ridge regression from Equation (11.55): \nUsing the matrix inversion lemma (Section 7.3.3), we can rewrite the ridge estimate as follows \nLet us define the following dual variables: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThen we can rewrite the primal variables as follows \nThis tells us that the solution vector is just a linear sum of the $N _ { mathcal { D } }$ training vectors. When we plug this in at test time to compute the predictive mean, we get \nWe can then use the kernel trick to rewrite this as \nwhere \nIn other words, \nwhere $pmb { k } = [ mathcal { K } ( pmb { x } , pmb { x } _ { 1 } ) , dots , mathcal { K } ( pmb { x } , pmb { x } _ { N } ) ]$ . This is called kernel ridge regression. \nThe trouble with the above approach is that the solution vector $alpha$ is not sparse, so predictions at test time will take $O ( N )$ time. We discuss a solution to this in Section 17.3.10. \n17.3.10 SVMs for regression \nConsider the following $ell _ { 2 }$ -regularized ERM problem: \nwhere $hat { y } _ { n } = w ^ { 1 } x _ { n } + w _ { 0 }$ . If we use the quadratic loss, $ell ( y , hat { y } ) = ( y - hat { y } ) ^ { 2 }$ , where $y , hat { y } in mathbb R$ , we recover ridge regression (Section 11.3). If we then apply the kernel trick, we recover kernel ridge regression (Section 17.3.9). \nThe problem with kernel ridge regression is that the solution depends on all $N$ training points, which makes it computationally intractable. However, by changing the loss function, we can make the optimal set of basis function coefficients, $alpha ^ { * }$ , be sparse, as we show below. \nIn particular, consider the following variant of the Huber loss function (Section 5.1.5.3) called the epsilon insensitive loss function: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "Kernel ridge regression"
    },
    {
        "content": "Then we can rewrite the primal variables as follows \nThis tells us that the solution vector is just a linear sum of the $N _ { mathcal { D } }$ training vectors. When we plug this in at test time to compute the predictive mean, we get \nWe can then use the kernel trick to rewrite this as \nwhere \nIn other words, \nwhere $pmb { k } = [ mathcal { K } ( pmb { x } , pmb { x } _ { 1 } ) , dots , mathcal { K } ( pmb { x } , pmb { x } _ { N } ) ]$ . This is called kernel ridge regression. \nThe trouble with the above approach is that the solution vector $alpha$ is not sparse, so predictions at test time will take $O ( N )$ time. We discuss a solution to this in Section 17.3.10. \n17.3.10 SVMs for regression \nConsider the following $ell _ { 2 }$ -regularized ERM problem: \nwhere $hat { y } _ { n } = w ^ { 1 } x _ { n } + w _ { 0 }$ . If we use the quadratic loss, $ell ( y , hat { y } ) = ( y - hat { y } ) ^ { 2 }$ , where $y , hat { y } in mathbb R$ , we recover ridge regression (Section 11.3). If we then apply the kernel trick, we recover kernel ridge regression (Section 17.3.9). \nThe problem with kernel ridge regression is that the solution depends on all $N$ training points, which makes it computationally intractable. However, by changing the loss function, we can make the optimal set of basis function coefficients, $alpha ^ { * }$ , be sparse, as we show below. \nIn particular, consider the following variant of the Huber loss function (Section 5.1.5.3) called the epsilon insensitive loss function: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis means that any point lying inside an $epsilon$ -tube around the prediction is not penalized, as in Figure 17.19. \nThe corresponding objective function is usually written in the following form \nwhere $hat { y } _ { n } = f ( pmb { x } _ { n } ) = pmb { w } ^ { 1 } pmb { x } _ { n } + w _ { 0 }$ and $C = 1 / lambda$ is a regularization constant. This objective is convex and unconstrained, but not differentiable, because of the absolute value function in the loss term. As in Section 11.4.9, where we discussed the lasso problem, there are several possible algorithms we could use. One popular approach is to formulate the problem as a constrained optimization problem. In particular, we introduce slack variables to represent the degree to which each point lies outside the tube: \nGiven this, we can rewrite the objective as follows: \nThis is a quadratic function of $mathbf { boldsymbol { w } }$ , and must be minimized subject to the linear constraints in Equations 17.112-17.113, as well as the positivity constraints $xi _ { n } ^ { + } ge 0$ and $xi _ { n } ^ { - } geq 0$ . This is a standard quadratic program in $2 N + D + 1$ variables. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nBy forming the Lagrangian and optimizing, as we did above, one can show that the optimal solution has the following form \nwhere $alpha _ { n } geq 0$ are the dual variables. (See e.g., [SS02] for details.) Fortunately, the $alpha$ vector is sparse, meaning that many of its entries are equal to 0. This is because the loss doesn’t care about errors which are smaller than $epsilon$ . The degree of sparsity is controlled by $C$ and $epsilon$ . \nThe ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ for which $alpha _ { n } > 0$ are called the support vectors; these are points for which the errors lie on or outside the $epsilon$ tube. These are the only training examples we need to keep for prediction at test time, since \nFinally, we can use the kernel trick to get \nThis overall technique is called support vector machine regression or SVM regression for short, and was first proposed in [VGS97]. \nIn Figure 17.20, we give an example where we use an RBF kernel with $gamma = 1$ . When $C$ is small, the model is heavily regularized; when $C$ is large, the model is less regularized and can fit the data better. We also see that when $epsilon$ is small, the tube is smaller, so there are more support vectors. \n17.4 Sparse vector machines \nGPs are very flexible models, but incur an $O ( N )$ time cost at prediction time, which can be prohibitive. SVMs solve that problem by estimating a sparse weight vector. However, SVMs do not give calibrated probabilistic outputs. \nWe can get the best of both worlds by using parametric models, where the feature vector is defined using basis functions centered on each of the training points, as follows: \nwhere $mathcal { K }$ is any similarity kernel, not necessarily a Mercer kernel. Equation (17.118) maps $mathbf { boldsymbol { x } } in mathcal { X }$ into $pmb { phi } ( pmb { x } ) in mathbb { R } ^ { N }$ . We can plug this new feature vector into any discriminative model, such as logistic regression. Since we have $D = N$ parameters, we need to use some kind of regularization, to prevent overfitting. If we fit such a model using $ell _ { 2 }$ regularization (which we will call L2VM, for $ell _ { 2 }$ -vector machine), the result often has good predictive performance, but the weight vector $mathbf { boldsymbol { w } }$ will be dense, and will depend on all $N$ training points. A natural solution is to impose a sparsity-promoting prior on $mathbf { boldsymbol { w } }$ , so that not all the exemplars need to be kept. We call such methods sparse vector machines. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Support vector machines (SVMs)",
        "subsubsection": "SVMs for regression"
    },
    {
        "content": "17.4.1 Relevance vector machines (RVMs) \nThe simplest way to ensure $mathbf { boldsymbol { w } }$ is sparse is to use $ell _ { 1 }$ regularization, as in Section 11.4. We call this L1VM or Laplace vector machine, since this approach is equivalent to using MAP estimation with a Laplace prior for $mathbf { boldsymbol { w } }$ . \nHowever, sometimes $ell _ { 1 }$ regularization does not result in a sufficient level of sparsity for a given level of accuracy. An alternative approach is based on the use of ARD or automatic relevancy determination, which uses type II maximum likelihood (aka empirical Bayes) to estimate a sparse weight vector [Mac95; Nea96]. If we apply this technique to a feature vector defined in terms of kernels, as in Equation (17.118), we get a method called the relevance vector machine or RVM [Tip01; TF03]. \n17.4.2 Comparison of sparse and dense kernel methods \nIn Figure 17.21, we compare L2VM, L1VM, RVM and an SVM using an RBF kernel on a binary classification problem in 2d. We use cross validation to pick $C = 1 / lambda$ for the SVM (see Section 17.3.8), \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 and then use the same value of the regularizer for L2VM and L1VM. We see that all the methods give similar predictive performance. However, we see that the RVM is the sparsest model, so it will be the fastest at run time.",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Sparse vector machines",
        "subsubsection": "Relevance vector machines (RVMs)"
    },
    {
        "content": "17.4.1 Relevance vector machines (RVMs) \nThe simplest way to ensure $mathbf { boldsymbol { w } }$ is sparse is to use $ell _ { 1 }$ regularization, as in Section 11.4. We call this L1VM or Laplace vector machine, since this approach is equivalent to using MAP estimation with a Laplace prior for $mathbf { boldsymbol { w } }$ . \nHowever, sometimes $ell _ { 1 }$ regularization does not result in a sufficient level of sparsity for a given level of accuracy. An alternative approach is based on the use of ARD or automatic relevancy determination, which uses type II maximum likelihood (aka empirical Bayes) to estimate a sparse weight vector [Mac95; Nea96]. If we apply this technique to a feature vector defined in terms of kernels, as in Equation (17.118), we get a method called the relevance vector machine or RVM [Tip01; TF03]. \n17.4.2 Comparison of sparse and dense kernel methods \nIn Figure 17.21, we compare L2VM, L1VM, RVM and an SVM using an RBF kernel on a binary classification problem in 2d. We use cross validation to pick $C = 1 / lambda$ for the SVM (see Section 17.3.8), \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 and then use the same value of the regularizer for L2VM and L1VM. We see that all the methods give similar predictive performance. However, we see that the RVM is the sparsest model, so it will be the fastest at run time. \n\nIn Figure 17.22, we compare L2VM, L1VM, RVM and an SVM using an RBF kernel on a 1d regression problem. Again, we see that predictions are quite similar, but RVM is the sparsest, then L1VM, then SVM. This is further illustrated in Figure 17.23. \nBeyond these small empirical examples, we provide a more general summary of the differen methods in Table 17.1. The columns of this table have the following meaning: \n• Optimize w: a key question is whether the objective $mathcal { L } ( pmb { w } ) = - log p ( mathcal { D } | pmb { w } ) - log p ( pmb { w } )$ is convex or not. L2VM, L1VM and SVMs have convex objectives. RVMs do not. GPs are Bayesian methods that integrate out the weights $mathbf { boldsymbol { w } }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n• Optimize kernel: all the methods require that we “tune” the kernel parameters, such as the bandwidth of the RBF kernel, as well as the level of regularization. For methods based on Gaussian priors, including L2VM, RVMs and GPs, we can use efficient gradient based optimizers to maximize the marginal likelihood. For SVMs and L1VMs, we must use cross validation, which is slower (see Section 17.3.8). Sparse: L1VM, RVMs and SVMs are sparse kernel methods, in that they only use a subset of the training examples. GPs and L2VM are not sparse: they use all the training examples. The principle advantage of sparsity is that prediction at test time is usually faster. However, this usually results in overconfidence in the predictions.   \n• Probabilistic: All the methods except for SVMs produce probabilistic output of the form $p ( boldsymbol { y } | boldsymbol { x } )$ . SVMs produce a “confidence” value that can be converted to a probability, but such probabilities are usually very poorly calibrated (see Section 17.3.5).   \n• Multiclass: All the methods except for SVMs naturally work in the multiclass setting, by using a categorical distribution instead of a Bernoulli. The SVM can be made into a multiclass classifier, but there are various difficulties with this approach, as discussed in Section 17.3.7. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • Mercer kernel: SVMs and GPs require that the kernel is positive definite; the other techniques do not, since the kernel function in Equation (17.118) can be an arbitrary function of two inputs. \n\n17.5 Exercises \nExercise 17.1 [Fitting an SVM classifier by hand $^ *$ ] \n(Source: Jaakkola.) Consider a dataset with 2 points in 1d: ( $x _ { 1 } = 0 , y _ { 1 } = - 1 .$ ) and $x _ { 2 } = { sqrt { 2 } } , y _ { 2 } = 1$ ). Consider mapping each point to 3d using the feature vector $phi ( x ) = [ 1 , sqrt { 2 } x , x ^ { 2 } ] ^ { T }$ . (This is equivalent to using \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Sparse vector machines",
        "subsubsection": "Comparison of sparse and dense kernel methods"
    },
    {
        "content": "17.5 Exercises \nExercise 17.1 [Fitting an SVM classifier by hand $^ *$ ] \n(Source: Jaakkola.) Consider a dataset with 2 points in 1d: ( $x _ { 1 } = 0 , y _ { 1 } = - 1 .$ ) and $x _ { 2 } = { sqrt { 2 } } , y _ { 2 } = 1$ ). Consider mapping each point to 3d using the feature vector $phi ( x ) = [ 1 , sqrt { 2 } x , x ^ { 2 } ] ^ { T }$ . (This is equivalent to using \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \na second order polynomial kernel.) The max margin classifier has the form \na. Write down a vector that is parallel to the optimal vector $mathbf { Delta } _ { mathbf { w } }$ . Hint: recall from Figure 7.8 (12Apr10 version) that $mathbf { Delta } _ { mathbf { w } }$ is perpendicular to the decision boundary between the two points in the 3d feature space.   \nb. What is the value of the margin that is achieved by this $mathbf { boldsymbol { w } }$ ? Hint: recall that the margin is the distance from each support vector to the decision boundary. Hint 2: think about the geometry of 2 points in space, with a line separating one from the other.   \nc. Solve for $mathbf { boldsymbol { w } }$ , using the fact that the margin is equal to $1 / | | w | |$ .   \nd. Solve for $w _ { 0 }$ using your value for $mathbf { Delta } _ { mathbf { w } }$ and Equations 17.119 to 17.121. Hint: the points will be on the decision boundary, so the inequalities will be tight.   \ne. Write down the form of the discriminant function $f ( x ) = w _ { 0 } + pmb { w } ^ { T } phi ( x )$ as an explicit function of $x$ . \n18 Trees, Forests, Bagging, and Boosting \n18.1 Classification and regression trees (CART) \nClassification and regression trees or CART models [BFO84], also called decision trees [Qui86; Qui93], are defined by recursively partitioning the input space, and defining a local model in each resulting region of input space. The overall model can be represented by a tree, with one leaf per region, as we explain below. \n18.1.1 Model definition \nWe start by considering regression trees, where all inputs are real-valued. The tree consists of a set of nested decision rules. At each node $i$ , the feature dimension $d _ { i }$ of the input vector $_ { x }$ is compared to a threshold value $t _ { i }$ , and the input is then passed down to the left or right branch, depending on whether it is above or below threshold. At the leaves of the tree, the model specifies the predicted output for any input that falls into that part of the input space. \nFor example, consider the regression tree in Figure 18.1(a). The first node asks if $x _ { 1 }$ is less than some threshold $t _ { 1 }$ . If yes, we then ask if $x _ { 2 }$ is less than some other threshold $t _ { 2 }$ . If yes, we enter the bottom left leaf node. This corresponds to the region of space defined by \nWe can associate this region with the predicted output computing using other branches of the tree define different regions in terms of axis parallel splits. The overall result is that we partition the 2d input space into 5 regions, as shown in Figure 18.1(b).1 We can now associate a mean response with each of these regions, resulting in the piecewise constant surface shown in Figure 18.1(b). For example, the output for region 1 can be estimated using \nFormally, a regression tree can be defined by",
        "chapter": "IV Nonparametric Models",
        "section": "Kernel Methods *",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "18 Trees, Forests, Bagging, and Boosting \n18.1 Classification and regression trees (CART) \nClassification and regression trees or CART models [BFO84], also called decision trees [Qui86; Qui93], are defined by recursively partitioning the input space, and defining a local model in each resulting region of input space. The overall model can be represented by a tree, with one leaf per region, as we explain below. \n18.1.1 Model definition \nWe start by considering regression trees, where all inputs are real-valued. The tree consists of a set of nested decision rules. At each node $i$ , the feature dimension $d _ { i }$ of the input vector $_ { x }$ is compared to a threshold value $t _ { i }$ , and the input is then passed down to the left or right branch, depending on whether it is above or below threshold. At the leaves of the tree, the model specifies the predicted output for any input that falls into that part of the input space. \nFor example, consider the regression tree in Figure 18.1(a). The first node asks if $x _ { 1 }$ is less than some threshold $t _ { 1 }$ . If yes, we then ask if $x _ { 2 }$ is less than some other threshold $t _ { 2 }$ . If yes, we enter the bottom left leaf node. This corresponds to the region of space defined by \nWe can associate this region with the predicted output computing using other branches of the tree define different regions in terms of axis parallel splits. The overall result is that we partition the 2d input space into 5 regions, as shown in Figure 18.1(b).1 We can now associate a mean response with each of these regions, resulting in the piecewise constant surface shown in Figure 18.1(b). For example, the output for region 1 can be estimated using \nFormally, a regression tree can be defined by \nwhere $R _ { j }$ is the region specified by the $j$ ’th leaf node, $w _ { j }$ is the predicted output for that node, and $pmb theta = { ( R _ { j } , w _ { j } ) : j = 1 : J }$ , where $J$ is the number of nodes. The regions themselves are defined by the feature dimensions that are used in each split, and the corresponding thresholds, on the path from the root to the leaf. For example, in Figure 18.1(a), we have $R _ { 1 } = [ ( d _ { 1 } leq t _ { 1 } ) , ( d _ { 2 } leq t _ { 2 } ) ]$ , $R _ { 2 } = [ ( d _ { 1 } leq t _ { 1 } ) , ( d _ { 2 } > t _ { 2 } ) , ( d _ { 3 } leq t _ { 3 } ) ]$ , etc. (For categorical inputs, we can define the splits based on comparing feature $d _ { i }$ to each of the possible values for that feature, rather than comparing to a numeric threshold.) We discuss how to learn these regions in Section 18.1.2. \nFor classification problems, the leaves contain a distribution over the class labels, rather than just the mean response. See Figure 18.2 for an example of a classification tree. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n18.1.2 Model fitting \nTo fit the model, we need to minimize the following loss: \nUnfortunately, this is not differentiable, because of the need to learn the discrete tree structure. Indeed, finding the optimal partitioning of the data is NP-complete [HR76]. The standard practice is to use a greedy procedure, in which we iteratively grow the tree one node at a time. This approach is used by CART [BFO84], C4.5 [Qui93], and ID3 [Qui86], which are three popular implementations of the method. \nThe idea is as follows. Suppose we are at node $i$ ; let $mathcal { D } _ { i } = { ( boldsymbol { x } _ { n } , y _ { n } ) in N _ { i } }$ be the set of examples that reach this node. We will consider how to split this node into a left branch and right branch so as to minimize the error in each child subtree. \nIf the $j$ ’the feature is a real-valued scalar, we can partition the data at node $i$ by comparing to a threshold $t$ . The set of possible thresholds $tau _ { j }$ for feature $j$ can be obtained by sorting the unique values of ${ x _ { n j } }$ . For example, if feature 1 has the values ${ 4 . 5 , - 1 2 , 7 2 , - 1 2 }$ , then we set $mathcal { T } _ { 1 } = { - 1 2 , 4 . 5 , 7 2 }$ . For each possible threshold, we define the left and right splits, $mathcal { D } _ { i } ^ { L } ( j , t ) = { ( x _ { n } , y _ { n } ) in N _ { i } : x _ { n , j } leq t }$ and $mathcal { D } _ { i } ^ { R } ( j , t ) = { ( x _ { n } , y _ { n } ) in N _ { i } : x _ { n , j } > t }$ . \nIf the $j$ ’th feature is categorical, with $K _ { j }$ possible values, then we check if the feature is equal to each of those values or not. This defines a set of $K _ { j }$ possible binary splits: $mathcal { D } _ { i } ^ { L } ( j , t ) = { ( x _ { n } , y _ { n } ) in N _ { i } :$ $x _ { n , j } = t }$ and $mathcal { D } _ { i } ^ { R } ( j , t ) = { ( { pmb x } _ { n } , y _ { n } ) in N _ { i } : x _ { n , j } neq t }$ .) (Alternatively, we could allow for a multi-way split, as in Figure 18.2(b). However, this may cause data fragmentation, in which too little data might “fall” into each subtree, resulting in overfitting. Therefore it is more common to use binary splits.) \nOnce we have computed $mathcal { D } _ { i } ^ { L } ( j , t )$ and $mathcal { D } _ { i } ^ { R } ( j , t )$ for each $j$ and $t$ at node $i$ , we choose the best feature $j _ { i }$ to split on, and the best value for that feature, $t _ { i }$ , as follows: \nWe now discuss the cost function $c ( mathcal { D } _ { i } )$ which is used to evaluate the cost of node $i$ . For regression, we can use the mean squared error \nwhere $begin{array} { r } { overline { { y } } = frac { 1 } { left| { mathcal { D } } right| } sum _ { n in { mathcal { D } } _ { i } } y _ { n } } end{array}$ is the mean of the response variable for examples reaching node $i$ . For classification, we first compute the empirical distribution over class labels for this node: \nGiven this, we can then compute the Gini index \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Classification and regression trees (CART)",
        "subsubsection": "Model definition"
    },
    {
        "content": "18.1.2 Model fitting \nTo fit the model, we need to minimize the following loss: \nUnfortunately, this is not differentiable, because of the need to learn the discrete tree structure. Indeed, finding the optimal partitioning of the data is NP-complete [HR76]. The standard practice is to use a greedy procedure, in which we iteratively grow the tree one node at a time. This approach is used by CART [BFO84], C4.5 [Qui93], and ID3 [Qui86], which are three popular implementations of the method. \nThe idea is as follows. Suppose we are at node $i$ ; let $mathcal { D } _ { i } = { ( boldsymbol { x } _ { n } , y _ { n } ) in N _ { i } }$ be the set of examples that reach this node. We will consider how to split this node into a left branch and right branch so as to minimize the error in each child subtree. \nIf the $j$ ’the feature is a real-valued scalar, we can partition the data at node $i$ by comparing to a threshold $t$ . The set of possible thresholds $tau _ { j }$ for feature $j$ can be obtained by sorting the unique values of ${ x _ { n j } }$ . For example, if feature 1 has the values ${ 4 . 5 , - 1 2 , 7 2 , - 1 2 }$ , then we set $mathcal { T } _ { 1 } = { - 1 2 , 4 . 5 , 7 2 }$ . For each possible threshold, we define the left and right splits, $mathcal { D } _ { i } ^ { L } ( j , t ) = { ( x _ { n } , y _ { n } ) in N _ { i } : x _ { n , j } leq t }$ and $mathcal { D } _ { i } ^ { R } ( j , t ) = { ( x _ { n } , y _ { n } ) in N _ { i } : x _ { n , j } > t }$ . \nIf the $j$ ’th feature is categorical, with $K _ { j }$ possible values, then we check if the feature is equal to each of those values or not. This defines a set of $K _ { j }$ possible binary splits: $mathcal { D } _ { i } ^ { L } ( j , t ) = { ( x _ { n } , y _ { n } ) in N _ { i } :$ $x _ { n , j } = t }$ and $mathcal { D } _ { i } ^ { R } ( j , t ) = { ( { pmb x } _ { n } , y _ { n } ) in N _ { i } : x _ { n , j } neq t }$ .) (Alternatively, we could allow for a multi-way split, as in Figure 18.2(b). However, this may cause data fragmentation, in which too little data might “fall” into each subtree, resulting in overfitting. Therefore it is more common to use binary splits.) \nOnce we have computed $mathcal { D } _ { i } ^ { L } ( j , t )$ and $mathcal { D } _ { i } ^ { R } ( j , t )$ for each $j$ and $t$ at node $i$ , we choose the best feature $j _ { i }$ to split on, and the best value for that feature, $t _ { i }$ , as follows: \nWe now discuss the cost function $c ( mathcal { D } _ { i } )$ which is used to evaluate the cost of node $i$ . For regression, we can use the mean squared error \nwhere $begin{array} { r } { overline { { y } } = frac { 1 } { left| { mathcal { D } } right| } sum _ { n in { mathcal { D } } _ { i } } y _ { n } } end{array}$ is the mean of the response variable for examples reaching node $i$ . For classification, we first compute the empirical distribution over class labels for this node: \nGiven this, we can then compute the Gini index \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis is the expected error rate. To see this, note that $hat { pi } _ { i c }$ is the probability a random entry in the leaf belongs to class $c$ , and $1 - hat { pi } _ { i c }$ is the probability it would be misclassified. \nAlternatively we can define cost as the entropy or deviance of the node: \nA node that is pure (i.e., only has examples of one class) will have 0 entropy. \nGiven one of the above cost functions, we can use Equation (18.5) to pick the best feature, and best threshold at each node. We then partition the data, and call the fitting algorithm recursively on each subset of the data. \n18.1.3 Regularization \nIf we let the tree become deep enough, it can achieve 0 error on the training set (assuming no label noise), by partioning the input space into sufficiently small regions where the output is constant. However, this will typically result in overfitting. To prevent this, there are two main approaches. The first is to stop the tree growing process according to some heuristic, such as having too few examples at a node, or reaching a maximum depth. The second approach is to grow the tree to its maximum depth, where no more splits are possible, and then to prune it back, by merging split subtrees back into their parent (see e.g., [BA97b]). This can partially overcome the greedy nature of top-down tree growing. (For example, consider applying the top-down approach to the xor data in Figure 13.1: the algorithm would never make any splits, since each feature on its own has no predictive power.) However, forward growing and backward pruning is slower than the greedy top-down approach. \n18.1.4 Handling missing input features \nIn general, it is hard for discriminative models, such as neural networks, to handle missing input features, as we discussed in Section 1.5.5. However, for trees, there are some simple heuristics that can work well. \nThe standard heuristic for handling missing inputs in decision trees is to look for a series of “backup” variables, which can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen variable is unobserved at test time. These are called surrogate splits. This method finds highly correlated features, and can be thought of as learning a local joint model of the input. This has the advantage over a generative model of not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler approach, applicable to categorical variables, is to code “missing” as a new value, and then to treat the data as fully observed. \n18.1.5 Pros and cons \nTree models are popular for several reasons: \n• They are easy to interpret.   \n• They can easily handle mixed discrete and continuous inputs. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • They are insensitive to monotone transformations of the inputs (because the split points are based on ranking the data points), so there is no need to standardize the data.",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Classification and regression trees (CART)",
        "subsubsection": "Model fitting"
    },
    {
        "content": "This is the expected error rate. To see this, note that $hat { pi } _ { i c }$ is the probability a random entry in the leaf belongs to class $c$ , and $1 - hat { pi } _ { i c }$ is the probability it would be misclassified. \nAlternatively we can define cost as the entropy or deviance of the node: \nA node that is pure (i.e., only has examples of one class) will have 0 entropy. \nGiven one of the above cost functions, we can use Equation (18.5) to pick the best feature, and best threshold at each node. We then partition the data, and call the fitting algorithm recursively on each subset of the data. \n18.1.3 Regularization \nIf we let the tree become deep enough, it can achieve 0 error on the training set (assuming no label noise), by partioning the input space into sufficiently small regions where the output is constant. However, this will typically result in overfitting. To prevent this, there are two main approaches. The first is to stop the tree growing process according to some heuristic, such as having too few examples at a node, or reaching a maximum depth. The second approach is to grow the tree to its maximum depth, where no more splits are possible, and then to prune it back, by merging split subtrees back into their parent (see e.g., [BA97b]). This can partially overcome the greedy nature of top-down tree growing. (For example, consider applying the top-down approach to the xor data in Figure 13.1: the algorithm would never make any splits, since each feature on its own has no predictive power.) However, forward growing and backward pruning is slower than the greedy top-down approach. \n18.1.4 Handling missing input features \nIn general, it is hard for discriminative models, such as neural networks, to handle missing input features, as we discussed in Section 1.5.5. However, for trees, there are some simple heuristics that can work well. \nThe standard heuristic for handling missing inputs in decision trees is to look for a series of “backup” variables, which can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen variable is unobserved at test time. These are called surrogate splits. This method finds highly correlated features, and can be thought of as learning a local joint model of the input. This has the advantage over a generative model of not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler approach, applicable to categorical variables, is to code “missing” as a new value, and then to treat the data as fully observed. \n18.1.5 Pros and cons \nTree models are popular for several reasons: \n• They are easy to interpret.   \n• They can easily handle mixed discrete and continuous inputs. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • They are insensitive to monotone transformations of the inputs (because the split points are based on ranking the data points), so there is no need to standardize the data.",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Classification and regression trees (CART)",
        "subsubsection": "Regularization"
    },
    {
        "content": "This is the expected error rate. To see this, note that $hat { pi } _ { i c }$ is the probability a random entry in the leaf belongs to class $c$ , and $1 - hat { pi } _ { i c }$ is the probability it would be misclassified. \nAlternatively we can define cost as the entropy or deviance of the node: \nA node that is pure (i.e., only has examples of one class) will have 0 entropy. \nGiven one of the above cost functions, we can use Equation (18.5) to pick the best feature, and best threshold at each node. We then partition the data, and call the fitting algorithm recursively on each subset of the data. \n18.1.3 Regularization \nIf we let the tree become deep enough, it can achieve 0 error on the training set (assuming no label noise), by partioning the input space into sufficiently small regions where the output is constant. However, this will typically result in overfitting. To prevent this, there are two main approaches. The first is to stop the tree growing process according to some heuristic, such as having too few examples at a node, or reaching a maximum depth. The second approach is to grow the tree to its maximum depth, where no more splits are possible, and then to prune it back, by merging split subtrees back into their parent (see e.g., [BA97b]). This can partially overcome the greedy nature of top-down tree growing. (For example, consider applying the top-down approach to the xor data in Figure 13.1: the algorithm would never make any splits, since each feature on its own has no predictive power.) However, forward growing and backward pruning is slower than the greedy top-down approach. \n18.1.4 Handling missing input features \nIn general, it is hard for discriminative models, such as neural networks, to handle missing input features, as we discussed in Section 1.5.5. However, for trees, there are some simple heuristics that can work well. \nThe standard heuristic for handling missing inputs in decision trees is to look for a series of “backup” variables, which can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen variable is unobserved at test time. These are called surrogate splits. This method finds highly correlated features, and can be thought of as learning a local joint model of the input. This has the advantage over a generative model of not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler approach, applicable to categorical variables, is to code “missing” as a new value, and then to treat the data as fully observed. \n18.1.5 Pros and cons \nTree models are popular for several reasons: \n• They are easy to interpret.   \n• They can easily handle mixed discrete and continuous inputs. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • They are insensitive to monotone transformations of the inputs (because the split points are based on ranking the data points), so there is no need to standardize the data.",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Classification and regression trees (CART)",
        "subsubsection": "Handling missing input features"
    },
    {
        "content": "This is the expected error rate. To see this, note that $hat { pi } _ { i c }$ is the probability a random entry in the leaf belongs to class $c$ , and $1 - hat { pi } _ { i c }$ is the probability it would be misclassified. \nAlternatively we can define cost as the entropy or deviance of the node: \nA node that is pure (i.e., only has examples of one class) will have 0 entropy. \nGiven one of the above cost functions, we can use Equation (18.5) to pick the best feature, and best threshold at each node. We then partition the data, and call the fitting algorithm recursively on each subset of the data. \n18.1.3 Regularization \nIf we let the tree become deep enough, it can achieve 0 error on the training set (assuming no label noise), by partioning the input space into sufficiently small regions where the output is constant. However, this will typically result in overfitting. To prevent this, there are two main approaches. The first is to stop the tree growing process according to some heuristic, such as having too few examples at a node, or reaching a maximum depth. The second approach is to grow the tree to its maximum depth, where no more splits are possible, and then to prune it back, by merging split subtrees back into their parent (see e.g., [BA97b]). This can partially overcome the greedy nature of top-down tree growing. (For example, consider applying the top-down approach to the xor data in Figure 13.1: the algorithm would never make any splits, since each feature on its own has no predictive power.) However, forward growing and backward pruning is slower than the greedy top-down approach. \n18.1.4 Handling missing input features \nIn general, it is hard for discriminative models, such as neural networks, to handle missing input features, as we discussed in Section 1.5.5. However, for trees, there are some simple heuristics that can work well. \nThe standard heuristic for handling missing inputs in decision trees is to look for a series of “backup” variables, which can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen variable is unobserved at test time. These are called surrogate splits. This method finds highly correlated features, and can be thought of as learning a local joint model of the input. This has the advantage over a generative model of not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler approach, applicable to categorical variables, is to code “missing” as a new value, and then to treat the data as fully observed. \n18.1.5 Pros and cons \nTree models are popular for several reasons: \n• They are easy to interpret.   \n• They can easily handle mixed discrete and continuous inputs. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • They are insensitive to monotone transformations of the inputs (because the split points are based on ranking the data points), so there is no need to standardize the data. \n\n• They perform automatic variable selection. \n• They are relatively robust to outliers. \n• They are fast to fit, and scale well to large data sets. \n• They can handle missing input features. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHowever, tree models also have some disadvantages. The primary one is that they do not predict very accurately compared to other kinds of model. This is in part due to the greedy nature of the tree construction algorithm. \nA related problem is that trees are unstable: small changes to the input data can have large effects on the structure of the tree, due to the hierarchical nature of the tree-growing process, causing errors at the top to affect the rest of the tree. For example, consider the tree in Figure 18.3b. Omitting even a single data point from the training set can result in a dramatically different decision surface, as shown in Figure 18.3c, due to the use of axis parallel splits. (Omitting features can also cause instability.) In Section 18.3 and Section 18.4, we will turn this instability into a virtue. \n18.2 Ensemble learning \nIn Section 18.1, we saw that decision trees can be quite unstable, in the sense that their predictions might vary a lot if the training data is perturbed. In other words, decision trees are a high variance estimator. A simple way to reduce variance is to average multiple models. This is called ensemble learning. The result model has the form \nwhere $f _ { m }$ is the $m$ ’th base model. The ensemble will have similar bias to the base models, but lower variance, generally resulting in improved overall performance (see Section 4.7.6.3 for details on the bias-variance tradeoff). \nAveraging is a sensible way to combine predictions from regression models. For classifiers, it can sometimes be better to take a majority vote of the outputs. (This is sometimes called a committee method.) To see why this can help, suppose each base model is a binary classifier with an accuracy of $theta$ , and suppose class 1 is the correct class. Let $Y _ { m } in { 0 , 1 }$ be the prediction for the $m$ ’th model, and let $begin{array} { r } { S = sum _ { m = 1 } ^ { M } Y _ { m } } end{array}$ be the number of votes for c ass 1. We define the final predictor to be the majority vote, i.e., class $^ { 1 }$ if $S > M / 2$ and class 0 otherwise. The probability that the ensemble will pick class 1 is \nwhere $B ( x , M , theta )$ is the cdf of the binomial distribution with parameters $M$ and $theta$ evaluated at $x$ .   \nFor $theta = 0 . 5 1$ and $M = 1 0 0 0$ , we get $p = 0 . 7 3$ and with $M = 1 0 , 0 0 0$ we get $p = 0 . 9 7$ . \nThe performance of the voting approach is dramatically improved, because we assumed each predictor made independent errors. In practice, their mistakes may be correlated, but as long as we ensemble sufficiently diverse models, we can still come out ahead. \n18.2.1 Stacking \nAn alternative to using an unweighted average or majority vote is to learn how to combine the base models, by using \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Classification and regression trees (CART)",
        "subsubsection": "Pros and cons"
    },
    {
        "content": "However, tree models also have some disadvantages. The primary one is that they do not predict very accurately compared to other kinds of model. This is in part due to the greedy nature of the tree construction algorithm. \nA related problem is that trees are unstable: small changes to the input data can have large effects on the structure of the tree, due to the hierarchical nature of the tree-growing process, causing errors at the top to affect the rest of the tree. For example, consider the tree in Figure 18.3b. Omitting even a single data point from the training set can result in a dramatically different decision surface, as shown in Figure 18.3c, due to the use of axis parallel splits. (Omitting features can also cause instability.) In Section 18.3 and Section 18.4, we will turn this instability into a virtue. \n18.2 Ensemble learning \nIn Section 18.1, we saw that decision trees can be quite unstable, in the sense that their predictions might vary a lot if the training data is perturbed. In other words, decision trees are a high variance estimator. A simple way to reduce variance is to average multiple models. This is called ensemble learning. The result model has the form \nwhere $f _ { m }$ is the $m$ ’th base model. The ensemble will have similar bias to the base models, but lower variance, generally resulting in improved overall performance (see Section 4.7.6.3 for details on the bias-variance tradeoff). \nAveraging is a sensible way to combine predictions from regression models. For classifiers, it can sometimes be better to take a majority vote of the outputs. (This is sometimes called a committee method.) To see why this can help, suppose each base model is a binary classifier with an accuracy of $theta$ , and suppose class 1 is the correct class. Let $Y _ { m } in { 0 , 1 }$ be the prediction for the $m$ ’th model, and let $begin{array} { r } { S = sum _ { m = 1 } ^ { M } Y _ { m } } end{array}$ be the number of votes for c ass 1. We define the final predictor to be the majority vote, i.e., class $^ { 1 }$ if $S > M / 2$ and class 0 otherwise. The probability that the ensemble will pick class 1 is \nwhere $B ( x , M , theta )$ is the cdf of the binomial distribution with parameters $M$ and $theta$ evaluated at $x$ .   \nFor $theta = 0 . 5 1$ and $M = 1 0 0 0$ , we get $p = 0 . 7 3$ and with $M = 1 0 , 0 0 0$ we get $p = 0 . 9 7$ . \nThe performance of the voting approach is dramatically improved, because we assumed each predictor made independent errors. In practice, their mistakes may be correlated, but as long as we ensemble sufficiently diverse models, we can still come out ahead. \n18.2.1 Stacking \nAn alternative to using an unweighted average or majority vote is to learn how to combine the base models, by using \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis is called stacking, which stands for “stacked generalization” [Wol92]. Note that the combination weights used by stacking need to be trained on a separate dataset, otherwise they would put all their mass on the best performing base model. \n18.2.2 Ensembling is not Bayes model averaging \nIt is worth noting that an ensemble of models is not the same as using Bayes model averaging over models (Section 4.6), as pointed out in [Min00]. An ensemble considers a larger hypothesis class of the form \nwhereas BMA uses \nThe key difference is that in the case of BMA, the weights $p ( m | mathcal { D } )$ sum to one, and in the limit of infinite data, only a single model will be chosen (namely the MAP model). By contrast, the ensemble weights $w _ { m }$ are arbitrary, and don’t collapse in this way to a single model. \n18.3 Bagging \nIn this section, we discuss bagging [Bre96], which stands for “bootstrap aggregating”. This is a simple form of ensemble learning in which we fit $M$ different base models to different randomly sampled versions of the data; this encourages the different models to make diverse predictions. The datasets are sampled with replacement (a technique known as bootstrap sampling, Section 4.7.3), so a given example may appear multiple times, until we have a total of $N$ examples per model (where $N$ is the number of original data points). \nThe disadvantage of bootstrap is that each base model only sees, on average, 63% of the unique input examples. To see why, note that the chance that a single item will not be selected from a set of size $N$ in any of $N$ draws is $( 1 - 1 / N ) ^ { N }$ . In the limit of large $N$ , this becomes $e ^ { - 1 } approx 0 . 3 7$ , which means only $1 - 0 . 3 7 = 0 . 6 3$ of the data points will be selected. \nThe $3 7 %$ of the training instances that are not used by a given base model are called out-of-bag instances (oob). We can use the predicted performance of the base model on these oob instances as an estimate of test set performance. This provides a useful alternative to cross validation. \nThe main advantage of bootstrap is that it prevents the ensemble from relying too much on any individual training example, which enhances robustness and generalization [Gra04]. For example, comparing Figure 18.3b and Figure 18.3c, we see that omitting a single example from the training set can have a large impact on the decision tree that we learn (even though the tree growing algorithm is otherwise deterministic). By averaging the predictions from both of these models, we get the more reasonable prediction model in Figure 18.3d. This advantage generally increases with the size of the ensemble, as shown in Figure 18.4. (Of course, larger ensembles take more memory and more time.) \nBagging does not always improve performance. In particular, it relies on the base models being unstable estimators, so that omitting some of the data significantly changes the resulting model fit. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Ensemble learning",
        "subsubsection": "Stacking"
    },
    {
        "content": "This is called stacking, which stands for “stacked generalization” [Wol92]. Note that the combination weights used by stacking need to be trained on a separate dataset, otherwise they would put all their mass on the best performing base model. \n18.2.2 Ensembling is not Bayes model averaging \nIt is worth noting that an ensemble of models is not the same as using Bayes model averaging over models (Section 4.6), as pointed out in [Min00]. An ensemble considers a larger hypothesis class of the form \nwhereas BMA uses \nThe key difference is that in the case of BMA, the weights $p ( m | mathcal { D } )$ sum to one, and in the limit of infinite data, only a single model will be chosen (namely the MAP model). By contrast, the ensemble weights $w _ { m }$ are arbitrary, and don’t collapse in this way to a single model. \n18.3 Bagging \nIn this section, we discuss bagging [Bre96], which stands for “bootstrap aggregating”. This is a simple form of ensemble learning in which we fit $M$ different base models to different randomly sampled versions of the data; this encourages the different models to make diverse predictions. The datasets are sampled with replacement (a technique known as bootstrap sampling, Section 4.7.3), so a given example may appear multiple times, until we have a total of $N$ examples per model (where $N$ is the number of original data points). \nThe disadvantage of bootstrap is that each base model only sees, on average, 63% of the unique input examples. To see why, note that the chance that a single item will not be selected from a set of size $N$ in any of $N$ draws is $( 1 - 1 / N ) ^ { N }$ . In the limit of large $N$ , this becomes $e ^ { - 1 } approx 0 . 3 7$ , which means only $1 - 0 . 3 7 = 0 . 6 3$ of the data points will be selected. \nThe $3 7 %$ of the training instances that are not used by a given base model are called out-of-bag instances (oob). We can use the predicted performance of the base model on these oob instances as an estimate of test set performance. This provides a useful alternative to cross validation. \nThe main advantage of bootstrap is that it prevents the ensemble from relying too much on any individual training example, which enhances robustness and generalization [Gra04]. For example, comparing Figure 18.3b and Figure 18.3c, we see that omitting a single example from the training set can have a large impact on the decision tree that we learn (even though the tree growing algorithm is otherwise deterministic). By averaging the predictions from both of these models, we get the more reasonable prediction model in Figure 18.3d. This advantage generally increases with the size of the ensemble, as shown in Figure 18.4. (Of course, larger ensembles take more memory and more time.) \nBagging does not always improve performance. In particular, it relies on the base models being unstable estimators, so that omitting some of the data significantly changes the resulting model fit. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Ensemble learning",
        "subsubsection": "Ensembling is not Bayes model averaging"
    },
    {
        "content": "This is called stacking, which stands for “stacked generalization” [Wol92]. Note that the combination weights used by stacking need to be trained on a separate dataset, otherwise they would put all their mass on the best performing base model. \n18.2.2 Ensembling is not Bayes model averaging \nIt is worth noting that an ensemble of models is not the same as using Bayes model averaging over models (Section 4.6), as pointed out in [Min00]. An ensemble considers a larger hypothesis class of the form \nwhereas BMA uses \nThe key difference is that in the case of BMA, the weights $p ( m | mathcal { D } )$ sum to one, and in the limit of infinite data, only a single model will be chosen (namely the MAP model). By contrast, the ensemble weights $w _ { m }$ are arbitrary, and don’t collapse in this way to a single model. \n18.3 Bagging \nIn this section, we discuss bagging [Bre96], which stands for “bootstrap aggregating”. This is a simple form of ensemble learning in which we fit $M$ different base models to different randomly sampled versions of the data; this encourages the different models to make diverse predictions. The datasets are sampled with replacement (a technique known as bootstrap sampling, Section 4.7.3), so a given example may appear multiple times, until we have a total of $N$ examples per model (where $N$ is the number of original data points). \nThe disadvantage of bootstrap is that each base model only sees, on average, 63% of the unique input examples. To see why, note that the chance that a single item will not be selected from a set of size $N$ in any of $N$ draws is $( 1 - 1 / N ) ^ { N }$ . In the limit of large $N$ , this becomes $e ^ { - 1 } approx 0 . 3 7$ , which means only $1 - 0 . 3 7 = 0 . 6 3$ of the data points will be selected. \nThe $3 7 %$ of the training instances that are not used by a given base model are called out-of-bag instances (oob). We can use the predicted performance of the base model on these oob instances as an estimate of test set performance. This provides a useful alternative to cross validation. \nThe main advantage of bootstrap is that it prevents the ensemble from relying too much on any individual training example, which enhances robustness and generalization [Gra04]. For example, comparing Figure 18.3b and Figure 18.3c, we see that omitting a single example from the training set can have a large impact on the decision tree that we learn (even though the tree growing algorithm is otherwise deterministic). By averaging the predictions from both of these models, we get the more reasonable prediction model in Figure 18.3d. This advantage generally increases with the size of the ensemble, as shown in Figure 18.4. (Of course, larger ensembles take more memory and more time.) \nBagging does not always improve performance. In particular, it relies on the base models being unstable estimators, so that omitting some of the data significantly changes the resulting model fit. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThis is the case for decision trees, but not for other models, such as nearest neighbor classifiers. For neural networks, the story is more mixed. They can be unstable wrt their training set. On the other hand, deep networks will underperform if they only see 63% of the data, so bagged DNNs do not usually work well [NTL20]. \n18.4 Random forests \nBagging relies on the assumption that re-running the same learning algorithm on different subsets of the data will result in sufficiently diverse base models. The technique known as random forests [Bre01] tries to decorrelate the base learners even further by learning trees based on a randomly chosen subset of input variables (at each node of the tree), as well as a randomly chosen subset of data cases. It does this by modifying Equation (18.5) so the the feature split dimension $j$ is optimized over a random subset of the features, $S _ { i } subset { 1 , ldots , D }$ . \nFor example, consider the email spam dataset [HTF09, p301]. This dataset contains 4601 email messages, each of which is classified as spam (1) or non-spam (0). The data was open sourced by George Forman from Hewlett-Packard (HP) Labs. \nThere are 57 quantitative (real-valued) features, as follows: \n• 48 features corresponding to the percentage of words in the email that match a given word, such as “remove” or “labs”. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • 6 features corresponding to the percentage of characters in the email that match a given character, namely ; . [ ! $ # • 3 features corresponding to the average length, max length, and sum of lengths of uninterrupted sequences of capital letters. (These features are called CAPAVE, CAPMAX and CAPTOT.)",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Bagging",
        "subsubsection": "N/A"
    },
    {
        "content": "This is the case for decision trees, but not for other models, such as nearest neighbor classifiers. For neural networks, the story is more mixed. They can be unstable wrt their training set. On the other hand, deep networks will underperform if they only see 63% of the data, so bagged DNNs do not usually work well [NTL20]. \n18.4 Random forests \nBagging relies on the assumption that re-running the same learning algorithm on different subsets of the data will result in sufficiently diverse base models. The technique known as random forests [Bre01] tries to decorrelate the base learners even further by learning trees based on a randomly chosen subset of input variables (at each node of the tree), as well as a randomly chosen subset of data cases. It does this by modifying Equation (18.5) so the the feature split dimension $j$ is optimized over a random subset of the features, $S _ { i } subset { 1 , ldots , D }$ . \nFor example, consider the email spam dataset [HTF09, p301]. This dataset contains 4601 email messages, each of which is classified as spam (1) or non-spam (0). The data was open sourced by George Forman from Hewlett-Packard (HP) Labs. \nThere are 57 quantitative (real-valued) features, as follows: \n• 48 features corresponding to the percentage of words in the email that match a given word, such as “remove” or “labs”. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 • 6 features corresponding to the percentage of characters in the email that match a given character, namely ; . [ ! $ # • 3 features corresponding to the average length, max length, and sum of lengths of uninterrupted sequences of capital letters. (These features are called CAPAVE, CAPMAX and CAPTOT.) \n\nFigure 18.5 shows that random forests work much better than bagged decision trees, because many input features are irrelevant. (We also see that a method called “boosting”, discussed in Section 18.5, works even better; however, this requires sequentially fitting trees, whereas random forests can be fit in parallel.) \n18.5 Boosting \nEnsembles of trees, whether fit by bagging or the random forest algorithm, corresponding to a model of the form \nwhere $F _ { m }$ is the $m$ ’th tree, and $beta _ { m }$ is the corresponding weight, often set to $beta _ { m } = 1 / M$ . We can generalize this by allowing the $F _ { m }$ functions to be general function approximators, such as neural networks, not just trees. The result is called an additive model [HTF09]. We can think of this as a linear model with adaptive basis functions. The goal, as usual, is to minimize the empirical loss (with an optional regularizer): \nBoosting [Sch90; FS96] is an algorithm for sequentially fitting additive models where each $F _ { m }$ is a binary classifier that returns $F _ { m } in { - 1 , + 1 }$ . In particular, we first fit $F _ { 1 }$ on the original data, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license and then we weight the data samples by the errors made by $F _ { 1 }$ , so misclassified examples get more weight. Next we fit $F _ { 2 }$ to this weighted data set. We keep repeating this process until we have fit the desired number $M$ of components. ( $M$ is a hyper-parameter that controls the complexity of the overall model, and can be chosen by monitoring performance on a validation set, and using early stopping.)",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Random forests",
        "subsubsection": "N/A"
    },
    {
        "content": "It can be shown that, as long as each $F _ { m }$ has an accuracy that is better than chance (even on the weighted dataset), then the final ensemble of classifiers will have higher accuracy than any given component. That is, if $F _ { m }$ is a weak learner (so its accuracy is only slightly better than $5 0 %$ ), then we can boost its performance using the above procedure so that the final $f$ becomes a strong learner. (See e.g., [SF12] for more details on the learning theory approach to boosting.) \nNote that boosting reduces the bias of the strong learner, by fitting trees that depend on each other, whereas bagging and RF reduce the variance by fitting independent trees. In many cases, boosting can work better. See Figure 18.5 for an example. \nThe original boosting algorithm focused on binary classification with a particular loss function that we will explain in Section 18.5.3, and was derived from the PAC learning theory framework (see Section 5.4.4). In the rest of this section, we focus on a more statistical version of boosting, due to [FHT00; Fri01], which works with arbitrary loss functions, making the method suitable for regression, multi-class classification, ranking, etc. Our presentation is based on [HTF09, ch10] and [BH07], which should be consulted for further details. \n18.5.1 Forward stagewise additive modeling \nIn this section, we discuss forward stagewise additive modeling, in which we sequentially optimize the objective in Equation (18.16) for general (differentiable) loss functions, where $f$ is an additive model as in Equation 18.15. That is, at iteration $m$ , we compute \nWe then set \n(Note that we do not adjust the parameters of previously added models.) The details on how to perform this optimization step depend on the loss function that we choose, and (in some cases) on the form of the weak learner $F$ , as we discuss below. \n18.5.2 Quadratic loss and least squares boosting \nSuppose we use squared error loss, $ell ( y , hat { y } ) = ( y - hat { y } ) ^ { 2 }$ . In this case, the $i$ ’th term in the objective at step $m$ becomes \nwhere $r _ { i m } = y _ { i } - f _ { m - 1 } ( pmb { x } _ { i } )$ is the residual of the current model on the $i$ ’th observation. We can minimize the above objective by simply setting $beta = 1$ , and fitting $F$ to the residual errors. This is called least squares boosting [BY03]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Boosting",
        "subsubsection": "Forward stagewise additive modeling"
    },
    {
        "content": "It can be shown that, as long as each $F _ { m }$ has an accuracy that is better than chance (even on the weighted dataset), then the final ensemble of classifiers will have higher accuracy than any given component. That is, if $F _ { m }$ is a weak learner (so its accuracy is only slightly better than $5 0 %$ ), then we can boost its performance using the above procedure so that the final $f$ becomes a strong learner. (See e.g., [SF12] for more details on the learning theory approach to boosting.) \nNote that boosting reduces the bias of the strong learner, by fitting trees that depend on each other, whereas bagging and RF reduce the variance by fitting independent trees. In many cases, boosting can work better. See Figure 18.5 for an example. \nThe original boosting algorithm focused on binary classification with a particular loss function that we will explain in Section 18.5.3, and was derived from the PAC learning theory framework (see Section 5.4.4). In the rest of this section, we focus on a more statistical version of boosting, due to [FHT00; Fri01], which works with arbitrary loss functions, making the method suitable for regression, multi-class classification, ranking, etc. Our presentation is based on [HTF09, ch10] and [BH07], which should be consulted for further details. \n18.5.1 Forward stagewise additive modeling \nIn this section, we discuss forward stagewise additive modeling, in which we sequentially optimize the objective in Equation (18.16) for general (differentiable) loss functions, where $f$ is an additive model as in Equation 18.15. That is, at iteration $m$ , we compute \nWe then set \n(Note that we do not adjust the parameters of previously added models.) The details on how to perform this optimization step depend on the loss function that we choose, and (in some cases) on the form of the weak learner $F$ , as we discuss below. \n18.5.2 Quadratic loss and least squares boosting \nSuppose we use squared error loss, $ell ( y , hat { y } ) = ( y - hat { y } ) ^ { 2 }$ . In this case, the $i$ ’th term in the objective at step $m$ becomes \nwhere $r _ { i m } = y _ { i } - f _ { m - 1 } ( pmb { x } _ { i } )$ is the residual of the current model on the $i$ ’th observation. We can minimize the above objective by simply setting $beta = 1$ , and fitting $F$ to the residual errors. This is called least squares boosting [BY03]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe give an example of this process in Figure 18.6, where we use a regression tree of depth 2 as the weak learner. On the left, we show the result of fitting the weak learner to the residuals, and on the right, we show the current strong learner. We see how each new weak learner that is added to the ensemble corrects the errors made by earlier versions of the model. \n18.5.3 Exponential loss and AdaBoost \nSuppose we are interested in binary classification, i.e., predicting $tilde { y } _ { i } in { - 1 , + 1 }$ . Let us assume the weak learner computes \nso $F ( { pmb x } )$ returns half the log odds. We know from Equation (10.13) that the negative log likelihood is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Boosting",
        "subsubsection": "Quadratic loss and least squares boosting"
    },
    {
        "content": "We give an example of this process in Figure 18.6, where we use a regression tree of depth 2 as the weak learner. On the left, we show the result of fitting the weak learner to the residuals, and on the right, we show the current strong learner. We see how each new weak learner that is added to the ensemble corrects the errors made by earlier versions of the model. \n18.5.3 Exponential loss and AdaBoost \nSuppose we are interested in binary classification, i.e., predicting $tilde { y } _ { i } in { - 1 , + 1 }$ . Let us assume the weak learner computes \nso $F ( { pmb x } )$ returns half the log odds. We know from Equation (10.13) that the negative log likelihood is given by \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can minimize this by ensuring that the margin $m ( { pmb x } ) = tilde { y } F ( { pmb x } )$ is as large as possible. We see from Figure 18.7 that the log loss is a smooth upper bound on the 0-1 loss. We also see that it penalizes negative margins more heavily than positive ones, as desired (since positive margins are already correctly classified). \nHowever, we can also use other loss functions. In this section, we consider the exponential loss \nWe see from Figure 18.7 that this is also a smooth upper bound on the 0-1 loss. In the population setting (with infinite sample size), the optimal solution to the exponential loss is the same as for log loss. To see this, we can just set the derivative of the expected loss (for each $_ { x }$ ) to zero: \nHowever, it turns out that the exponential loss is easier to optimize in the boosting setting, as we show below. (We consider the log loss case in Section 18.5.4.) \nWe now discuss how to solve for the $m$ ’th weak learner, $F _ { m }$ , when we use exponential loss. We will assume that the base classifier $F _ { m }$ returns a binary class label; the resulting algorithm is called discrete AdaBoost [FHT00]. If $F _ { m }$ returns a probability instead, a modified algorithm, known as real AdaBoost, can be used [FHT00]. \nAt step $m$ we have to minimize \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $omega _ { i , m } triangleq exp bigl ( - tilde { y } _ { i } f _ { m - 1 } ( pmb { x } _ { i } ) bigr )$ is a weight applied to datacase $i$ , and $tilde { y } _ { i } in { - 1 , + 1 }$ . We can rewrite this objective as follows: \nConsequently the optimal function to add is \nThis can be found by applying the weak learner to a weighted version of the dataset, with weights ωi,m. \nAll that remains is to solve for the size of the update, $beta$ . Subsituting $F _ { m }$ into $L _ { m }$ and solving for $beta$ we find \nwhere \nTherefore overall update becomes \nAfter updating the strong learner, we need to recompute the weights for the next iteration, as follows: \nIf $tilde { y } _ { i } = F _ { m } ( pmb { x } _ { i } )$ , then $tilde { y } _ { i } F _ { m } ( { pmb x } _ { i } ) = 1$ , and if $tilde { y } _ { i } neq F _ { m } ( pmb { x } _ { i } )$ , then $tilde { y } _ { i } F _ { m } ( { pmb x } _ { i } ) = - 1$ . Hence $begin{array} { r l } { - tilde { y } _ { i } F _ { m } ( pmb { x } _ { i } ) = } end{array}$ $2 mathbb { I } left( tilde { y } _ { i } neq F _ { m } ( pmb { x } _ { o } ) right) - 1$ , so the update becomes \nSince the $e ^ { - beta _ { m } }$ is constant across all examples, it can be dropped. If we then define $alpha _ { m } = 2 beta _ { m }$ , the update becomes \nThus we see that we exponentially increase weights of misclassified examples. The resulting algorithm shown in Algorithm 8, and is known as Adaboost.M1 [FS96]. \nA multiclass generalization of exponential loss, and an adaboost-like algorithm to minimize it, known as SAMME (stagewise additive modeling using a multiclass exponential loss function), is described in [Has+09]. This is implemented in scikit learn (the AdaBoostClassifier class). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n\n18.5.4 LogitBoost \nThe trouble with exponential loss is that it puts a lot of weight on misclassified examples, as is apparent from the exponential blowup on the left hand side of Figure 18.7. This makes the method very sensitive to outliers (mislabeled examples). In addition, $e ^ { - tilde { y } f }$ is not the logarithm of any pmf for binary variables $tilde { y } in { - 1 , + 1 }$ ; consequently we cannot recover probability estimates from $f ( { pmb x } )$ . A natural alternative is to use log loss, as we discussed in Section 18.5.3. This only punishes mistakes linearly, as is clear from Figure 18.7. Furthermore, it means that we will be able to extract probabilities from the final learned function, using \nThe goal is to minimze the expected log-loss, given by \nBy performing a Newton update on this objective (similar to IRLS), one can derive the algorithm shown in Algorithm 9. This is known as logitBoost [FHT00]. The key subroutine is the ability of the weak learner $F$ to solve a weighted least squares problem. This method can be generalized to the multi-class setting, as explained in [FHT00]. \n18.5.5 Gradient boosting \nRather than deriving new versions of boosting for every different loss function, it is possible to derive a generic version, known as gradient boosting [Fri01; Mas+00]. To explain this, imagine solving $hat { pmb f } = mathrm { a r g m i n } _ { pmb f } mathcal { L } ( pmb f )$ by performing gradient descent in the space of functions. Since functions are infinite dimensional objects, we will represent them by their values on the training set, ${ pmb f } = ( f ( { pmb x } _ { 1 } ) , dots , f ( { pmb x } _ { N } ) )$ . At step $m$ , let ${ bf { nabla } } _ { bf { { g } } } _ { m }$ be the gradient of $mathcal { L } ( f )$ evaluated at $f = f _ { m - 1 }$ : \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Boosting",
        "subsubsection": "Exponential loss and AdaBoost"
    },
    {
        "content": "18.5.4 LogitBoost \nThe trouble with exponential loss is that it puts a lot of weight on misclassified examples, as is apparent from the exponential blowup on the left hand side of Figure 18.7. This makes the method very sensitive to outliers (mislabeled examples). In addition, $e ^ { - tilde { y } f }$ is not the logarithm of any pmf for binary variables $tilde { y } in { - 1 , + 1 }$ ; consequently we cannot recover probability estimates from $f ( { pmb x } )$ . A natural alternative is to use log loss, as we discussed in Section 18.5.3. This only punishes mistakes linearly, as is clear from Figure 18.7. Furthermore, it means that we will be able to extract probabilities from the final learned function, using \nThe goal is to minimze the expected log-loss, given by \nBy performing a Newton update on this objective (similar to IRLS), one can derive the algorithm shown in Algorithm 9. This is known as logitBoost [FHT00]. The key subroutine is the ability of the weak learner $F$ to solve a weighted least squares problem. This method can be generalized to the multi-class setting, as explained in [FHT00]. \n18.5.5 Gradient boosting \nRather than deriving new versions of boosting for every different loss function, it is possible to derive a generic version, known as gradient boosting [Fri01; Mas+00]. To explain this, imagine solving $hat { pmb f } = mathrm { a r g m i n } _ { pmb f } mathcal { L } ( pmb f )$ by performing gradient descent in the space of functions. Since functions are infinite dimensional objects, we will represent them by their values on the training set, ${ pmb f } = ( f ( { pmb x } _ { 1 } ) , dots , f ( { pmb x } _ { N } ) )$ . At step $m$ , let ${ bf { nabla } } _ { bf { { g } } } _ { m }$ be the gradient of $mathcal { L } ( f )$ evaluated at $f = f _ { m - 1 }$ : \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Boosting",
        "subsubsection": "LogitBoost"
    },
    {
        "content": "18.5.4 LogitBoost \nThe trouble with exponential loss is that it puts a lot of weight on misclassified examples, as is apparent from the exponential blowup on the left hand side of Figure 18.7. This makes the method very sensitive to outliers (mislabeled examples). In addition, $e ^ { - tilde { y } f }$ is not the logarithm of any pmf for binary variables $tilde { y } in { - 1 , + 1 }$ ; consequently we cannot recover probability estimates from $f ( { pmb x } )$ . A natural alternative is to use log loss, as we discussed in Section 18.5.3. This only punishes mistakes linearly, as is clear from Figure 18.7. Furthermore, it means that we will be able to extract probabilities from the final learned function, using \nThe goal is to minimze the expected log-loss, given by \nBy performing a Newton update on this objective (similar to IRLS), one can derive the algorithm shown in Algorithm 9. This is known as logitBoost [FHT00]. The key subroutine is the ability of the weak learner $F$ to solve a weighted least squares problem. This method can be generalized to the multi-class setting, as explained in [FHT00]. \n18.5.5 Gradient boosting \nRather than deriving new versions of boosting for every different loss function, it is possible to derive a generic version, known as gradient boosting [Fri01; Mas+00]. To explain this, imagine solving $hat { pmb f } = mathrm { a r g m i n } _ { pmb f } mathcal { L } ( pmb f )$ by performing gradient descent in the space of functions. Since functions are infinite dimensional objects, we will represent them by their values on the training set, ${ pmb f } = ( f ( { pmb x } _ { 1 } ) , dots , f ( { pmb x } _ { N } ) )$ . At step $m$ , let ${ bf { nabla } } _ { bf { { g } } } _ { m }$ be the gradient of $mathcal { L } ( f )$ evaluated at $f = f _ { m - 1 }$ : \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nAlgorithm 9: LogitBoost, for binary classification with log-loss \n1 $omega _ { i } = 1 / N$ , $pi _ { i } = 1 / 2$   \n2 for $m = 1 : M$ do   \n3 Compute the working response zi = πiy(i∗1−πii)   \n4 Compute the weights $omega _ { i } = pi _ { i } ( 1 - pi _ { i } )$   \n5 $begin{array} { r } { F _ { m } = mathrm { a r g m i n } _ { F } sum _ { i = 1 } ^ { N } omega _ { i } ( z _ { i } - F ( pmb { x } _ { i } ) ) ^ { 2 } } end{array}$   \n6 Update $begin{array} { r } { f ( pmb { x } )  f ( pmb { x } ) + frac { 1 } { 2 } F _ { m } ( pmb { x } ) } end{array}$   \n7 Compute $pi _ { i } = 1 / ( 1 + exp ( - 2 f ( pmb { x } _ { i } ) ) )$ ; \n\nTable 18.1: Some commonly used loss functions, their gradients, and their population minimizers $F ^ { * }$ . For binary classification problems, we assume $tilde { y } _ { i } in { - 1 , + 1 }$ , and $pi _ { i } = sigma ( 2 f ( pmb { x } _ { i } ) )$ . For regression problems, we assume $y _ { i } in mathbb { R }$ . Adapted from [HTF09, p360] and [BH07, p483]. \nGradients of some common loss functions are given in Table 18.1. We then make the update \nwhere $beta _ { m }$ is the step length, chosen by \nIn its current form, this is not much use, since it only optimizes $f$ at a fixed set of $N$ points, so we do not learn a function that can generalize. However, we can modify the algorithm by fitting a weak learner to approximate the negative gradient signal. That is, we use this update \nThe overall algorithm is summarized in Algorithm 10. We have omitted the line search step for $beta _ { m }$ , which is not strictly necessary, as argued in [BH07]. However, we have introduced a learning rate or shrinkage factor $0 < nu leq 1$ , to control the size of the updates, for regularization purposes. \nIf we apply this algorithm using squared loss, we recover L2Boosting, since $- g _ { i m } = y _ { i } - f _ { m - 1 } ( { bf x } _ { i } )$ is just the residual error. We can also apply this algorithm to other loss functions, such as absolute loss or Huber loss (Section 5.1.5.3), which is useful for robust regression problems. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nAlgorithm 10: Gradient boosting \nFor classification, we can use log-loss. In this case, we get an algorithm known as BinomialBoost [BH07]. The advantage of this over LogitBoost is that it does not need to be able to do weighted fitting: it just applies any black-box regression model to the gradient vector. To apply this to multi-class classification, we can fit $C$ separate regression trees, using the pseudo residual of the form \nAlthough the trees are fit separately, their predictions are combined via a softmax transform \nWhen we have large datasets, we can use a stochastic variant in which we subsample (without replacement) a random fraction of the data to pass to the regression tree at each iteration. This is called stochastic gradient boosting [Fri99]. Not only is it faster, but it can also generalize better, because subsampling the data is a form of regularization. \n18.5.5.1 Gradient tree boosting \nIn practice, gradient boosting nearly always assumes that the weak learner is a regression tree, which is a model of the form \nwhere $w _ { j m }$ is the predicted output for region $R _ { j m }$ . (In general, $w _ { j m }$ could be a vector.) This combination is called gradient boosted regression trees, or gradient tree boosting. (A related version is known as MART, which stands for “multivariate additive regression trees” [FM03].) To use this in gradient boosting, we first find good regions $R _ { j m }$ for tree $m$ using standard regression tree learning (see Section 18.1) on the residuals; we then (re)solve for the weights of each leaf by solving \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nFor squared error (as used by gradient boosting), the optimal weight $hat { w } _ { j m }$ is the just the mean of the residuals in that leaf. \n18.5.5.2 XGBoost \nXGBoost (https://github.com/dmlc/xgboost), which stands for “extreme gradient boosting”, is a very efficient and widely used implementation of gradient boosted trees, that adds a few more improvements beyond the description in Section 18.5.5.1. The details can be found in [CG16], but in brief, the extensions are as follows: it adds a regularizer on the tree complexity, it uses a second order approximation of the loss (from [FHT00]) instead of just a linear approximation, it samples features at internal nodes (as in random forests), and it uses various computer science methods (such as handling out-of-core computation for large datasets) to ensure scalability.2 \nIn more detail, XGBoost optimizes the following regularized objective \nwhere \nis the regularizer, where $J$ is the number of leaves, and $gamma geq 0$ and $lambda geq 0$ are regularization coefficients. At the $m$ ’th step, the loss is given by \nWe can compute a second order Taylor expansion of this as follows: \nwhere $h _ { i m }$ is the Hessian \nIn the case of regression trees, we have $F ( pmb { x } ) = w _ { q ( pmb { x } ) }$ , where $q : mathbb { R } ^ { D }  { 1 , ldots , J }$ specifies which leaf node $_ { x }$ belongs to, and $boldsymbol { w } in mathbb { R } ^ { J }$ are the leaf weights. Hence we can rewrite Equation (18.49) as \nfollows, dropping terms that are independent of $F _ { m }$ : \nwhere $I _ { j } = { i : q ( { pmb x } _ { i } ) = j }$ is the set of indices of data points assigned to the $j$ ’th leaf. Let us define $begin{array} { r } { G _ { j m } = sum _ { i in I _ { j } } g _ { i m } } end{array}$ and $begin{array} { r } { H _ { j m } = sum _ { i in I _ { j } } h _ { i m } } end{array}$ . Then the above simplifies to \nThis is a quadratic in each $w _ { j } mathrm { m }$ so the optimal weights are given by \nThe loss for evaluating different tree structures $q$ then becomes \nWe can greedily optimize this using a recursive node splitting procedure, as in Section 18.1. Specifically, for a given leaf $j$ , we consider splitting it into a left and right half, $I = I _ { L } cup I _ { R }$ . We can compute the gain (reduction in loss) of such a split as follows: \nwhere $begin{array} { r } { G _ { L } = sum _ { i in I _ { L } } g _ { i m } } end{array}$ , $begin{array} { r } { G _ { R } = sum _ { i in I _ { R } } g _ { i m } } end{array}$ , $begin{array} { r } { H _ { L } = sum _ { i in I _ { L } } h _ { i m } } end{array}$ , and $begin{array} { r } { H _ { R } = sum _ { i in I _ { R } } h _ { i m } } end{array}$ . Thus we see that it is not worth splitting a node if the gain is negative (i.e., the first term is less than $gamma$ ). \nA fast approximation for evaluating this objective, that does not require sorting the features (for choosing the optimal threshold to split on), is described in [CG16]. \n18.6 Interpreting tree ensembles \nTrees are popular because they are interpretable. Unfortunately, ensembles of trees (whether in the form of bagging, random forests, or boosting) lose that property. Fortunately, there are some simple methods we can use to interpret what function has been learned. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Boosting",
        "subsubsection": "Gradient boosting"
    },
    {
        "content": "18.6.1 Feature importance \nFor a single decision tree $T$ , [BFO84] proposed the following measure for feature importance of feature $k$ : \nwhere the sum is over all non-leaf (internal) nodes, $G _ { j }$ is the gain in accuracy (reduction in cost) at node $j$ , and $v _ { j } = k$ if node $j$ uses feature $k$ . We can get a more reliable estimate by averaging over all trees in the ensemble: \nAfter computing these scores, we can normalize them so the largest value is $1 0 0 %$ . We give some examples below. \nFigure 18.8 gives an example of estimating feature importance for a classifier trained to distinguish MNIST digits from classes 0 and 8. We see that it focuses on the parts of the image that differ between these classes. \nIn Figure 18.9, we plot the relative importance of each of the features for the spam dataset (Section 18.4). Not surprisingly, we find that the most important features are the words “george” (the name of the recipient) and “hp” (the company he worked for), as well as the characters ! and $$ 9$ . (Note it can be the presence or absence of these features that is informative.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n18.6.2 Partial dependency plots \nAfter we have identified the most relevant input features, we can try to assess the impact they have on the output. A partial dependency plot for feature $k$ is a plot of \nvs $x _ { k }$ . Thus we marginalize out all features except $k$ . In the case of a binary classifier, we can convert this to log odds, $log p ( y = 1 | x _ { k } ) / p ( y = 0 | x _ { k } )$ , before plotting. We illustrate this for our spam example in Figure 18.10a for 4 different features. We see that as the frequency of ! and “remove” increases, so does the probability of spam. Conversely, as the frequency of “edu” or “hp” increases, the probability of spam decreases. \nWe can also try to capture interaction effects between features $j$ and $k$ by computing \nWe illustrate this for our spam example in Figure 18.10b for hp and !. We see that higher frequency of ! makes it more likely to be spam, but much more so if the word “hp” is missing.",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Interpreting tree ensembles",
        "subsubsection": "Feature importance"
    },
    {
        "content": "18.6.2 Partial dependency plots \nAfter we have identified the most relevant input features, we can try to assess the impact they have on the output. A partial dependency plot for feature $k$ is a plot of \nvs $x _ { k }$ . Thus we marginalize out all features except $k$ . In the case of a binary classifier, we can convert this to log odds, $log p ( y = 1 | x _ { k } ) / p ( y = 0 | x _ { k } )$ , before plotting. We illustrate this for our spam example in Figure 18.10a for 4 different features. We see that as the frequency of ! and “remove” increases, so does the probability of spam. Conversely, as the frequency of “edu” or “hp” increases, the probability of spam decreases. \nWe can also try to capture interaction effects between features $j$ and $k$ by computing \nWe illustrate this for our spam example in Figure 18.10b for hp and !. We see that higher frequency of ! makes it more likely to be spam, but much more so if the word “hp” is missing. \nPart V \nBeyond Supervised Learning",
        "chapter": "IV Nonparametric Models",
        "section": "Trees, Forests, Bagging, and Boosting",
        "subsection": "Interpreting tree ensembles",
        "subsubsection": "Partial dependency plots"
    },
    {
        "content": "19 Learning with Fewer Labeled Examples \nMany ML models, especially neural networks, often have many more parameters than we have labeled training examples. For example, a ResNet CNN (Section 14.3.4) with 50 layers has 23 million parameters. Transformer models (Section 15.5) can be even bigger. Of course these parameters are highly correlated, so they are not independent “degrees of freedom”. Nevertheless, such big models are slow to train and, more importantly, they may easily overfit. This is particularly a problem when you do not have a large labeled training set. In this chapter, we discuss some ways to tackle this issue, beyond the generic regularization techniques we discussed in Section 13.5 such as early stopping, weight decay and dropout. \n19.1 Data augmentation \nSuppose we just have a single small labeled dataset. In some cases, we may be able to create artificially modified versions of the input vectors, which capture the kinds of variations we expect to see at test time, while keeping the original labels unchanged. This is called data augmentation.1 We give some examples below, and then discuss why this approach works. \n19.1.1 Examples \nFor image classification tasks, standard data augmentation methods include random crops, zooms, and mirror image flips, as illustrated in Figure 19.1. [GVZ16] gives a more sophisticated example, where they render text characters onto an image in a realistic way, thereby creating a very large dataset of text “in the wild”. They used this to train a state of the art visual text localization and reading system. Other examples of data augmentation include artifically adding background noise to clean speech signals, and artificially replacing characters or words at random in text documents. \nIf we afford to train and test the model many times using different versions of the data, we can learn which augmentations work best, using blackbox optimization methods such as RL (see e.g., [Cub+19]) or Bayesian optimization (see e.g., [Lim+19]); this is called AutoAugment. We can also learn to combine multiple augmentations together; this is called AutoAugment [Cub+19]. \nFor some examples of augmentation in NLP, see e.g., [Fen+21]. \n19.1.2 Theoretical justification \nData augmentation often significantly improves performance (predictive accuracy, robustness, etc). At first this might seem like we are getting something for nothing, since we have not provided additional data. However, the data augmentation mechanism can be viewed as a way to algorithmically inject prior knowledge. \nTo see this, recall that in standard ERM training, we minimize the empirical risk \nwhere we approximate $p ^ { * } ( { pmb x } , { pmb y } )$ by the empirical distribution \nWe can think of data augmentation as replacing the empirical distribution with the following algorithmically smoothed distribution \nwhere $A$ is the data augmentation algorithm, which generates a sample $_ { x }$ from a training point ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , such that the label (“semantics”) is not changed. (A very simple example would be a Gaussian kernel, $p ( pmb { x } | pmb { x } _ { n } , A ) = mathcal { N } ( pmb { x } | pmb { x } _ { n } , sigma ^ { 2 } mathbf { I } )$ .) This has been called vicinal risk minimization [Cha+01], since we are minimizing the risk in the vicinity of each training point $_ { ast }$ . For more details on this perspective, see [Zha+17b; CDL19; Dao+19]. \n19.2 Transfer learning \nThis section is coauthored with Colin Raffel. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Data augmentation",
        "subsubsection": "Examples"
    },
    {
        "content": "19.1.2 Theoretical justification \nData augmentation often significantly improves performance (predictive accuracy, robustness, etc). At first this might seem like we are getting something for nothing, since we have not provided additional data. However, the data augmentation mechanism can be viewed as a way to algorithmically inject prior knowledge. \nTo see this, recall that in standard ERM training, we minimize the empirical risk \nwhere we approximate $p ^ { * } ( { pmb x } , { pmb y } )$ by the empirical distribution \nWe can think of data augmentation as replacing the empirical distribution with the following algorithmically smoothed distribution \nwhere $A$ is the data augmentation algorithm, which generates a sample $_ { x }$ from a training point ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , such that the label (“semantics”) is not changed. (A very simple example would be a Gaussian kernel, $p ( pmb { x } | pmb { x } _ { n } , A ) = mathcal { N } ( pmb { x } | pmb { x } _ { n } , sigma ^ { 2 } mathbf { I } )$ .) This has been called vicinal risk minimization [Cha+01], since we are minimizing the risk in the vicinity of each training point $_ { ast }$ . For more details on this perspective, see [Zha+17b; CDL19; Dao+19]. \n19.2 Transfer learning \nThis section is coauthored with Colin Raffel. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Data augmentation",
        "subsubsection": "Theoretical justification"
    },
    {
        "content": "Many data-poor tasks have some high-level structural similarity to other data-rich tasks. For example, consider the task of fine-grained visual classification of endangered bird species. Given that endangered birds are by definition rare, it is unlikely that a large quantity of diverse labeled images of these birds exist. However, birds bear many structural similarities across species - for example, most birds have wings, feathers, beaks, claws, etc. We therefore might expect that first training a model on a large dataset of non-endangered bird species and then continuing to train it on a small dataset of endangered species could produce better performance than training on the small dataset alone. \nThis is called transfer learning, since we are transferring information from one dataset to another, via a shared set of parameters. More precisely, we first perform a pre-training phase, in which we train a model with parameters $pmb theta$ on a large source dataset $mathcal { D } _ { p }$ ; this may be labeled or unlabeled. We then perform a second fine-tuning phase on the small labeled target dataset $mathcal { D } _ { q }$ of interest. We discuss these two phases in more detail below, but for more information, see e.g., [Tan+18; Zhu+21] for recent surveys. \n19.2.1 Fine-tuning \nSuppose, for now, that we already have a pretrained classifier, $p ( boldsymbol { y } | mathbf { x } , boldsymbol { theta } _ { p } )$ , such as a CNN, that works well for inputs $pmb { x } in mathcal { X } _ { p }$ (e.g. natural images) and outputs $boldsymbol { y } in mathcal { V } _ { p }$ (e.g., ImageNet labels), where the data comes from a distribution $p ( { pmb x } , { pmb y } )$ similar to the one used in training. Now we want to create a new model $q ( y | mathbf { x } , pmb { theta } _ { q } )$ that works well for inputs $pmb { x } in mathcal { X } _ { q }$ (e.g. bird images) and outputs $y in mathcal { V } _ { q }$ (e.g., fine-grained bird labels), where the data comes from a distribution $q ( { pmb x } , { pmb y } )$ which may be different from $p$ . \nWe will assume that the set of possible inputs is the same, so $mathcal { X } _ { q } approx mathcal { X } _ { p }$ (e.g., both are RGB images), or that we can easily transform inputs from domain $p$ to domain $q$ (e.g., we can convert an RGB image to grayscale by dropping the chrominance channels and just keeping luminance). (If this is not \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license the case, then we may need to use a method called domain adaptation, that modifies models to map between modalities, as discussed in Section 19.2.5.) \n\nHowever, the output domains are usually different, i.e., $mathcal { V } _ { q } not = mathcal { V } _ { p }$ . For example, $mathcal { V } _ { p }$ might be Imagenet labels and $mathcal { V } _ { q }$ might be medical labels (e.g., types of diabetic retinopathy [Arc+19]). In this case, we need to “translate” the output of the pre-trained model to the new domain. This is easy to do with neural networks: we simply “chop off” the final layer of the original model, and add a new “head” to model the new class labels, as illustrated in Figure 19.2. For example, suppose $p ( boldsymbol { y } | mathbf { x } , pmb { theta } _ { p } ) = mathrm { s o f t m a x } ( boldsymbol { y } | mathbf { W } _ { 2 } boldsymbol { h } ( mathbf { x } ; pmb { theta } _ { 1 } ) + b _ { 2 } )$ , where $pmb { theta } _ { p } = ( mathbf { W } _ { 2 } , pmb { b } _ { 2 } , pmb { theta } _ { 1 } )$ . Then we can construct 9 $begin{array} { r } { mathbf { Theta } _ { l } ( y | pmb { theta } _ { q } ) = mathrm { s o f t m a x } ( y | mathbf { W } _ { 3 } pmb { h } ( pmb { x } ; pmb { theta } _ { 1 } ) + pmb { b } _ { 3 } ) } end{array}$ , where $pmb { theta } _ { q } = ( mathbf { W } _ { 3 } , pmb { b } _ { 3 } , pmb { theta } _ { 1 } )$ and $pmb { h } ( pmb { x } ; pmb { theta } _ { 1 } )$ is the shared nonlinear feature extractor. \nAfter performing this “model surgery”, we can fine-tune the new model with parameters $theta _ { q } = $ $( theta _ { 1 } , theta _ { 3 } )$ , where $pmb { theta } _ { 1 }$ parameterizes the feature extractor, and $pmb { theta } _ { 3 }$ parameterizes the final linear layer that maps features to the new set of labels. If we treat $pmb { theta } _ { 1 }$ as “frozen parameters”, then the resulting model $q ( y | mathbf { x } , pmb { theta } _ { q } )$ is linear in its parameters, so we have a convex optimization problem for which many simple and efficient fitting methods exist (see Part II). This is particularly helpful in the long-tail setting, where some classes are very rare [Kan+20]. However, a linear “decoder” may be too limiting, so we can also allow $pmb { theta } _ { 1 }$ to be fine-tuned as well, but using a lower learning rate, to prevent the values moving too far from the values estimated on $mathcal { D } _ { p }$ . \n19.2.2 Adapters \nOne disadvantage of fine-tuning all the model parameters of a pre-trained model is that it can be slow, since there are often many parameters, and we may need to use a small learning rate to prevent the low-level feature extractors from diverging too far from their prior values. In addition, every new task requires a new model to be trained, making task sharing hard. An alternative approach is to keep the pre-trained model untouched, but to add new parameters to modify its internal behavior to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 customize the feature extraction process for each task. This idea is called adapters, and has been explored in several papers (e.g., [RBV17; RBV18; Hou+19]).",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Transfer learning",
        "subsubsection": "Fine-tuning"
    },
    {
        "content": "However, the output domains are usually different, i.e., $mathcal { V } _ { q } not = mathcal { V } _ { p }$ . For example, $mathcal { V } _ { p }$ might be Imagenet labels and $mathcal { V } _ { q }$ might be medical labels (e.g., types of diabetic retinopathy [Arc+19]). In this case, we need to “translate” the output of the pre-trained model to the new domain. This is easy to do with neural networks: we simply “chop off” the final layer of the original model, and add a new “head” to model the new class labels, as illustrated in Figure 19.2. For example, suppose $p ( boldsymbol { y } | mathbf { x } , pmb { theta } _ { p } ) = mathrm { s o f t m a x } ( boldsymbol { y } | mathbf { W } _ { 2 } boldsymbol { h } ( mathbf { x } ; pmb { theta } _ { 1 } ) + b _ { 2 } )$ , where $pmb { theta } _ { p } = ( mathbf { W } _ { 2 } , pmb { b } _ { 2 } , pmb { theta } _ { 1 } )$ . Then we can construct 9 $begin{array} { r } { mathbf { Theta } _ { l } ( y | pmb { theta } _ { q } ) = mathrm { s o f t m a x } ( y | mathbf { W } _ { 3 } pmb { h } ( pmb { x } ; pmb { theta } _ { 1 } ) + pmb { b } _ { 3 } ) } end{array}$ , where $pmb { theta } _ { q } = ( mathbf { W } _ { 3 } , pmb { b } _ { 3 } , pmb { theta } _ { 1 } )$ and $pmb { h } ( pmb { x } ; pmb { theta } _ { 1 } )$ is the shared nonlinear feature extractor. \nAfter performing this “model surgery”, we can fine-tune the new model with parameters $theta _ { q } = $ $( theta _ { 1 } , theta _ { 3 } )$ , where $pmb { theta } _ { 1 }$ parameterizes the feature extractor, and $pmb { theta } _ { 3 }$ parameterizes the final linear layer that maps features to the new set of labels. If we treat $pmb { theta } _ { 1 }$ as “frozen parameters”, then the resulting model $q ( y | mathbf { x } , pmb { theta } _ { q } )$ is linear in its parameters, so we have a convex optimization problem for which many simple and efficient fitting methods exist (see Part II). This is particularly helpful in the long-tail setting, where some classes are very rare [Kan+20]. However, a linear “decoder” may be too limiting, so we can also allow $pmb { theta } _ { 1 }$ to be fine-tuned as well, but using a lower learning rate, to prevent the values moving too far from the values estimated on $mathcal { D } _ { p }$ . \n19.2.2 Adapters \nOne disadvantage of fine-tuning all the model parameters of a pre-trained model is that it can be slow, since there are often many parameters, and we may need to use a small learning rate to prevent the low-level feature extractors from diverging too far from their prior values. In addition, every new task requires a new model to be trained, making task sharing hard. An alternative approach is to keep the pre-trained model untouched, but to add new parameters to modify its internal behavior to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 customize the feature extraction process for each task. This idea is called adapters, and has been explored in several papers (e.g., [RBV17; RBV18; Hou+19]). \n\nFigure 19.3a illustrates adapters for transformer networks (Section 15.5), as proposed in [Hou+19]. The basic idea is to insert two shallow bottleneck MLPs inside each transformer layer, one after the multi-head attention and once after the feed-forward layers. Note that these MLPs have skip connections, so that they can be initialized to implement the identity mapping. If the transformer layer has features of dimensionality $D$ , and the adapter uses a bottleneck of size $M$ , this introduces $O ( D M )$ new parameters per layer. These adapter MLPs, as well as the layer norm parameters and final output head, are trained for each new task, but the all remaining parameters are frozen. Empirically on several NLP benchmarks, this is found to give better performance than fine tuning, while only needing about 1-10% of the original parameters. \nFigure 19.3b illustrates adapters for residual networks (Section 14.3.4), as proposed in [RBV17; RBV18]. The basic idea is to add a 1x1 convolution layer $alpha$ , which is analogous to the MLP adapter in the transformer case, to the internal layers of the CNN. This can be added in series or in parallel, as shown in the diagram. If we denote the adapter layer by $rho ( { pmb x } )$ , we can define the series adapter to be \nwhere $mathrm { d i a g } _ { 1 } ( { pmb { alpha } } ) in mathbb { R } ^ { 1 times 1 times C times D }$ reshapes a matrix $pmb { alpha } in mathbb { R } ^ { C times D }$ into a matrix that can be applied to each spatial location in parallel. (We have omitted batch normalization for simplicity.) If we insert this after a regular convolution layer $f circledast x$ we get \nThis can be interpreted as a low-rank multiplicative perturbation to the original filter $f$ . The parallel adapter can be defined by \nThis can be interpreted as a low-rank additive perturbation to the original filter $f$ . In both cases, setting $alpha = 0$ ensures the adapter layers can be initialized to the identity transformation. In addition, both methods required $O ( C ^ { 2 } )$ parameters per layer. \n19.2.3 Supervised pre-training \nThe pre-training task may be supervised or unsupervised; the main requirements are that it can teach the model basic structure about the problem domain and that it is sufficiently similar to the downstream fine-tuning task. The notion of task similarity is not rigorously defined, but in practice the domain of the pre-training task is often more broad than that of the fine-tuning task (e.g., pre-train on all bird species and fine-tune on endangered ones). \nThe most straightforward form of transfer learning is the case where a large labeled dataset is suitable for pre-training. For example, it is very common to use the ImageNet dataset (Section 1.5.1.2) to pretrain CNNs, which can then be used for an a variety of downstream tasks and datasets (see e.g., [Kol+19]). Imagenet has 1.28 million natural images, each associated with a label from one of 1,000 classes. The classes constitute a wide variety of different concepts, including animals, foods, buildings, musical instruments, clothing, and so on. The images themselves are diverse in the sense \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license that they contain objects from many angles and in many sizes with a wide variety of backgrounds. This diversity and scale may partially explain why it has become a de-facto pre-training task for transfer learning in computer vision. (See finetune_cnn_jax.ipynb for some example code.)",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Transfer learning",
        "subsubsection": "Adapters"
    },
    {
        "content": "Figure 19.3a illustrates adapters for transformer networks (Section 15.5), as proposed in [Hou+19]. The basic idea is to insert two shallow bottleneck MLPs inside each transformer layer, one after the multi-head attention and once after the feed-forward layers. Note that these MLPs have skip connections, so that they can be initialized to implement the identity mapping. If the transformer layer has features of dimensionality $D$ , and the adapter uses a bottleneck of size $M$ , this introduces $O ( D M )$ new parameters per layer. These adapter MLPs, as well as the layer norm parameters and final output head, are trained for each new task, but the all remaining parameters are frozen. Empirically on several NLP benchmarks, this is found to give better performance than fine tuning, while only needing about 1-10% of the original parameters. \nFigure 19.3b illustrates adapters for residual networks (Section 14.3.4), as proposed in [RBV17; RBV18]. The basic idea is to add a 1x1 convolution layer $alpha$ , which is analogous to the MLP adapter in the transformer case, to the internal layers of the CNN. This can be added in series or in parallel, as shown in the diagram. If we denote the adapter layer by $rho ( { pmb x } )$ , we can define the series adapter to be \nwhere $mathrm { d i a g } _ { 1 } ( { pmb { alpha } } ) in mathbb { R } ^ { 1 times 1 times C times D }$ reshapes a matrix $pmb { alpha } in mathbb { R } ^ { C times D }$ into a matrix that can be applied to each spatial location in parallel. (We have omitted batch normalization for simplicity.) If we insert this after a regular convolution layer $f circledast x$ we get \nThis can be interpreted as a low-rank multiplicative perturbation to the original filter $f$ . The parallel adapter can be defined by \nThis can be interpreted as a low-rank additive perturbation to the original filter $f$ . In both cases, setting $alpha = 0$ ensures the adapter layers can be initialized to the identity transformation. In addition, both methods required $O ( C ^ { 2 } )$ parameters per layer. \n19.2.3 Supervised pre-training \nThe pre-training task may be supervised or unsupervised; the main requirements are that it can teach the model basic structure about the problem domain and that it is sufficiently similar to the downstream fine-tuning task. The notion of task similarity is not rigorously defined, but in practice the domain of the pre-training task is often more broad than that of the fine-tuning task (e.g., pre-train on all bird species and fine-tune on endangered ones). \nThe most straightforward form of transfer learning is the case where a large labeled dataset is suitable for pre-training. For example, it is very common to use the ImageNet dataset (Section 1.5.1.2) to pretrain CNNs, which can then be used for an a variety of downstream tasks and datasets (see e.g., [Kol+19]). Imagenet has 1.28 million natural images, each associated with a label from one of 1,000 classes. The classes constitute a wide variety of different concepts, including animals, foods, buildings, musical instruments, clothing, and so on. The images themselves are diverse in the sense \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license that they contain objects from many angles and in many sizes with a wide variety of backgrounds. This diversity and scale may partially explain why it has become a de-facto pre-training task for transfer learning in computer vision. (See finetune_cnn_jax.ipynb for some example code.) \n\nHowever, Imagenet pre-training has been shown to be less helpful when the domain of the finetuning task is quite different from natural images (e.g. medical images [Rag+19]). And in some cases where it is helpful (e.g., training object detection systems), it seems to be more of a speedup trick (by warm-starting optimization at a good point) rather than something that is essential, in the sense that one can achieve comparable performance on the downstream task when training from scratch, if done for long enough [HGD19]. \nSupervised pre-training is somewhat less common in non-vision applications. One notable exception is to pre-train on natural language inference data (i.e. whether a sentence implies or contradicts another) to learn vector representations of sentences [Con+17], though this approach has largely been supplanted by unsupervised methods (Section 19.2.4). Another non-vision application of transfer learning is to pre-train a speech recognition on a large English-labeled corpus before fine-tuning on low-resource languages [Ard+20]. \n19.2.4 Unsupervised pre-training (self-supervised learning) \nIt is increasingly common to use unsupervised pre-training, because unlabeled data is often easy to acquire, e.g., unlabeled images or text documents from the web. \nFor a short period of time it was common to pre-train deep neural networks using an unsupervised objective (e.g., reconstruction error, as discussed in Section 20.3) over the labeled dataset (i.e. ignoring the labels) before proceeding with standard supervised training [HOT06; Vin+10b; Erh+10]. While this technique is also called unsupervised pre-training, it differs from the form of pre-training for transfer learning we discuss in this section, which uses a (large) unlabeled dataset for pre-training before fine-tuning on a different (smaller) labeled dataset. \nPre-training tasks that use unlabeled data are often called self-supervised rather than unsupervised. This term is used because the labels are created by the algorithm, rather than being provided externally by a human, as in standard supervised learning. Both supervised and self-supervised learning are discriminative tasks, since they require predicting outputs given inputs. By contrast, other unsupervised approaches, such as some of those discussed in Chapter 20, are generative, since they predict outputs unconditionally. \nThere are many different self-supervised learning heuristics that have been tried (see e.g., [GR18; JT19; Ren19] for a review, and https://github.com/jason718/awesome-self-supervised-learning for an extensive list of papers). We can identify at least three main broad groups, which we discuss below. \n19.2.4.1 Imputation tasks \nOne approach to self-supervised learning is to solve imputation tasks. In this approach, we partition the input vector $_ { x }$ into two parts, $pmb { x } = ( pmb { x } _ { h } , pmb { x } _ { v } )$ , and then try to predict the hidden part $_ { x h }$ given the remaining visible part, ${ bf { sigma } } _ { x }$ , using a model of the form $hat { pmb { x } } _ { h } = f ( pmb { x } _ { v } , pmb { x } _ { h } = mathbf { 0 } )$ . We can think of this as a “fill-in-the-blank” task; in the NLP community, this is called a cloze task. See Figure 19.4 for some visual examples, and Section 15.7.2 for some NLP examples. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Transfer learning",
        "subsubsection": "Supervised pre-training"
    },
    {
        "content": "However, Imagenet pre-training has been shown to be less helpful when the domain of the finetuning task is quite different from natural images (e.g. medical images [Rag+19]). And in some cases where it is helpful (e.g., training object detection systems), it seems to be more of a speedup trick (by warm-starting optimization at a good point) rather than something that is essential, in the sense that one can achieve comparable performance on the downstream task when training from scratch, if done for long enough [HGD19]. \nSupervised pre-training is somewhat less common in non-vision applications. One notable exception is to pre-train on natural language inference data (i.e. whether a sentence implies or contradicts another) to learn vector representations of sentences [Con+17], though this approach has largely been supplanted by unsupervised methods (Section 19.2.4). Another non-vision application of transfer learning is to pre-train a speech recognition on a large English-labeled corpus before fine-tuning on low-resource languages [Ard+20]. \n19.2.4 Unsupervised pre-training (self-supervised learning) \nIt is increasingly common to use unsupervised pre-training, because unlabeled data is often easy to acquire, e.g., unlabeled images or text documents from the web. \nFor a short period of time it was common to pre-train deep neural networks using an unsupervised objective (e.g., reconstruction error, as discussed in Section 20.3) over the labeled dataset (i.e. ignoring the labels) before proceeding with standard supervised training [HOT06; Vin+10b; Erh+10]. While this technique is also called unsupervised pre-training, it differs from the form of pre-training for transfer learning we discuss in this section, which uses a (large) unlabeled dataset for pre-training before fine-tuning on a different (smaller) labeled dataset. \nPre-training tasks that use unlabeled data are often called self-supervised rather than unsupervised. This term is used because the labels are created by the algorithm, rather than being provided externally by a human, as in standard supervised learning. Both supervised and self-supervised learning are discriminative tasks, since they require predicting outputs given inputs. By contrast, other unsupervised approaches, such as some of those discussed in Chapter 20, are generative, since they predict outputs unconditionally. \nThere are many different self-supervised learning heuristics that have been tried (see e.g., [GR18; JT19; Ren19] for a review, and https://github.com/jason718/awesome-self-supervised-learning for an extensive list of papers). We can identify at least three main broad groups, which we discuss below. \n19.2.4.1 Imputation tasks \nOne approach to self-supervised learning is to solve imputation tasks. In this approach, we partition the input vector $_ { x }$ into two parts, $pmb { x } = ( pmb { x } _ { h } , pmb { x } _ { v } )$ , and then try to predict the hidden part $_ { x h }$ given the remaining visible part, ${ bf { sigma } } _ { x }$ , using a model of the form $hat { pmb { x } } _ { h } = f ( pmb { x } _ { v } , pmb { x } _ { h } = mathbf { 0 } )$ . We can think of this as a “fill-in-the-blank” task; in the NLP community, this is called a cloze task. See Figure 19.4 for some visual examples, and Section 15.7.2 for some NLP examples. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n19.2.4.2 Proxy tasks \nAnother approach to SSL is to solve proxy tasks, also called pretext tasks. In this setup, we create pairs of inputs, $( pmb { x } _ { 1 } , pmb { x } _ { 2 } )$ , and then train a Siamese network classifier (Figure 16.5a) of the form $p ( y | pmb { x } _ { 1 } , pmb { x } _ { 2 } ) = p ( y | r [ f ( pmb { x } _ { 1 } ) , f ( pmb { x } _ { 2 } ) ] )$ , where $f ( { pmb x } )$ is some function that performs “representation learning” [BCV13], and $y$ is some label that captures the relationship between $mathbf { Delta x } _ { 1 }$ and $scriptstyle { pmb { x } } _ { 2 }$ , which is predicted by $r ( f _ { 1 } , f _ { 2 } )$ . For example, suppose ${ bf { x } } _ { 1 }$ is an image patch, and ${ pmb x } _ { 2 } = t ( { pmb x } _ { 1 } )$ is some transformation of ${ bf { x } } _ { 1 }$ that we control, such as a random rotation; then we define $y$ to be the rotation angle that we used [GSK18]. \n19.2.4.3 Contrastive tasks \nThe currently most popular approach to self-supervised learning is to use various kinds of contrastive tasks. The basic idea is to create pairs of examples that are semantically similar to each other, using data augmentation methods (Section 19.1), and then to ensure that the distance between their representations is closer (in embedding space) than the distance between two unrelated examples. This is exactly the same idea that is used in deep metric learning (Section 16.2.2) — the only difference is that the algorithm creates its own similar pairs, rather than relying on an externally provided measure of similarity, such as labels. We give some examples of this in Section 19.2.4.4 and Section 19.2.4.5. \n19.2.4.4 SimCLR \nIn this section, we discuss SimCLR, which stands for “Simple contrastive learning of visual representations” [Che+20b; Che+20c]. This has shown state of the art performance on transfer learning and semi-supervised learning. The basic idea is as follows. Each input $pmb { x } in mathbb { R } ^ { D }$ is converted to two augmented “views’ ${ pmb x } _ { 1 } = t _ { 1 } ( { pmb x } )$ , ${ pmb x } _ { 2 } = t _ { 2 } ( { pmb x } )$ , which are “semantically equivalent” versions of the input generated by some transformations $t _ { 1 } , t _ { 2 }$ . For example, if $_ { x }$ is an image, these could be small perturbations to the image, such as random crops, as discussed in Section 19.1. In addition, we sample “negative” examples $pmb { x } _ { 1 } ^ { - } , ldots , pmb { x } _ { n } ^ { - } in N ( pmb { x } )$ from the dataset which represent “semantically different” images (in practice, these are the other examples in the minibatch). Next we define some feature mapping $F : mathbb { R } ^ { D }  mathbb { R } ^ { E }$ , where $D$ is the size of the input, and $E$ is the size of the embedding. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe then try to maximize the similarity of the similar views, while minimizing the similarity of the different views, for each input $_ { x }$ : \nIn practice, we use cosine similarity, so we $ell _ { 2 }$ -normalize the representations produced by $F$ before taking inner products, but this is omitted in the above equation. See Figure 19.5a for an illustration. (In this figure, we assume $F ( { pmb x } ) = g ( r ( { pmb x } ) )$ , where the intermediate representation $boldsymbol { h } = boldsymbol { r } ( boldsymbol { x } )$ is the one that will be later used for fine-tuning, and $g$ is an additional transformation applied during training.) Interestingly, we can interpret this as a form of conditional energy based model of the form \nwhere $mathcal { E } ( { pmb x } _ { 2 } | { pmb x } _ { 1 } ) = - F ( { pmb x } _ { 2 } ) ^ { 1 } F ( { pmb x } _ { 1 } )$ is the energy, and \nis the normalization constant, known as the partition function. The conditional log likelihood under this model has the form \nThe only difference from Equation (19.7) is that we replace the integral with a Monte Carlo upper bound derived from the negative samples. Thus we can think of contrastive learning as approximate maximum likelihood estimation of a conditional energy based generative model [Gra+20]. More details on such models can be found in the sequel to this book, [Mur23]. \nA critical ingredient to the success of SimCLR is the choice of data augmentation methods. By using random cropping, they can force the model to predict local views from global views, as well as to predict adjacent views of the same image (see Figure 19.5). After cropping, all images are resized back to the same size. In addition, they randomly flip the image some fraction of the time.2 \nSimCLR relies on large batch training, in order to ensure a sufficiently diverse set of negatives. When this is not possible, we can use a memory bank of past (negative) embeddings, which can be updated using exponential moving averaging (Section 4.4.2.2). This is known as momentum contrastive learning or MoCo [He+20]. \n19.2.4.5 CLIP \nIn this section, we describe CLIP, which stands for “Contrastive Language-Image Pre-training” [Rad+]. This is a contrastive approach to representation learning which uses a massive corpus of \n400M (image, text) pairs extracted from the web. Let ${ boldsymbol { x } } _ { i }$ be the $textit { textbf {  i } }$ ’th image and $mathbf { nabla } _ { mathbf { boldsymbol { y } } _ { i } }$ be its matching text. Rather than trying to predict the exact words associated with the image, it is simpler to just determine if $pmb { y } _ { i }$ is more likely to be the correct text compared to ${ bf { y } } _ { j }$ , for some other text string $j$ in the minibatch. Similarly, the model can try to determine if image ${ bf { x } } _ { i }$ is more likely to be matched than $boldsymbol { mathscr { x } } _ { j }$ to a given text $mathbf { nabla } _ { mathbf { mathcal { Y } } _ { i } }$ . \nMore precisely, let ${ f } _ { I } ( { pmb x } _ { i } )$ be the embedding of the image, $f _ { T } ( pmb { y } _ { j } )$ be the embedding of the text, $mathbf { I } _ { i } = { f _ { I } ( pmb { x } _ { i } ) } / { | | pmb { f } _ { I } ( pmb { x } _ { i } ) | | _ { 2 } }$ be the unit-norm version of the image embedding, and $mathbf { T } _ { j } = { f _ { T } ( pmb { y } _ { j } ) } / { | | pmb { f } _ { T } ( pmb { y } _ { j } ) | | _ { 2 } }$ be the unit-norm version of the text embedding. Define the vector of pairwise logits (similarity scores) to be \nWe now train the parameters of the two embedding functions $f _ { I }$ and $f _ { T }$ to minimize the following loss, averaged over minibatches of size $N$ : \nwhere CE is the cross entropy loss \nand ${ bf 1 } _ { i }$ is a one-hot encoding of label $i$ . See Figure 19.7a for an illustration. (In practice, the normalized embeddings are scaled by a temperature parameter which is also learned; this controls the sharpness of the softmax.) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn their paper, they considered using a ResNet (Section 14.3.4) and a vision transformer (Section 15.5.6) for the function $f _ { I }$ , and a text transformer (Section 15.5) for $f _ { T }$ . They used a very large minibatch of $N sim 3 2 k$ , and trained for many days on 100s of GPUs. \nAfter the model is trained, it can be used for zero-shot classification of an image $_ { x }$ as follows. First each of the $K$ possible class labels for a given dataset is converted into a text string $mathbf { mathcal { { y } } } _ { k }$ that might occur on the web. For example, “dog” becomes “a photo of a dog”. Second, we compute the normalized emebddings $mathbf { I } propto f _ { I } ( { pmb x } )$ and $mathbf { T } _ { k } propto mathbf { f } _ { T } ( mathbf { mathbf { y } } _ { k } )$ . Third, we compute the softmax probabilites \nSee Figure 19.7b for an illustration. (A similar approach was adopted in the visual n-grams paper [Li+17].) \nRemarkably, this approach can perform as well as standard supervised learning on tasks such as ImageNet classification, without ever being explicitly trained on specific labeled datasets. Of course, the images in ImageNet come from the web, and were found using text-based web-search, so the model has seen similar data before. Nevertheless, its generalization to new tasks, and robustness to distribution shift, are quite impressive (see the paper for examples). \nOne drawback of the approach, however, is that it is sensitive to how class labels are converted to textual form. For example, to make the model work on food classification, it is necessary to use text strings of the form “a photo of guacamole, a type of food”, “a photo of ceviche, a type of food”, etc. Disambiguating phrases such as “a type of food” are currently added by hand, on a per-dataset basis. This is called prompt engineering, and is needed since the raw class names can be ambiguous across (and sometimes within) a dataset. \n19.2.5 Domain adaptation \nConsider a problem in which we have inputs from different domains, such as a source domain $mathcal { X } _ { s }$ and target domain $mathcal { X } _ { t }$ , but a common set of output labels, $_ { mathcal { V } }$ . (This is the “dual” of transfer learning, since the input domains are different, but the output domains the same.) For example, the domains might be images from a computer graphics system and real images, or product reviews and movie reviews. We assume we do not have labeled examples from the target domain. Our goal is to fit the model on the source domain, and then modify its parameters so it works on the target domain. This is called (unsupervised) domain adaptation (see e.g., [KL21] for a review). \nA common approach to this problem is to train the source classifier in such a way that it cannot distinguish whether the input is coming from the source or target distribution; in this case, it will only be able to use features that are common to both domains. This is called domain adversarial learning [Gan+16]. More formally, let $d _ { n } in { s , t }$ be a label that specifies if the data example $n$ comes from domain $s$ or $t$ . We want to optimize \nwhere $N _ { s } ~ = ~ | D _ { s } |$ , $N _ { t } ~ = ~ | D _ { t } |$ , $f$ maps $mathcal { X } _ { s } cup mathcal { X } _ { t } to mathcal { H }$ , and $g$ maps $mathcal { H }  mathcal { V } _ { t }$ . The objective in Equation (19.15) minimizes the loss on the desired task of classifying $y$ , but maximizes the loss on the auxiliary task of classifying the source domain $d$ . This can be implemented by the gradient sign reversal trick, and is related to GANs (generative adversarial networks). See e.g., [Csu17; Wu+19] for some other approaches to domain adaptation. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Transfer learning",
        "subsubsection": "Unsupervised pre-training (self-supervised learning)"
    },
    {
        "content": "In their paper, they considered using a ResNet (Section 14.3.4) and a vision transformer (Section 15.5.6) for the function $f _ { I }$ , and a text transformer (Section 15.5) for $f _ { T }$ . They used a very large minibatch of $N sim 3 2 k$ , and trained for many days on 100s of GPUs. \nAfter the model is trained, it can be used for zero-shot classification of an image $_ { x }$ as follows. First each of the $K$ possible class labels for a given dataset is converted into a text string $mathbf { mathcal { { y } } } _ { k }$ that might occur on the web. For example, “dog” becomes “a photo of a dog”. Second, we compute the normalized emebddings $mathbf { I } propto f _ { I } ( { pmb x } )$ and $mathbf { T } _ { k } propto mathbf { f } _ { T } ( mathbf { mathbf { y } } _ { k } )$ . Third, we compute the softmax probabilites \nSee Figure 19.7b for an illustration. (A similar approach was adopted in the visual n-grams paper [Li+17].) \nRemarkably, this approach can perform as well as standard supervised learning on tasks such as ImageNet classification, without ever being explicitly trained on specific labeled datasets. Of course, the images in ImageNet come from the web, and were found using text-based web-search, so the model has seen similar data before. Nevertheless, its generalization to new tasks, and robustness to distribution shift, are quite impressive (see the paper for examples). \nOne drawback of the approach, however, is that it is sensitive to how class labels are converted to textual form. For example, to make the model work on food classification, it is necessary to use text strings of the form “a photo of guacamole, a type of food”, “a photo of ceviche, a type of food”, etc. Disambiguating phrases such as “a type of food” are currently added by hand, on a per-dataset basis. This is called prompt engineering, and is needed since the raw class names can be ambiguous across (and sometimes within) a dataset. \n19.2.5 Domain adaptation \nConsider a problem in which we have inputs from different domains, such as a source domain $mathcal { X } _ { s }$ and target domain $mathcal { X } _ { t }$ , but a common set of output labels, $_ { mathcal { V } }$ . (This is the “dual” of transfer learning, since the input domains are different, but the output domains the same.) For example, the domains might be images from a computer graphics system and real images, or product reviews and movie reviews. We assume we do not have labeled examples from the target domain. Our goal is to fit the model on the source domain, and then modify its parameters so it works on the target domain. This is called (unsupervised) domain adaptation (see e.g., [KL21] for a review). \nA common approach to this problem is to train the source classifier in such a way that it cannot distinguish whether the input is coming from the source or target distribution; in this case, it will only be able to use features that are common to both domains. This is called domain adversarial learning [Gan+16]. More formally, let $d _ { n } in { s , t }$ be a label that specifies if the data example $n$ comes from domain $s$ or $t$ . We want to optimize \nwhere $N _ { s } ~ = ~ | D _ { s } |$ , $N _ { t } ~ = ~ | D _ { t } |$ , $f$ maps $mathcal { X } _ { s } cup mathcal { X } _ { t } to mathcal { H }$ , and $g$ maps $mathcal { H }  mathcal { V } _ { t }$ . The objective in Equation (19.15) minimizes the loss on the desired task of classifying $y$ , but maximizes the loss on the auxiliary task of classifying the source domain $d$ . This can be implemented by the gradient sign reversal trick, and is related to GANs (generative adversarial networks). See e.g., [Csu17; Wu+19] for some other approaches to domain adaptation. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n19.3 Semi-supervised learning \nThis section is co-authored with Colin Raffel. \nMany recent successful applications of machine learning are in the supervised learning setting, where a large dataset of labeled examples are available for training a model. However, in many practical applications it is expensive to obtain this labeled data. Consider the case of automatic speech recognition: Modern datasets contain thousands of hours of audio recordings [Pan+15; Ard+20]. The process of annotating the words spoken in a recording is many times slower than realtime, potentially resulting in a long (and costly) annotation process. To make matters worse, in some applications data must be labeled by an expert (such as a doctor in medical applications) which can further increase costs. \nSemi-supervised learning can alleviate the need for labeled data by taking advantage of unlabeled data. The general goal of semi-supervised learning is to allow the model to learn the high-level structure of the data distribution from unlabeled data and only rely on the labeled data for learning the fine-grained details of a given task. Whereas in standard supervised learning we assume that we have access to samples from the joint distribution of data and labels $pmb { x } , y sim p ( pmb { x } , y )$ , semi-supervised learning assumes that we additionally have access to samples from the marginal distribution of $_ { x }$ , namely $mathbf { boldsymbol { x } } sim p ( mathbf { boldsymbol { x } } )$ , as illustrated in Figure 19.8. Further, it is generally assumed that we have many more of these unlabeled samples since they are typically cheaper to obtain. Continuing the example of automatic speech recognition, it is often much cheaper to simply record people talking (which would produce unlabeled data) than it is to transcribe recorded speech. Semi-supervised learning is a good fit for the scenario where a large amount of unlabeled data has been collected and the practitioner would like to avoid having to label all of it. \n19.3.1 Self-training and pseudo-labeling \nAn early and straightforward approach to semi-supervised learning is self-training [Scu65; Agr70; McL75]. The basic idea behind self-training is to use the model itself to infer predictions on unlabeled \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 data, and then treat these predictions as labels for subsequent training. Self-training has endured as a semi-supervised learning method because of its simplicity and general applicability; i.e. it is applicable to any model that can generate predictions for the unlabeled data. Recently, it has become common to refer to this approach as “pseudo-labeling” [Lee13] because the inferred labels for unlabeled data are only “pseudo-correct” in comparison with the true, ground-truth targets used in supervised learning.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Transfer learning",
        "subsubsection": "Domain adaptation"
    },
    {
        "content": "19.3 Semi-supervised learning \nThis section is co-authored with Colin Raffel. \nMany recent successful applications of machine learning are in the supervised learning setting, where a large dataset of labeled examples are available for training a model. However, in many practical applications it is expensive to obtain this labeled data. Consider the case of automatic speech recognition: Modern datasets contain thousands of hours of audio recordings [Pan+15; Ard+20]. The process of annotating the words spoken in a recording is many times slower than realtime, potentially resulting in a long (and costly) annotation process. To make matters worse, in some applications data must be labeled by an expert (such as a doctor in medical applications) which can further increase costs. \nSemi-supervised learning can alleviate the need for labeled data by taking advantage of unlabeled data. The general goal of semi-supervised learning is to allow the model to learn the high-level structure of the data distribution from unlabeled data and only rely on the labeled data for learning the fine-grained details of a given task. Whereas in standard supervised learning we assume that we have access to samples from the joint distribution of data and labels $pmb { x } , y sim p ( pmb { x } , y )$ , semi-supervised learning assumes that we additionally have access to samples from the marginal distribution of $_ { x }$ , namely $mathbf { boldsymbol { x } } sim p ( mathbf { boldsymbol { x } } )$ , as illustrated in Figure 19.8. Further, it is generally assumed that we have many more of these unlabeled samples since they are typically cheaper to obtain. Continuing the example of automatic speech recognition, it is often much cheaper to simply record people talking (which would produce unlabeled data) than it is to transcribe recorded speech. Semi-supervised learning is a good fit for the scenario where a large amount of unlabeled data has been collected and the practitioner would like to avoid having to label all of it. \n19.3.1 Self-training and pseudo-labeling \nAn early and straightforward approach to semi-supervised learning is self-training [Scu65; Agr70; McL75]. The basic idea behind self-training is to use the model itself to infer predictions on unlabeled \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 data, and then treat these predictions as labels for subsequent training. Self-training has endured as a semi-supervised learning method because of its simplicity and general applicability; i.e. it is applicable to any model that can generate predictions for the unlabeled data. Recently, it has become common to refer to this approach as “pseudo-labeling” [Lee13] because the inferred labels for unlabeled data are only “pseudo-correct” in comparison with the true, ground-truth targets used in supervised learning. \n\nAlgorithmically, self-training typically follows one of the following two procedures. In the first approach, pseudo-labels are first predicted for the entire collection of unlabeled data and the model is re-trained (possibly from scratch) to convergence on the combination of the labeled and (pseudolabeled) unlabeled data. Then, the unlabeled data is re-labeled by the model and the process repeats itself until a suitable solution is found. The second approach instead continually generates predictions on randomly-chosen batches of unlabeled data and immediately trains the model against these pseudo-labels. Both approaches are currently common in practice; the first “offline” variant has been shown to be particularly successful when leveraging giant collections of unlabeled data [Yal+19; Xie+20] whereas the “online” approach is often used as one component of more sophisticated semisupervised learning methods [Soh+20]. Neither variant is fundamentally better than the other. Offline self-training can result in training the model on “stale” pseudo-labels, since they are only updated each time the model converges. However, online pseudo-labeling can incur larger computational costs since it involves constantly “re-labeling” unlabeled data. \nSelf-training can suffer from an obvious problem: If the model generates incorrect predictions for unlabeled data and then is re-trained on these incorrect predictions, it can become progressively worse and worse at the intended classification task until it eventually learns a totally invalid solution. This issue has been dubbed confirmation bias [TV17] because the model is continually confirming its own (incorrect) bias about the decision rule. \nA common way to mitigate confirmation bias is to use a “selection metric” [RHS05] which heuristically tries to only retain pseudo-labels that are correct. For example, assuming that a model outputs probabilities for each possible class, a frequently-used selection metric is to only retain pseudo-labels whose largest class probability is above a threshold [Yar95; RHS05]. If the model’s class probability estimates are well-calibrated, then this selection metric will only retain labels that are highly likely to be correct (according to the model, at least). More sophisticated selection metrics can be designed according to the problem domain. \n19.3.2 Entropy minimization \nSelf-training has the implicit effect of encouraging the model to output low-entropy (i.e. highconfidence) predictions. This effect is most apparent in the online setting with a cross-entropy loss, where the model minimizes the following loss function $mathcal { L }$ on unlabeled data: \nwhere $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ is the model’s class probability distribution given input $_ { x }$ . This function is minimized when the model assigns all of its class probability to a single class $c ^ { * }$ , i.e. $p ( y = c ^ { * } | x ) = 1$ and $p ( y neq c ^ { * } | x ) = 0$ . \nA closely-related semi-supervised learning method is entropy minimization [GB05], which \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Self-training and pseudo-labeling"
    },
    {
        "content": "Algorithmically, self-training typically follows one of the following two procedures. In the first approach, pseudo-labels are first predicted for the entire collection of unlabeled data and the model is re-trained (possibly from scratch) to convergence on the combination of the labeled and (pseudolabeled) unlabeled data. Then, the unlabeled data is re-labeled by the model and the process repeats itself until a suitable solution is found. The second approach instead continually generates predictions on randomly-chosen batches of unlabeled data and immediately trains the model against these pseudo-labels. Both approaches are currently common in practice; the first “offline” variant has been shown to be particularly successful when leveraging giant collections of unlabeled data [Yal+19; Xie+20] whereas the “online” approach is often used as one component of more sophisticated semisupervised learning methods [Soh+20]. Neither variant is fundamentally better than the other. Offline self-training can result in training the model on “stale” pseudo-labels, since they are only updated each time the model converges. However, online pseudo-labeling can incur larger computational costs since it involves constantly “re-labeling” unlabeled data. \nSelf-training can suffer from an obvious problem: If the model generates incorrect predictions for unlabeled data and then is re-trained on these incorrect predictions, it can become progressively worse and worse at the intended classification task until it eventually learns a totally invalid solution. This issue has been dubbed confirmation bias [TV17] because the model is continually confirming its own (incorrect) bias about the decision rule. \nA common way to mitigate confirmation bias is to use a “selection metric” [RHS05] which heuristically tries to only retain pseudo-labels that are correct. For example, assuming that a model outputs probabilities for each possible class, a frequently-used selection metric is to only retain pseudo-labels whose largest class probability is above a threshold [Yar95; RHS05]. If the model’s class probability estimates are well-calibrated, then this selection metric will only retain labels that are highly likely to be correct (according to the model, at least). More sophisticated selection metrics can be designed according to the problem domain. \n19.3.2 Entropy minimization \nSelf-training has the implicit effect of encouraging the model to output low-entropy (i.e. highconfidence) predictions. This effect is most apparent in the online setting with a cross-entropy loss, where the model minimizes the following loss function $mathcal { L }$ on unlabeled data: \nwhere $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ is the model’s class probability distribution given input $_ { x }$ . This function is minimized when the model assigns all of its class probability to a single class $c ^ { * }$ , i.e. $p ( y = c ^ { * } | x ) = 1$ and $p ( y neq c ^ { * } | x ) = 0$ . \nA closely-related semi-supervised learning method is entropy minimization [GB05], which \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nminimizes the following loss function: \nNote that this function is also minimized when the model assigns all of its class probability to a single class. We can make the entropy-minimization loss in Equation (19.17) equivalent to the online self-training loss in Equation (19.16) by replacing the first $p _ { theta } ( y = c | mathbf { x } )$ term with a “one-hot” vector that assigns a probability of 1 for the class that was assigned the highest probability. In other words, online self-training minimizes the cross-entropy between the model’s output and the “hard” target arg max $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ , whereas entropy minimization uses the the “soft” target $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ . One way to trade off between these two extremes is to adjust the “temperature” of the target distribution by raising each probability to the power of $1 / T$ and renormalizing; this is the basis of the mixmatch method of [Ber+19b; Ber+19a; Xie+19]. At $T = 1$ , this is equivalent to entropy minimization; as $T  0$ , it becomes hard online self-training. A comparison of these loss functions is shown in Figure 19.9. \n19.3.2.1 The cluster assumption \nWhy is entropy minimization a good idea? A basic assumption of many semi-supervised learning methods is that the decision boundary between classes should fall in a low-density region of the data manifold. This effectively assumes that the data corresponding to different classes are clustered together. A good decision boundary, therefore, should not pass through clusters; it should simply separate them. Semi-supervised learning methods that make the “cluster assumption” can be thought of as using unlabeled data to estimate the shape of the data manifold and moving the decision boundary away from it. \nEntropy minimization is one such method. To see why, first assume that the decision boundary between two classes is “smooth”, i.e. the model does not abruptly change its class prediction anywhere in its domain. This is true in practice for simple and/or regularized models. In this case, if the decision boundary passes through a high-density region of data, it will by necessity produce highentropy predictions for some samples from the data distribution. Entropy minimization will therefore encourage the model to place its decision boundary in low-density regions of the input space to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 avoid transitioning from one class to another in a region of space where data may be sampled. A visualization of this behavior is shown in Figure 19.10. \n\n19.3.2.2 Input-output mutual information \nAn alternative justification for the entropy minimization objective was proposed by Bridle, Heading, and MacKay [BHM92], where it was shown that it naturally arises from maximizing the mutual information (Section 6.3) between the data and the label (i.e. the input and output of a model). Denoting $_ { x }$ as the input and $y$ as the target, the input-output mutual information can be written as \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nNote that the first integral is equivalent to taking an expectation over $_ { x }$ , and the second integral is equivalent to summing over all possible values of the class $y$ . Using these relations, we obtain \nSince we had initially sought to maximize the mutual information, and we typically minimize loss functions, we can convert this to a suitable loss function by negating it: \nThe first term is exactly the entropy minimization objective in expectation. The second term specifies that we should maximize the entropy of the expected class prediction, i.e. the average class prediction over our training set. This encourages the model to predict each possible class with equal probability, which is only appropriate when we know a priori that all classes are equally likely. \n19.3.3 Co-training \nCo-training [BM98] is also similar to self-training, but makes an additional assumption that there are two complementary “views” (i.e. independent sets of features) of the data, both of which can be used separately to train a reasonable model. After training two models separately on each view, unlabeled data is classified by each model to obtain candidate pseudo-labels. If a particular pseudolabel receives a low-entropy prediction (indicating high confidence) from one model and a high-entropy prediction (indicating low confidence) from the other, then that pseudo-labeled datapoint is added to the training set for the low-confidence model. Then, the process is repeated with the new, larger training datasets. The procedure of only retaining pseudo-labels when one of the models is confident ideally builds up the training sets with correctly-labeled data. \nCo-training makes the strong assumption that there are two informative-but-independent views of the data, which may not be true for many problems. The Tri-Training algorithm [ZL05] circumvents this issue by instead using three models that are first trained on independently-sampled (with replacement) subsets of the labeled data. Ideally, initially training on different collections of labeled data results in models that do not always agree on their predictions. Then, pseudo-labels are generated for the unlabeled data independently by each of the three models. For a given unlabeled datapoint, if two of the models agree on the pseudo-label, it is added to the training set for the third model. This can be seen as a selection metric, because it only retains pseudo-labels where two (differently initialized) models agree on the correct label. The models are then re-trained on the combination of the labeled data and the new pseudo-labels, and the whole process is repeated iteratively. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Entropy minimization"
    },
    {
        "content": "Note that the first integral is equivalent to taking an expectation over $_ { x }$ , and the second integral is equivalent to summing over all possible values of the class $y$ . Using these relations, we obtain \nSince we had initially sought to maximize the mutual information, and we typically minimize loss functions, we can convert this to a suitable loss function by negating it: \nThe first term is exactly the entropy minimization objective in expectation. The second term specifies that we should maximize the entropy of the expected class prediction, i.e. the average class prediction over our training set. This encourages the model to predict each possible class with equal probability, which is only appropriate when we know a priori that all classes are equally likely. \n19.3.3 Co-training \nCo-training [BM98] is also similar to self-training, but makes an additional assumption that there are two complementary “views” (i.e. independent sets of features) of the data, both of which can be used separately to train a reasonable model. After training two models separately on each view, unlabeled data is classified by each model to obtain candidate pseudo-labels. If a particular pseudolabel receives a low-entropy prediction (indicating high confidence) from one model and a high-entropy prediction (indicating low confidence) from the other, then that pseudo-labeled datapoint is added to the training set for the low-confidence model. Then, the process is repeated with the new, larger training datasets. The procedure of only retaining pseudo-labels when one of the models is confident ideally builds up the training sets with correctly-labeled data. \nCo-training makes the strong assumption that there are two informative-but-independent views of the data, which may not be true for many problems. The Tri-Training algorithm [ZL05] circumvents this issue by instead using three models that are first trained on independently-sampled (with replacement) subsets of the labeled data. Ideally, initially training on different collections of labeled data results in models that do not always agree on their predictions. Then, pseudo-labels are generated for the unlabeled data independently by each of the three models. For a given unlabeled datapoint, if two of the models agree on the pseudo-label, it is added to the training set for the third model. This can be seen as a selection metric, because it only retains pseudo-labels where two (differently initialized) models agree on the correct label. The models are then re-trained on the combination of the labeled data and the new pseudo-labels, and the whole process is repeated iteratively. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n19.3.4 Label propagation on graphs \nIf two datapoints are “similar” in some meaningful way, we might expect that they share a label. This idea has been referred to as the manifold assumption. Label propagation is a semi-supervised learning technique that leverages the manifold assumption to assign labels to unlabeled data. Label propagation first constructs a graph where the nodes are the data examples and the edge weights represent the degree of similarity. The node labels are known for nodes corresponding to labeled data but are unknown for unlabeled data. Label propagation then propagates the known labels across edges of the graph in such a way that there is minimal disagreement in the labels of a given node’s neighbors. This provides label guesses for the unlabeled data, which can then be used in the usual way for supervised training of a model. \nMore specifically, the basic label propagation algorithm [ZG02] proceeds as follows: First, let $w _ { i , j }$ denote a non-negative edge weight between ${ boldsymbol { x } } _ { i }$ and $boldsymbol { mathscr { x } } _ { j }$ that provides a measure of similarity for the two (labeled or unlabeled) datapoints. Assuming that we have $M$ labeled datapoints and $N$ unlabeled datapoints, define the $( M + N ) times ( M + N )$ transition matrix $mathbf { T }$ as having entries \n$mathbf { T } _ { i , j }$ represents the probability of propagating the label for node $j$ to node $i$ . Further, define the $( M + N ) times C$ label matrix $mathbf { Y }$ , where $C$ is the number of possible classes. The $i$ th row of $mathbf { Y }$ represents the class probability distribution of datapoint $i$ . Then, repeat the following steps until the values in $mathbf { Y }$ do not change significantly: First, use the transition matrix $mathbf { T }$ to propagate labels in $mathbf { Y }$ by setting $mathbf { Y }  mathbf { T Y }$ . Then, re-normalize the rows of Y by setting $mathbf { Y } _ { i , c }  mathbf { Y } _ { i , c } / sum _ { k } mathbf { Y } _ { i , k }$ . Finally, replace the rows of $mathbf { Y }$ corresponding to labeled datapoints with their one-hot representation (i.e. $mathbf { Y } _ { i , c } = 1$ if datapoint $i$ has ground-truth label $c$ and 0 otherwise). After convergence, guessed labels are chosen based on the highest class probability for each datapoint in $mathbf { Y }$ . \nThis algorithm iteratively uses the similarity of datapoints (encoded in the weights used to construct the transition matrix) to propagate information from the (fixed) labels onto the unlabeled data. At each iteration, the label distribution for a given datapoint is computed as the weighted average of the label distributions for all of its connected datapoints, where the weighting corresponds to the edge weights in $mathbf { T }$ . It can be shown that this procedure converges to a single fixed point, whose computational cost mainly involves the inversion of the matrix of unlabled-to-unlabled transition probabilities [ZG02]. \nThe overall approach can be seen as a form of transductive learning, since it is learning to predict labels for a fixed unlabeled dataset, rather than learning a model that generalizes. However, given the induced labeling. we can perform inductive learning in the usual way. \nThe success of label propagation depends heavily on the notion of similarity used to construct the weights between different nodes (datapoints). For simple data, measuring the Euclidean distance between datapoints can be sufficient. However, for complex and high-dimensional data the Euclidean distance might not meaningfully reflect the likelihood that two datapoints share the same class. The similarity weights can also be set arbitrarily according to problem-specific knowledge. For a few examples of different ways of constructing the similarity graph, see Zhu [Zhu05, chapter 3]. For some recent papers that use this approach in conjunction with deep learning, see e.g., [BRR18; Isc+19]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Co-training"
    },
    {
        "content": "19.3.4 Label propagation on graphs \nIf two datapoints are “similar” in some meaningful way, we might expect that they share a label. This idea has been referred to as the manifold assumption. Label propagation is a semi-supervised learning technique that leverages the manifold assumption to assign labels to unlabeled data. Label propagation first constructs a graph where the nodes are the data examples and the edge weights represent the degree of similarity. The node labels are known for nodes corresponding to labeled data but are unknown for unlabeled data. Label propagation then propagates the known labels across edges of the graph in such a way that there is minimal disagreement in the labels of a given node’s neighbors. This provides label guesses for the unlabeled data, which can then be used in the usual way for supervised training of a model. \nMore specifically, the basic label propagation algorithm [ZG02] proceeds as follows: First, let $w _ { i , j }$ denote a non-negative edge weight between ${ boldsymbol { x } } _ { i }$ and $boldsymbol { mathscr { x } } _ { j }$ that provides a measure of similarity for the two (labeled or unlabeled) datapoints. Assuming that we have $M$ labeled datapoints and $N$ unlabeled datapoints, define the $( M + N ) times ( M + N )$ transition matrix $mathbf { T }$ as having entries \n$mathbf { T } _ { i , j }$ represents the probability of propagating the label for node $j$ to node $i$ . Further, define the $( M + N ) times C$ label matrix $mathbf { Y }$ , where $C$ is the number of possible classes. The $i$ th row of $mathbf { Y }$ represents the class probability distribution of datapoint $i$ . Then, repeat the following steps until the values in $mathbf { Y }$ do not change significantly: First, use the transition matrix $mathbf { T }$ to propagate labels in $mathbf { Y }$ by setting $mathbf { Y }  mathbf { T Y }$ . Then, re-normalize the rows of Y by setting $mathbf { Y } _ { i , c }  mathbf { Y } _ { i , c } / sum _ { k } mathbf { Y } _ { i , k }$ . Finally, replace the rows of $mathbf { Y }$ corresponding to labeled datapoints with their one-hot representation (i.e. $mathbf { Y } _ { i , c } = 1$ if datapoint $i$ has ground-truth label $c$ and 0 otherwise). After convergence, guessed labels are chosen based on the highest class probability for each datapoint in $mathbf { Y }$ . \nThis algorithm iteratively uses the similarity of datapoints (encoded in the weights used to construct the transition matrix) to propagate information from the (fixed) labels onto the unlabeled data. At each iteration, the label distribution for a given datapoint is computed as the weighted average of the label distributions for all of its connected datapoints, where the weighting corresponds to the edge weights in $mathbf { T }$ . It can be shown that this procedure converges to a single fixed point, whose computational cost mainly involves the inversion of the matrix of unlabled-to-unlabled transition probabilities [ZG02]. \nThe overall approach can be seen as a form of transductive learning, since it is learning to predict labels for a fixed unlabeled dataset, rather than learning a model that generalizes. However, given the induced labeling. we can perform inductive learning in the usual way. \nThe success of label propagation depends heavily on the notion of similarity used to construct the weights between different nodes (datapoints). For simple data, measuring the Euclidean distance between datapoints can be sufficient. However, for complex and high-dimensional data the Euclidean distance might not meaningfully reflect the likelihood that two datapoints share the same class. The similarity weights can also be set arbitrarily according to problem-specific knowledge. For a few examples of different ways of constructing the similarity graph, see Zhu [Zhu05, chapter 3]. For some recent papers that use this approach in conjunction with deep learning, see e.g., [BRR18; Isc+19]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n19.3.5 Consistency regularization \nConsistency regularization leverages the simple idea that perturbing a given datapoint (or the model itself) should not cause the model’s output to change dramatically. Since measuring consistency in this way only makes use of the model’s outputs (and not ground-truth labels), it is readily applicable to unlabeled data and therefore can be used to create appropriate loss functions for semi-supervised learning. This idea was first proposed under the framework of “learning with pseudo-ensembles” [BAP14], with similar variants following soon thereafter [LA16; SJT16]. \nIn its most general form, both the model $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ and the transformations applied to the input can be stochastic. For example, in computer vision problems we may transform the input by using data augmentation like randomly rotating or adding noise the input image, and the network may include stochastic components like dropout (Section 13.5.4) or weight noise [Gra11]. A common and simple form of consistency regularization first samples ${ pmb x } ^ { prime } sim q ( { pmb x } ^ { prime } | { pmb x } )$ (where $q ( { pmb x } ^ { prime } | { pmb x } )$ is the distribution induced by the stochastic input transformations) and then minimizes the loss $| p _ { boldsymbol { theta } } ( y | mathbf { x } ) - p _ { boldsymbol { theta } } ( y | mathbf { x } ^ { prime } ) | ^ { 2 }$ . In practice, the first term $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ is typically treated as fixed (i.e. gradients are not propagated through it). In the semi-supervised setting, the combined loss function over a batch of labeled data $( { pmb x } _ { 1 } , y _ { 1 } ) , ( { pmb x } _ { 2 } , y _ { 2 } ) , dots , ( { pmb x } _ { M } , y _ { M } )$ and unlabeled data ${ pmb x } _ { 1 } , { pmb x } _ { 2 } , ldots , { pmb x } _ { N }$ is \nwhere $lambda$ is a scalar hyperparameter that balances the importance of the loss on unlabeled data and, for simplicity, we write $pmb { x } _ { j } ^ { prime }$ to denote a sample drawn from $q ( pmb { x } ^ { prime } | pmb { x } _ { j } )$ . \nThe basic form of consistency regularization in Equation (19.27) reveals many design choices that impact the success of this semi-supervised learning approach. First, the value chosen for the $lambda$ hyperparameter is important. If it is too large, then the model may not give enough weight to learning the supervised task and will instead start to reinforce its own bad predictions (as with confirmation bias in self-training). Since the model is often poor at the start of training before it has been trained on much labeled data, it is common in practice to initialize set $lambda$ to zero and increase its value over the course of training. \nA second important consideration are the random transformations applied to the input, i.e., $q ( { pmb x } ^ { prime } | { pmb x } )$ . Generally speaking, these transformations should be designed so that they do not change the label of $_ { x }$ . As mentioned above, a common choice is to use domain-specific data augmentations. It has recently been shown that using strong data augmentations that heavily corrupt the input (but, arguably, still do not change the label) can produce particularly strong results [Xie+19; Ber+19a; Soh+20]. \nThe use of data augmentation requires expert knowledge to determine what kinds of transformations are label-preserving and appropriate for a given problem. An alternative technique, called virtual adversarial training (VAT), instead transforms the input using an analytically-found perturbation designed to maximally change the model’s output. Specifically, VAT computes a perturbation $pmb { delta }$ that approximates $begin{array} { r } { delta = mathrm { a r g m a x } _ { delta } D _ { mathbb { K L } } left( p _ { theta } ( y | pmb { x } ) parallel p _ { theta } ( y | pmb { x } + pmb { delta } ) right) } end{array}$ . The approximation is done by sampling $textbf { em d }$ from a multivariate Gaussian distribution, initializing $delta = d$ , and then setting \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Label propagation on graphs"
    },
    {
        "content": "19.3.5 Consistency regularization \nConsistency regularization leverages the simple idea that perturbing a given datapoint (or the model itself) should not cause the model’s output to change dramatically. Since measuring consistency in this way only makes use of the model’s outputs (and not ground-truth labels), it is readily applicable to unlabeled data and therefore can be used to create appropriate loss functions for semi-supervised learning. This idea was first proposed under the framework of “learning with pseudo-ensembles” [BAP14], with similar variants following soon thereafter [LA16; SJT16]. \nIn its most general form, both the model $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ and the transformations applied to the input can be stochastic. For example, in computer vision problems we may transform the input by using data augmentation like randomly rotating or adding noise the input image, and the network may include stochastic components like dropout (Section 13.5.4) or weight noise [Gra11]. A common and simple form of consistency regularization first samples ${ pmb x } ^ { prime } sim q ( { pmb x } ^ { prime } | { pmb x } )$ (where $q ( { pmb x } ^ { prime } | { pmb x } )$ is the distribution induced by the stochastic input transformations) and then minimizes the loss $| p _ { boldsymbol { theta } } ( y | mathbf { x } ) - p _ { boldsymbol { theta } } ( y | mathbf { x } ^ { prime } ) | ^ { 2 }$ . In practice, the first term $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ is typically treated as fixed (i.e. gradients are not propagated through it). In the semi-supervised setting, the combined loss function over a batch of labeled data $( { pmb x } _ { 1 } , y _ { 1 } ) , ( { pmb x } _ { 2 } , y _ { 2 } ) , dots , ( { pmb x } _ { M } , y _ { M } )$ and unlabeled data ${ pmb x } _ { 1 } , { pmb x } _ { 2 } , ldots , { pmb x } _ { N }$ is \nwhere $lambda$ is a scalar hyperparameter that balances the importance of the loss on unlabeled data and, for simplicity, we write $pmb { x } _ { j } ^ { prime }$ to denote a sample drawn from $q ( pmb { x } ^ { prime } | pmb { x } _ { j } )$ . \nThe basic form of consistency regularization in Equation (19.27) reveals many design choices that impact the success of this semi-supervised learning approach. First, the value chosen for the $lambda$ hyperparameter is important. If it is too large, then the model may not give enough weight to learning the supervised task and will instead start to reinforce its own bad predictions (as with confirmation bias in self-training). Since the model is often poor at the start of training before it has been trained on much labeled data, it is common in practice to initialize set $lambda$ to zero and increase its value over the course of training. \nA second important consideration are the random transformations applied to the input, i.e., $q ( { pmb x } ^ { prime } | { pmb x } )$ . Generally speaking, these transformations should be designed so that they do not change the label of $_ { x }$ . As mentioned above, a common choice is to use domain-specific data augmentations. It has recently been shown that using strong data augmentations that heavily corrupt the input (but, arguably, still do not change the label) can produce particularly strong results [Xie+19; Ber+19a; Soh+20]. \nThe use of data augmentation requires expert knowledge to determine what kinds of transformations are label-preserving and appropriate for a given problem. An alternative technique, called virtual adversarial training (VAT), instead transforms the input using an analytically-found perturbation designed to maximally change the model’s output. Specifically, VAT computes a perturbation $pmb { delta }$ that approximates $begin{array} { r } { delta = mathrm { a r g m a x } _ { delta } D _ { mathbb { K L } } left( p _ { theta } ( y | pmb { x } ) parallel p _ { theta } ( y | pmb { x } + pmb { delta } ) right) } end{array}$ . The approximation is done by sampling $textbf { em d }$ from a multivariate Gaussian distribution, initializing $delta = d$ , and then setting \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nFigure 19.11: Comparison of the squared error and KL divergence lossses for a consistency regularization. This visualization is for a binary classification problem where it is assumed that the model’s output for the unperturbed input is 1. The figure plots the loss incurred for a particular value of the logit (i.e. the pre-activation fed into the output sigmoid nonlinearity) for the perturbed input. As the logit grows towards infinity, the model predicts a class label of 1 (in agreement with the prediction for the unperturbed input); as it grows towards negative infinity, the model predictions class 0. The squared error loss saturates (and has zero gradients) when the model predicts one class or the other with high probability, but the KL divergence grows without bound as the model predicts class 0 with more and more confidence. \nwhere $xi$ is a small constant, typically $1 0 ^ { - 6 }$ . VAT then sets \nand proceeds as usual with consistency regularization (as in Equation (19.27)), where $epsilon$ is a scalar hyperparameter that sets the L2-norm of the perturbation applied to $_ { x }$ . \nConsistency regularization can also profoundly affect the geometry properties of the training objective, and the trajectory of SGD, such that performance can particularly benefit from nonstandard training procedures. For example, the Euclidean distances between weights at different training epochs is significantly larger for objectives that use consistency regularization. Athiwaratkun et al. [Ath+19] show that a variant of stochastic weight averaging (SWA) [Izm+18] can achieve state-of-the-art performance on semi-supervised learning tasks by exploiting the geometric properties of consistency regularization. \nA final consideration when using consistency regularization is the function used to measure the difference between the network’s output with and without perturbations. Equation (19.27) uses the squared L2 distance (also referred to as the Brier score), which is a common choice [SJT16; TV17; LA16; Ber+19b]. It is also common to use the KL divergence $D _ { mathbb { K } mathbb { L } } ( p _ { theta } ( y | pmb { x } ) ~ | ~ p _ { theta } ( y | pmb { x } ^ { prime } )$ in analogy with the cross-entropy loss (i.e. KL divergence between ground-truth label and prediction) used for labeled examples [Miy+18; Ber+19a; Xie+19]. The gradient of the squared-error loss approaches zero as the model’s predictions on the perturbed and unperturbed input differ more and more, assuming the model uses a softmax nonlinearity on its output. Using the squared-error loss therefore has a possible advantage that the model is not updated when its predictions are very unstable. However, the KL divergence has the same scale as the cross-entropy loss used for labeled data, which makes for more intuitive tuning of the unlabeled loss hyperparameter $lambda$ . A comparison of the two loss functions is shown in Figure 19.11. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n19.3.6 Deep generative models * \nGenerative models provide a natural way of making use of unlabeled data through learning a model of the marginal distribution by minimizing $begin{array} { r } { mathcal { L } _ { U } = - sum _ { n } log p _ { pmb { theta } } ( pmb { x } _ { n } ) } end{array}$ . Various approaches have leveraged generative models for semi-supervised by developing ways to use the model of $p _ { pmb { theta } } ( pmb { x } _ { n } )$ to help produce a better supervised model. \n19.3.6.1 Variational autoencoders \nIn Section 20.3.5, we describe the variational autoencoder (VAE), which defines a probabilistic model of the joint distribution of data $_ { x }$ and latent variables $boldsymbol { z }$ . Data is assumed to be generated by first sampling $z sim p ( z )$ and then sampling $begin{array} { r } { pmb { x } sim p ( pmb { x } | pmb { z } ) } end{array}$ . For learning, the VAE uses an encoder $pmb { q } _ { lambda } ( pmb { z } | pmb { x } )$ to approximate the posterior and a decoder $p _ { theta } ( { pmb x } | z )$ to approximate the likelihood. The encoder and decoder are typically deep neural networks. The parameters of the encoder and decoder can be jointly trained by maximizing the evidence lower bound (ELBO) of data. \nThe marginal distribution of latent variables $p ( z )$ is often chosen to be a simple distribution like a diagonal-covariance Gaussian. In practice, this can make the latent variables $mathscr { z }$ more amenable to downstream classification thanks to the facts that $mathscr { z }$ is typically lower-dimensional than $_ { x }$ , that $boldsymbol { z }$ is constructed via cascaded nonlinear transformations, and that the dimensions of the latent variables are designed to be independent. In other words, the latent variables can provide a (learned) representation where data may be more easily separable. In [Kin+14], this approach is called M1 and it is indeed shown that the latent variables can be used to train stronger models when labels are scarce. (The general idea of unsupervised learning of representations to help with downstream classification tasks is described further in Section 19.2.4.) \nAn alternative approach to leveraging VAEs, also proposed in [Kin+14] and called M2, has the form \nwhere $mathscr { z }$ is a latent variable, $begin{array} { r } { p _ { pmb { theta } } ( z ) = mathcal { N } ( z | pmb { mu } _ { pmb { theta } } , pmb { Sigma } _ { pmb { theta } } ) } end{array}$ is the latent prior (typically we fix $mu _ { theta } = 0$ and $Sigma _ { theta } = mathbf { I }$ ), $p _ { pmb { theta } } ( y ) = mathrm { C a t } ( y | pmb { pi } _ { pmb { theta } } )$ the label prior, and $p _ { pmb theta } ( pmb x | y , z ) = p ( pmb x | f _ { pmb theta } ( y , z ) )$ is the likelihood, such as a Gaussian, with parameters computed by $f$ (a deep neural network). The main innovation of this approach is to assume that data is generated according to both a latent class variable $y$ as well as the continuous latent variable $_ { z }$ . The class variable $y$ is observed for labeled data and unobserved for unlabled data. \nTo compute the likelihood for the labeled data, $p _ { pmb { theta } } ( pmb { x } , pmb { y } )$ , we need to marginalize over $boldsymbol { z }$ , which we can do by using an inference network of the form \nWe then use the following variational lower bound \nas is standard for VAEs (see Section 20.3.5). The only difference is that we observe two kinds of data: $_ { x }$ and $y$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Consistency regularization"
    },
    {
        "content": "19.3.6 Deep generative models * \nGenerative models provide a natural way of making use of unlabeled data through learning a model of the marginal distribution by minimizing $begin{array} { r } { mathcal { L } _ { U } = - sum _ { n } log p _ { pmb { theta } } ( pmb { x } _ { n } ) } end{array}$ . Various approaches have leveraged generative models for semi-supervised by developing ways to use the model of $p _ { pmb { theta } } ( pmb { x } _ { n } )$ to help produce a better supervised model. \n19.3.6.1 Variational autoencoders \nIn Section 20.3.5, we describe the variational autoencoder (VAE), which defines a probabilistic model of the joint distribution of data $_ { x }$ and latent variables $boldsymbol { z }$ . Data is assumed to be generated by first sampling $z sim p ( z )$ and then sampling $begin{array} { r } { pmb { x } sim p ( pmb { x } | pmb { z } ) } end{array}$ . For learning, the VAE uses an encoder $pmb { q } _ { lambda } ( pmb { z } | pmb { x } )$ to approximate the posterior and a decoder $p _ { theta } ( { pmb x } | z )$ to approximate the likelihood. The encoder and decoder are typically deep neural networks. The parameters of the encoder and decoder can be jointly trained by maximizing the evidence lower bound (ELBO) of data. \nThe marginal distribution of latent variables $p ( z )$ is often chosen to be a simple distribution like a diagonal-covariance Gaussian. In practice, this can make the latent variables $mathscr { z }$ more amenable to downstream classification thanks to the facts that $mathscr { z }$ is typically lower-dimensional than $_ { x }$ , that $boldsymbol { z }$ is constructed via cascaded nonlinear transformations, and that the dimensions of the latent variables are designed to be independent. In other words, the latent variables can provide a (learned) representation where data may be more easily separable. In [Kin+14], this approach is called M1 and it is indeed shown that the latent variables can be used to train stronger models when labels are scarce. (The general idea of unsupervised learning of representations to help with downstream classification tasks is described further in Section 19.2.4.) \nAn alternative approach to leveraging VAEs, also proposed in [Kin+14] and called M2, has the form \nwhere $mathscr { z }$ is a latent variable, $begin{array} { r } { p _ { pmb { theta } } ( z ) = mathcal { N } ( z | pmb { mu } _ { pmb { theta } } , pmb { Sigma } _ { pmb { theta } } ) } end{array}$ is the latent prior (typically we fix $mu _ { theta } = 0$ and $Sigma _ { theta } = mathbf { I }$ ), $p _ { pmb { theta } } ( y ) = mathrm { C a t } ( y | pmb { pi } _ { pmb { theta } } )$ the label prior, and $p _ { pmb theta } ( pmb x | y , z ) = p ( pmb x | f _ { pmb theta } ( y , z ) )$ is the likelihood, such as a Gaussian, with parameters computed by $f$ (a deep neural network). The main innovation of this approach is to assume that data is generated according to both a latent class variable $y$ as well as the continuous latent variable $_ { z }$ . The class variable $y$ is observed for labeled data and unobserved for unlabled data. \nTo compute the likelihood for the labeled data, $p _ { pmb { theta } } ( pmb { x } , pmb { y } )$ , we need to marginalize over $boldsymbol { z }$ , which we can do by using an inference network of the form \nWe then use the following variational lower bound \nas is standard for VAEs (see Section 20.3.5). The only difference is that we observe two kinds of data: $_ { x }$ and $y$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo compute the likelihood for the unlabeled data, $p _ { pmb { theta } } ( pmb { x } )$ , we need to marginalize over $boldsymbol { z }$ and $y$ , which we can do by using an inference network of the form \nNote that $q _ { phi } ( y | mathbf { x } )$ acts like a discriminative classifier, that imputes the missing labels. We then use the following variational lower bound: \nNote that the discriminative classifier $q _ { phi } ( y | mathbf { x } )$ is only used to compute the log-likelihood of the unlabeled data, which is undesirable. We can therefore add an extra classification loss on the supervised data, to get the following overall objective function: \nwhere $alpha$ is a hyperparameter that controls the relative weight of generative and discriminative learning. \nOf course, the probablistic model used in M2 is just one of many ways to decompose the dependencies between the observed data, the class labels, and the continuous latent variables. There are also many ways other than variational inference to perform approximate inference. The best technique will be problem dependent, but overall the main advantage of the generative approach is that we can incorporate domain knowledge. For example, we can model the missing data mechanism, since the absence of a label may be informative about the underlying data (e.g., people may be reluctant to answer a survey question about their health if they are unwell). \n19.3.6.2 Generative adversarial networks \nGenerative adversarial networks (GANs) (described in more detail in the sequel to this book, [Mur23]) are a popular class of generative models that learn an implicit model of the data distribution. They consist of a generator network, which maps samples from a simple latent distribution to the data space, and a critic network, which attempts to distinguish between the outputs of the generator and samples from the true data distribution. The generator is trained to generate samples that the critic classifies as “real”. \nSince standard GANs do not produce a learned latent representation of a given datapoint and do not learn an explicit model of the data distribution, we cannot use the same approaches as were used for VAEs. Instead, semi-supervised learning with GANs is typically done by modifying the critic so that it outputs either a class label or “fake” instead of simply classifying real vs. fake [Sal+16; Ode16]. For labeled real data, the critic is trained to output the appropriate class label, and for unlabeled real data, it is trained to raise the probability of any of the class labels. As with standard GAN training, the critic is trained to classify outputs from the generator as fake and the generator is trained to fool the critic. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIn more detail, let $p _ { theta } ( y | mathbf { boldsymbol { x } } )$ denote the critic with $C + 1$ outputs corresponding to $C$ classes plus a “fake” class, and let $G ( z )$ denote the generator which takes as input samples from the prior distribution $p ( z )$ . Let us assume that we are using the standard cross-entropy GAN loss as originally proposed in [Goo+14]. Then the critic’s loss is \nThis tries to maximize the probability of the correct class for the labeled examples, to minimize the probability of the fake class for real unlabeled examples, and to maximize the probability of the fake class for generated examples. The generator’s loss is simpler, namely \nA diagram visualizing the semi-supervised GAN framework is shown in Figure 19.12. \n19.3.6.3 Normalizing flows \nNormalizing flows (described in more detail in the sequel to this book, [Mur23]) are a tractable way to define deep generative models. More precisely, they define an invertible mapping $f _ { theta } : mathcal { X }  mathcal { Z }$ , with parameters $theta$ , from the data space $mathcal { X }$ to the latent space $mathcal { Z }$ . The density in data space can be written starting from the density in the latent space using the change of variables formula: \nWe can extend this to semi-supervised learning, as proposed in [Izm+20]. For class labels $y in { 1 ldots { mathcal { C } } }$ , we can specify the latent distribution, conditioned on a label $k$ , as Gaussian with mean $mu _ { k }$ and covariance $Sigma _ { k }$ : $p ( z | y = k ) = mathcal { N } ( z | mu _ { k } , Sigma _ { k } )$ . The marginal distribution of $z$ is then a Gaussian mixture. The likelihood for labeled data is then \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nand the likelihood for data with unknown label is $begin{array} { r } { p ( x ) = sum _ { k } p ( x | y = k ) p ( y = k ) } end{array}$ \nFor semi-supervised learning we can then maximize the joint likelihood of the labeled $mathcal { D } _ { ell }$ and unlabeled data $mathcal { D } _ { u }$ : \nover the parameters $theta$ of the bijective function $f$ , which learns a density model for a Bayes classifier. Given a test point $x$ , the model predictive distribution is given by \nwhere we have assumed $p ( y = c ) = 1 / C$ . We can make predictions for a test point $x$ with the Bayes decision rule $y = arg operatorname* { m a x } _ { c in { 1 , . . . , C } } p ( y = c | x )$ . \n19.3.7 Combining self-supervised and semi-supervised learning \nIt is possible to combine self-supervised and semi-supervised learning. For example, [Che+20c] using SimCLR (Section 19.2.4.4) to perform self-supervised representation learning on the unlabeled data, they then fine-tune this representation on a small labeled dataset (as in transfer learning, Section 19.2), and finally, they apply the trained model back to the original unlabeled dataset, and distill the predictions from this teacher model $T$ into a student model $S$ . (Knowledge distillation is the name given to the approach of training one model on the predictions of another, as originally proposed in [HVD14].) That is, after fine-tuning $T$ , they train $S$ by minimizing \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $tau > 0$ is a temperature parameter applied to the softmax output, which is used to perform label smoothing. If $S$ has the same form as $T$ , this is known as self-training, as discussed in Section 19.3.1. However, normally the student $S$ is smaller than the teacher $T$ . (For example, $T$ might be a high capacity model, and $S$ is a lightweight version that runs on a phone.) See Figure 19.13 for an illustration of the overall approach.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Deep generative models *"
    },
    {
        "content": "and the likelihood for data with unknown label is $begin{array} { r } { p ( x ) = sum _ { k } p ( x | y = k ) p ( y = k ) } end{array}$ \nFor semi-supervised learning we can then maximize the joint likelihood of the labeled $mathcal { D } _ { ell }$ and unlabeled data $mathcal { D } _ { u }$ : \nover the parameters $theta$ of the bijective function $f$ , which learns a density model for a Bayes classifier. Given a test point $x$ , the model predictive distribution is given by \nwhere we have assumed $p ( y = c ) = 1 / C$ . We can make predictions for a test point $x$ with the Bayes decision rule $y = arg operatorname* { m a x } _ { c in { 1 , . . . , C } } p ( y = c | x )$ . \n19.3.7 Combining self-supervised and semi-supervised learning \nIt is possible to combine self-supervised and semi-supervised learning. For example, [Che+20c] using SimCLR (Section 19.2.4.4) to perform self-supervised representation learning on the unlabeled data, they then fine-tune this representation on a small labeled dataset (as in transfer learning, Section 19.2), and finally, they apply the trained model back to the original unlabeled dataset, and distill the predictions from this teacher model $T$ into a student model $S$ . (Knowledge distillation is the name given to the approach of training one model on the predictions of another, as originally proposed in [HVD14].) That is, after fine-tuning $T$ , they train $S$ by minimizing \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license where $tau > 0$ is a temperature parameter applied to the softmax output, which is used to perform label smoothing. If $S$ has the same form as $T$ , this is known as self-training, as discussed in Section 19.3.1. However, normally the student $S$ is smaller than the teacher $T$ . (For example, $T$ might be a high capacity model, and $S$ is a lightweight version that runs on a phone.) See Figure 19.13 for an illustration of the overall approach. \n\n19.4 Active learning \nIn active learning, the goal is to identify the true predictive mapping $y = f ( { pmb x } )$ by querying as few $( { pmb x } , y )$ points as possible. There are three main variants. In query synthesis, the algorithm gets to choose any input $_ { x }$ , and can ask for its corresponding output $y = f ( { pmb x } )$ . In pool-based active learning, there is a large, but fixed, set of unlabeled data points, and the algorithm gets to ask for a label for one or more of these points. Finally, in stream-based active learning, the incoming data is arriving continuously, and the algorithm must choose whether it wants to request a label for the current input or not. \nThere are various closely related problems. In Bayesian optimization the goal is to estimate the location of the global optimum $begin{array} { r } { { pmb x } ^ { * } = mathrm { a r g m i n } _ { pmb x } f ( { pmb x } ) } end{array}$ in as few queries as possible; typically we fit a surrogate (response surface) model to the intermediate $( { pmb x } , y )$ queries, to decide which question to ask next. In experiment design, the goal is to infer a parameter vector of some model, using carefully chosen data samples $mathcal { D } = { pmb { x } _ { 1 } , ldots , pmb { x } _ { N } }$ , i.e. we want to estimate $p ( pmb { theta } | mathcal { D } )$ using as little data as possible. (This can be thought of as an unsupervised, or generalized, form of active learning.) \nIn this section, we give a brief review of the pool based approach to active learning. For more details, see e.g., [Set12] for a review. \n19.4.1 Decision-theoretic approach \nIn the decision theoretic approach to active learning, proposed in [KHB07; RM01], we define the utility of querying $_ { x }$ in terms of the value of information. In particular, we define the utility of issuing query $_ { x }$ as \nwhere $R ( a | mathcal { D } ) = mathbb { E } _ { p ( theta | mathcal { D } ) } left[ ell ( theta , a ) right]$ is the posterior expected loss of taking some future action $a$ given the data $mathcal { D }$ observed so far. Unfortunately, evaluating $U ( { pmb x } )$ for each $_ { x }$ is quite expensive, since for each possible response $y$ we might observe, we have to update our beliefs given $( { pmb x } , y )$ to see what effect it might have on our future decisions (similar to look ahead search technique applied to belief states). \n19.4.2 Information-theoretic approach \nIn the information theoretic approach to active supervised learning, we avoid using task-specific loss functions, and instead focus on learning our model as well as we can. In particular, [Lin56] proposed to define the utility of querying $_ { x }$ in terms of information gain about the parameters $pmb theta$ , i.e., the reduction in entropy: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Semi-supervised learning",
        "subsubsection": "Combining self-supervised and semi-supervised learning"
    },
    {
        "content": "19.4 Active learning \nIn active learning, the goal is to identify the true predictive mapping $y = f ( { pmb x } )$ by querying as few $( { pmb x } , y )$ points as possible. There are three main variants. In query synthesis, the algorithm gets to choose any input $_ { x }$ , and can ask for its corresponding output $y = f ( { pmb x } )$ . In pool-based active learning, there is a large, but fixed, set of unlabeled data points, and the algorithm gets to ask for a label for one or more of these points. Finally, in stream-based active learning, the incoming data is arriving continuously, and the algorithm must choose whether it wants to request a label for the current input or not. \nThere are various closely related problems. In Bayesian optimization the goal is to estimate the location of the global optimum $begin{array} { r } { { pmb x } ^ { * } = mathrm { a r g m i n } _ { pmb x } f ( { pmb x } ) } end{array}$ in as few queries as possible; typically we fit a surrogate (response surface) model to the intermediate $( { pmb x } , y )$ queries, to decide which question to ask next. In experiment design, the goal is to infer a parameter vector of some model, using carefully chosen data samples $mathcal { D } = { pmb { x } _ { 1 } , ldots , pmb { x } _ { N } }$ , i.e. we want to estimate $p ( pmb { theta } | mathcal { D } )$ using as little data as possible. (This can be thought of as an unsupervised, or generalized, form of active learning.) \nIn this section, we give a brief review of the pool based approach to active learning. For more details, see e.g., [Set12] for a review. \n19.4.1 Decision-theoretic approach \nIn the decision theoretic approach to active learning, proposed in [KHB07; RM01], we define the utility of querying $_ { x }$ in terms of the value of information. In particular, we define the utility of issuing query $_ { x }$ as \nwhere $R ( a | mathcal { D } ) = mathbb { E } _ { p ( theta | mathcal { D } ) } left[ ell ( theta , a ) right]$ is the posterior expected loss of taking some future action $a$ given the data $mathcal { D }$ observed so far. Unfortunately, evaluating $U ( { pmb x } )$ for each $_ { x }$ is quite expensive, since for each possible response $y$ we might observe, we have to update our beliefs given $( { pmb x } , y )$ to see what effect it might have on our future decisions (similar to look ahead search technique applied to belief states). \n19.4.2 Information-theoretic approach \nIn the information theoretic approach to active supervised learning, we avoid using task-specific loss functions, and instead focus on learning our model as well as we can. In particular, [Lin56] proposed to define the utility of querying $_ { x }$ in terms of information gain about the parameters $pmb theta$ , i.e., the reduction in entropy: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Active learning",
        "subsubsection": "Decision-theoretic approach"
    },
    {
        "content": "19.4 Active learning \nIn active learning, the goal is to identify the true predictive mapping $y = f ( { pmb x } )$ by querying as few $( { pmb x } , y )$ points as possible. There are three main variants. In query synthesis, the algorithm gets to choose any input $_ { x }$ , and can ask for its corresponding output $y = f ( { pmb x } )$ . In pool-based active learning, there is a large, but fixed, set of unlabeled data points, and the algorithm gets to ask for a label for one or more of these points. Finally, in stream-based active learning, the incoming data is arriving continuously, and the algorithm must choose whether it wants to request a label for the current input or not. \nThere are various closely related problems. In Bayesian optimization the goal is to estimate the location of the global optimum $begin{array} { r } { { pmb x } ^ { * } = mathrm { a r g m i n } _ { pmb x } f ( { pmb x } ) } end{array}$ in as few queries as possible; typically we fit a surrogate (response surface) model to the intermediate $( { pmb x } , y )$ queries, to decide which question to ask next. In experiment design, the goal is to infer a parameter vector of some model, using carefully chosen data samples $mathcal { D } = { pmb { x } _ { 1 } , ldots , pmb { x } _ { N } }$ , i.e. we want to estimate $p ( pmb { theta } | mathcal { D } )$ using as little data as possible. (This can be thought of as an unsupervised, or generalized, form of active learning.) \nIn this section, we give a brief review of the pool based approach to active learning. For more details, see e.g., [Set12] for a review. \n19.4.1 Decision-theoretic approach \nIn the decision theoretic approach to active learning, proposed in [KHB07; RM01], we define the utility of querying $_ { x }$ in terms of the value of information. In particular, we define the utility of issuing query $_ { x }$ as \nwhere $R ( a | mathcal { D } ) = mathbb { E } _ { p ( theta | mathcal { D } ) } left[ ell ( theta , a ) right]$ is the posterior expected loss of taking some future action $a$ given the data $mathcal { D }$ observed so far. Unfortunately, evaluating $U ( { pmb x } )$ for each $_ { x }$ is quite expensive, since for each possible response $y$ we might observe, we have to update our beliefs given $( { pmb x } , y )$ to see what effect it might have on our future decisions (similar to look ahead search technique applied to belief states). \n19.4.2 Information-theoretic approach \nIn the information theoretic approach to active supervised learning, we avoid using task-specific loss functions, and instead focus on learning our model as well as we can. In particular, [Lin56] proposed to define the utility of querying $_ { x }$ in terms of information gain about the parameters $pmb theta$ , i.e., the reduction in entropy: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n(Note that the first term is a constant wrt $_ { x }$ , but we include it for later convenience.) Exercise 19.1 asks you to show that this objective is identical to the expected change in the posterior over the parameters which is given by \nUsing symmetry of the mutual information, we can rewrite Equation (19.47) as follows: \nThe advantage of this approach is that we now only have to reason about the uncertainty of the predictive distribution over outputs $y$ , not over the parameters $pmb theta$ . \nEquation (19.51) has an interesting interpretation. The first term prefers examples $_ { x }$ for which there is uncertainty in the predicted label. Just using this as a selection criterion is called maximum entropy sampling [SW87]. However, this can have problems with examples which are inherently ambiguous or mislabeled. The second term in Equation (19.51) will discourage such behavior, since it prefers examples $_ { x }$ for which the predicted label is fairly certain once we know $pmb theta$ ; this will avoid picking inherently hard-to-predict examples. In other words, Equation (19.51) will select examples $_ { x }$ for which the model makes confident predictions which are highly diverse. This approach has therefore been called Bayesian active learning by disagreement or BALD [Hou+12]. \nThis method can be used to train classifiers for other domains where expert labels are hard to acquire, such as medical images or astronomical images [Wal+20]. \n19.4.3 Batch active learning \nSo far, we have assumed a greedy or myopic strategy, in which we select a single example $_ { ast }$ , as if it were the last datapoint to be selected. But sometimes we have a budget to collect a set of $B$ samples, call them $( mathbf { X } , mathbf { Y } )$ . In this case, the information gain criterion becomes ${ cal U } ( { bf X } ) = $ $mathbb { H } left( p ( pmb { theta } | mathcal { D } ) right) - mathbb { E } _ { p ( mathbf { Y } | mathbf { X } , mathcal { D } ) } left[ mathbb { H } left( p ( pmb { theta } | mathbf { Y } , mathbf { X } , mathcal { D } ) right) right]$ . Unfortunately, optimizing this is NP-hard in the horizon length $B$ [KLQ95; KG05]. \nFortunately, under certain conditions, the greedy strategy is near-optimal, as we now explain. First note that, for any given $mathbf { X }$ , the information gain function $f ( mathbf { Y } ) triangleq mathbb { H } left( p ( pmb { theta } | mathcal { D } ) right) - mathbb { H } left( p ( pmb { theta } | mathbf { Y } , mathbf { X } , mathcal { D } ) right)$ maps a set of labels $mathbf { Y }$ to a scalar. It is clear that $f ( varnothing ) = 0$ , and that $f$ is non-decreasing, meaning $f ( Y ^ { mathrm { l a r g e } } ) geq f ( Y ^ { mathrm { s m a l l } } )$ , due to the “more information never hurts” principle. Furthermore, [KG05] proved that $f$ is submodular. As a consequence, a sequential greedy approach is within a constant factor of optimal. If we combine this greedy technique with the BALD objective, we get a method called BatchBALD [KAG19]. \n19.5 Meta-learning \nWe can think of a learning algorithm as a function $A$ that maps data to a parameter estimate, $theta = A ( mathcal { D } )$ . The function $A$ usually has its own parameter — call them $phi$ — such as the initial values for $theta$ , or the learning rate, etc. We denote this by $theta = A ( mathcal { D } ; phi )$ . We can imagine learning $phi$ itself, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license given a collection of datasets $mathcal { D } _ { 1 : J }$ and some meta-learning algorithm $M$ , i.e., $phi = M ( mathcal { D } _ { 1 : J } )$ . We can then apply $A ( cdot ; phi )$ to learn the parameters $theta _ { J + 1 }$ on some new dataset $mathcal { D } _ { J + 1 }$ . There are many techniques for meta-learning — see e.g., [Van18; HRP21] for recent reviews. Below we discuss one particularly popular method. (Note that meta-learning is also called learning to learn [TP97].)",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Active learning",
        "subsubsection": "Information-theoretic approach"
    },
    {
        "content": "(Note that the first term is a constant wrt $_ { x }$ , but we include it for later convenience.) Exercise 19.1 asks you to show that this objective is identical to the expected change in the posterior over the parameters which is given by \nUsing symmetry of the mutual information, we can rewrite Equation (19.47) as follows: \nThe advantage of this approach is that we now only have to reason about the uncertainty of the predictive distribution over outputs $y$ , not over the parameters $pmb theta$ . \nEquation (19.51) has an interesting interpretation. The first term prefers examples $_ { x }$ for which there is uncertainty in the predicted label. Just using this as a selection criterion is called maximum entropy sampling [SW87]. However, this can have problems with examples which are inherently ambiguous or mislabeled. The second term in Equation (19.51) will discourage such behavior, since it prefers examples $_ { x }$ for which the predicted label is fairly certain once we know $pmb theta$ ; this will avoid picking inherently hard-to-predict examples. In other words, Equation (19.51) will select examples $_ { x }$ for which the model makes confident predictions which are highly diverse. This approach has therefore been called Bayesian active learning by disagreement or BALD [Hou+12]. \nThis method can be used to train classifiers for other domains where expert labels are hard to acquire, such as medical images or astronomical images [Wal+20]. \n19.4.3 Batch active learning \nSo far, we have assumed a greedy or myopic strategy, in which we select a single example $_ { ast }$ , as if it were the last datapoint to be selected. But sometimes we have a budget to collect a set of $B$ samples, call them $( mathbf { X } , mathbf { Y } )$ . In this case, the information gain criterion becomes ${ cal U } ( { bf X } ) = $ $mathbb { H } left( p ( pmb { theta } | mathcal { D } ) right) - mathbb { E } _ { p ( mathbf { Y } | mathbf { X } , mathcal { D } ) } left[ mathbb { H } left( p ( pmb { theta } | mathbf { Y } , mathbf { X } , mathcal { D } ) right) right]$ . Unfortunately, optimizing this is NP-hard in the horizon length $B$ [KLQ95; KG05]. \nFortunately, under certain conditions, the greedy strategy is near-optimal, as we now explain. First note that, for any given $mathbf { X }$ , the information gain function $f ( mathbf { Y } ) triangleq mathbb { H } left( p ( pmb { theta } | mathcal { D } ) right) - mathbb { H } left( p ( pmb { theta } | mathbf { Y } , mathbf { X } , mathcal { D } ) right)$ maps a set of labels $mathbf { Y }$ to a scalar. It is clear that $f ( varnothing ) = 0$ , and that $f$ is non-decreasing, meaning $f ( Y ^ { mathrm { l a r g e } } ) geq f ( Y ^ { mathrm { s m a l l } } )$ , due to the “more information never hurts” principle. Furthermore, [KG05] proved that $f$ is submodular. As a consequence, a sequential greedy approach is within a constant factor of optimal. If we combine this greedy technique with the BALD objective, we get a method called BatchBALD [KAG19]. \n19.5 Meta-learning \nWe can think of a learning algorithm as a function $A$ that maps data to a parameter estimate, $theta = A ( mathcal { D } )$ . The function $A$ usually has its own parameter — call them $phi$ — such as the initial values for $theta$ , or the learning rate, etc. We denote this by $theta = A ( mathcal { D } ; phi )$ . We can imagine learning $phi$ itself, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license given a collection of datasets $mathcal { D } _ { 1 : J }$ and some meta-learning algorithm $M$ , i.e., $phi = M ( mathcal { D } _ { 1 : J } )$ . We can then apply $A ( cdot ; phi )$ to learn the parameters $theta _ { J + 1 }$ on some new dataset $mathcal { D } _ { J + 1 }$ . There are many techniques for meta-learning — see e.g., [Van18; HRP21] for recent reviews. Below we discuss one particularly popular method. (Note that meta-learning is also called learning to learn [TP97].)",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Active learning",
        "subsubsection": "Batch active learning"
    },
    {
        "content": "19.5.1 Model-agnostic meta-learning (MAML) \nA natural approach to meta learning is to use a hierarchical Bayesian model, as illustrated in Figure 19.14. The parameters for each task $pmb { theta } _ { j }$ are assumed to come from a common prior, $p ( pmb theta _ { j } | phi )$ , which can be used to help pool statistical strength from multiple data-poor problems. Meta-learning becomes equivalent to learning the prior $phi$ . Rather than performing full Bayesian inference in this model, a more efficient approach is to use the following empirical Bayes (Section 4.6.5.3) approximation: \nwhere $begin{array} { r } { hat { pmb { theta } } _ { j } = hat { pmb { theta } } ( phi , mathcal { D } _ { mathrm { t r a i n } } ^ { j } ) } end{array}$ is a point estimate of the parameters for task $j$ based on $mathcal { D } _ { mathrm { t r a i n } } ^ { j }$ and prior $phi$ , and where we use a cross-validation approximation to the marginal likelihood (Section 5.2.4). \nTo compute the point estimate of the parameters for the target task $hat { pmb { theta } } _ { J + 1 }$ , we use $K$ steps of a gradient ascent procedure starting at $phi$ with a learning rate of $eta$ . This is known as model-agnostic meta-learning or MAML [FAL17]. This can be shown to be equivalent to an approximate MAP estimate using a Gaussian prior centered at $phi$ , where the strength of the prior is controlled by the number of gradient steps [San96; Gra+18]. (This is an example of fast adapation of the task specific weights starting from the shared prior $phi$ .) \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n19.6 Few-shot learning \nPeople can learn to predict from very few labeled examples. This is called few-shot learning. In the extreme in which the person or system learns from a single example of each class, this is called one-shot learning, and if no labeled examples are given, it is called zero-shot learning. \nA common way to evaluate methods for FSL is to use C-way N-shot classification, in which the system is expected to learn to classify $C$ classes using just $N$ training examples of each class. Typically $N$ and $C$ are very small, e.g., Figure 19.15 illustrates the case where we have $C = 3$ classes, each with $N = 2$ examples. Since the amount of data from the new domain (here, ducks, dolphins and hens) is so small, we cannot expect to learn from scratch. Therefore we turn to meta-learning. \nDuring training, the meta-algorithm $M$ trains on a labeled support set from group $j$ , returns a predictor $f ^ { j }$ , which is then evaluated on a disjoint query set also from group $j$ . We optimize $M$ over all $J$ groups. Finally we can apply $M$ to our new labeled support set to get $f ^ { mathrm { t e s t } }$ , which is applied to the query set from the test domain. This is illustrated in Figure 19.15. We see that there is no overlap between the classes in the two training tasks ( ${ mathrm { c a t } , mathrm { l a m b } , mathrm { p i g } }$ and ${ mathrm { d o g } , mathrm { s h a r k } , mathrm { l i o n } }$ ) and those in the test task ( duck, dolphin, hen ). Thus the algorithm $M$ must learn to predict image classes in general rather than any particular set of labels. \nThere are many approaches to few-shot learning. We discuss one such method in Section 19.6.1. For more methods, see e.g., [Wan+20b]. \n19.6.1 Matching networks \nOne approach to few shot learning is to learn a distance metric on some other dataset, and then to use $d _ { pmb theta } ( pmb x , pmb x ^ { prime } )$ inside of a nearest neighbor classifier. Essentially this defines a semi-parametric model of the form $p _ { pmb theta } ( y | pmb x , S )$ , where $boldsymbol { S }$ is the small labeled dataset (known as the support set), and $pmb theta$ are the parameters of the distance function. This approach is widely used for fine-grained classification \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license tasks, where there are many different visually similar categories, such as face images from a gallery, or product images from a catalog.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Meta-learning",
        "subsubsection": "Model-agnostic meta-learning (MAML)"
    },
    {
        "content": "19.6 Few-shot learning \nPeople can learn to predict from very few labeled examples. This is called few-shot learning. In the extreme in which the person or system learns from a single example of each class, this is called one-shot learning, and if no labeled examples are given, it is called zero-shot learning. \nA common way to evaluate methods for FSL is to use C-way N-shot classification, in which the system is expected to learn to classify $C$ classes using just $N$ training examples of each class. Typically $N$ and $C$ are very small, e.g., Figure 19.15 illustrates the case where we have $C = 3$ classes, each with $N = 2$ examples. Since the amount of data from the new domain (here, ducks, dolphins and hens) is so small, we cannot expect to learn from scratch. Therefore we turn to meta-learning. \nDuring training, the meta-algorithm $M$ trains on a labeled support set from group $j$ , returns a predictor $f ^ { j }$ , which is then evaluated on a disjoint query set also from group $j$ . We optimize $M$ over all $J$ groups. Finally we can apply $M$ to our new labeled support set to get $f ^ { mathrm { t e s t } }$ , which is applied to the query set from the test domain. This is illustrated in Figure 19.15. We see that there is no overlap between the classes in the two training tasks ( ${ mathrm { c a t } , mathrm { l a m b } , mathrm { p i g } }$ and ${ mathrm { d o g } , mathrm { s h a r k } , mathrm { l i o n } }$ ) and those in the test task ( duck, dolphin, hen ). Thus the algorithm $M$ must learn to predict image classes in general rather than any particular set of labels. \nThere are many approaches to few-shot learning. We discuss one such method in Section 19.6.1. For more methods, see e.g., [Wan+20b]. \n19.6.1 Matching networks \nOne approach to few shot learning is to learn a distance metric on some other dataset, and then to use $d _ { pmb theta } ( pmb x , pmb x ^ { prime } )$ inside of a nearest neighbor classifier. Essentially this defines a semi-parametric model of the form $p _ { pmb theta } ( y | pmb x , S )$ , where $boldsymbol { S }$ is the small labeled dataset (known as the support set), and $pmb theta$ are the parameters of the distance function. This approach is widely used for fine-grained classification \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license tasks, where there are many different visually similar categories, such as face images from a gallery, or product images from a catalog. \n\nAn extension of this approach is to learn a function of the form \nwhere $a _ { pmb theta } ( pmb x , pmb x _ { n } ; pmb S ) in mathbb { R } ^ { + }$ is some kind of adaptive similarity kernel. For example, we can use an attention kernel of the form \nwhere $c ( { pmb u } , { pmb v } )$ is the cosine distance. (We can make $f$ and $g$ be the same function if we want.) Intuitively, the attention kernel will compare $_ { x }$ to ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ in the context of all the labeled examples, which provides an implicit signal about which feature dimensions are relevant. (We discuss attention mechanisms in more detail in Section 15.4.) This is called a matching network [Vin+16]. See Figure 19.16 for an illustration. \nWe can train the $f$ and $g$ functions using multiple small datasets, as in meta-learning (Section 19.5). More precisely, let $mathcal { D }$ be a large labeled dataset (e.g., ImageNet), and let $p ( mathcal { L } )$ be a distribution over its labels. We create a task by sampling a small set of labels (say 25), ${ mathcal { L } } sim p ( { mathcal { L } } )$ , and then sampling a small support set of examples from $mathcal { D }$ with those labels, $s sim mathcal { L }$ , and finally sampling a small test set with those same labels, $tau sim mathcal L$ . We then train the model to predict the test labels given the support set, i.e., we optimize the following objective: \nAfter training, we freeze $pmb theta$ , and apply Equation (19.53) to a test support set $boldsymbol { S }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n19.7 Weakly supervised learning \nThe term weakly supervised learning refers to scenarios where we do not have an exact label associated with every feature vector in the training set. \nOne scenario is when we have a distribution over labels for each case, rather than a single label. Fortunately, we can still do maximum likelihood training: we just have to minimize the cross entropy, \nwhere $p ( boldsymbol { y } | mathbf { x } _ { n } )$ is the label distribution for case $n$ , and $q _ { pmb { theta } } ( pmb { y } | mathbf { x } _ { n } )$ is the predicted distribution. Indeed, it is often useful to artificially replace exact labels with a “soft” version, in which we replace the delta function with a distribution that puts, say, $9 0 %$ of its mass on the observed label, and spreads the remaining mass uniformly over the other choices. This is called label smoothing, and is a useful form of regularization (see e.g., [MKH19]). \nAnother scenario is when we have a set, or bag, of instances, $pmb { x } _ { n } = { pmb { x } _ { n , 1 } , dotsc , pmb { x } _ { n , B } }$ , but we only have a label for the entire bag, $y _ { n }$ , not for the members of the bag, $y _ { n b }$ . We often assume that if any member of the bag is positive, the whole bag is labeled positive, so $y _ { n } = vee _ { b = 1 } ^ { B } y _ { n b }$ , but we do not know which member “caused” the positive outcome. However, if all the members are negative, the entire bag is negative. This is known as multi-instance learning [DLLP97]. (For a recent example of this in the context of COVID-19 risk score learning, see [MKS21].) Various algorthms can be used to solve the MIL problem, depending on what assumptions we make about the correlation between the labels in each bag, and the fraction of positive members we expect to see (see e.g., [KF05]). \nYet another scenario is known as distant supervision [Min+09], which is often used to train information extraction systems. The idea is that we have some fact, such as “Married(B,M)”, that we know to be true (since it is stored in a database). We use this to label every sentence (in our unlabeled training corpus) in which the entities B and M are mentioned as being a positive example of the “Married” relation. For example, the sentence “B and M invited 100 people to their wedding” will be labeled positive. But this heuristic may include false positives, for example “B and M went out to dinner” will also be labeled positive. Thus the resulting labels will be noisy. We discuss some ways to handle label noise in Section 10.4. \n19.8 Exercises \nExercise 19.1 [Information gain equations]   \nConsider the following two objectives for evaluating the utility of querying a datapoint $_ { ast }$ in an active learning   \nsetting: \nProve that these are equal. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Few-shot learning",
        "subsubsection": "Matching networks"
    },
    {
        "content": "19.7 Weakly supervised learning \nThe term weakly supervised learning refers to scenarios where we do not have an exact label associated with every feature vector in the training set. \nOne scenario is when we have a distribution over labels for each case, rather than a single label. Fortunately, we can still do maximum likelihood training: we just have to minimize the cross entropy, \nwhere $p ( boldsymbol { y } | mathbf { x } _ { n } )$ is the label distribution for case $n$ , and $q _ { pmb { theta } } ( pmb { y } | mathbf { x } _ { n } )$ is the predicted distribution. Indeed, it is often useful to artificially replace exact labels with a “soft” version, in which we replace the delta function with a distribution that puts, say, $9 0 %$ of its mass on the observed label, and spreads the remaining mass uniformly over the other choices. This is called label smoothing, and is a useful form of regularization (see e.g., [MKH19]). \nAnother scenario is when we have a set, or bag, of instances, $pmb { x } _ { n } = { pmb { x } _ { n , 1 } , dotsc , pmb { x } _ { n , B } }$ , but we only have a label for the entire bag, $y _ { n }$ , not for the members of the bag, $y _ { n b }$ . We often assume that if any member of the bag is positive, the whole bag is labeled positive, so $y _ { n } = vee _ { b = 1 } ^ { B } y _ { n b }$ , but we do not know which member “caused” the positive outcome. However, if all the members are negative, the entire bag is negative. This is known as multi-instance learning [DLLP97]. (For a recent example of this in the context of COVID-19 risk score learning, see [MKS21].) Various algorthms can be used to solve the MIL problem, depending on what assumptions we make about the correlation between the labels in each bag, and the fraction of positive members we expect to see (see e.g., [KF05]). \nYet another scenario is known as distant supervision [Min+09], which is often used to train information extraction systems. The idea is that we have some fact, such as “Married(B,M)”, that we know to be true (since it is stored in a database). We use this to label every sentence (in our unlabeled training corpus) in which the entities B and M are mentioned as being a positive example of the “Married” relation. For example, the sentence “B and M invited 100 people to their wedding” will be labeled positive. But this heuristic may include false positives, for example “B and M went out to dinner” will also be labeled positive. Thus the resulting labels will be noisy. We discuss some ways to handle label noise in Section 10.4. \n19.8 Exercises \nExercise 19.1 [Information gain equations]   \nConsider the following two objectives for evaluating the utility of querying a datapoint $_ { ast }$ in an active learning   \nsetting: \nProve that these are equal. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Weakly supervised learning",
        "subsubsection": "N/A"
    },
    {
        "content": "19.7 Weakly supervised learning \nThe term weakly supervised learning refers to scenarios where we do not have an exact label associated with every feature vector in the training set. \nOne scenario is when we have a distribution over labels for each case, rather than a single label. Fortunately, we can still do maximum likelihood training: we just have to minimize the cross entropy, \nwhere $p ( boldsymbol { y } | mathbf { x } _ { n } )$ is the label distribution for case $n$ , and $q _ { pmb { theta } } ( pmb { y } | mathbf { x } _ { n } )$ is the predicted distribution. Indeed, it is often useful to artificially replace exact labels with a “soft” version, in which we replace the delta function with a distribution that puts, say, $9 0 %$ of its mass on the observed label, and spreads the remaining mass uniformly over the other choices. This is called label smoothing, and is a useful form of regularization (see e.g., [MKH19]). \nAnother scenario is when we have a set, or bag, of instances, $pmb { x } _ { n } = { pmb { x } _ { n , 1 } , dotsc , pmb { x } _ { n , B } }$ , but we only have a label for the entire bag, $y _ { n }$ , not for the members of the bag, $y _ { n b }$ . We often assume that if any member of the bag is positive, the whole bag is labeled positive, so $y _ { n } = vee _ { b = 1 } ^ { B } y _ { n b }$ , but we do not know which member “caused” the positive outcome. However, if all the members are negative, the entire bag is negative. This is known as multi-instance learning [DLLP97]. (For a recent example of this in the context of COVID-19 risk score learning, see [MKS21].) Various algorthms can be used to solve the MIL problem, depending on what assumptions we make about the correlation between the labels in each bag, and the fraction of positive members we expect to see (see e.g., [KF05]). \nYet another scenario is known as distant supervision [Min+09], which is often used to train information extraction systems. The idea is that we have some fact, such as “Married(B,M)”, that we know to be true (since it is stored in a database). We use this to label every sentence (in our unlabeled training corpus) in which the entities B and M are mentioned as being a positive example of the “Married” relation. For example, the sentence “B and M invited 100 people to their wedding” will be labeled positive. But this heuristic may include false positives, for example “B and M went out to dinner” will also be labeled positive. Thus the resulting labels will be noisy. We discuss some ways to handle label noise in Section 10.4. \n19.8 Exercises \nExercise 19.1 [Information gain equations]   \nConsider the following two objectives for evaluating the utility of querying a datapoint $_ { ast }$ in an active learning   \nsetting: \nProve that these are equal. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n20 Dimensionality Reduction \nA common form of unsupervised learning is dimensionality reduction, in which we learn a mapping from the high-dimensional visible space, $pmb { x } in mathbb { R } ^ { D }$ , to a low-dimensional latent space, $z in mathbb { R } ^ { L }$ . This mapping can either be a parametric model $z = f ( boldsymbol { x } ; boldsymbol { theta } )$ which can be applied to any input, or it can be a nonparametric mapping where we compute an embedding $z _ { n }$ for each input ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ in the data set, but not for any other points. This latter approach is mostly used for data visualization, whereas the former approach can also be used as a preprocessing step for other kinds of learning algorithms. For example, we might first reduce the dimensionality by learning a mapping from $_ { x }$ to $boldsymbol { z }$ , and then learn a simple linear classifier on this embedding, by mapping $mathscr { z }$ to $y$ . \n20.1 Principal components analysis (PCA) \nThe simplest and most widely used form of dimensionality reduction is principal components analysis or PCA. The basic idea is to find a linear and orthogonal projection of the high dimensional data $pmb { x } in mathbb { R } ^ { D }$ to a low dimensional subspace $z in mathbb { R } ^ { L }$ , such that the low dimensional representation is a “good approximation” to the original data, in the following sense: if we project or encode $_ { x }$ to get $z = mathbf { W } ^ { vert } x$ , and then unproject or decode $boldsymbol { z }$ to get $hat { mathbf { Omega } } _ { hat { mathbf { x } } } = mathbf { W } z$ , then we want $hat { pmb x }$ to be close to $_ { x }$ in $ell _ { 2 }$ distance. In particular, we can define the following reconstruction error or distortion: \nwhere the encode and decoding stages are both linear maps, as we explain below. \nIn Section 20.1.2, we show that we can minimize this objective by setting $hat { mathbf { W } } = mathbf { U } _ { L }$ , where $mathbf { U } _ { L }$ contains the $L$ eigenvectors with largest eigenvalues of the empirical covariance matrix \nwhere $mathbf { X } _ { c }$ is a centered version of the $N times D$ design matrix. In Section 20.2.2, we show that this is equivalent to maximizing the likelihood of a latent linear Gaussian model known as probabilistic PCA. \n20.1.1 Examples \nBefore giving the details, we start by showing some examples.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Learning with Fewer Labeled Examples",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "20 Dimensionality Reduction \nA common form of unsupervised learning is dimensionality reduction, in which we learn a mapping from the high-dimensional visible space, $pmb { x } in mathbb { R } ^ { D }$ , to a low-dimensional latent space, $z in mathbb { R } ^ { L }$ . This mapping can either be a parametric model $z = f ( boldsymbol { x } ; boldsymbol { theta } )$ which can be applied to any input, or it can be a nonparametric mapping where we compute an embedding $z _ { n }$ for each input ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ in the data set, but not for any other points. This latter approach is mostly used for data visualization, whereas the former approach can also be used as a preprocessing step for other kinds of learning algorithms. For example, we might first reduce the dimensionality by learning a mapping from $_ { x }$ to $boldsymbol { z }$ , and then learn a simple linear classifier on this embedding, by mapping $mathscr { z }$ to $y$ . \n20.1 Principal components analysis (PCA) \nThe simplest and most widely used form of dimensionality reduction is principal components analysis or PCA. The basic idea is to find a linear and orthogonal projection of the high dimensional data $pmb { x } in mathbb { R } ^ { D }$ to a low dimensional subspace $z in mathbb { R } ^ { L }$ , such that the low dimensional representation is a “good approximation” to the original data, in the following sense: if we project or encode $_ { x }$ to get $z = mathbf { W } ^ { vert } x$ , and then unproject or decode $boldsymbol { z }$ to get $hat { mathbf { Omega } } _ { hat { mathbf { x } } } = mathbf { W } z$ , then we want $hat { pmb x }$ to be close to $_ { x }$ in $ell _ { 2 }$ distance. In particular, we can define the following reconstruction error or distortion: \nwhere the encode and decoding stages are both linear maps, as we explain below. \nIn Section 20.1.2, we show that we can minimize this objective by setting $hat { mathbf { W } } = mathbf { U } _ { L }$ , where $mathbf { U } _ { L }$ contains the $L$ eigenvectors with largest eigenvalues of the empirical covariance matrix \nwhere $mathbf { X } _ { c }$ is a centered version of the $N times D$ design matrix. In Section 20.2.2, we show that this is equivalent to maximizing the likelihood of a latent linear Gaussian model known as probabilistic PCA. \n20.1.1 Examples \nBefore giving the details, we start by showing some examples. \nFigure 20.1 shows a very simple example, where we project 2d data to a 1d line. This direction captures most of the variation in the data. \nIn Figure 20.2, we show what happens when we project some MNIST images of the digit 9 down to 2d. Although the inputs are high dimensional (specifically $2 8 times 2 8 = 7 8 4$ dimensional), the number of “effective degrees of freedom” is much less, since the pixels are correlated, and many digits look similar. Therefore we can represent each image as a point in a low dimensional linear space. \nIn general, it can be hard to interpet the latent dimensions to which the data is projected. However, by looking at several projected points along a given direction, and the examples from which they are derived, we see that the first principal component (horizontal direction) seems to capture the orientation of the digit, and the second component (vertical direction) seems to capture line thickness. \nIn Figure 20.3, we show PCA applied to another image dataset, known as the Olivetti face dataset, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 which is a set of $6 4 times 6 4$ grayscale images. We project these to a 3d subspace. The resulting basis vectors (columns of the projection matrix W) are shown as images in in Figure 20.3b; these are known as eigenfaces [Tur13], for reasons that will be explained in Section 20.1.2. We see that the main modes of variation in the data are related to overall lighting, and then differences in the eyebrow region of the face. If we use enough dimensions (but fewer than the 4096 we started with), we can use the representation $z = mathbf { W } ^ { top } x$ as input to a nearest-neighbor classifier to perform face recognition; this is faster and more reliable than working in pixel space [MWP98]. \n\n20.1.2 Derivation of the algorithm \nSuppose we have an (unlabeled) dataset $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ , where $pmb { x } _ { n } in mathbb { R } ^ { D }$ . We can represent this as an $N times D$ data matrix $mathbf { X }$ . We will assume $begin{array} { r } { overline { { pmb { x } } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } pmb { x } _ { n } = mathbf { 0 } } end{array}$ , which we can ensure by centering the data. \nWe would like to approximate each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ by a low dimensional representation, $z _ { n } in mathbb { R } ^ { L }$ . We assume where each that each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ ${ pmb w } _ { k } in mathbb { R } ^ { D }$ can be “explained” in terms of a weighted combination of basis functions , and where the weights are given by $z _ { n } in mathbb { R } ^ { L }$ , i.e., we assume $begin{array} { r } { pmb { x } _ { n } approx sum _ { k = 1 } ^ { L } z _ { n k } pmb { w } _ { k } } end{array}$ $pmb { w } _ { 1 } , ldots , pmb { w } _ { L }$ , The vector $z _ { n }$ is the low dimensional representation of ${ pmb x } _ { n }$ , and is known as the latent vector, since it consists of latent or “hidden” values that are not observed in the data. The collection of these latent variables are called the latent factors. \nWe can measure the error produced by this approximation as follows: \nwhere the rows of $mathbf { Z }$ contain the low dimension versions of the rows of $mathbf { X }$ . This is known as the (average) reconstruction error, since we are approximating each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ by $hat { pmb x } _ { n } = { bf W } pmb z _ { n }$ . \nWe want to minimize this subject to the constraint that $mathbf { W }$ is an orthogonal matrix. Below we show that the optimal solution is obtained by setting $hat { mathbf { W } } = mathbf { U } _ { L }$ , where $mathbf { U } _ { L }$ contains the $L$ eigenvectors with largest eigenvalues of the empirical covariance matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Principal components analysis (PCA)",
        "subsubsection": "Examples"
    },
    {
        "content": "20.1.2 Derivation of the algorithm \nSuppose we have an (unlabeled) dataset $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ , where $pmb { x } _ { n } in mathbb { R } ^ { D }$ . We can represent this as an $N times D$ data matrix $mathbf { X }$ . We will assume $begin{array} { r } { overline { { pmb { x } } } = frac { 1 } { N } sum _ { n = 1 } ^ { N } pmb { x } _ { n } = mathbf { 0 } } end{array}$ , which we can ensure by centering the data. \nWe would like to approximate each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ by a low dimensional representation, $z _ { n } in mathbb { R } ^ { L }$ . We assume where each that each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ ${ pmb w } _ { k } in mathbb { R } ^ { D }$ can be “explained” in terms of a weighted combination of basis functions , and where the weights are given by $z _ { n } in mathbb { R } ^ { L }$ , i.e., we assume $begin{array} { r } { pmb { x } _ { n } approx sum _ { k = 1 } ^ { L } z _ { n k } pmb { w } _ { k } } end{array}$ $pmb { w } _ { 1 } , ldots , pmb { w } _ { L }$ , The vector $z _ { n }$ is the low dimensional representation of ${ pmb x } _ { n }$ , and is known as the latent vector, since it consists of latent or “hidden” values that are not observed in the data. The collection of these latent variables are called the latent factors. \nWe can measure the error produced by this approximation as follows: \nwhere the rows of $mathbf { Z }$ contain the low dimension versions of the rows of $mathbf { X }$ . This is known as the (average) reconstruction error, since we are approximating each ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ by $hat { pmb x } _ { n } = { bf W } pmb z _ { n }$ . \nWe want to minimize this subject to the constraint that $mathbf { W }$ is an orthogonal matrix. Below we show that the optimal solution is obtained by setting $hat { mathbf { W } } = mathbf { U } _ { L }$ , where $mathbf { U } _ { L }$ contains the $L$ eigenvectors with largest eigenvalues of the empirical covariance matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n20.1.2.1 Base case \nLet us start by estimating the best 1d solution, ${ pmb w } _ { 1 } in mathbb { R } ^ { D }$ . We will find the remaining basis vectors $_ { w _ { 2 } }$ , $_ { w _ { 3 } }$ , etc. later. \nLet the coefficients for each of the data points associated with the first basis vector be denoted by $tilde { mathbf { z } } _ { 1 } = [ z _ { 1 1 } , dots , z _ { N 1 } ] in mathbb { R } ^ { N _ { mathcal { D } } }$ . The reconstruction error is given by \nsince $pmb { w } _ { 1 } ^ { vert } pmb { w } _ { 1 } = 1$ (by the orthonormality assumption). Taking derivatives wrt $z _ { n 1 }$ and equating to zero gives \nSo the optimal embedding is obtained by orthogonally projecting the data onto ${ pmb w } _ { 1 }$ (see Figure 20.1(a)). Plugging this back in gives the loss for the weights: \nTo solve for ${ pmb w } _ { 1 }$ , note that \nwhere $pmb { Sigma }$ is the empirical covariance matrix (since we assumed the data is centered). We can trivially optimize this by letting $| | pmb { w } _ { 1 } | |  infty$ , so we impose the constraint $| | pmb { w } _ { 1 } | | = 1$ and instead optimize \nwhere $lambda _ { 1 }$ is a Lagrange multiplier (see Section 8.5.1). Taking derivatives and equating to zero we have \nHence the optimal direction onto which we should project the data is an eigenvector of the covariance matrix. Left multiplying by $pmb { w } _ { 1 } ^ { 1 }$ (and using $pmb { w } _ { 1 } ^ { vert } pmb { w } _ { 1 } = 1$ ) we find \nSince we want to maximize this quantity (minimize the loss), we pick the eigenvector which corresponds to the largest eigenvalue. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n20.1.2.2 Optimal weight vector maximizes the variance of the projected data \nBefore continuing, we make an interesting observation. Since the data has been centered, we have \nHence variance of the projected data is given by \nFrom this, we see that minimizing the reconstruction error is equivalent to maximizing the variance of the projected data: \nThis is why it is often said that PCA finds the directions of maximal variance. (See Figure 20.4 for an illustration.) However, the minimum error formulation is easier to understand and is more general. \n20.1.2.3 Induction step \nNow let us find another direction ${ pmb w } _ { 2 }$ to further minimize the reconstruction error, subject to $pmb { w } _ { 1 } ^ { top } pmb { w } _ { 2 } = 0$ and $pmb { w } _ { 2 } ^ { top } pmb { w } _ { 2 } = 1$ . The error is \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nOptimizing wrt ${ pmb w } _ { 1 }$ and $z _ { 1 }$ gives the same solution as before. Exercise 20.3 asks you to show that ∂zL = 0 yields zn2 = w2Txn. Substituting in yields \nDropping the constant term, plugging in the optimal ${ pmb w } _ { 1 }$ and adding the constraints yields \nExercise 20.3 asks you to show that the solution is given by the eigenvector with the second largest eigenvalue: \nThe proof continues in this way to show that $hat { mathbf { W } } = mathbf { U } _ { L }$ . \n20.1.3 Computational issues \nIn this section, we discuss various practical issues related to using PCA. \n20.1.3.1 Covariance matrix vs correlation matrix \nWe have been working with the eigendecomposition of the covariance matrix. However, it is better to use the correlation matrix instead. The reason is that otherwise PCA can be “misled” by directions in which the variance is high merely because of the measurement scale. Figure 20.5 shows an example of this. On the left, we see that the vertical axis uses a larger range than the horizontal axis. This results in a first principal component that looks somewhat “unnatural”. On the right, we show the results of PCA after standardizing the data (which is equivalent to using the correlation matrix instead of the covariance matrix); the results look much better. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Principal components analysis (PCA)",
        "subsubsection": "Derivation of the algorithm"
    },
    {
        "content": "Optimizing wrt ${ pmb w } _ { 1 }$ and $z _ { 1 }$ gives the same solution as before. Exercise 20.3 asks you to show that ∂zL = 0 yields zn2 = w2Txn. Substituting in yields \nDropping the constant term, plugging in the optimal ${ pmb w } _ { 1 }$ and adding the constraints yields \nExercise 20.3 asks you to show that the solution is given by the eigenvector with the second largest eigenvalue: \nThe proof continues in this way to show that $hat { mathbf { W } } = mathbf { U } _ { L }$ . \n20.1.3 Computational issues \nIn this section, we discuss various practical issues related to using PCA. \n20.1.3.1 Covariance matrix vs correlation matrix \nWe have been working with the eigendecomposition of the covariance matrix. However, it is better to use the correlation matrix instead. The reason is that otherwise PCA can be “misled” by directions in which the variance is high merely because of the measurement scale. Figure 20.5 shows an example of this. On the left, we see that the vertical axis uses a larger range than the horizontal axis. This results in a first principal component that looks somewhat “unnatural”. On the right, we show the results of PCA after standardizing the data (which is equivalent to using the correlation matrix instead of the covariance matrix); the results look much better. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n20.1.3.2 Dealing with high-dimensional data \nWe have presented PCA as the problem of finding the eigenvectors of the $D times D$ covariance matrix $mathbf { X } ^ { mathsf { I } } mathbf { X }$ . If $D > N$ , it is faster to work with the $N times N$ Gram matrix $mathbf { X X ^ { parallel } }$ . We now show how to do this. \nFirst, let $mathbf { U }$ be an orthogonal matrix containing the eigenvectors of $mathbf { X X ^ { mathsf { T } } }$ with corresponding eigenvalues in $pmb { Lambda }$ . By definition we have $( mathbf { X } mathbf { X } ^ { ! } ) mathbf { U } = mathbf { U } mathbf { A }$ . Pre-multiplying by $mathbf { X } ^ { mathsf { I } }$ gives \nfrom which we see that the eigenvectors of $mathbf { X } ^ { mathsf { I } } mathbf { X }$ are $mathbf { V } = mathbf { X } ^ { mathsf { I } } mathbf { U }$ , with eigenvalues given by $pmb { Lambda }$ as before. However, these eigenvectors are not normalized, since $| | pmb { v } _ { j } | | ^ { 2 } = pmb { u } _ { j } ^ { top } mathbf { X } mathbf { X } ^ { top } pmb { u } _ { j } = lambda _ { j } pmb { u } _ { j } ^ { top } pmb { u } _ { j } = lambda _ { j }$ . The normalized eigenvectors are given by \nThis provides an alternative way to compute the PCA basis. It also allows us to use the kernel trick, as we discuss in Section 20.4.6. \n20.1.3.3 Computing PCA using SVD \nIn this section, we show the equivalence between PCA as computed using eigenvector methods (Section 20.1) and the truncated SVD.1 \nLet ${ bf U } _ { Sigma } pmb { Lambda } _ { Sigma } mathbf { U } _ { Sigma } ^ { top }$ be the top $L$ eigendecomposition of the covariance matrix $begin{array} { r } { pmb { Sigma } propto mathbf { X } ^ { vert } mathbf { X } } end{array}$ (we assume $mathbf { X }$ is centered). Recall from Section 20.1.2 that the optimal estimate of the projection weights $mathbf { W }$ is given by the top $L$ eigenvalues, so $mathbf { W } = mathbf { U } _ { Sigma }$ . \nNow let $mathbf { U } _ { X } mathbf { S } _ { X } mathbf { V } _ { X } ^ { sf T } approx mathbf { X }$ be the $L$ -truncated SVD approximation to the data matrix $mathbf { X }$ . From Equation (7.184), we know that the right singular vectors of $mathbf { X }$ are the eigenvectors of $mathbf { X } ^ { mathsf { I } } mathbf { X }$ , so $mathbf { V } _ { X } = mathbf { U } _ { Sigma } = mathbf { W }$ . (In addition, the eigenvalues of the covariance matrix are related to the singular values of the data matrix via $lambda _ { k } = s _ { k } ^ { 2 } / N$ .) \nNow suppose we are interested in the projected points (also called the principal components or PC scores), rather than the projection matrix. We have \nFinally, if we want to approximately reconstruct the data, we have \nThis is precisely the same as a truncated SVD approximation (Section 7.5.5). \nThus we see that we can perform PCA either using an eigendecomposition of $pmb { Sigma }$ or an SVD decomposition of $mathbf { X }$ . The latter is often preferable, for computational reasons. For very high dimensional problems, we can use a randomized SVD algorithm, see e.g., [HMT11; SKT14; DM16]. For example, the randomized solver used by sklearn takes $O ( N L ^ { 2 } ) + O ( L ^ { 3 } )$ time for $N$ examples and $L$ principal components, whereas exact SVD takes $O ( N D ^ { 2 } ) + O ( D ^ { 3 } )$ time. \n20.1.4 Choosing the number of latent dimensions \nIn this section, we discuss how to choose the number of latent dimensions $L$ for PCA. \n20.1.4.1 Reconstruction error \nLet us define the reconstruction error on some dataset $mathcal { D }$ incurred by the model when using $L$ dimensions: \nwhere the reconstruction is given by by ${ hat { pmb x } } _ { n } = mathbf { W } { boldsymbol z } _ { n } + { boldsymbol mu }$ , where $z _ { n } = mathbf { W } ^ { parallel } ( { pmb x } _ { n } - { pmb mu } )$ and $mu$ is the empirical mean, and $mathbf { W }$ is estimated as above. Figure 20.6(a) plots $mathcal { L } _ { L }$ vs $L$ on the MNIST training data. We see that it drops off quite quickly, indicating that we can capture most of the empirical correlation of the pixels with a small number of factors. \nOf course, if we use $L = { mathrm { r a n k } } ( mathbf { X } )$ , we get zero reconstruction error on the training set. To avoid overfitting, it is natural to plot reconstruction error on the test set. This is shown in Figure 20.6(b). Here we see that the error continues to go down even as the model becomes more complex! Thus we do not get the usual U-shaped curve that we typically expect to see in supervised learning. The problem is that PCA is not a proper generative model of the data: If you give it more latent dimensions, it will be able to approximate the test data more accurately. (A similar problem arises if we plot reconstruction error on the test set using K-means clustering, as discussed in Section 21.3.7.) We discuss some solutions to this below. \n20.1.4.2 Scree plots \nA common alternative to plotting reconstruction error vs $L$ is to use something called a scree plot, which is a plot of the eigenvalues $lambda _ { j }$ vs $j$ in order of decreasing magnitude. One can show \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Principal components analysis (PCA)",
        "subsubsection": "Computational issues"
    },
    {
        "content": "20.1.4 Choosing the number of latent dimensions \nIn this section, we discuss how to choose the number of latent dimensions $L$ for PCA. \n20.1.4.1 Reconstruction error \nLet us define the reconstruction error on some dataset $mathcal { D }$ incurred by the model when using $L$ dimensions: \nwhere the reconstruction is given by by ${ hat { pmb x } } _ { n } = mathbf { W } { boldsymbol z } _ { n } + { boldsymbol mu }$ , where $z _ { n } = mathbf { W } ^ { parallel } ( { pmb x } _ { n } - { pmb mu } )$ and $mu$ is the empirical mean, and $mathbf { W }$ is estimated as above. Figure 20.6(a) plots $mathcal { L } _ { L }$ vs $L$ on the MNIST training data. We see that it drops off quite quickly, indicating that we can capture most of the empirical correlation of the pixels with a small number of factors. \nOf course, if we use $L = { mathrm { r a n k } } ( mathbf { X } )$ , we get zero reconstruction error on the training set. To avoid overfitting, it is natural to plot reconstruction error on the test set. This is shown in Figure 20.6(b). Here we see that the error continues to go down even as the model becomes more complex! Thus we do not get the usual U-shaped curve that we typically expect to see in supervised learning. The problem is that PCA is not a proper generative model of the data: If you give it more latent dimensions, it will be able to approximate the test data more accurately. (A similar problem arises if we plot reconstruction error on the test set using K-means clustering, as discussed in Section 21.3.7.) We discuss some solutions to this below. \n20.1.4.2 Scree plots \nA common alternative to plotting reconstruction error vs $L$ is to use something called a scree plot, which is a plot of the eigenvalues $lambda _ { j }$ vs $j$ in order of decreasing magnitude. One can show \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n(Exercise 20.4) that \nThus as the number of dimensions increases, the eigenvalues get smaller, and so does the reconstruction error, as shown in Figure 20.7a).2 A related quantity is the fraction of variance explained, defined as \nThis captures the same information as the scree plot, but goes up with $L$ (see Figure 20.7b). \n20.1.4.3 Profile likelihood \nAlthough there is no U-shape in the reconstruction error plot, there is sometimes a “knee” or “elbow” in the curve, where the error suddenly changes from relatively large errors to relatively small. The idea is that for $L < L ^ { * }$ , where $L ^ { * }$ is the “true” latent dimensionality (or number of clusters), the rate of decrease in the error function will be high, whereas for $L > L ^ { * }$ , the gains will be smaller, since the model is already sufficiently complex to capture the true distribution. \nOne way to automate the detection of this change in the gradient of the curve is to compute the profile likelihood, as proposed in [ZG06]. The idea is this. Let $lambda _ { L }$ be some measure of the error incurred by a model of size $L$ , such that $lambda _ { 1 } geq lambda _ { 2 } geq cdot cdot cdot geq lambda _ { L ^ { operatorname* { m a x } } }$ . In PCA, these are the eigenvalues, but the method can also be applied to the reoconstruction error from K-means clustering (see Section 21.3.7). Now consider partitioning these values into two groups, depending on whether $k < L$ or $k > L$ , where $L$ is some threshold which we will determine. To measure the quality of $L$ , we will use a simple change-point model, where $lambda _ { k } sim mathcal N ( mu _ { 1 } , sigma ^ { 2 } )$ if $k leq L$ , and $lambda _ { k } sim mathcal N ( mu _ { 2 } , sigma ^ { 2 } )$ if $k > L$ . (It is important that $sigma ^ { 2 }$ be the same in both models, to prevent overfitting in the case where one regime has less data than the other.) Within each of the two regimes, we assume the $lambda _ { k }$ are iid, which is obviously incorrect, but is adequate for our present purposes. We can fit this model for each $L = 1 : L ^ { operatorname* { m a x } }$ by partitioning the data and computing the MLEs, using a pooled estimate of the variance: \n\nWe can then evaluate the profile log likelihood \nThis is illustrated in Figure 20.8. We see that the peak $L ^ { * } = operatorname { a r g m a x } ell ( L )$ is well determined. \n20.2 Factor analysis * \nPCA is a simple method for computing a linear low-dimensional representation of data. In this section, we present a generalization of PCA known as factor analysis. This is based on a probabilistic model, which means we can treat it as a building block for more complex models, such as the mixture of FA models in Section 20.2.6, or the nonlinear FA model in Section 20.3.5. We can recover PCA as a special limiting case of FA, as we discuss in Section 20.2.2. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Principal components analysis (PCA)",
        "subsubsection": "Choosing the number of latent dimensions"
    },
    {
        "content": "20.2.1 Generative model \nFactor analysis corresponds to the following linear-Gaussian latent variable generative model: \nwhere $mathbf { W }$ is a $D times L$ matrix, known as the factor loading matrix, and $Psi$ is a diagonal $D times D$ covariance matrix. \nFA can be thought of as a low-rank version of a Gaussian distribution. To see this, note that the induced marginal distribution $p ( { pmb x } | { pmb theta } )$ is a Gaussian (see Equation (3.38) for the derivation): \nHence $mathbb { E } left[ pmb { x } right] = mathbf { W } pmb { mu } _ { 0 } + pmb { mu }$ and $operatorname { C o v } left[ mathbf { { x } } right] = mathbf { W } operatorname { C o v } left[ { boldsymbol { z } } right] mathbf { W } ^ { mathsf { T } } + Psi = mathbf { W } pmb { Sigma } _ { 0 } mathbf { W } ^ { mathsf { T } } + Psi$ . From this, we see that we can set ${ boldsymbol { mu } } _ { 0 } = mathbf { 0 }$ without loss of generality, since we can always absorb $mathbf { W } pmb { mu } _ { 0 }$ into $pmb { mu }$ . Similarly, we can set $pmb { Sigma } _ { 0 } = mathbf { I }$ without loss of generality, since we can always absorb a correlated prior by using a new weight matrix, $tilde { mathbf { W } } = mathbf { W } boldsymbol { Sigma } _ { 0 } ^ { - frac { 1 } { 2 } }$ . After these simplifications we have \nFor example, suppose where $L = 1$ , $D = 2$ and $boldsymbol { Psi } = sigma ^ { 2 } mathbf { I }$ . We illustrate the generative process in this case in Figure 20.9. We can think of this as taking an isotropic Gaussian “spray can”, representing the likelihood $p ( pmb { x } | pmb { z } )$ , and “sliding it along” the 1d line defined by $boldsymbol { w z } + boldsymbol { mu }$ as we vary the 1d latent prior $z$ . This induces an elongated (and hence correlated) Gaussian in 2d. That is, the induced distribution has the form $p ( { pmb x } ) = mathcal { N } ( { pmb x } | { pmb mu } , { pmb w } { pmb w } ^ { 1 } + sigma ^ { 2 } { bf I } )$ . \nIn general, FA approximates the covariance matrix of the visible vector using a low-rank decomposition: \nThis only uses $O ( L D )$ parameters, which allows a flexible compromise between a full covariance Gaussian, with $O ( D ^ { 2 } )$ parameters, and a diagonal covariance, with $O ( D )$ parameters. \nFrom Equation (20.39), we see that we should restrict $Psi$ to be diagonal, otherwise we could set $mathbf { W } = mathbf { 0 }$ , thus ignoring the latent factors, while still being able to model any covariance. The marginal variance of each visible variable is given by $begin{array} { r } { mathbb { V } left[ x _ { d } right] = sum _ { k = 1 } ^ { L } w _ { d k } ^ { 2 } + psi _ { d } } end{array}$ , where the first term is the variance due to the common factors, and the second $psi _ { d }$ term is called the uniqueness, and is the variance term that is specific to that dimension. \nWe can estimate the parameters of an FA model using EM (see Section 20.2.3). Once we have fit the model, we can compute probabilistic latent embeddings using $p ( boldsymbol { z } | boldsymbol { x } )$ . Using Bayes rule for Gaussians we have \nwhere $mathbf { C }$ is defined in Equation (20.39). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFigure 20.9: Illustration of the $F A$ generative process, where we have $L = 1$ latent dimension generating $D = 2$ observed dimensions; we assume $scriptstyle Psi  =  sigma ^ { 2 } mathbf { I }$ . The latent factor has value $z in mathbb { R }$ , sampled from $p ( z )$ ; this gets mapped to a 2d offset ${ pmb { delta } } = z { pmb { w } }$ , where $pmb { w } in mathbb { R } ^ { 2 }$ , which gets added to $pmb { mu }$ to define a Gaussian $p ( pmb { x } | z ) = mathcal { N } ( pmb { x } | pmb { mu } + pmb { delta } , sigma ^ { 2 } mathbf { I } )$ . By integrating over $z$ , we “slide” this circular Gaussian “spray can” along the principal component axis $pmb { w }$ , which induces elliptical Gaussian contours in $scriptstyle { pmb x }$ space centered on $pmb { mu }$ . Adapted from Figure 12.9 of [Bis06]. \n20.2.2 Probabilistic PCA \nIn this section, we consider a special case of the factor analysis model in which W has orthonormal columns, $boldsymbol { Psi } = sigma ^ { 2 } mathbf { I }$ and ${ pmb mu } = { bf 0 }$ . This model is called probabilistic principal components analysis (PPCA) [TB99], or sensible PCA [Row97]. The marginal distribution on the visible variables has the form \nwhere \nThe log likelihood for PPCA is given by \nThe MLE for $pmb { mu }$ is ${ pmb x }$ . Plugging in gives \nwhere $begin{array} { r } { mathbf { S } = frac { 1 } { N } sum _ { n = 1 } ^ { N } ( { pmb x } _ { n } - { overline { { pmb x } } } ) ( { pmb x } _ { n } - { overline { { pmb x } } } ) ^ { top } } end{array}$ is the empirical covariance matrix. \nIn [TB99; Row97] they show that the maximum of this objective must satisfy \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $mathbf { U } _ { L }$ is a $D times L$ matrix whose columns are given by the $L$ eigenvectors of $mathbf { s }$ with largest eigenvalues, $mathbf { L } _ { L }$ is the $L times L$ diagonal matrix of eigenvalues, and $mathbf { R }$ is an arbitrary $L times L$ orthogonal matrix, which (WLOG) we can take to be $mathbf R = mathbf I$ . In the noise-free limit, where $sigma ^ { 2 } = 0$ , we see that $mathbf { W } _ { mathrm { m l e } } = mathbf { U } _ { L } mathbf { L } _ { L } ^ { frac { 1 } { 2 } }$ , which is proportional to the PCA solution.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Generative model"
    },
    {
        "content": "Figure 20.9: Illustration of the $F A$ generative process, where we have $L = 1$ latent dimension generating $D = 2$ observed dimensions; we assume $scriptstyle Psi  =  sigma ^ { 2 } mathbf { I }$ . The latent factor has value $z in mathbb { R }$ , sampled from $p ( z )$ ; this gets mapped to a 2d offset ${ pmb { delta } } = z { pmb { w } }$ , where $pmb { w } in mathbb { R } ^ { 2 }$ , which gets added to $pmb { mu }$ to define a Gaussian $p ( pmb { x } | z ) = mathcal { N } ( pmb { x } | pmb { mu } + pmb { delta } , sigma ^ { 2 } mathbf { I } )$ . By integrating over $z$ , we “slide” this circular Gaussian “spray can” along the principal component axis $pmb { w }$ , which induces elliptical Gaussian contours in $scriptstyle { pmb x }$ space centered on $pmb { mu }$ . Adapted from Figure 12.9 of [Bis06]. \n20.2.2 Probabilistic PCA \nIn this section, we consider a special case of the factor analysis model in which W has orthonormal columns, $boldsymbol { Psi } = sigma ^ { 2 } mathbf { I }$ and ${ pmb mu } = { bf 0 }$ . This model is called probabilistic principal components analysis (PPCA) [TB99], or sensible PCA [Row97]. The marginal distribution on the visible variables has the form \nwhere \nThe log likelihood for PPCA is given by \nThe MLE for $pmb { mu }$ is ${ pmb x }$ . Plugging in gives \nwhere $begin{array} { r } { mathbf { S } = frac { 1 } { N } sum _ { n = 1 } ^ { N } ( { pmb x } _ { n } - { overline { { pmb x } } } ) ( { pmb x } _ { n } - { overline { { pmb x } } } ) ^ { top } } end{array}$ is the empirical covariance matrix. \nIn [TB99; Row97] they show that the maximum of this objective must satisfy \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $mathbf { U } _ { L }$ is a $D times L$ matrix whose columns are given by the $L$ eigenvectors of $mathbf { s }$ with largest eigenvalues, $mathbf { L } _ { L }$ is the $L times L$ diagonal matrix of eigenvalues, and $mathbf { R }$ is an arbitrary $L times L$ orthogonal matrix, which (WLOG) we can take to be $mathbf R = mathbf I$ . In the noise-free limit, where $sigma ^ { 2 } = 0$ , we see that $mathbf { W } _ { mathrm { m l e } } = mathbf { U } _ { L } mathbf { L } _ { L } ^ { frac { 1 } { 2 } }$ , which is proportional to the PCA solution. \n\nThe MLE for the observation variance is \nwhich is the average distortion associated with the discarded dimensions. If $L  =  D$ , then the estimated noise is $0$ , since the model collapses to $z = x$ . \nTo compute the likelihood $p ( mathbf { X } | boldsymbol { mu } , mathbf { W } , boldsymbol { sigma } ^ { 2 } )$ , we need to evaluate $mathbf { C } ^ { - 1 }$ and $log | mathbf { C } |$ , where $mathbf { C }$ is a $D times D$ matrix. To do this efficiently, we can use the matrix inversion lemma to write \nwhere the $L times L$ dimensional matrix $mathbf { M }$ is given by \nWhen we plug in the MLE for $mathbf { W }$ from Equation (20.45) (using $mathbf R = mathbf I$ ) we find \nand hence \nThus we can avoid all matrix inversions (since $Lambda _ { L } ^ { - 1 } = mathrm { d i a g } ( 1 / lambda _ { j } ) )$ . \nTo use PPCA as an alternative to PCA, we need to compute the posterior mean $mathbb { E } left[ boldsymbol { z } | boldsymbol { x } right]$ , which is the equivalent of the encoder model. Using Bayes rule for Gaussians we have \nwhere $mathbf { M }$ is defined in Equation (20.48). In the $sigma ^ { 2 } = 0$ limit, the posterior mean using the MLE parameters becomes \nwhich is the orthogonal projection of the data into the latent space, as in standard PCA. \n20.2.3 EM algorithm for FA/PPCA \nIn this section, we describe one method for computing the MLE for the FA model using the EM algorithm, based on [RT82; GH96]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Probabilistic PCA"
    },
    {
        "content": "The MLE for the observation variance is \nwhich is the average distortion associated with the discarded dimensions. If $L  =  D$ , then the estimated noise is $0$ , since the model collapses to $z = x$ . \nTo compute the likelihood $p ( mathbf { X } | boldsymbol { mu } , mathbf { W } , boldsymbol { sigma } ^ { 2 } )$ , we need to evaluate $mathbf { C } ^ { - 1 }$ and $log | mathbf { C } |$ , where $mathbf { C }$ is a $D times D$ matrix. To do this efficiently, we can use the matrix inversion lemma to write \nwhere the $L times L$ dimensional matrix $mathbf { M }$ is given by \nWhen we plug in the MLE for $mathbf { W }$ from Equation (20.45) (using $mathbf R = mathbf I$ ) we find \nand hence \nThus we can avoid all matrix inversions (since $Lambda _ { L } ^ { - 1 } = mathrm { d i a g } ( 1 / lambda _ { j } ) )$ . \nTo use PPCA as an alternative to PCA, we need to compute the posterior mean $mathbb { E } left[ boldsymbol { z } | boldsymbol { x } right]$ , which is the equivalent of the encoder model. Using Bayes rule for Gaussians we have \nwhere $mathbf { M }$ is defined in Equation (20.48). In the $sigma ^ { 2 } = 0$ limit, the posterior mean using the MLE parameters becomes \nwhich is the orthogonal projection of the data into the latent space, as in standard PCA. \n20.2.3 EM algorithm for FA/PPCA \nIn this section, we describe one method for computing the MLE for the FA model using the EM algorithm, based on [RT82; GH96]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n20.2.3.1 EM for FA \nIn the $mathrm { E }$ step, we compute the posterior embeddings \nIn the M step, it is easiest to estimate $pmb { mu }$ and $mathbf { W }$ at the same time, by defining $tilde { mathbf { W } } = ( mathbf { W } , pmb { mu } )$ , $tilde { z } = ( z , 1 )$ , Also, define \nThen the M step is as follows: \nNote that these updates are for “vanilla” EM. A much faster version of this algorithm, based on ECM, is described in [ZY08]. \n20.2.3.2 EM for (P)PCA \nWe can also use EM to fit the PPCA model, which provides a useful alternative to eigenvector methods. This relies on the probabilistic formulation of PCA. However the algorithm continues to work in the zero noise limit, $sigma ^ { 2 } = 0$ , as shown by [Row97]. \nIn particular, let $tilde { mathbf { Z } } = mathbf { Z } ^ { top }$ be a $L times N _ { mathcal { D } }$ matrix storing the posterior means (low-dimensional representations) along its columns. Similarly, let $tilde { mathbf { X } } = mathbf { X } ^ { top }$ store the original data along its columns. From Equation (20.52), when $sigma ^ { 2 } = 0$ , we have \nThis constitutes the $mathrm { E }$ step. Notice that this is just an orthogonal projection of the data. From Equation 20.59, the M step is given by \nwhere we exploited the fact that $Sigma = mathrm { C o v } left[ z _ { i } | boldsymbol { x } _ { i } , pmb { theta } right] = 0 mathbf { I }$ when $sigma ^ { 2 } = 0$ . \nIt is worth comparing this expression to the MLE for multi-output linear regression (Equation 11.2), which has the form $begin{array} { r } { { bf W } = ( sum _ { i } pmb { y } _ { i } pmb { x } _ { i } ^ { T } ) ( sum _ { i } pmb { x } _ { i } pmb { x } _ { i } ^ { T } ) ^ { - 1 } } end{array}$ . Thus we see that the M step is like linear regression where we replace the observed inputs by the expected values of the latent variables. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn summary, here is the entire algorithm: \n[TB99] showed that the only stable fixed point of the EM algorithm is the globally optimal solution. That is, the EM algorithm converges to a solution where $mathbf { W }$ spans the same linear subspace as that defined by the first $L$ eigenvectors. However, if we want $mathbf { W }$ to be orthogonal, and to contain the eigenvectors in descending order of eigenvalue, we have to orthogonalize the resulting matrix (which can be done quite cheaply). Alternatively, we can modify EM to give the principal basis directly [AO03]. \nThis algorithm has a simple physical analogy in the case $D = 2$ and $L = 1$ [Row97]. Consider some points in $mathbb { R } ^ { 2 }$ attached by springs to a rigid rod, whose orientation is defined by a vector $mathbf { boldsymbol { w } }$ . Let $z _ { i }$ be the location where the $i$ ’th spring attaches to the rod. In the E step, we hold the rod fixed, and let the attachment points slide around so as to minimize the spring energy (which is proportional to the sum of squared residuals). In the M step, we hold the attachment points fixed and let the rod rotate so as to minimize the spring energy. See Figure 20.10 for an illustration. \n20.2.3.3 Advantages \nEM for PCA has the following advantages over eigenvector methods: \n• EM can be faster. In particular, assuming $N _ { mathit { D } } , D gg L$ , the dominant cost of EM is the projection operation in the $mathrm { E }$ step, so the overall time is $O ( T L N _ { mathcal { D } } D )$ , where $T$ is the number of iterations. [Row97] showed experimentally that the number of iterations is usually very small (the mean was 3.6), regardless of $N$ or $D$ . (This result depends on the ratio of eigenvalues of the empirical covariance matrix.) This is much faster than the $O ( operatorname* { m i n } ( N D ^ { 2 } , D N ^ { 2 } ) )$ time required by straightforward eigenvector methods, although more sophisticated eigenvector methods, such as the Lanczos algorithm, have running times comparable to EM.   \n• EM can be implemented in an online fashion, i.e., we can update our estimate of $mathbf { W }$ as the data streams in.   \n• EM can handle missing data in a simple way (see e.g., [IR10; DJ15]).   \n• EM can be extended to handle mixtures of PPCA/ FA models (see Section 20.2.6).   \n• EM can be modified to variational EM or to variational Bayes EM to fit more complex models (see e.g., Section 20.2.7). \n20.2.4 Unidentifiability of the parameters \nThe parameters of a FA model are unidentifiable. To see this, consider a model with weights $mathbf { W }$ and observation covariance $Psi$ . We have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "EM algorithm for FA/PPCA"
    },
    {
        "content": "In summary, here is the entire algorithm: \n[TB99] showed that the only stable fixed point of the EM algorithm is the globally optimal solution. That is, the EM algorithm converges to a solution where $mathbf { W }$ spans the same linear subspace as that defined by the first $L$ eigenvectors. However, if we want $mathbf { W }$ to be orthogonal, and to contain the eigenvectors in descending order of eigenvalue, we have to orthogonalize the resulting matrix (which can be done quite cheaply). Alternatively, we can modify EM to give the principal basis directly [AO03]. \nThis algorithm has a simple physical analogy in the case $D = 2$ and $L = 1$ [Row97]. Consider some points in $mathbb { R } ^ { 2 }$ attached by springs to a rigid rod, whose orientation is defined by a vector $mathbf { boldsymbol { w } }$ . Let $z _ { i }$ be the location where the $i$ ’th spring attaches to the rod. In the E step, we hold the rod fixed, and let the attachment points slide around so as to minimize the spring energy (which is proportional to the sum of squared residuals). In the M step, we hold the attachment points fixed and let the rod rotate so as to minimize the spring energy. See Figure 20.10 for an illustration. \n20.2.3.3 Advantages \nEM for PCA has the following advantages over eigenvector methods: \n• EM can be faster. In particular, assuming $N _ { mathit { D } } , D gg L$ , the dominant cost of EM is the projection operation in the $mathrm { E }$ step, so the overall time is $O ( T L N _ { mathcal { D } } D )$ , where $T$ is the number of iterations. [Row97] showed experimentally that the number of iterations is usually very small (the mean was 3.6), regardless of $N$ or $D$ . (This result depends on the ratio of eigenvalues of the empirical covariance matrix.) This is much faster than the $O ( operatorname* { m i n } ( N D ^ { 2 } , D N ^ { 2 } ) )$ time required by straightforward eigenvector methods, although more sophisticated eigenvector methods, such as the Lanczos algorithm, have running times comparable to EM.   \n• EM can be implemented in an online fashion, i.e., we can update our estimate of $mathbf { W }$ as the data streams in.   \n• EM can handle missing data in a simple way (see e.g., [IR10; DJ15]).   \n• EM can be extended to handle mixtures of PPCA/ FA models (see Section 20.2.6).   \n• EM can be modified to variational EM or to variational Bayes EM to fit more complex models (see e.g., Section 20.2.7). \n20.2.4 Unidentifiability of the parameters \nThe parameters of a FA model are unidentifiable. To see this, consider a model with weights $mathbf { W }$ and observation covariance $Psi$ . We have \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nNow consider a different model with weights $tilde { mathbf { W } } = mathbf { W } mathbf { R } mathbf { Lambda }$ , where $mathbf { R }$ is an arbitrary orthogonal rotation matrix, satisfying $mathbf { R R } ^ { mathsf { I } } = mathbf { I }$ . This has the same likelihood, since \nGeometrically, multiplying $mathbf { W }$ by an orthogonal matrix is like rotating $mathscr { z }$ before generating $_ { x }$ ; but since $boldsymbol { z }$ is drawn from an isotropic Gaussian, this makes no difference to the likelihood. Consequently, we cannot uniquely identify $mathbf { W }$ , and therefore cannot uniquely identify the latent factors, either. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTo break this symmetry, several solutions can be used, as we discuss below. \n• Forcing W to have orthonormal columns. Perhaps the simplest solution to the identifiability problem is to force W to have orthonormal columns. This is the approach adopted by PCA. The resulting posterior estimate will then be unique, up to permutation of the latent dimensions. (In PCA, this ordering ambiguity is resolved by sorting the dimensions in order of decreasing eigenvalues of W.)   \n• Forcing W to be lower triangular. One way to resolve permutation unidentifiability, which is popular in the Bayesian community (e.g., [LW04c]), is to ensure that the first visible feature is only generated by the first latent factor, the second visible feature is only generated by the first two latent factors, and so on. For example, if $L = 3$ and $D = 4$ , the corresponding factor loading matrix is given by \nWe also require that $w _ { k k } > 0$ for $k = 1 : L$ . The total number of parameters in this constrained matrix is $D + D L - L ( L - 1 ) / 2$ , which is equal to the number of uniquely identifiable parameters in FA.3 The disadvantage of this method is that the first $L$ visible variables, known as the founder variables, affect the interpretation of the latent factors, and so must be chosen carefully. \n• Sparsity promoting priors on the weights. Instead of pre-specifying which entries in $mathbf { W }$ are zero, we can encourage the entries to be zero, using $ell _ { 1 }$ regularization [ZHT06], ARD [Bis99; AB08], or spike-and-slab priors [Rat+09]. This is called sparse factor analysis. This does not necessarily ensure a unique MAP estimate, but it does encourage interpretable solutions.   \n• Choosing an informative rotation matrix. There are a variety of heuristic methods that try to find rotation matrices $mathbf { R }$ which can be used to modify $mathbf { W }$ (and hence the latent factors) so as to try to increase the interpretability, typically by encouraging them to be (approximately) sparse. One popular method is known as varimax [Kai58].   \n• Use of non-Gaussian priors for the latent factors. If we replace the prior on the latent variables, $p ( z )$ , with a non-Gaussian distribution, we can sometimes uniquely identify $mathbf { W }$ , as well as the latent factors. See e.g., [KKH20] for details. \n20.2.5 Nonlinear factor analysis\nThe FA model assumes the observed data can be modeled as arising from a linear mapping from a low-dimensional set of Gaussian factors. One way to relax this assumption is to let the mapping from $boldsymbol { z }$ to $_ { x }$ be a nonlinear model, such as a neural network. That is, the model becomes",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Unidentifiability of the parameters"
    },
    {
        "content": "To break this symmetry, several solutions can be used, as we discuss below. \n• Forcing W to have orthonormal columns. Perhaps the simplest solution to the identifiability problem is to force W to have orthonormal columns. This is the approach adopted by PCA. The resulting posterior estimate will then be unique, up to permutation of the latent dimensions. (In PCA, this ordering ambiguity is resolved by sorting the dimensions in order of decreasing eigenvalues of W.)   \n• Forcing W to be lower triangular. One way to resolve permutation unidentifiability, which is popular in the Bayesian community (e.g., [LW04c]), is to ensure that the first visible feature is only generated by the first latent factor, the second visible feature is only generated by the first two latent factors, and so on. For example, if $L = 3$ and $D = 4$ , the corresponding factor loading matrix is given by \nWe also require that $w _ { k k } > 0$ for $k = 1 : L$ . The total number of parameters in this constrained matrix is $D + D L - L ( L - 1 ) / 2$ , which is equal to the number of uniquely identifiable parameters in FA.3 The disadvantage of this method is that the first $L$ visible variables, known as the founder variables, affect the interpretation of the latent factors, and so must be chosen carefully. \n• Sparsity promoting priors on the weights. Instead of pre-specifying which entries in $mathbf { W }$ are zero, we can encourage the entries to be zero, using $ell _ { 1 }$ regularization [ZHT06], ARD [Bis99; AB08], or spike-and-slab priors [Rat+09]. This is called sparse factor analysis. This does not necessarily ensure a unique MAP estimate, but it does encourage interpretable solutions.   \n• Choosing an informative rotation matrix. There are a variety of heuristic methods that try to find rotation matrices $mathbf { R }$ which can be used to modify $mathbf { W }$ (and hence the latent factors) so as to try to increase the interpretability, typically by encouraging them to be (approximately) sparse. One popular method is known as varimax [Kai58].   \n• Use of non-Gaussian priors for the latent factors. If we replace the prior on the latent variables, $p ( z )$ , with a non-Gaussian distribution, we can sometimes uniquely identify $mathbf { W }$ , as well as the latent factors. See e.g., [KKH20] for details. \n20.2.5 Nonlinear factor analysis\nThe FA model assumes the observed data can be modeled as arising from a linear mapping from a low-dimensional set of Gaussian factors. One way to relax this assumption is to let the mapping from $boldsymbol { z }$ to $_ { x }$ be a nonlinear model, such as a neural network. That is, the model becomes \nThis is called nonlinear factor analysis. Unfortunately we can no longer compute the posterior or the MLE exactly, so we need to use approximate methods. In Section 20.3.5, we discuss variational autoencoders, which is the most common way to approximate a nonlinear FA model. \n20.2.6 Mixtures of factor analysers \nThe factor analysis model (Section 20.2) assumes the observed data can be modeled as arising from a linear mapping from a low-dimensional set of Gaussian factors. One way to relax this assumption is to assume the model is only locally linear, so the overall model becomes a (weighted) combination of FA models; this is called a mixture of factor analysers. The overall model for the data is a mixture of linear manifolds, which can be used to approximate an overall curved manifold. \nMore precisely, let latent indicator $m _ { n } in { 1 , ldots , K }$ , specifying which subspace (cluster) we should use to generate the data. If $m _ { n } = k$ , we sample $z _ { n }$ from a Gaussian prior and pass it through the $mathbf { W } _ { k }$ matrix and add noise, where $mathbf { W } _ { k }$ maps from the $L$ -dimensional subspace to the $D$ -dimensional visible space.4 More precisely, the model is as follows: \nThis is called a mixture of factor analysers (MFA) [GH96]. The corresponding distribution in the visible space is given by \nIn the special case that $Psi _ { k } = sigma ^ { 2 } mathbf { I }$ , we get a mixture of PPCA models (although it is difficult to ensure orthogonality of the $mathbf { W } _ { k }$ in this case). See Figure 20.12 for an example of the method applied to some 2d data. \nWe can think of this as a low-rank version of a mixture of Gaussians. In particular, this model needs $O ( K L D )$ parameters instead of the $O ( K D ^ { 2 } )$ parameters needed for a mixture of full covariance Gaussians. This can reduce overfitting.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Nonlinear factor analysis"
    },
    {
        "content": "This is called nonlinear factor analysis. Unfortunately we can no longer compute the posterior or the MLE exactly, so we need to use approximate methods. In Section 20.3.5, we discuss variational autoencoders, which is the most common way to approximate a nonlinear FA model. \n20.2.6 Mixtures of factor analysers \nThe factor analysis model (Section 20.2) assumes the observed data can be modeled as arising from a linear mapping from a low-dimensional set of Gaussian factors. One way to relax this assumption is to assume the model is only locally linear, so the overall model becomes a (weighted) combination of FA models; this is called a mixture of factor analysers. The overall model for the data is a mixture of linear manifolds, which can be used to approximate an overall curved manifold. \nMore precisely, let latent indicator $m _ { n } in { 1 , ldots , K }$ , specifying which subspace (cluster) we should use to generate the data. If $m _ { n } = k$ , we sample $z _ { n }$ from a Gaussian prior and pass it through the $mathbf { W } _ { k }$ matrix and add noise, where $mathbf { W } _ { k }$ maps from the $L$ -dimensional subspace to the $D$ -dimensional visible space.4 More precisely, the model is as follows: \nThis is called a mixture of factor analysers (MFA) [GH96]. The corresponding distribution in the visible space is given by \nIn the special case that $Psi _ { k } = sigma ^ { 2 } mathbf { I }$ , we get a mixture of PPCA models (although it is difficult to ensure orthogonality of the $mathbf { W } _ { k }$ in this case). See Figure 20.12 for an example of the method applied to some 2d data. \nWe can think of this as a low-rank version of a mixture of Gaussians. In particular, this model needs $O ( K L D )$ parameters instead of the $O ( K D ^ { 2 } )$ parameters needed for a mixture of full covariance Gaussians. This can reduce overfitting. \n20.2.7 Exponential family factor analysis \nSo far we have assumed the observed data is real-valued, so $pmb { x } _ { n } in mathbb { R } ^ { D }$ . If we want to model other kinds of data (e.g., binary or categorical), we can simply replace the Gaussian output distribution with a suitable member of the exponential family, where the natural parameters are given by a linear function of $z _ { n }$ . That is, we use \nwhere the $N times D$ matrix of natural parameters is assumed to be given by the low rank decomposition $mathbf { Theta } Theta = mathbf { Z } mathbf { W }$ , where $mathbf { Z }$ is $N times L$ and $mathbf { W }$ is $L times D$ . The resulting model is called exponential family factor analysis. \nUnlike the linear-Gaussian FA, we cannot compute the exact posterior $p ( boldsymbol { z } _ { n } | boldsymbol { x } _ { n } , mathbf { W } )$ due to the lack of conjugacy between the expfam likelihood and the Gaussian prior. Furthermore, we cannot compute the exact marginal likelihood either, which prevents us from finding the optimal MLE. \n[CDS02] proposed a coordinate ascent method for a deterministic variant of this model, known as exponential family PCA. This alternates between computing a point estimate of $z _ { n }$ and $mathbf { W }$ . This can be regarded as a degenerate version of variational EM, where the E step uses a delta function posterior for $z _ { n }$ . [GS08] present an improved algorithm that finds the global optimum, and [Ude+16] presents an extension called generalized low rank models, that covers many different kinds of loss function. \nHowever, it is often preferable to use a probabilistic version of the model, rather than computing point estimates of the latent factors. In this case, we must represent the posterior using a nondegenerate distribution to avoid overfitting, since the number of latent variables is proportional to the number of datacases [WCS08]. Fortunately, we can use a non-degenerate posterior, such as a Gaussian, by optimizing the variational lower bound. We give some examples of this below. \n20.2.7.1 Example: binary PCA \nConsider a factored Bernoulli likelihood: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Mixtures of factor analysers"
    },
    {
        "content": "20.2.7 Exponential family factor analysis \nSo far we have assumed the observed data is real-valued, so $pmb { x } _ { n } in mathbb { R } ^ { D }$ . If we want to model other kinds of data (e.g., binary or categorical), we can simply replace the Gaussian output distribution with a suitable member of the exponential family, where the natural parameters are given by a linear function of $z _ { n }$ . That is, we use \nwhere the $N times D$ matrix of natural parameters is assumed to be given by the low rank decomposition $mathbf { Theta } Theta = mathbf { Z } mathbf { W }$ , where $mathbf { Z }$ is $N times L$ and $mathbf { W }$ is $L times D$ . The resulting model is called exponential family factor analysis. \nUnlike the linear-Gaussian FA, we cannot compute the exact posterior $p ( boldsymbol { z } _ { n } | boldsymbol { x } _ { n } , mathbf { W } )$ due to the lack of conjugacy between the expfam likelihood and the Gaussian prior. Furthermore, we cannot compute the exact marginal likelihood either, which prevents us from finding the optimal MLE. \n[CDS02] proposed a coordinate ascent method for a deterministic variant of this model, known as exponential family PCA. This alternates between computing a point estimate of $z _ { n }$ and $mathbf { W }$ . This can be regarded as a degenerate version of variational EM, where the E step uses a delta function posterior for $z _ { n }$ . [GS08] present an improved algorithm that finds the global optimum, and [Ude+16] presents an extension called generalized low rank models, that covers many different kinds of loss function. \nHowever, it is often preferable to use a probabilistic version of the model, rather than computing point estimates of the latent factors. In this case, we must represent the posterior using a nondegenerate distribution to avoid overfitting, since the number of latent variables is proportional to the number of datacases [WCS08]. Fortunately, we can use a non-degenerate posterior, such as a Gaussian, by optimizing the variational lower bound. We give some examples of this below. \n20.2.7.1 Example: binary PCA \nConsider a factored Bernoulli likelihood: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nSuppose we observe $N _ { mathrm { } D } = 1 5 0$ bit vectors of length $D = 6$ . Each example is generated by choosing one of three binary prototype vectors, and then by flipping bits at random. See Figure 20.13(a) for the data. We can fit this using the variational EM algorithm (see [Tip98] for details). We use $L = 2$ latent dimensions to allow us to visualize the latent space. In Figure 20.13(b), we plot $mathbb { E } left[ z _ { n } | boldsymbol { mathbf { mathit { x } } } _ { n } , hat { boldsymbol { mathbf { W } } } right]$ . We see that the projected points group into three distinct clusters, as is to be expected. In Figure 20.13(c), we plot the reconstructed version of the data, which is computed as follows: \nIf we threshold these probabilities at 0.5 (corresponding to a MAP estimate), we get the “denoised” version of the data in Figure 20.13(d). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n20.2.7.2 Example: categorical PCA \nWe can generalize the model in Section 20.2.7.1 to handle categorical data by using the following likelihood: \nWe call this categorical PCA (CatPCA). A variational EM algorithm for fitting this is described in [Kha+10]. \n20.2.8 Factor analysis models for paired data \nIn this section, we discuss linear-Gaussian factor analysis models when we have two kinds of observed variables, $pmb { x } in mathbb { R } ^ { D _ { x } }$ and $boldsymbol { y } in mathbb { R } ^ { D _ { y } }$ , which are paired. These often correspond to different sensors or modalities (e.g., images and sound). We follow the presentation of [Vir10]. \n20.2.8.1 Supervised PCA \nIn supervised PCA [Yu+06], we model the joint $p ( { pmb x } , { pmb y } )$ using a shared low-dimensional representation using the following linear Gaussian model: \nThis is illustrated as a graphical model in Figure 20.14a. The intuition is that $z _ { n }$ is a shared latent subspace, that captures features that ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and ${ bf { y } } _ { n }$ have in common. The variance terms $sigma _ { x }$ and $sigma _ { y }$ control how much emphasis the model puts on the two different signals. If we put a prior on the parameters $pmb { theta } = ( mathbf { W } _ { x } , mathbf { W } _ { y } , sigma _ { x } , sigma _ { y } )$ , we recover the Bayesian factor regression model of [Wes03]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Exponential family factor analysis"
    },
    {
        "content": "20.2.7.2 Example: categorical PCA \nWe can generalize the model in Section 20.2.7.1 to handle categorical data by using the following likelihood: \nWe call this categorical PCA (CatPCA). A variational EM algorithm for fitting this is described in [Kha+10]. \n20.2.8 Factor analysis models for paired data \nIn this section, we discuss linear-Gaussian factor analysis models when we have two kinds of observed variables, $pmb { x } in mathbb { R } ^ { D _ { x } }$ and $boldsymbol { y } in mathbb { R } ^ { D _ { y } }$ , which are paired. These often correspond to different sensors or modalities (e.g., images and sound). We follow the presentation of [Vir10]. \n20.2.8.1 Supervised PCA \nIn supervised PCA [Yu+06], we model the joint $p ( { pmb x } , { pmb y } )$ using a shared low-dimensional representation using the following linear Gaussian model: \nThis is illustrated as a graphical model in Figure 20.14a. The intuition is that $z _ { n }$ is a shared latent subspace, that captures features that ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and ${ bf { y } } _ { n }$ have in common. The variance terms $sigma _ { x }$ and $sigma _ { y }$ control how much emphasis the model puts on the two different signals. If we put a prior on the parameters $pmb { theta } = ( mathbf { W } _ { x } , mathbf { W } _ { y } , sigma _ { x } , sigma _ { y } )$ , we recover the Bayesian factor regression model of [Wes03]. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can marginalize out $z _ { n }$ to get $p ( pmb { y } _ { n } | pmb { x } _ { n } )$ . If ${ bf { y } } _ { n }$ is a scalar, this becomes \nTo apply this to the classification setting, we can use supervised ePCA [Guo09], in which we replace the Gaussian $p ( pmb { y } | pmb { z } )$ with a logistic regression model. \nThis model is completely symmetric in $_ { x }$ and $pmb { y }$ . If our goal is to predict $mathbf { Delta } _ { mathbf { mathcal { Y } } }$ from $_ { x }$ via the latent bottleneck $boldsymbol { z }$ , then we might want to upweight the likelihood term for $pmb { y }$ , as proposed in [Ris+08]. This gives \nwhere $alpha leq 1$ controls the relative importance of modeling the two sources. The value of $alpha$ can be chosen by cross-validation. \n20.2.8.2 Partial least squares \nAnother way to improve the predictive performance in supervised tasks is to allow the inputs $_ { x }$ to have their own “private” noise source that is independent on the target variable, since not all variation in $_ { x }$ is relevant for predictive purposes. We can do this by introducing an extra latent variable $z _ { n } ^ { x }$ just for the inputs, that is different from $z _ { n } ^ { s }$ that is the shared bottleneck between ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ and ${ pmb y } _ { n }$ . In the Gaussian case, the overall model has the form \nSee Figure 20.14b. MLE for $pmb theta$ in this model is equivalent to the technique of partial least squares (PLS) [Gus01; Nou+02; Sun+09]. \n20.2.8.3 Canonical correlation analysis \nIn some cases, we want to use a fully symmetric model, so we can capture the dependence between $_ { x }$ and $mathbf { nabla } _ { mathbf { boldsymbol { y } } }$ , while allowing for domain-specific or “private” noise sources We can do this by introducing a latent variable $z _ { n } ^ { x }$ just for ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ , a latent variable $z _ { n } ^ { y }$ just for ${ bf { y } } _ { n }$ , and a shared latent variable $z _ { n } ^ { s }$ . In the Gaussian case, the overall model has the form \nwhere $mathbf { W } _ { x }$ and $mathbf { W } _ { y }$ are $L ^ { s } times D$ dimensional, $mathbf { V } _ { x }$ is $L ^ { x } times D$ dimensional, and $mathbf { V } _ { y }$ is $L ^ { y } times D$ dimensional.   \nSee Figure 20.15 for the PGM. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIf we marginalize out all the latent variables, we get the following distribution on the visibles (where we assume $sigma _ { x } = sigma _ { y } = sigma$ ): \nwhere $pmb { mu } = ( mu _ { x } ; pmb { mu } _ { y } )$ , and $mathbf { W } = | mathbf { W } _ { x } ; mathbf { W } _ { y } |$ . Thus the induced covariance is the following low rank matrix: \n[BJ05] showed that MLE for this model is equivalent to a classical statistical method known as canonical correlation analysis or CCA [Hot36]. However, the PGM perspective allows us to easily generalize to multiple kinds of observations (this is known as generalized CCA [Hor61]) or to nonlinear models (this is known as deep CCA [WLL16; SNM16]), or exponential family CCA [KVK10]. See [Uur+17] for further discussion of CCA and its extensions. \n20.3 Autoencoders \nWe can think of PCA (Section 20.1) and factor analysis (Section 20.2) as learning a (linear) mapping from $x  z$ , called the encoder, $f _ { e }$ , and learning another (linear) mapping $z  x$ , called the decoder, $f _ { d }$ . The overall reconstruction function has the form $r ( { pmb x } ) = f _ { d } ( f _ { e } ( { pmb x } ) )$ . The model is trained to minimize $mathcal { L } ( pmb { theta } ) = | | boldsymbol { r } ( pmb { x } ) - pmb { x } | | _ { 2 } ^ { 2 }$ . More generally, we can use $begin{array} { r } { mathcal { L } ( pmb { theta } ) = - log p ( pmb { x } | r ( pmb { x } ) ) } end{array}$ . \nIn this section, we consider the case where the encoder and decoder are nonlinear mappings implemented by neural networks. This is called an autoencoder. If we use an MLP with one hidden layer, we get the model shown Figure 20.16. We can think of the hidden units in the middle as a low-dimensional bottleneck between the input and its reconstruction. \nOf course, if the hidden layer is wide enough, there is nothing to stop this model from learning the identity function. To prevent this degenerate solution, we have to restrict the model in some way. The simplest approach is to use a narrow bottleneck layer, with $L ll D$ ; this is called an undercomplete representation. The other approach is to use $L gg D$ , known as an overcomplete representation, but to impose some other kind of regularization, such as adding noise to the inputs, forcing the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license activations of the hidden units to be sparse, or imposing a penalty on the derivatives of the hidden units. We discuss these options in more detail below.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Factor analysis *",
        "subsubsection": "Factor analysis models for paired data"
    },
    {
        "content": "20.3.1 Bottleneck autoencoders \nWe start by considering the special case of a linear autoencoder, in which there is one hidden layer, the hidden units are computed using $boldsymbol { z } = mathbf { W } _ { 1 } boldsymbol { x }$ , and the output is reconstructed using $hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } } hat { mathbf { Delta } }$ , where $mathbf { W } _ { 1 }$ is a $L times D$ matrix, $mathbf { W } _ { 2 }$ is a $D times L$ matrix, and $L < D$ . Hence $hat { pmb { x } } = mathbf { W } _ { 2 } mathbf { W } _ { 1 } pmb { x } = mathbf { W } pmb { x }$ is the output of the model. If we train this model to minimize the squared reconstruction error, $begin{array} { r } { mathcal { L } ( mathbf { W } ) = sum _ { n = 1 } ^ { N } | | pmb { x } _ { n } - mathbf { W } pmb { x } _ { n } | | _ { 2 } ^ { 2 } } end{array}$ , one can show [BH89; KJ95] that $hat { mathbf { W } }$ is an orthogonal projection onto the first $L$ eigenvectors of the empirical covariance matrix of the data. This is therefore equivalent to PCA. \nIf we introduce nonlinearities into the autoencoder, we get a model that is strictly more powerful than PCA, as proved in [JHG00]. Such methods can learn very useful low dimensional representations of data. \nConsider fitting an autoencoder to the Fashion MNIST dataset. We consider both an MLP architecture (with 2 layers and a bottleneck of size 30), and a CNN based architecture (with 3 layers and a 3d bottleneck with 64 channels). We use a Bernoulli likelihood model and binary cross entropy as the loss. Figure 20.17 shows some test images and their reconstructions. We see that the CNN model reconstructs the images more accurately than the MLP model. However, both models are small, and were only trained for 5 epochs; results can be improved by using larger models, and training for longer. \nFigure 20.18 visualizes the first 2 (of 30) latent dimensions produced by the MLP-AE. More precisely, we plot the tSNE embeddings (see Section 20.4.10), color coded by class label. We also show some corresponding images from the dataset, from which the embeddings were derived. We see that the method has done a good job of separating the classes in a fully unsupervised way. We also see that the latent space of the MLP and CNN models is very similar (at least when viewed through this 2d projection). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n20.3.2 Denoising autoencoders \nOne useful way to control the capacity of an autoencoder is to add noise to its input, and then train the model to reconstruct a clean (uncorrupted) version of the original input. This is called a denoising autoencoder [Vin+10a]. \nWe can implement this by adding Gaussian noise, or using Bernoulli dropout. Figure 20.19 shows some reconstructions of corrupted images computed using a DAE. We see that the model is able to “hallucinate” details that are missing in the input, since it has seen similar images before, and can store this information in the parameters of the model. \nSuppose we train a DAE using Gaussian corruption and squared error reconstruction, i.e., we use $p _ { c } ( tilde { mathbf { boldsymbol { x } } } | mathbf { boldsymbol { x } } ) = mathcal { N } ( tilde { mathbf { boldsymbol { x } } } | mathbf { boldsymbol { x } } , sigma ^ { 2 } mathbf { I } )$ and $ell ( pmb { x } , r ( tilde { pmb { x } } ) ) = | | pmb { e } | | _ { 2 } ^ { 2 }$ , where $e ( { pmb x } ) = r ( { tilde { pmb x } } ) - { pmb x }$ is the residual error for example \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license $_ { x }$ . Then one can show [AB14] the remarkable result that, as $sigma  0$ (and with a sufficiently powerful model and enough data), the residuals approximate the score function, which is the log probability of the data, i.e., $e ( pmb { x } ) approx nabla _ { pmb { x } } log p ( pmb { x } )$ . That is, the DAE learns a vector field, corresponding to the gradient of the log data density. Thus points that are close to the data manifold will be projected onto it via the sampling process. See Figure 20.20 for an illustration.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Autoencoders",
        "subsubsection": "Bottleneck autoencoders"
    },
    {
        "content": "20.3.2 Denoising autoencoders \nOne useful way to control the capacity of an autoencoder is to add noise to its input, and then train the model to reconstruct a clean (uncorrupted) version of the original input. This is called a denoising autoencoder [Vin+10a]. \nWe can implement this by adding Gaussian noise, or using Bernoulli dropout. Figure 20.19 shows some reconstructions of corrupted images computed using a DAE. We see that the model is able to “hallucinate” details that are missing in the input, since it has seen similar images before, and can store this information in the parameters of the model. \nSuppose we train a DAE using Gaussian corruption and squared error reconstruction, i.e., we use $p _ { c } ( tilde { mathbf { boldsymbol { x } } } | mathbf { boldsymbol { x } } ) = mathcal { N } ( tilde { mathbf { boldsymbol { x } } } | mathbf { boldsymbol { x } } , sigma ^ { 2 } mathbf { I } )$ and $ell ( pmb { x } , r ( tilde { pmb { x } } ) ) = | | pmb { e } | | _ { 2 } ^ { 2 }$ , where $e ( { pmb x } ) = r ( { tilde { pmb x } } ) - { pmb x }$ is the residual error for example \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license $_ { x }$ . Then one can show [AB14] the remarkable result that, as $sigma  0$ (and with a sufficiently powerful model and enough data), the residuals approximate the score function, which is the log probability of the data, i.e., $e ( pmb { x } ) approx nabla _ { pmb { x } } log p ( pmb { x } )$ . That is, the DAE learns a vector field, corresponding to the gradient of the log data density. Thus points that are close to the data manifold will be projected onto it via the sampling process. See Figure 20.20 for an illustration. \n\n20.3.3 Contractive autoencoders \nA different way to regularize autoencoders is by adding the penalty term \nto the reconstruction loss, where $h _ { k }$ is the value of the $k$ ’th hidden embedding unit. That is, we penalize the Frobenius norm of the encoder’s Jacobian. This is called a contractive autoencoder \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 [Rif+11]. (A linear operator with Jacobian $mathbf { J }$ is called a contraction if $| | mathbf { J } pmb { x } | | leq 1$ for all unit-norm inputs $_ { x }$ .)",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Autoencoders",
        "subsubsection": "Denoising autoencoders"
    },
    {
        "content": "20.3.3 Contractive autoencoders \nA different way to regularize autoencoders is by adding the penalty term \nto the reconstruction loss, where $h _ { k }$ is the value of the $k$ ’th hidden embedding unit. That is, we penalize the Frobenius norm of the encoder’s Jacobian. This is called a contractive autoencoder \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 [Rif+11]. (A linear operator with Jacobian $mathbf { J }$ is called a contraction if $| | mathbf { J } pmb { x } | | leq 1$ for all unit-norm inputs $_ { x }$ .) \n\nTo understand why this is useful, consider Figure 20.20. We can approximate the curved lowdimensional manifold by a series of locally linear manifolds. These linear approximations can be computed using the Jacobian of the encoder at each point. By encouraging these to be contractive, we ensure the model “pushes” inputs that are off the manifold to move back towards it. \nAnother way to think about CAEs is as follows. To minimize the penalty term, the model would like to ensure the encoder is a constant function. However, if it was completely constant, it would ignore its input, and hence incur high reconstruction cost. Thus the two terms together encourage the model to learn a representation where only a few units change in response to the most significant variations in the input. \nOne possible degenerate solution is that the encoder simply learns to multiply the input by a small constant $epsilon$ (which scales down the Jacobian), followed by a decoder that divides by $epsilon$ (which reconstructs perfectly). To avoid this, we can tie the weights of the encoder and decoder, by setting the weight matrix for layer $ell$ of $f _ { d }$ to be the transpose of the weight matrix for layer $ell$ of $f _ { e }$ , but using untied bias terms. Unfortunately CAEs are slow to train, because of the expense of computing the Jacobian. \n20.3.4 Sparse autoencoders \nYet another way to regularize autoencoders is to add a sparsity penalty to the latent activations of the form $Omega ( z ) = lambda | | z | | _ { 1 }$ . (This is called activity regularization.) \nAn alternative way to implement sparsity, that often gives better results, is to use logistic units, and then to compute the expected fraction of time each unit $k$ is on within a minibatch (call this $q _ { k }$ ), and ensure that this is close to a desired target value $p$ , as proposed in [GBB11]. In particular, we use the regularizer $begin{array} { r } { Omega ( z _ { 1 : L , 1 : N } ) = lambda sum _ { k } D _ { mathbb { K L } } ( pmb { p } |  pmb { q } _ { k } ) } end{array}$ for latent dimensions $1 : L$ and examples $1 : N$ , where $pmb { p } = ( p , 1 - p )$ is the desired target distribution, and $pmb q _ { k } = ( q _ { k } , 1 - q _ { k } )$ is the empirical distribution for unit $k$ , computed using $begin{array} { r } { q _ { k } = frac { 1 } { N } sum _ { n = 1 } ^ { N } mathbb { I } left( z _ { n , k } = 1 right) } end{array}$ ). \nFigure 20.21 shows the results when fitting an AE-MLP (with 300 hidden units) to Fashion MNIST. If we set $lambda = 0$ (i.e., if we don’t impose a sparsity penalty), we see that the average activation value is about 0.4, with most neurons being partially activated most of the time. With the $ell _ { 1 }$ penalty, we see that most units are off all the time, which means they are not being used at all. With the KL penalty, we see that about $7 0 %$ of neurons are off on average, but unlike the $ell _ { 1 }$ case, we don’t see units being permanently turned off (the average activation level is 0.1). This latter kind of sparse firing pattern is similar to that observed in biological brains (see e.g., [Bey+19]). \n20.3.5 Variational autoencoders \nIn this section, we discuss the variational autoencoder or VAE [KW14; RMW14; KW19a], which can be thought of as a probabilistic version of a deterministic autoencoder (Section 20.3) The principal advantage is that a VAE is a generative model that can create new samples, whereas an autoencoder just computes embeddings of input vectors. \nWe discuss VAEs in detail in the sequel to this book, [Mur23]. However, in brief, the VAE combines two key ideas. First we create a non-linear extension of the factor analysis generative model, i.e., we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license replace $p ( pmb { x } | z ) = mathcal { N } ( pmb { x } | mathbf { W } z , sigma ^ { 2 } mathbf { I } )$ with",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Autoencoders",
        "subsubsection": "Contractive autoencoders"
    },
    {
        "content": "To understand why this is useful, consider Figure 20.20. We can approximate the curved lowdimensional manifold by a series of locally linear manifolds. These linear approximations can be computed using the Jacobian of the encoder at each point. By encouraging these to be contractive, we ensure the model “pushes” inputs that are off the manifold to move back towards it. \nAnother way to think about CAEs is as follows. To minimize the penalty term, the model would like to ensure the encoder is a constant function. However, if it was completely constant, it would ignore its input, and hence incur high reconstruction cost. Thus the two terms together encourage the model to learn a representation where only a few units change in response to the most significant variations in the input. \nOne possible degenerate solution is that the encoder simply learns to multiply the input by a small constant $epsilon$ (which scales down the Jacobian), followed by a decoder that divides by $epsilon$ (which reconstructs perfectly). To avoid this, we can tie the weights of the encoder and decoder, by setting the weight matrix for layer $ell$ of $f _ { d }$ to be the transpose of the weight matrix for layer $ell$ of $f _ { e }$ , but using untied bias terms. Unfortunately CAEs are slow to train, because of the expense of computing the Jacobian. \n20.3.4 Sparse autoencoders \nYet another way to regularize autoencoders is to add a sparsity penalty to the latent activations of the form $Omega ( z ) = lambda | | z | | _ { 1 }$ . (This is called activity regularization.) \nAn alternative way to implement sparsity, that often gives better results, is to use logistic units, and then to compute the expected fraction of time each unit $k$ is on within a minibatch (call this $q _ { k }$ ), and ensure that this is close to a desired target value $p$ , as proposed in [GBB11]. In particular, we use the regularizer $begin{array} { r } { Omega ( z _ { 1 : L , 1 : N } ) = lambda sum _ { k } D _ { mathbb { K L } } ( pmb { p } |  pmb { q } _ { k } ) } end{array}$ for latent dimensions $1 : L$ and examples $1 : N$ , where $pmb { p } = ( p , 1 - p )$ is the desired target distribution, and $pmb q _ { k } = ( q _ { k } , 1 - q _ { k } )$ is the empirical distribution for unit $k$ , computed using $begin{array} { r } { q _ { k } = frac { 1 } { N } sum _ { n = 1 } ^ { N } mathbb { I } left( z _ { n , k } = 1 right) } end{array}$ ). \nFigure 20.21 shows the results when fitting an AE-MLP (with 300 hidden units) to Fashion MNIST. If we set $lambda = 0$ (i.e., if we don’t impose a sparsity penalty), we see that the average activation value is about 0.4, with most neurons being partially activated most of the time. With the $ell _ { 1 }$ penalty, we see that most units are off all the time, which means they are not being used at all. With the KL penalty, we see that about $7 0 %$ of neurons are off on average, but unlike the $ell _ { 1 }$ case, we don’t see units being permanently turned off (the average activation level is 0.1). This latter kind of sparse firing pattern is similar to that observed in biological brains (see e.g., [Bey+19]). \n20.3.5 Variational autoencoders \nIn this section, we discuss the variational autoencoder or VAE [KW14; RMW14; KW19a], which can be thought of as a probabilistic version of a deterministic autoencoder (Section 20.3) The principal advantage is that a VAE is a generative model that can create new samples, whereas an autoencoder just computes embeddings of input vectors. \nWe discuss VAEs in detail in the sequel to this book, [Mur23]. However, in brief, the VAE combines two key ideas. First we create a non-linear extension of the factor analysis generative model, i.e., we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license replace $p ( pmb { x } | z ) = mathcal { N } ( pmb { x } | mathbf { W } z , sigma ^ { 2 } mathbf { I } )$ with",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Autoencoders",
        "subsubsection": "Sparse autoencoders"
    },
    {
        "content": "To understand why this is useful, consider Figure 20.20. We can approximate the curved lowdimensional manifold by a series of locally linear manifolds. These linear approximations can be computed using the Jacobian of the encoder at each point. By encouraging these to be contractive, we ensure the model “pushes” inputs that are off the manifold to move back towards it. \nAnother way to think about CAEs is as follows. To minimize the penalty term, the model would like to ensure the encoder is a constant function. However, if it was completely constant, it would ignore its input, and hence incur high reconstruction cost. Thus the two terms together encourage the model to learn a representation where only a few units change in response to the most significant variations in the input. \nOne possible degenerate solution is that the encoder simply learns to multiply the input by a small constant $epsilon$ (which scales down the Jacobian), followed by a decoder that divides by $epsilon$ (which reconstructs perfectly). To avoid this, we can tie the weights of the encoder and decoder, by setting the weight matrix for layer $ell$ of $f _ { d }$ to be the transpose of the weight matrix for layer $ell$ of $f _ { e }$ , but using untied bias terms. Unfortunately CAEs are slow to train, because of the expense of computing the Jacobian. \n20.3.4 Sparse autoencoders \nYet another way to regularize autoencoders is to add a sparsity penalty to the latent activations of the form $Omega ( z ) = lambda | | z | | _ { 1 }$ . (This is called activity regularization.) \nAn alternative way to implement sparsity, that often gives better results, is to use logistic units, and then to compute the expected fraction of time each unit $k$ is on within a minibatch (call this $q _ { k }$ ), and ensure that this is close to a desired target value $p$ , as proposed in [GBB11]. In particular, we use the regularizer $begin{array} { r } { Omega ( z _ { 1 : L , 1 : N } ) = lambda sum _ { k } D _ { mathbb { K L } } ( pmb { p } |  pmb { q } _ { k } ) } end{array}$ for latent dimensions $1 : L$ and examples $1 : N$ , where $pmb { p } = ( p , 1 - p )$ is the desired target distribution, and $pmb q _ { k } = ( q _ { k } , 1 - q _ { k } )$ is the empirical distribution for unit $k$ , computed using $begin{array} { r } { q _ { k } = frac { 1 } { N } sum _ { n = 1 } ^ { N } mathbb { I } left( z _ { n , k } = 1 right) } end{array}$ ). \nFigure 20.21 shows the results when fitting an AE-MLP (with 300 hidden units) to Fashion MNIST. If we set $lambda = 0$ (i.e., if we don’t impose a sparsity penalty), we see that the average activation value is about 0.4, with most neurons being partially activated most of the time. With the $ell _ { 1 }$ penalty, we see that most units are off all the time, which means they are not being used at all. With the KL penalty, we see that about $7 0 %$ of neurons are off on average, but unlike the $ell _ { 1 }$ case, we don’t see units being permanently turned off (the average activation level is 0.1). This latter kind of sparse firing pattern is similar to that observed in biological brains (see e.g., [Bey+19]). \n20.3.5 Variational autoencoders \nIn this section, we discuss the variational autoencoder or VAE [KW14; RMW14; KW19a], which can be thought of as a probabilistic version of a deterministic autoencoder (Section 20.3) The principal advantage is that a VAE is a generative model that can create new samples, whereas an autoencoder just computes embeddings of input vectors. \nWe discuss VAEs in detail in the sequel to this book, [Mur23]. However, in brief, the VAE combines two key ideas. First we create a non-linear extension of the factor analysis generative model, i.e., we \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license replace $p ( pmb { x } | z ) = mathcal { N } ( pmb { x } | mathbf { W } z , sigma ^ { 2 } mathbf { I } )$ with \n\nwhere $f _ { d }$ is the decoder. For binary observations we should use a Bernoulli likelihood: \nSecond, we create another model, $q ( boldsymbol { z } | boldsymbol { x } )$ , called the recognition network or inference network, that is trained simultaneously with the generative model to do approximate posterior inference. If we assume the posterior is Gaussian, with diagonal covariance, we get \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $f _ { e }$ is the encoder. See Figure 20.22 for a sketch. \n\nThe idea of training an inference network to “invert” a generative network, rather than running an optimization algorithm to infer the latent code, is called amortized inference. This idea was first proposed in the Helmholtz machine [Day+95]. However, that paper did not present a single unified objective function for inference and generation, but instead used the wake sleep method for training, which alternates between optimizing the generative model and inference model. By contrast, the VAE optimizes a variational lower bound on the log-likelihood, which is more principled, since it is a single unified objective. \n20.3.5.1 Training VAEs \nWe cannot compute the exact marginal likelihood $p ( { pmb x } | { pmb theta } )$ needed for MLE training, because posterior inference in a nonlinear FA model is intractable. However, we can use the inference network to compute an approximate posterior, $q ( boldsymbol { z } | boldsymbol { x } )$ . We can then use this to compute the evidence lower bound or ELBO. For a single example $_ { x }$ , this is given by \nThis can be interpreted as the expected log likelihood, plus a regularizer, that penalizes the posterior from deviating too much from the prior. (This is different than the approach in Section 20.3.4, where we applied the KL penalty to the aggregate posterior in each minibatch.) \nThe ELBO is a lower bound of the log marginal likelihood (aka evidence), as can be seen from Jensen’s inequality: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThus for fixed inference network parameters $phi$ , increasing the ELBO should increase the log likelihood of the data, similar to EM Section 8.7.2. \n20.3.5.2 The reparameterization trick \nIn this section, we discuss how to compute the ELBO and its gradient. For simplicity, let us suppose that the inference network estimates the parameters of a Gaussian posterior. Since $q _ { phi } ( z | pmb { x } )$ is Gaussian, we can write \nwhere $mathbf { epsilon } gets mathcal { N } ( mathbf { 0 } , mathbf { I } )$ . Hence \nNow the expectation is independent of the parameters of the model, so we can safely push gradients inside and use backpropagation for training in the usual way, by minimizing $- mathbb { E } _ { { pmb x } sim mathcal { D } } left[ mathbb { E } ( { pmb theta } , { pmb phi } | { pmb x } ) right]$ wrt $pmb theta$ and $phi$ . This is known as the reparameterization trick. See Figure 20.23 for an illustration. \nThe first term in the ELBO can be approximated by sampling $epsilon$ , scaling it by the output of the inference network to get $_ { z }$ , and then evaluating $log p ( { pmb x } | z )$ using the decoder network. \nThe second term in the ELBO is the KL of two Gaussians, which has a closed form solution. In particular, inserting $p ( z ) = mathcal { N } ( z | mathbf { 0 } , mathbf { I } )$ and $q ( z ) = mathcal { N } ( z | mu , mathrm { d i a g } ( pmb { sigma } ) )$ into Equation (6.33), we get \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n0 0   \n20 5041q213 20 504/q213   \n40 14353617 40 4353617   \n60 60 0 50 100 150 200 250 0 50 100 150 200 0 0   \n20 50419213 20 30488413   \n40 60 19353617 40 60 （363457 0 50 100 150 200 250 0 50 100 150 200 250 (a) (b) \n20.3.5.3 Comparison of VAEs and autoencoders \nVAEs are very similar to autoencoders. In particular, the generative model, $p _ { pmb { theta } } ( pmb { x } | pmb { z } )$ , acts like the decoder, and the inference network, $q _ { phi } ( z | pmb { x } )$ , acts like the encoder. The reconstruction abilities of both models are similar, as can be seen by comparing Figure 20.24a with Figure 20.24b. \nThe primary advantage of the VAE is that it can be used to generate new data from random noise. In particular, we sample $mathscr { z }$ from the Gaussian prior $mathcal { N } ( z | mathbf { 0 } , mathbf { I } )$ , and then pass this through the decoder to get $mathbb { E } left[ { pmb { x } } | { pmb z } right] = f _ { d } ( { pmb z } ; { pmb theta } )$ . The VAE’s decoder is trained to convert random points in the embedding space (generated by perturbing the input encodings) to sensible outputs. By contrast, the decoder for the deterministic autoencoder only ever gets as inputs the exact encodings of the training set, so it does not know what to do with random inputs that are outside what it was trained on. So a standard autoencoder cannot create new samples. This difference can be seen by comparing Figure 20.25a with Figure 20.25b. \nThe reason the VAE is better at sample is that it it embeds images into Gaussians in latent space, whereas the AE embeds images into points, which are like delta functions. The advantage of using a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n20 22883355 20 43333333 0 50 100 150 200 250 0 50 100 150 200 250 (a) (b) \nlatent distribution is that it encourages local smoothness, since a given image may map to multiple nearby places, depending on the stochastic sampling. By contrast, in an AE, the latent space is typically not smooth, so images from different classes often end up next to each other. This difference can be seen by comparing Figure 20.26a with Figure 20.26b. \nWe can leverage the smoothness of the latent space to perform image interpolation. Rather than working in pixel space, we can work in the latent space of the model. Specifically, let $pmb { x } _ { 1 }$ and $mathbf { delta x } _ { 2 }$ be two images, and let $z _ { 1 } = mathbb { E } _ { q ( z | mathbf { x } _ { 1 } ) } left[ z right]$ and $z _ { 2 } = mathbb { E } _ { q ( z | mathbf { x } _ { 2 } ) } left[ z right]$ be their encodings. We can now generate new images that interpolate between these two anchors by computing $z = lambda z _ { 1 } + ( 1 - lambda ) z _ { 2 }$ , where $0 leq lambda leq 1$ , and then decoding by computing $mathbb { E } left[ pmb { x } | pmb { z } right]$ . This is called latent space interpolation. (The justification for taking a linear interpolation is that the learned manifold has approximately zero curvature, as shown in [SKTF18].) A VAE is more useful for latent space interpolation than an AE because its latent space is smoother, and because the model can generate from almost any point in latent space. This difference can be seen by comparing Figure 20.27a with Figure 20.27b. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n20.4 Manifold learning * \nIn this section, we discuss the problem of recovering the underlying low-dimensional structure in a high-dimensional dataset. This structure is often assumed to be a curved manifold (explained in Section 20.4.1), so this problem is called manifold learning or nonlinear dimensionality reduction. The key difference from methods such as autoencoders (Section 20.3) is that we will focus on non-parametric methods, in which we compute an embedding for each point in the training set, as opposed to learning a generic model that can embed any input vector. That is, the methods we discuss do not (easily) support out-of-sample generalization. However, they can be easier to fit, and are quite flexible. Such methods can be a useful for unsupervised learning (knowledge discovery), data visualization, and as a preprocessing step for supervised learning. See [AAB21] for a recent review of this field. \n20.4.1 What are manifolds? \nRoughly speaking, a manifold is a topological space which is locally Euclidean. One of the simplest examples is the surface of the earth, which is a curved 2d surface embedded in a 3d space. At each local point on the surface, the earth seems flat. \nMore formally, a $d$ -dimensional manifold $mathcal { X }$ is a space in which each point $x in mathcal { X }$ has a neighborhood which is topologically equivalent to a $d$ -dimensional Euclidean space, called the tangent space, denoted $mathcal { T } _ { x } = T _ { x } mathcal { X }$ . This is illustrated in Figure 20.28. \nA Riemannian manifold is a differentiable manifold that associates an inner product operator at each point $x$ in tangent space; this is assumed to depend smoothly on the position $x$ . The inner product induces a notion of distance, angles, and volume. The collection of these inner products is called a Riemannian metric. It can be shown that any sufficiently smooth Riemannian manifold can be embedded into a Euclidean space of potentially higher dimension; the Riemannian inner product at a point then becomes Euclidean inner product in that tangent space. \n20.4.2 The manifold hypothesis \nMost “naturally occuring” high dimensional dataset lie a low dimensional manifold. This is called the manifold hypothesis [FMN16]. For example, consider the case of an image. Figure 20.29a shows a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license single image of size $6 4 times 5 7$ . This is a vector in a 3,648-dimensional space, where each dimension corresponds to a pixel intensity. Suppose we try to generate an image by drawing a random point in this space; it is unlikely to look like the image of a digit, as shown in Figure 20.29b. However, the pixels are not independent of each other, since they are generated by some lower dimensional structure, namely the shape of the digit 6.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Autoencoders",
        "subsubsection": "Variational autoencoders"
    },
    {
        "content": "20.4 Manifold learning * \nIn this section, we discuss the problem of recovering the underlying low-dimensional structure in a high-dimensional dataset. This structure is often assumed to be a curved manifold (explained in Section 20.4.1), so this problem is called manifold learning or nonlinear dimensionality reduction. The key difference from methods such as autoencoders (Section 20.3) is that we will focus on non-parametric methods, in which we compute an embedding for each point in the training set, as opposed to learning a generic model that can embed any input vector. That is, the methods we discuss do not (easily) support out-of-sample generalization. However, they can be easier to fit, and are quite flexible. Such methods can be a useful for unsupervised learning (knowledge discovery), data visualization, and as a preprocessing step for supervised learning. See [AAB21] for a recent review of this field. \n20.4.1 What are manifolds? \nRoughly speaking, a manifold is a topological space which is locally Euclidean. One of the simplest examples is the surface of the earth, which is a curved 2d surface embedded in a 3d space. At each local point on the surface, the earth seems flat. \nMore formally, a $d$ -dimensional manifold $mathcal { X }$ is a space in which each point $x in mathcal { X }$ has a neighborhood which is topologically equivalent to a $d$ -dimensional Euclidean space, called the tangent space, denoted $mathcal { T } _ { x } = T _ { x } mathcal { X }$ . This is illustrated in Figure 20.28. \nA Riemannian manifold is a differentiable manifold that associates an inner product operator at each point $x$ in tangent space; this is assumed to depend smoothly on the position $x$ . The inner product induces a notion of distance, angles, and volume. The collection of these inner products is called a Riemannian metric. It can be shown that any sufficiently smooth Riemannian manifold can be embedded into a Euclidean space of potentially higher dimension; the Riemannian inner product at a point then becomes Euclidean inner product in that tangent space. \n20.4.2 The manifold hypothesis \nMost “naturally occuring” high dimensional dataset lie a low dimensional manifold. This is called the manifold hypothesis [FMN16]. For example, consider the case of an image. Figure 20.29a shows a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license single image of size $6 4 times 5 7$ . This is a vector in a 3,648-dimensional space, where each dimension corresponds to a pixel intensity. Suppose we try to generate an image by drawing a random point in this space; it is unlikely to look like the image of a digit, as shown in Figure 20.29b. However, the pixels are not independent of each other, since they are generated by some lower dimensional structure, namely the shape of the digit 6.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "What are manifolds?"
    },
    {
        "content": "20.4 Manifold learning * \nIn this section, we discuss the problem of recovering the underlying low-dimensional structure in a high-dimensional dataset. This structure is often assumed to be a curved manifold (explained in Section 20.4.1), so this problem is called manifold learning or nonlinear dimensionality reduction. The key difference from methods such as autoencoders (Section 20.3) is that we will focus on non-parametric methods, in which we compute an embedding for each point in the training set, as opposed to learning a generic model that can embed any input vector. That is, the methods we discuss do not (easily) support out-of-sample generalization. However, they can be easier to fit, and are quite flexible. Such methods can be a useful for unsupervised learning (knowledge discovery), data visualization, and as a preprocessing step for supervised learning. See [AAB21] for a recent review of this field. \n20.4.1 What are manifolds? \nRoughly speaking, a manifold is a topological space which is locally Euclidean. One of the simplest examples is the surface of the earth, which is a curved 2d surface embedded in a 3d space. At each local point on the surface, the earth seems flat. \nMore formally, a $d$ -dimensional manifold $mathcal { X }$ is a space in which each point $x in mathcal { X }$ has a neighborhood which is topologically equivalent to a $d$ -dimensional Euclidean space, called the tangent space, denoted $mathcal { T } _ { x } = T _ { x } mathcal { X }$ . This is illustrated in Figure 20.28. \nA Riemannian manifold is a differentiable manifold that associates an inner product operator at each point $x$ in tangent space; this is assumed to depend smoothly on the position $x$ . The inner product induces a notion of distance, angles, and volume. The collection of these inner products is called a Riemannian metric. It can be shown that any sufficiently smooth Riemannian manifold can be embedded into a Euclidean space of potentially higher dimension; the Riemannian inner product at a point then becomes Euclidean inner product in that tangent space. \n20.4.2 The manifold hypothesis \nMost “naturally occuring” high dimensional dataset lie a low dimensional manifold. This is called the manifold hypothesis [FMN16]. For example, consider the case of an image. Figure 20.29a shows a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license single image of size $6 4 times 5 7$ . This is a vector in a 3,648-dimensional space, where each dimension corresponds to a pixel intensity. Suppose we try to generate an image by drawing a random point in this space; it is unlikely to look like the image of a digit, as shown in Figure 20.29b. However, the pixels are not independent of each other, since they are generated by some lower dimensional structure, namely the shape of the digit 6. \n\nAs we vary the shape, we will generate different images. We can often characterize the space of shape variations using a low-dimensional manifold. This is illustrated in Figure 20.29c, where we apply PCA (Section 20.1) to project a dataset of 360 images, each one a slightly rotated version of the digit 6, into a 2d space. We see that most of the variation in the data is captured by an underlying curved 2d manifold. We say that the intrinsic dimensionality $d$ of the data is 2, even though the ambient dimensionality $D$ is 3,648. \n20.4.3 Approaches to manifold learning \nIn the rest of this section, we discuss ways to learn manifolds from data. There are many different algorithms that have been proposed, which make different assumptions about the nature of the manifold, and which have different computational properties. We discuss a few of these methods in the following sections. For more details, see e.g., [Bur10]. \nThe methods can be categorized as shown in Table 20.1. The term “nonparametric” refers to methods that learn a low dimensional embedding $z _ { i }$ for each datapoint ${ bf { x } } _ { i }$ , but do not learn a mapping function which can be applied to an out-of-sample datapoint. (However, [Ben+04b] discusses how to extend many of these methods beyond the training set by learning a kernel.) \nIn the sections below, we compare some of these methods using 2 different datasets: a set of 1000 3d-points sampled from the 2d “Swiss roll” manifold, and a set of 1797 64-dimensional points sampled from the UCI digits dataset. See Figure 20.30 for an illustration of the data. We will learn a 2d manifold, so we can visualize the data. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "The manifold hypothesis"
    },
    {
        "content": "As we vary the shape, we will generate different images. We can often characterize the space of shape variations using a low-dimensional manifold. This is illustrated in Figure 20.29c, where we apply PCA (Section 20.1) to project a dataset of 360 images, each one a slightly rotated version of the digit 6, into a 2d space. We see that most of the variation in the data is captured by an underlying curved 2d manifold. We say that the intrinsic dimensionality $d$ of the data is 2, even though the ambient dimensionality $D$ is 3,648. \n20.4.3 Approaches to manifold learning \nIn the rest of this section, we discuss ways to learn manifolds from data. There are many different algorithms that have been proposed, which make different assumptions about the nature of the manifold, and which have different computational properties. We discuss a few of these methods in the following sections. For more details, see e.g., [Bur10]. \nThe methods can be categorized as shown in Table 20.1. The term “nonparametric” refers to methods that learn a low dimensional embedding $z _ { i }$ for each datapoint ${ bf { x } } _ { i }$ , but do not learn a mapping function which can be applied to an out-of-sample datapoint. (However, [Ben+04b] discusses how to extend many of these methods beyond the training set by learning a kernel.) \nIn the sections below, we compare some of these methods using 2 different datasets: a set of 1000 3d-points sampled from the 2d “Swiss roll” manifold, and a set of 1797 64-dimensional points sampled from the UCI digits dataset. See Figure 20.30 for an illustration of the data. We will learn a 2d manifold, so we can visualize the data. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nTable 20.1: A list of some approaches to dimensionality reduction. If a method is convex, we specify in parentheses whether it requires solving a sparse or dense eigenvalue problem. \n20.4.4 Multi-dimensional scaling (MDS) \nThe simplest approach to manifold learning is multidimensional scaling (MDS). This tries to find a set of low dimensional vectors ${ z _ { i } in mathbb { R } ^ { L } : i = 1 : N }$ such that the pairwise distances between these vectors is as similar as possible to a set of pairwise dissimilarities $mathbf { D } = { d _ { i j } }$ provided by the user. There are several variants of MDS, one of which turns out to be equivalent to PCA, as we discuss below. \n20.4.4.1 Classical MDS \nSuppose we start an $N times D$ data matrix $mathbf { X }$ with rows ${ bf { x } } _ { i }$ . Let us define the centered Gram (similarity) matrix as follows: \nIn matrix notation, we have $tilde { mathbf { K } } = tilde { mathbf { X } } tilde { mathbf { X } } ^ { top }$ , where $ddot { mathbf { X } } = mathbf { C } _ { N } mathbf { X }$ and $begin{array} { r } { { bf { C } } _ { N } = { bf { I } } _ { N } - frac { 1 } { N } { bf { 1 } } _ { N } { bf { 1 } } _ { N } ^ { sf t } } end{array}$ is the centering matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Approaches to manifold learning"
    },
    {
        "content": "Table 20.1: A list of some approaches to dimensionality reduction. If a method is convex, we specify in parentheses whether it requires solving a sparse or dense eigenvalue problem. \n20.4.4 Multi-dimensional scaling (MDS) \nThe simplest approach to manifold learning is multidimensional scaling (MDS). This tries to find a set of low dimensional vectors ${ z _ { i } in mathbb { R } ^ { L } : i = 1 : N }$ such that the pairwise distances between these vectors is as similar as possible to a set of pairwise dissimilarities $mathbf { D } = { d _ { i j } }$ provided by the user. There are several variants of MDS, one of which turns out to be equivalent to PCA, as we discuss below. \n20.4.4.1 Classical MDS \nSuppose we start an $N times D$ data matrix $mathbf { X }$ with rows ${ bf { x } } _ { i }$ . Let us define the centered Gram (similarity) matrix as follows: \nIn matrix notation, we have $tilde { mathbf { K } } = tilde { mathbf { X } } tilde { mathbf { X } } ^ { top }$ , where $ddot { mathbf { X } } = mathbf { C } _ { N } mathbf { X }$ and $begin{array} { r } { { bf { C } } _ { N } = { bf { I } } _ { N } - frac { 1 } { N } { bf { 1 } } _ { N } { bf { 1 } } _ { N } ^ { sf t } } end{array}$ is the centering matrix. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nNow define the strain of a set of embeddings as follows: \nwhere $begin{array} { r } { dot { z } _ { i } = z _ { i } - overline { { z } } } end{array}$ is the centered embedding vector. Intuitively this measures how well similarities in the high-dimensional data space, $tilde { K } _ { i j }$ , are matched by similarities in the low-dimensional embedding space, $langle tilde { z } _ { i } , tilde { z } _ { j } rangle$ . Minimizing this loss is called classical MDS. \nWe know from Section 7.5 that the best rank $L$ approximation to a matrix is its truncated SVD representation, $tilde { mathbf { K } } = mathbf { U S V } ^ { mathsf { T } }$ . Since $tilde { bf K }$ is positive semi definite, we have that $mathbf { V } = mathbf { U }$ . Hence the optimal embedding satisfies \nThus we can set the embedding vectors to be the rows of $tilde { mathbf { Z } } = mathbf { U } mathbf { S } ^ { frac { 1 } { 2 } }$ . \nNow we describe how to apply classical MDS to a dataset where we just have Euclidean distances, rather than raw features. First we compute a matrix of squared Euclidean distances, $mathbf { D } ^ { ( 2 ) } = mathbf { D } odot mathbf { D }$ , which has the following entries: \nWe see that $mathbf { D } ^ { ( 2 ) }$ only differs from $ddot { bf K }$ by some row and column constants (and a factor of -2). Hence we can compute $ddot { bf K }$ by double centering $mathbf { D } ^ { ( 2 ) }$ using Equation (7.89) to get $begin{array} { r } { tilde { mathbf { K } } = - frac { 1 } { 2 } mathbf { C } _ { N } mathbf { D } ^ { ( 2 ) } mathbf { C } _ { N } } end{array}$ . In other words, \nWe can then compute the embeddings as before. \nIt turns out that classical MDS is equivalent to PCA (Section 20.1). To see this, let $tilde { mathbf { K } } = mathbf { U } _ { L } mathbf { S } _ { L } mathbf { U } _ { L } ^ { top }$ be the rank $L$ truncated SVD of the centered kernel matrix. The MDS embedding is given by $mathbf { Z } _ { mathrm { M D S } } = mathbf { U } _ { L } mathbf { S } _ { L } ^ { frac { 1 } { 2 } }$ . Now consider the rank $L$ SVD of the centered data matrix, $ddot { mathbf { X } } = mathbf { U } _ { X } mathbf { S } _ { X } mathbf { V } _ { X } ^ { top }$ . The PCA embedding is $mathbf { Z } _ { mathrm { P C A } } = mathbf { U } _ { X } mathbf { S } _ { X }$ . Now \nHence $mathbf { U } _ { X } = mathbf { U } _ { L }$ and $mathbf { S } _ { X } = mathbf { S } _ { L } ^ { 2 }$ , and so ${ bf Z } _ { mathrm { P C A } } = { bf Z } _ { mathrm { M D S } }$ . \n20.4.4.2 Metric MDS \nClassical MDS assumes Euclidean distances. We can generalize it to allow for any dissimilarity measure by defining the stress function \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 where $hat { d } _ { i j } = | | z _ { i } - z _ { j } | |$ . This is called metric MDS. Note that this is a different objective than the one used by classical MDS, so even if $d _ { i j }$ are Euclidean distances, the results will be different. \n\nWe can use gradient descent to solve the optimization problem. However, it is better to use a bound optimization algorithm (Section 8.7) called SMACOF [Lee77], which stands for “Scaling by MAjorizing a COmplication Function”. (This is the method implemented in scikit-learn.) See Figure 20.31 for the results of applying this to our running example. \n20.4.4.3 Non-metric MDS \nInstead of trying to match the distance between points, we can instead just try to match the ranking of how similar points are. To do this, let $f ( d )$ be a monotonic transformation from distances to ranks. Now define the loss \nwhere $hat { d } _ { i j } = | | z _ { i } - z _ { j } | |$ . Minimizing this is known as non-metric MDS. \nThis objective can be optimized iteratively. First the function $f$ is optimized, for a given $mathbf { Z }$ , using isotonic regression; this finds the optimal monotonic transformation of the input distances to match the current embedding distances. Then the embeddings $mathbf { Z }$ are optimized, for a given $f$ , using gradient descent, and the process repeats. \n20.4.4.4 Sammon mapping \nMetric MDS tries to minimize the sum of squared distances, so it puts the most emphasis on large distances. However, for many embedding methods, small distances matter more, since they capture local structure. One way to capture this is to divide each term of the loss by $d _ { i j }$ , so small distances get upweighted: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nMinimizing this results in a Sammon mapping. (The coefficient in front of the sum is just to simplify the gradient of the loss.) Unfortunately this is a non-convex objective, and it arguably puts too much emphasis on getting very small distances exactly right. We will discuss better methods for capturing local structure later on. \n20.4.5 Isomap \nIf the high-dimensional data lies on or near a curved manifold, such as the Swiss roll example, then MDS might consider two points to be close even if their distance along the manifold is large. This is illustrated in Figure 20.32a. \nOne way to capture this is to create the $K$ -nearest neighbor graph between datapoints5, and then approximate the manifold distance between a pair of points by the shortest distance along this graph; this can be computed efficiently using Dijkstra’s shortest path algorithm. See Figure 20.32b for an illustration. Once we have computed this new distance metric, we can apply classical MDS (i.e., PCA). This is a way to capture local structure while avoiding local optima. The overall method is called isomap [TSL00]. \nSee Figure 20.33 for the results of this method on our running example. We see that they are quite reasonable. However, if the data is noisy, there can be “false” edges in the nearest neighbor graph, which can result in “short circuits” which significantly distort the embedding, as shown in Figure 20.34. This problem is known as “topological instability” [BS02]. Choosing a very small neighborhood does not solve this problem, since this can fragment the manifold into a large number of disconnected regions. Various other solutions have been proposed, e.g., [CC07]. \n20.4.6 Kernel PCA \nPCA (and classical MDS) finds the best linear projection of the data, so as to preserve pairwise similarities between all the points. In this section, we consider nonlinear projections. The key idea is to solve PCA by finding the eigenvectors of the inner product (Gram) matrix $mathbf { K } = mathbf { X } mathbf { X } ^ { mathsf { I } }$ , as in Section 20.1.3.2, and then to use the kernel trick (Section 17.3.4), which lets us replace inner products such as ${ pmb x } _ { i } ^ { 1 } { pmb x } _ { j }$ with a kernel function, $K _ { i j } = mathcal { K } ( pmb { x } _ { i } , pmb { x } _ { k } )$ . This is known as kernel PCA [SSM98].",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Multi-dimensional scaling (MDS)"
    },
    {
        "content": "Minimizing this results in a Sammon mapping. (The coefficient in front of the sum is just to simplify the gradient of the loss.) Unfortunately this is a non-convex objective, and it arguably puts too much emphasis on getting very small distances exactly right. We will discuss better methods for capturing local structure later on. \n20.4.5 Isomap \nIf the high-dimensional data lies on or near a curved manifold, such as the Swiss roll example, then MDS might consider two points to be close even if their distance along the manifold is large. This is illustrated in Figure 20.32a. \nOne way to capture this is to create the $K$ -nearest neighbor graph between datapoints5, and then approximate the manifold distance between a pair of points by the shortest distance along this graph; this can be computed efficiently using Dijkstra’s shortest path algorithm. See Figure 20.32b for an illustration. Once we have computed this new distance metric, we can apply classical MDS (i.e., PCA). This is a way to capture local structure while avoiding local optima. The overall method is called isomap [TSL00]. \nSee Figure 20.33 for the results of this method on our running example. We see that they are quite reasonable. However, if the data is noisy, there can be “false” edges in the nearest neighbor graph, which can result in “short circuits” which significantly distort the embedding, as shown in Figure 20.34. This problem is known as “topological instability” [BS02]. Choosing a very small neighborhood does not solve this problem, since this can fragment the manifold into a large number of disconnected regions. Various other solutions have been proposed, e.g., [CC07]. \n20.4.6 Kernel PCA \nPCA (and classical MDS) finds the best linear projection of the data, so as to preserve pairwise similarities between all the points. In this section, we consider nonlinear projections. The key idea is to solve PCA by finding the eigenvectors of the inner product (Gram) matrix $mathbf { K } = mathbf { X } mathbf { X } ^ { mathsf { I } }$ , as in Section 20.1.3.2, and then to use the kernel trick (Section 17.3.4), which lets us replace inner products such as ${ pmb x } _ { i } ^ { 1 } { pmb x } _ { j }$ with a kernel function, $K _ { i j } = mathcal { K } ( pmb { x } _ { i } , pmb { x } _ { k } )$ . This is known as kernel PCA [SSM98].",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Isomap"
    },
    {
        "content": "Minimizing this results in a Sammon mapping. (The coefficient in front of the sum is just to simplify the gradient of the loss.) Unfortunately this is a non-convex objective, and it arguably puts too much emphasis on getting very small distances exactly right. We will discuss better methods for capturing local structure later on. \n20.4.5 Isomap \nIf the high-dimensional data lies on or near a curved manifold, such as the Swiss roll example, then MDS might consider two points to be close even if their distance along the manifold is large. This is illustrated in Figure 20.32a. \nOne way to capture this is to create the $K$ -nearest neighbor graph between datapoints5, and then approximate the manifold distance between a pair of points by the shortest distance along this graph; this can be computed efficiently using Dijkstra’s shortest path algorithm. See Figure 20.32b for an illustration. Once we have computed this new distance metric, we can apply classical MDS (i.e., PCA). This is a way to capture local structure while avoiding local optima. The overall method is called isomap [TSL00]. \nSee Figure 20.33 for the results of this method on our running example. We see that they are quite reasonable. However, if the data is noisy, there can be “false” edges in the nearest neighbor graph, which can result in “short circuits” which significantly distort the embedding, as shown in Figure 20.34. This problem is known as “topological instability” [BS02]. Choosing a very small neighborhood does not solve this problem, since this can fragment the manifold into a large number of disconnected regions. Various other solutions have been proposed, e.g., [CC07]. \n20.4.6 Kernel PCA \nPCA (and classical MDS) finds the best linear projection of the data, so as to preserve pairwise similarities between all the points. In this section, we consider nonlinear projections. The key idea is to solve PCA by finding the eigenvectors of the inner product (Gram) matrix $mathbf { K } = mathbf { X } mathbf { X } ^ { mathsf { I } }$ , as in Section 20.1.3.2, and then to use the kernel trick (Section 17.3.4), which lets us replace inner products such as ${ pmb x } _ { i } ^ { 1 } { pmb x } _ { j }$ with a kernel function, $K _ { i j } = mathcal { K } ( pmb { x } _ { i } , pmb { x } _ { k } )$ . This is known as kernel PCA [SSM98]. \n\nRecall from Mercer’s theorem that the use of a kernel implies some underlying feature space, so we are implicitly replacing ${ boldsymbol { x } } _ { i }$ with $phi ( pmb { x } _ { i } ) = phi _ { i }$ . Let $Phi$ be the corresponding (notional) design matrix, and $mathbf { K } = mathbf { X } mathbf { X } ^ { mathsf { T } }$ be the Gram matrix. Finally, let $begin{array} { r } { { bf S } _ { phi } = frac { 1 } { N } sum _ { i } phi _ { i } phi _ { i } ^ { top } } end{array}$ be the covariance matrix in feature space. (We are assuming for now the features are centered.) From Equation (20.22), the normalized eigenvectors of $mathbf { s }$ are given by $mathbf { V } _ { mathrm { k P C A } } = Phi ^ { mathsf { T } } mathbf { U } mathbf { A } ^ { - frac { 1 } { 2 } }$ , where $mathbf { U }$ and $pmb { Lambda }$ contain the eigenvectors and eigenvalues of $mathbf { K }$ . Of course, we can’t actually compute $mathbf { V } _ { mathrm { k P C A } }$ , since $phi _ { i }$ is potentially infinite dimensional. However, we can compute the projection of a test vector ${ pmb x } _ { * }$ onto the feature space as follows: \nwhere $pmb { k } _ { * } = [ mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { 1 } ) , dots , mathcal { K } ( pmb { x } _ { * } , pmb { x } _ { N } ) ]$ . \nThere is one final detail to worry about. The covariance matrix is only given by $mathbf { S } = Phi ^ { prime } bar { Phi }$ if the features is zero-mean. Thus we can only use the Gram matrix $mathbf { K } = Phi mathbf { Phi } mathbf { Phi } ^ { mathsf { T } }$ if $mathbb { E } left[ phi _ { i } right] = mathbf { 0 }$ . Unfortunately, \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license we cannot simply subtract off the mean in feature space, since it may be infinite dimensional. However, there is a trick we can use. Define the centered feature vector as $begin{array} { r } { tilde { phi } _ { i } = phi ( pmb { x } _ { i } ) - frac { 1 } { N } sum _ { j = 1 } ^ { N } phi ( pmb { x } _ { j } ) } end{array}$ . The Gram matrix of the centered feature vectors is given by $tilde { K } _ { i j } = tilde { phi } _ { i } ^ { top } tilde { phi } _ { j }$ . Using the double centering trick from Equation (7.89), we can write this in matrix form as $tilde { mathbf { K } } = mathbf { C } _ { N } mathbf { K } mathbf { C } _ { N }$ , where $begin{array} { r } { { bf C } _ { N } triangleq { bf I } _ { N } - frac { 1 } { N } { bf 1 } _ { N } { bf 1 } _ { N } ^ { sf } } end{array}$ is the centering matrix. \n\nIf we apply kPCA with a linear kernel, we recover regular PCA (classical MDS). This is limited to using $L leq D$ embedding dimensions. If we use a non-degenerate kernel, we can use up to $N$ components, since the size of $Phi$ is $N times D ^ { * }$ , where $D ^ { * }$ is the (potentially infinite) dimensionality of embedded feature vectors. Figure 20.35 gives an example of the method applied to some $D = 2$ dimensional data using an RBF kernel. We project points in the unit grid onto the first 8 components and visualize the corresponding surfaces using a contour plot. We see that the first two components separate the three clusters, and the following components split the clusters. \nSee Figure 20.36 for some the results on kPCA (with an RBF kernel) on our running example. In this case, the results are arguably not very useful. In fact, it can be shown that kPCA with an RBF kernel expands the feature space instead of reducing it [WSS04], as we saw in Figure 20.35, which makes it not very useful as a method for dimensionality reduction. We discuss a solution to this in Section 20.4.7. \n20.4.7 Maximum variance unfolding (MVU) \nkPCA with certain kernels, such as RBF, might not result in a low dimensional embedding, as discussed in Section 20.4.6. This observation led to the development of the semidefinite embedding \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Kernel PCA"
    },
    {
        "content": "If we apply kPCA with a linear kernel, we recover regular PCA (classical MDS). This is limited to using $L leq D$ embedding dimensions. If we use a non-degenerate kernel, we can use up to $N$ components, since the size of $Phi$ is $N times D ^ { * }$ , where $D ^ { * }$ is the (potentially infinite) dimensionality of embedded feature vectors. Figure 20.35 gives an example of the method applied to some $D = 2$ dimensional data using an RBF kernel. We project points in the unit grid onto the first 8 components and visualize the corresponding surfaces using a contour plot. We see that the first two components separate the three clusters, and the following components split the clusters. \nSee Figure 20.36 for some the results on kPCA (with an RBF kernel) on our running example. In this case, the results are arguably not very useful. In fact, it can be shown that kPCA with an RBF kernel expands the feature space instead of reducing it [WSS04], as we saw in Figure 20.35, which makes it not very useful as a method for dimensionality reduction. We discuss a solution to this in Section 20.4.7. \n20.4.7 Maximum variance unfolding (MVU) \nkPCA with certain kernels, such as RBF, might not result in a low dimensional embedding, as discussed in Section 20.4.6. This observation led to the development of the semidefinite embedding \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nalgorithm [WSS04], also called maximum variance unfolding, which tries to learn an embedding $left{ z _ { i } right}$ such that \nwhere $G$ is the nearest neighbor graph (as in Isomap). This approach explicitly tries to ’unfold’ th data manifold while respecting the nearest neighbor constraints. \nThis can be reformulated as a semidefinite programming (SDP) problem by defining the kernel matrix $mathbf { K } = mathbf { Z } mathbf { Z } ^ { mathsf { T } }$ and then optimizing \nThe resulting kernel is then passed to kPCA, and the resulting eigenvectors give the low dimensional embedding. \n20.4.8 Local linear embedding (LLE) \nThe techniques we have discussed so far all rely on an eigendecomposition of a full matrix of pairwise similarities, either in the ambient space (PCA), in feature space (kPCA), or along the KNN graph (Isomap). In this section, we discuss local linear embedding (LLE) [RS00], a technique that solves a sparse eigenproblem, thus focusing more on local structure in the data. \nLLE assumes the data manifold around each point ${ bf { chi } } _ { i }$ is locally linear. The best linear approximation can be found by predicting ${ bf { chi } } _ { i }$ as a linear combination of its $K$ nearest neighbors using reconstruction weights ${ pmb w } _ { i }$ . This can be found by solving \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Maximum variance unfolding (MVU)"
    },
    {
        "content": "algorithm [WSS04], also called maximum variance unfolding, which tries to learn an embedding $left{ z _ { i } right}$ such that \nwhere $G$ is the nearest neighbor graph (as in Isomap). This approach explicitly tries to ’unfold’ th data manifold while respecting the nearest neighbor constraints. \nThis can be reformulated as a semidefinite programming (SDP) problem by defining the kernel matrix $mathbf { K } = mathbf { Z } mathbf { Z } ^ { mathsf { T } }$ and then optimizing \nThe resulting kernel is then passed to kPCA, and the resulting eigenvectors give the low dimensional embedding. \n20.4.8 Local linear embedding (LLE) \nThe techniques we have discussed so far all rely on an eigendecomposition of a full matrix of pairwise similarities, either in the ambient space (PCA), in feature space (kPCA), or along the KNN graph (Isomap). In this section, we discuss local linear embedding (LLE) [RS00], a technique that solves a sparse eigenproblem, thus focusing more on local structure in the data. \nLLE assumes the data manifold around each point ${ bf { chi } } _ { i }$ is locally linear. The best linear approximation can be found by predicting ${ bf { chi } } _ { i }$ as a linear combination of its $K$ nearest neighbors using reconstruction weights ${ pmb w } _ { i }$ . This can be found by solving \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nNote that we need the sum-to-one constraint on the weights to prevent the trivial solution $mathbf { W } = mathbf { 0 }$ The resulting vector of weights ${ mathbf { } } { mathbf { } } w _ { i } ,$ ,: constitute the barycentric coordinates of ${ bf { x } } _ { i }$ . \nAny linear mapping of this hyperplane to a lower dimensional space preserves the reconstruction weights, and thus the local geometry. Thus we can solve for the low-dimensional embeddings for each point by solving \nwhere $hat { w } _ { i j } = 0$ if $j$ is not one of the $K$ nearest neighbors of $i$ . We can rewrite this loss as \nThus the solution is given by the eigenvectors of $left( mathbf { I } - mathbf { W } right) ^ { parallel } left( mathbf { I } - mathbf { W } right)$ corresponding to the smallest nonzero eigenvalues, as shown in Section 7.4.8. \nSee Figure 20.37 for some the results on LLE on our running example. In this case, the results do not seem as good as those produced by Isomap. However, the method tends to be somewhat less sensitive to short-circuiting (noise). \n20.4.9 Laplacian eigenmaps \nIn this section, we describe Laplacian eigenmaps or spectral embedding [BN01]. The idea is to compute a low-dimensional representation of the data in which the weighted distances between a datapoint and its $K$ nearest neighbors are minimized. We put more weight on the first nearest neighbor than the second, etc. We give the details below. \n20.4.9.1 Using eigenvectors of the graph Laplacian to compute embeddings \nWe want to find embeddings which minimize \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Local linear embedding (LLE)"
    },
    {
        "content": "Note that we need the sum-to-one constraint on the weights to prevent the trivial solution $mathbf { W } = mathbf { 0 }$ The resulting vector of weights ${ mathbf { } } { mathbf { } } w _ { i } ,$ ,: constitute the barycentric coordinates of ${ bf { x } } _ { i }$ . \nAny linear mapping of this hyperplane to a lower dimensional space preserves the reconstruction weights, and thus the local geometry. Thus we can solve for the low-dimensional embeddings for each point by solving \nwhere $hat { w } _ { i j } = 0$ if $j$ is not one of the $K$ nearest neighbors of $i$ . We can rewrite this loss as \nThus the solution is given by the eigenvectors of $left( mathbf { I } - mathbf { W } right) ^ { parallel } left( mathbf { I } - mathbf { W } right)$ corresponding to the smallest nonzero eigenvalues, as shown in Section 7.4.8. \nSee Figure 20.37 for some the results on LLE on our running example. In this case, the results do not seem as good as those produced by Isomap. However, the method tends to be somewhat less sensitive to short-circuiting (noise). \n20.4.9 Laplacian eigenmaps \nIn this section, we describe Laplacian eigenmaps or spectral embedding [BN01]. The idea is to compute a low-dimensional representation of the data in which the weighted distances between a datapoint and its $K$ nearest neighbors are minimized. We put more weight on the first nearest neighbor than the second, etc. We give the details below. \n20.4.9.1 Using eigenvectors of the graph Laplacian to compute embeddings \nWe want to find embeddings which minimize \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $begin{array} { r } { W _ { i j } = exp bigl ( { - frac { 1 } { 2 sigma ^ { 2 } } | | pmb { x } _ { i } - pmb { x } _ { j } | | _ { 2 } ^ { 2 } } bigr ) } end{array}$ if $i - j$ are neighbors in the KNN graph and 0 otherwise. We add the constraint ${ bf Z } ^ { 1 } { bf D } { bf Z } = { bf I }$ to avoid the degenerate solution where $mathbf { Z } = mathbf { 0 }$ , where $mathbf { D }$ is the diagonal weight matrix storing the degree of each node, $begin{array} { r } { D _ { i i } = sum _ { j } W _ { i , j } } end{array}$ . \nWe can rewrite the above objective as follows: \nwhere $mathbf { L } = mathbf { D } - mathbf { W }$ is the graph Laplacian (see Section 20.4.9.2). One can show that minimizing this is equivalent to solving the (generalized) eigenvalue problem $mathbf { L } z _ { i } = lambda _ { i } mathbf { D } z _ { i }$ for the $L$ smallest nonzero eigenvalues. \nSee Figure 20.38 for the results of applying this method (with an RBF kernel) to our running example. \n20.4.9.2 What is the graph Laplacian? \nWe saw above that we can compute the eigenvectors of the graph Laplacian in order to learn a good embedding of the high dimensional points. In this section, we give some intuition as to why this works. \nLet W be a symmetric weight matrix for a graph, where $W _ { i j } = W _ { j i } ge 0$ . Let $mathbf { D } = mathrm { d i a g } ( d _ { i } )$ be a diagonal matrix containing the weighted degree of each node, $begin{array} { r } { d _ { i } = sum _ { j } w _ { i j } } end{array}$ . We define the graph Laplacian as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFigure 20.39: Illustration of the Laplacian matrix derived from an undirected graph. From https: // en.   \nwikipedia. org/ wiki/ Laplacian_ matrix . Used with kind permission of Wikipedia author AzaToth. \nThus the elements of $mathbf { L }$ are given by \nSee Figure 20.39 for an example of how to compute this. \nSuppose we associate a value $f _ { i } in mathbb { R }$ with each node $i$ in the graph (see Figure 20.40 for example). Then we can use the graph Laplacian as a difference operator, to compute a discrete derivative of the function at a point: \nwhere $mathrm { n b r } _ { i }$ is the set of neighbors of node $i$ . We can also compute an overall measure of “smoothness” \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nof the function $f$ by computing its Dirichlet energy as follows: \nBy studying the eigenvalues and eigenvectors of the Laplacian matrix, we can determine various useful properties of the function. (Applying linear algebra to study the adjacency matrix of a graph, or related matrices, is called spectral graph theory [Chu97].) For example, we see that $mathbf { L }$ is symmetric and positive semi-definite, since we have $f ^ { T } mathbf { L } f geq 0$ for all $pmb { f } in mathbb { R } ^ { N }$ , which follows from Equation (20.128) due to the assumption that $w _ { i j } geq 0$ . Consequently $mathbf { L }$ has $N$ non-negative, real-valued eigenvalues, $0 le lambda _ { 1 } le lambda _ { 2 } le . . . le lambda _ { N }$ . The corresponding eigenvectors form an orthogonal basis for the function $f$ defined on the graph, in order of decreasing smoothness. \nIn Section 20.4.9.1, we discuss Laplacian eigenmaps, which is a way to learn low dimensional embeddings for high dimensional data vectors. The approach is to let $z _ { i d } = f _ { i } ^ { d }$ be the $d { mathrm { ~ } }$ ’th embedding dimension for input $i$ , and then to find a basis for these functions (i.e., embedding of the points) that varies smoothly over the graph, thus respecting distance of the points in ambient space. \nThere are many other applications of the graph Laplacian in ML. For example, in Section 21.5.1, we discuss normalized cuts, which is a way to learn a clustering of high dimensional data vectors based on pairwise similarity; and [WTN19] discusses how to use the eigenvectors of the state transition matrix to learn representations for RL. \n20.4.10 t-SNE \nIn this section, we describe a very popular nonconvex technique for learning low dimensional embeddings called t-SNE [MH08]. This extends the earlier stochastic neighbor embedding method of [HR03], so we first describe SNE, before describing the t-SNE extension. \n20.4.10.1 Stochastic neighborhood embedding (SNE) \nThe basic idea in SNE is to convert high-dimensional Euclidean distances into conditional probabilities that represent similarities. More precisely, we define $p _ { j | i }$ to be the probability that point $i$ would pick point $j$ as its neighbor if neighbors were picked in proportion to their probability under a Gaussian centered at ${ bf { chi } } _ { i }$ : \nHere $sigma _ { i } ^ { 2 }$ is the variance for data point $i$ , which can be used to “magnify” the scale of points in dense regions of input space, and diminish the scale in sparser regions. (We discuss how to estimate the length scales $sigma _ { i } ^ { 2 }$ shortly). \nLet $z _ { i }$ be the low dimensional embedding representing ${ boldsymbol { x } } _ { i }$ . We define similarities in the low \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "Laplacian eigenmaps"
    },
    {
        "content": "of the function $f$ by computing its Dirichlet energy as follows: \nBy studying the eigenvalues and eigenvectors of the Laplacian matrix, we can determine various useful properties of the function. (Applying linear algebra to study the adjacency matrix of a graph, or related matrices, is called spectral graph theory [Chu97].) For example, we see that $mathbf { L }$ is symmetric and positive semi-definite, since we have $f ^ { T } mathbf { L } f geq 0$ for all $pmb { f } in mathbb { R } ^ { N }$ , which follows from Equation (20.128) due to the assumption that $w _ { i j } geq 0$ . Consequently $mathbf { L }$ has $N$ non-negative, real-valued eigenvalues, $0 le lambda _ { 1 } le lambda _ { 2 } le . . . le lambda _ { N }$ . The corresponding eigenvectors form an orthogonal basis for the function $f$ defined on the graph, in order of decreasing smoothness. \nIn Section 20.4.9.1, we discuss Laplacian eigenmaps, which is a way to learn low dimensional embeddings for high dimensional data vectors. The approach is to let $z _ { i d } = f _ { i } ^ { d }$ be the $d { mathrm { ~ } }$ ’th embedding dimension for input $i$ , and then to find a basis for these functions (i.e., embedding of the points) that varies smoothly over the graph, thus respecting distance of the points in ambient space. \nThere are many other applications of the graph Laplacian in ML. For example, in Section 21.5.1, we discuss normalized cuts, which is a way to learn a clustering of high dimensional data vectors based on pairwise similarity; and [WTN19] discusses how to use the eigenvectors of the state transition matrix to learn representations for RL. \n20.4.10 t-SNE \nIn this section, we describe a very popular nonconvex technique for learning low dimensional embeddings called t-SNE [MH08]. This extends the earlier stochastic neighbor embedding method of [HR03], so we first describe SNE, before describing the t-SNE extension. \n20.4.10.1 Stochastic neighborhood embedding (SNE) \nThe basic idea in SNE is to convert high-dimensional Euclidean distances into conditional probabilities that represent similarities. More precisely, we define $p _ { j | i }$ to be the probability that point $i$ would pick point $j$ as its neighbor if neighbors were picked in proportion to their probability under a Gaussian centered at ${ bf { chi } } _ { i }$ : \nHere $sigma _ { i } ^ { 2 }$ is the variance for data point $i$ , which can be used to “magnify” the scale of points in dense regions of input space, and diminish the scale in sparser regions. (We discuss how to estimate the length scales $sigma _ { i } ^ { 2 }$ shortly). \nLet $z _ { i }$ be the low dimensional embedding representing ${ boldsymbol { x } } _ { i }$ . We define similarities in the low \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \ndimensional space in an analogous way: \nIn this case, the variance is fixed to a constant; changing it would just rescale the learned map, and not change its topology. \nIf the embedding is a good one, then $q _ { j | i }$ should match $p _ { j | i }$ . Therefore, SNE defines the objective to be \nwhere $P _ { i }$ is the conditional distribution over all other data points given ${ boldsymbol { x } } _ { i }$ , $Q _ { i }$ is the conditional distribution over all other latent points given $z _ { i }$ , and $D _ { mathbb { K L } } left( P _ { i } parallel Q _ { i } right)$ is the KL divergence (Section 6.2) between the distributions. \nNote that this is an asymmetric objective. In particular, there is a large cost if a small $q _ { j | i }$ is used to model a large $p _ { j | i }$ . This objective will prefer to pull distant points together rather than push nearby points apart. We can get a better idea of the geometry by looking at the gradient for each embedding vector, which is given by \nThus points are pulled towards each other if the $p$ ’s are bigger than the $q$ ’s, and repelled if the $q$ ’s are bigger than the $p$ ’s. \nAlthough this is an intuitively sensible objective, it is not convex. Nevertheless it can be minimized using SGD. In practice, it helps to add Gaussian noise to the embedding points, and to gradually anneal the amount of noise. [Hin13] recommends to “spend a long time at the noise level at which the global structure starts to form from the hot plasma of map points” before reducing it.6 \n20.4.10.2 Symmetric SNE \nThere is a slightly simpler version of SNE that minimizes a single KL between the joint distribution $P$ in high dimensional space and $Q$ in low dimensional space: \nThis is called symmetric SNE. \nThe obvious way to define $p _ { i j }$ is to use \nWe can define $q _ { i j }$ similarily. \nThe corresponding gradient becomes \nAs before, points are pulled towards each other if the $p$ ’s are bigger than the $q$ ’s, and repelled if the $q$ ’s are bigger than the $p$ ’s. \nAlthough symmetric SNE is slightly easier to implement, it loses the nice property of regular SNE that the data is its own optimal embedding if the embedding dimension $L$ is set equal to the ambient dimension $D$ . Nevertheless, the methods seems to give similar results in practice on real datasets where $L ll D$ . \n20.4.10.3 t-distributed SNE \nA fundamental problem with SNE and many other embedding techniques is that they tend to squeeze points that are relatively far away in the high dimensional space close together in the low dimensional (usually 2d) embedding space; this is called the crowding problem, and arises due to the use of squared errors (or Gaussian probabilities). \nOne solution to this is to use a probability distribution in latent space that has heavier tails, which eliminates the unwanted attractive forces between points that are relatively far in the high dimensional space. An obvious choice is the Student-t distribution (Section 2.7.1). In t-SNE, they set the degree of freedom parameter to $nu = 1$ , so the distribution becomes equivalent to a Cauchy: \nWe can use the same global KL objective as in Equation (20.133). For t-SNE, the gradient turns out to be \nThe gradient for symmetric (Gaussian) SNE is the same, but lacks the $( 1 + | | z _ { i } - z _ { j } | | ^ { 2 } ) ^ { - 1 }$ term. This term is useful because $( 1 + | | z _ { i } - z _ { j } | | ^ { 2 } ) ^ { - 1 }$ acts like an inverse square law. This means that points in embedding space act like stars and galaxies, forming many well-separated clusters (galaxies) each of which has many stars tightly packed inside. This can be useful for separating different classes of data in an unsupervised way (see Figure 20.41 for an example). \n20.4.10.4 Choosing the length scale \nAn important parameter in t-SNE is the local bandwidth $sigma _ { i } ^ { 2 }$ . This is usually chosen so that $P _ { i }$ has a perplexity chosen by the user.7 This can be interpreted as a smooth measure of the effective number of neighbors. \nUnfortunately, the results of t-SNE can be quite sensitive to the perplexity parameter, so it is wise to run the algorithm with many different values. This is illustrated in Figure 20.42. The input data is 2d, so there is no distortion generating by mapping to a 2d latent space. If the perplexity is too small, the method tends to find structure within each cluster which is not truly present. At perplexity 30 (the default for scikit-learn), the clusters seem equi-distant in embedding space, even though some are closer than others in the data space. Many other caveats in interpreting t-SNE plots can be found in [WVJ16]. \n20.4.10.5 Computational issues \nThe naive implementation of t-SNE takes $O ( N ^ { 2 } )$ time, as can be seen from the gradient term in Equation (20.137). A faster version can be created by leveraging an analogy to N-body simulation in physics. In particular, the gradient requires computing the force of $N$ points on each of $N$ points. However, points that are far away can be grouped into clusters (computationally speaking), and their effective force can be approximated by a few representative points per cluster. We can then approximate the forces using the Barnes-Hut algorithm [BH86], which takes $O ( N log N )$ time, as proposed in [Maa14]. Unfortunately, this only works well for low dimensional embeddings, such as $L = 2$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n20.4.10.6 UMAP \nVarious extensions of tSNE have been proposed, that try to improve its speed, the quality of the embedding space, or the ability to embed into more than 2 dimensions. \nOne popular recent extension is called UMAP (which stands for “Uniform Manifold Approximation and Projection”), was proposed in [MHM18]. At a high level, this is similar to tSNE, but it tends to preserve global structure better, and it is much faster. This makes it easier to try multiple values of the hyperparameters. For an interactive tutorial on UMAP, and a comparison to tSNE, see [CP19]. \n20.5 Word embeddings \nWords are categorical random variables, so their corresponding one-hot vector representations are sparse. The problem with this binary representation is that semantically similar words may have very different vector representations. For example, the pair of related words “man” and “woman” will be Hamming distance 1 apart, as will the pair of unrelated words “man” and “banana”. \nThe standard way to solve this problem is to use word embeddings, in which we map each sparse one-hot vector, $pmb { s } _ { n , t } in { 0 , 1 } ^ { M }$ , representing the $t$ ’th word in document $n$ , to a lower-dimensional dense vector, $z _ { n , t } in mathbb { R } ^ { D }$ , such that semantically similar words are placed close by. This can significantly help with data sparsity. There are many ways to learn such embeddings, as we discuss below. \nBefore discussing methods, we have to define what we mean by “semantically similar” words. We will assume that two words are semantically similar if they occur in similar contexts. This is known as the distributional hypothesis [Har54], which is often summarized by the phase (originally from [Fir57]) “a word is characterized by the company it keeps”. Thus the methods we discuss will all learn a mapping from a word’s context to an embedding vector for that word. \n20.5.1 Latent semantic analysis / indexing \nIn this section, we discuss a simple way to learn word embeddings based on singular value decomposition (Section 7.5) of a term-frequency count matrix. \n20.5.1.1 Latent semantic indexing (LSI) \nLet $C _ { i j }$ be the number of times “term” $i$ occurs in “context” $j$ . The definition of what we mean by “term” is application-specific. In English, we often take it to be the set of unique tokens that are separated by punctuation or whitespace; for simplicity, we will call these “words”. However, we may preprocess the text data to remove very frequent or infrequent words, or perform other kinds of preprocessing. as we discuss in Section 1.5.4.1. \nThe definition of what we mean by “context” is also application-specific. In this section, we count how many times word $i$ occurs in each document $j in { 1 , ldots , N }$ from a set or corpus of documents; the resulting matrx $mathbf { C }$ is called a term-document frequency matrix, as in Figure 1.15. (Sometimes we apply the TF-IDF transformation to the counts, as discussed in Section 1.5.4.2.) \nLet $mathbf { C } in mathbb { R } ^ { M times N }$ be the count matrix, and let $hat { mathbf { C } }$ be the rank $K$ approximation that minimizes the following loss: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Manifold learning *",
        "subsubsection": "t-SNE"
    },
    {
        "content": "20.4.10.6 UMAP \nVarious extensions of tSNE have been proposed, that try to improve its speed, the quality of the embedding space, or the ability to embed into more than 2 dimensions. \nOne popular recent extension is called UMAP (which stands for “Uniform Manifold Approximation and Projection”), was proposed in [MHM18]. At a high level, this is similar to tSNE, but it tends to preserve global structure better, and it is much faster. This makes it easier to try multiple values of the hyperparameters. For an interactive tutorial on UMAP, and a comparison to tSNE, see [CP19]. \n20.5 Word embeddings \nWords are categorical random variables, so their corresponding one-hot vector representations are sparse. The problem with this binary representation is that semantically similar words may have very different vector representations. For example, the pair of related words “man” and “woman” will be Hamming distance 1 apart, as will the pair of unrelated words “man” and “banana”. \nThe standard way to solve this problem is to use word embeddings, in which we map each sparse one-hot vector, $pmb { s } _ { n , t } in { 0 , 1 } ^ { M }$ , representing the $t$ ’th word in document $n$ , to a lower-dimensional dense vector, $z _ { n , t } in mathbb { R } ^ { D }$ , such that semantically similar words are placed close by. This can significantly help with data sparsity. There are many ways to learn such embeddings, as we discuss below. \nBefore discussing methods, we have to define what we mean by “semantically similar” words. We will assume that two words are semantically similar if they occur in similar contexts. This is known as the distributional hypothesis [Har54], which is often summarized by the phase (originally from [Fir57]) “a word is characterized by the company it keeps”. Thus the methods we discuss will all learn a mapping from a word’s context to an embedding vector for that word. \n20.5.1 Latent semantic analysis / indexing \nIn this section, we discuss a simple way to learn word embeddings based on singular value decomposition (Section 7.5) of a term-frequency count matrix. \n20.5.1.1 Latent semantic indexing (LSI) \nLet $C _ { i j }$ be the number of times “term” $i$ occurs in “context” $j$ . The definition of what we mean by “term” is application-specific. In English, we often take it to be the set of unique tokens that are separated by punctuation or whitespace; for simplicity, we will call these “words”. However, we may preprocess the text data to remove very frequent or infrequent words, or perform other kinds of preprocessing. as we discuss in Section 1.5.4.1. \nThe definition of what we mean by “context” is also application-specific. In this section, we count how many times word $i$ occurs in each document $j in { 1 , ldots , N }$ from a set or corpus of documents; the resulting matrx $mathbf { C }$ is called a term-document frequency matrix, as in Figure 1.15. (Sometimes we apply the TF-IDF transformation to the counts, as discussed in Section 1.5.4.2.) \nLet $mathbf { C } in mathbb { R } ^ { M times N }$ be the count matrix, and let $hat { mathbf { C } }$ be the rank $K$ approximation that minimizes the following loss: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nOne can show that the minimizer of this is given by the rank $K$ truncated SVD approximation, $hat { mathbf { C } } = mathbf { U } mathbf { S } mathbf { V }$ . This means we can represent each $c _ { i j }$ as a bilinear product: \nWe define $mathbf { delta } mathbf { u } _ { i }$ to be the embedding for word $i$ , and $mathbf { boldsymbol { s } } odot mathbf { boldsymbol { v } } _ { j }$ to be the embedding for context $j$ . \nWe can use these embeddings for document retrieval. The idea is to compute an embedding for the query words using $mathbf { Delta } mathbf { u } _ { i }$ , and to compare this to the embedding of all the documents or contexts ${ pmb v } _ { j }$ . This is known as latent semantic indexing or LSI [Dee+90]. \n$begin{array} { r } { pmb q = frac { 1 } { B } sum _ { b = 1 } ^ { B } pmb u _ { w _ { b } } } end{array}$ In more detail, suppose the query is a bag of words , where $mathbf { Delta } mathbf { u } _ { w _ { b } }$ is tchoesienmebseidmdilnagrfiotry wor $w _ { 1 } , ldots , w _ { B }$ $w _ { b }$ ; we represent this by the vector ocument $j$ be represented by $boldsymbol { v } _ { j }$ by \nwhere $| | { pmb q } | | = sqrt { sum _ { i } q _ { i } ^ { 2 } }$ is the $ell _ { 2 }$ -norm of $mathbf { pmb { q } }$ . This measures the angles between the two vectors, as shown in Figure 20.43. Note that if the vectors are unit norm, cosine similarity is the same as inner product; it is also equal to the squared Euclidean distance, up to a change of sign and an irrelevant additive constant: \n20.5.1.2 Latent semantic analysis (LSA) \nNow suppose we define context more generally to be some local neighborhood of words $j in { 1 , dots , M ^ { h } }$ , where $h$ is the window size. Thus $C _ { i j }$ is how many times word $i$ occurs in a neighborhood of type $j$ . We can compute the SVD of this matrix as before, to get $begin{array} { r } { c _ { i j } approx sum _ { k = 1 } ^ { K } u _ { i k } s _ { k } v _ { j k } } end{array}$ . We define ${ pmb u } _ { i }$ to be \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 the embedding for word $i$ , and $mathbf { boldsymbol { s } } odot mathbf { boldsymbol { v } } _ { j }$ to be the embedding for context $j$ . This is known as latent semantic analysis or LSA [Dee+90]. \n\nFor example, suppose we compute $mathbf { C }$ on the British National Corpus.8 For each word, let us retrieve the $K$ nearest neighbors in embedding space ranked by cosine similarity (i.e., normalized inner product). If the query word is “dog”, and we use $h = 2$ or $h = 3 0$ , the nearest neighbors are as follows: \nh=2: cat, horse, fox, pet, rabbit, pig, animal, mongrel, sheep, pigeon h=30: kennel, puppy, pet, bitch, terrier, rottweiler, canine, cat, to bark \nThe 2-word context window is more sensitive to syntax, while the 30-word window is more sensitive to semantics. The “optimal” value of context size $h$ depends on the application. \n20.5.1.3 PMI \nIn practice LSA (and other similar methods) give much better results if we replace the raw counts $C _ { i j }$ with pointwise mutual information (PMI) [CH90], defined as \nIf word $i$ is strongly associated with context $j$ , we will have $mathbb { P } mathbb { M } mathbb { I } ( i , j ) > 0$ . If the PMI is negative, it means $i$ and $j$ co-occur less often that if they were independent; however, such negative correlations can be unreliable, so it is common to use the positive PMI: $mathbb { P } mathbb { P } mathbb { M } [ ( i , j ) = operatorname* { m a x } ( mathbb { P } mathbb { M } mathbb { I } ( i , j ) , 0 )$ . In [BL07b], they show that SVD applied to the PPMI matrix results in word embeddings that perform well on a many tasks related to word meaning. See Section 20.5.5 for a theoretical model that explains this empirical performance. \n20.5.2 Word2vec \nIn this section, we discuss the popular word2vec model from [Mik+13a; Mik+13b], which are “shallow” neural nets for predicting a word given its context. In Section 20.5.5, we will discuss the connections with SVD of the PMI matrix. \nThere are two versions of the word2vec model. The first is called CBOW, which stands for “continuous bag of words”. The second is called skipgram. We discuss both of these below. \n20.5.2.1 Word2vec CBOW model \nIn the continuous bag of words (CBOW) model (see Figure 20.44(a)), the log likelihood of a sequence of words is computed using the following model: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Word embeddings",
        "subsubsection": "Latent semantic analysis / indexing"
    },
    {
        "content": "For example, suppose we compute $mathbf { C }$ on the British National Corpus.8 For each word, let us retrieve the $K$ nearest neighbors in embedding space ranked by cosine similarity (i.e., normalized inner product). If the query word is “dog”, and we use $h = 2$ or $h = 3 0$ , the nearest neighbors are as follows: \nh=2: cat, horse, fox, pet, rabbit, pig, animal, mongrel, sheep, pigeon h=30: kennel, puppy, pet, bitch, terrier, rottweiler, canine, cat, to bark \nThe 2-word context window is more sensitive to syntax, while the 30-word window is more sensitive to semantics. The “optimal” value of context size $h$ depends on the application. \n20.5.1.3 PMI \nIn practice LSA (and other similar methods) give much better results if we replace the raw counts $C _ { i j }$ with pointwise mutual information (PMI) [CH90], defined as \nIf word $i$ is strongly associated with context $j$ , we will have $mathbb { P } mathbb { M } mathbb { I } ( i , j ) > 0$ . If the PMI is negative, it means $i$ and $j$ co-occur less often that if they were independent; however, such negative correlations can be unreliable, so it is common to use the positive PMI: $mathbb { P } mathbb { P } mathbb { M } [ ( i , j ) = operatorname* { m a x } ( mathbb { P } mathbb { M } mathbb { I } ( i , j ) , 0 )$ . In [BL07b], they show that SVD applied to the PPMI matrix results in word embeddings that perform well on a many tasks related to word meaning. See Section 20.5.5 for a theoretical model that explains this empirical performance. \n20.5.2 Word2vec \nIn this section, we discuss the popular word2vec model from [Mik+13a; Mik+13b], which are “shallow” neural nets for predicting a word given its context. In Section 20.5.5, we will discuss the connections with SVD of the PMI matrix. \nThere are two versions of the word2vec model. The first is called CBOW, which stands for “continuous bag of words”. The second is called skipgram. We discuss both of these below. \n20.5.2.1 Word2vec CBOW model \nIn the continuous bag of words (CBOW) model (see Figure 20.44(a)), the log likelihood of a sequence of words is computed using the following model: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nwhere ${ pmb v } _ { w _ { t } }$ is the vector for the word at location $w _ { t }$ , $nu$ is the set of all words, $m$ is the context size, and \nis the average of the word vectors in the window around word $w _ { t }$ . Thus we try to predict each word given its context. The model is called CBOW because it uses a bag of words assumption for the context, and represents each word by a continuous embedding. \n20.5.2.2 Word2vec Skip-gram model \nIn CBOW, each word is predicted from its context. A variant of this is to predict the context (surrounding words) given each word. This yields the following objective: \nwhere $m$ is the context window length. We define the log probability of some other context word $w _ { o }$ given the central word $w _ { c }$ to be \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere $nu$ is the vocabulary. Here $mathbf { delta } mathbf { u } _ { i }$ is the embedding of a word if used as context, and ${ pmb v } _ { i }$ is the embedding of a word if used as a central (target) word to be predicted. This model is known as the skipgram model. See Figure 20.44(b) for an illustration. \n20.5.2.3 Negative sampling \nComputing the conditional probability of each word using Equation (20.148) is expensive, due to the need to normalize over all possible words in the vocabulary. This makes it slow to compute the log likelihood and its gradient, for both the CBOW and skip-gram models. \nIn [Mik+13b], they propose a fast approximation, called skip-gram with negative sampling (SGNS). The basic idea is to create a set of $K + 1$ context words for each central word $w _ { t }$ , and to label the one that actually occurs as positive, and the rest as negative. The negative words are called noise words, and can be sampled from a reweighted unigram distribution, $p ( w ) propto mathrm { f r e q } ( w ) ^ { 3 / 4 }$ , which has the effect of redistributing probability mass from common to rare words. The conditional probability is now approximated by \nwhere $w _ { k } sim p ( w )$ are noise words, and $D = 1$ is the event that the word pair actually occurs in the data, and $D = 0$ is the event that the word pair does not occur. The binary probabilities are given by \nTo train this model, we just need to compute the contexts for each central word, and a set of negative noise words. We associate a label of 1 with the context words, and a label of 0 with the noise words. We can then compute the log probability of the data, and optimize the embedding vectors ${ pmb u } _ { i }$ and ${ pmb v } _ { i }$ for each word using SGD. See skipgram_jax.ipynb for some sample code. \n20.5.3 GloVE \nA popular alternative to Skipgram is the GloVe model of [PSM14a]. (GloVe stands for “global vectors for word representation”.) This method uses a simpler objective, which is much faster to optimize. \nTo explain the method, recall that in the skipgram model, the predicted conditional probability of word $j$ occuring in the context window of central word $i$ as \nLet $x _ { i j }$ be the number of times word $j$ occurs in any context window of $i$ . (Note that if word $i$ occurs in the window of $j$ , then $j$ will occur in the window of $i$ , so we have $x _ { i j } = x _ { j i }$ .) Then we can rewrite Equation (20.147) as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Word embeddings",
        "subsubsection": "Word2vec"
    },
    {
        "content": "where $nu$ is the vocabulary. Here $mathbf { delta } mathbf { u } _ { i }$ is the embedding of a word if used as context, and ${ pmb v } _ { i }$ is the embedding of a word if used as a central (target) word to be predicted. This model is known as the skipgram model. See Figure 20.44(b) for an illustration. \n20.5.2.3 Negative sampling \nComputing the conditional probability of each word using Equation (20.148) is expensive, due to the need to normalize over all possible words in the vocabulary. This makes it slow to compute the log likelihood and its gradient, for both the CBOW and skip-gram models. \nIn [Mik+13b], they propose a fast approximation, called skip-gram with negative sampling (SGNS). The basic idea is to create a set of $K + 1$ context words for each central word $w _ { t }$ , and to label the one that actually occurs as positive, and the rest as negative. The negative words are called noise words, and can be sampled from a reweighted unigram distribution, $p ( w ) propto mathrm { f r e q } ( w ) ^ { 3 / 4 }$ , which has the effect of redistributing probability mass from common to rare words. The conditional probability is now approximated by \nwhere $w _ { k } sim p ( w )$ are noise words, and $D = 1$ is the event that the word pair actually occurs in the data, and $D = 0$ is the event that the word pair does not occur. The binary probabilities are given by \nTo train this model, we just need to compute the contexts for each central word, and a set of negative noise words. We associate a label of 1 with the context words, and a label of 0 with the noise words. We can then compute the log probability of the data, and optimize the embedding vectors ${ pmb u } _ { i }$ and ${ pmb v } _ { i }$ for each word using SGD. See skipgram_jax.ipynb for some sample code. \n20.5.3 GloVE \nA popular alternative to Skipgram is the GloVe model of [PSM14a]. (GloVe stands for “global vectors for word representation”.) This method uses a simpler objective, which is much faster to optimize. \nTo explain the method, recall that in the skipgram model, the predicted conditional probability of word $j$ occuring in the context window of central word $i$ as \nLet $x _ { i j }$ be the number of times word $j$ occurs in any context window of $i$ . (Note that if word $i$ occurs in the window of $j$ , then $j$ will occur in the window of $i$ , so we have $x _ { i j } = x _ { j i }$ .) Then we can rewrite Equation (20.147) as follows: \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nIf we define $p _ { i j } = x _ { i j } / x _ { i }$ to be the empirical probability of word $j$ occuring in the context window of central word $i$ , we can rewrite the skipgram loss as a cross entropy loss: \nThe problem with this objective is that computing $q _ { i j }$ is expensive, due to the need to normalize over all words. In GloVe, we work with unnormalized probabilities, $p _ { i j } ^ { prime } = x _ { i j }$ and $q _ { i j } ^ { prime } = exp ( { boldsymbol { u } _ { j } ^ { scriptscriptstyle 1 } } boldsymbol { v } _ { i } + boldsymbol { b } _ { i } + boldsymbol { c } _ { j } )$ , where $b _ { i }$ and $c _ { j }$ are bias terms to capture marginal probabilities. In addition, we minimize the squared loss, $( log p _ { i j } ^ { prime } - log q _ { i j } ^ { prime } ) ^ { 2 }$ , which is more robust to errors in estimating small probablities than log loss. Finally, we upweight rare words for which $x _ { i j } < c$ , where $c = 1 0 0$ , by weighting the squared errors by $h ( x _ { i j } )$ , where $h ( x ) = ( x / c ) ^ { 0 . 7 5 }$ if $x < c$ , and $h ( x ) = 1$ otherwise. This gives the final GloVe objective: \nWe can precompute $x _ { i j }$ offline, and then optimize the above objective using SGD. After training, we define the embedding of word $i$ to be the average of ${ pmb v } _ { i }$ and $mathbf { delta } mathbf { u } _ { i }$ . \nEmpirically GloVe gives similar results to skigram, but it is faster to train. See Section 20.5.5 for a theoretical model that explains why these methods work. \n20.5.4 Word analogies\nOne of the most remarkable properties of word embeddings produced by word2vec, GloVe, and other similar methods is that the learned vector space seems to capture relational semantics in terms of simple vector addition. For example, consider the word analogy problem “man is to woman as king is to queen”, often written as man:woman::king:queen. Suppose we are given the words $a { = }$ man, $b$ =woman, $c { = }$ king; how do we find $d$ =queen? Let $pmb { delta } = pmb { v } _ { b } - pmb { v } _ { a }$ be the vector representing the concept of “converting the gender from male to female”. Intuitively we can find word $d$ by computing \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 ${ pmb v } _ { d } = { pmb c } + delta$ , and then finding the closest word in the vocabulary to ${ pmb v } _ { d }$ . See Figure 20.45 for an illustration of this process, and word_analogies_jax.ipynb for some code.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Word embeddings",
        "subsubsection": "GloVE"
    },
    {
        "content": "If we define $p _ { i j } = x _ { i j } / x _ { i }$ to be the empirical probability of word $j$ occuring in the context window of central word $i$ , we can rewrite the skipgram loss as a cross entropy loss: \nThe problem with this objective is that computing $q _ { i j }$ is expensive, due to the need to normalize over all words. In GloVe, we work with unnormalized probabilities, $p _ { i j } ^ { prime } = x _ { i j }$ and $q _ { i j } ^ { prime } = exp ( { boldsymbol { u } _ { j } ^ { scriptscriptstyle 1 } } boldsymbol { v } _ { i } + boldsymbol { b } _ { i } + boldsymbol { c } _ { j } )$ , where $b _ { i }$ and $c _ { j }$ are bias terms to capture marginal probabilities. In addition, we minimize the squared loss, $( log p _ { i j } ^ { prime } - log q _ { i j } ^ { prime } ) ^ { 2 }$ , which is more robust to errors in estimating small probablities than log loss. Finally, we upweight rare words for which $x _ { i j } < c$ , where $c = 1 0 0$ , by weighting the squared errors by $h ( x _ { i j } )$ , where $h ( x ) = ( x / c ) ^ { 0 . 7 5 }$ if $x < c$ , and $h ( x ) = 1$ otherwise. This gives the final GloVe objective: \nWe can precompute $x _ { i j }$ offline, and then optimize the above objective using SGD. After training, we define the embedding of word $i$ to be the average of ${ pmb v } _ { i }$ and $mathbf { delta } mathbf { u } _ { i }$ . \nEmpirically GloVe gives similar results to skigram, but it is faster to train. See Section 20.5.5 for a theoretical model that explains why these methods work. \n20.5.4 Word analogies\nOne of the most remarkable properties of word embeddings produced by word2vec, GloVe, and other similar methods is that the learned vector space seems to capture relational semantics in terms of simple vector addition. For example, consider the word analogy problem “man is to woman as king is to queen”, often written as man:woman::king:queen. Suppose we are given the words $a { = }$ man, $b$ =woman, $c { = }$ king; how do we find $d$ =queen? Let $pmb { delta } = pmb { v } _ { b } - pmb { v } _ { a }$ be the vector representing the concept of “converting the gender from male to female”. Intuitively we can find word $d$ by computing \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 ${ pmb v } _ { d } = { pmb c } + delta$ , and then finding the closest word in the vocabulary to ${ pmb v } _ { d }$ . See Figure 20.45 for an illustration of this process, and word_analogies_jax.ipynb for some code. \n\nIn [PSM14a], they conjecture that $a : b : : c : d$ holds iff for every word $w$ in the vocabulary, we have \nIn [Aro+16], they show that this follows from the RAND-WALK modeling assumptions in Section 20.5.5. See also [AH19; EDH19] for other explanations of why word analogies work, based on different modeling assumptions. \n20.5.5 RAND-WALK model of word embeddings \nWord embeddings significantly improve the performance of various kinds of NLP models compared to using one-hot encodings for words. It is natural to wonder why the above word embeddings work so well. In this section, we give a simple generative model for text documents that explains this phenomenon, based on [Aro+16]. \nConsider a sequence of words $w _ { 1 } , dots , w _ { T }$ . We assume each word is generated by a latent context or discourse vector $boldsymbol { z } _ { t } in mathbb { R } ^ { D }$ using the following log bilinear language model, similar to [MH07]: \nwhere ${ pmb v } _ { w } in mathbb { R } ^ { D }$ is the embedding for word $w$ , and $Z ( z _ { t } )$ is the partition function. We assume $D < M$ the number of words in the vocabulary. \nLet us further assume the prior for the word embeddings ${ pmb v } _ { w }$ is an isotropic Gaussian, and that the latent topic $z _ { t }$ undergoes a slow Gaussian random walk. (This is therefore called the RAND-WALK model.) Under this model, one can show that $Z ( z _ { t } )$ is approximately equal to a fixed constant, $Z$ , independent of the context. This is known as the self-normalization property of log-linear models [AK15]. Furthermore, one can show that the pointwise mutual information of predictions from the model is given by \nWe can therefore fit the RAND-WALK model by matching the model’s predicted values for PMI with the empirical values, i.e., we minimize \nwhere $X _ { w , w ^ { prime } }$ is the number of times $w$ and $w ^ { prime }$ occur next to each other. This objective can be seen as a frequency-weighted version of the SVD loss in Equation (20.138). (See [LG14] for more connections between word embeddings and SVD.) \nFurthermore, some additional approximations can be used to show that the NLL for the RANDWALK model is equivalent to the CBOW and SGNS word2vec objectives. We can also derive the objective for GloVE from this approach. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Word embeddings",
        "subsubsection": "Word analogies"
    },
    {
        "content": "In [PSM14a], they conjecture that $a : b : : c : d$ holds iff for every word $w$ in the vocabulary, we have \nIn [Aro+16], they show that this follows from the RAND-WALK modeling assumptions in Section 20.5.5. See also [AH19; EDH19] for other explanations of why word analogies work, based on different modeling assumptions. \n20.5.5 RAND-WALK model of word embeddings \nWord embeddings significantly improve the performance of various kinds of NLP models compared to using one-hot encodings for words. It is natural to wonder why the above word embeddings work so well. In this section, we give a simple generative model for text documents that explains this phenomenon, based on [Aro+16]. \nConsider a sequence of words $w _ { 1 } , dots , w _ { T }$ . We assume each word is generated by a latent context or discourse vector $boldsymbol { z } _ { t } in mathbb { R } ^ { D }$ using the following log bilinear language model, similar to [MH07]: \nwhere ${ pmb v } _ { w } in mathbb { R } ^ { D }$ is the embedding for word $w$ , and $Z ( z _ { t } )$ is the partition function. We assume $D < M$ the number of words in the vocabulary. \nLet us further assume the prior for the word embeddings ${ pmb v } _ { w }$ is an isotropic Gaussian, and that the latent topic $z _ { t }$ undergoes a slow Gaussian random walk. (This is therefore called the RAND-WALK model.) Under this model, one can show that $Z ( z _ { t } )$ is approximately equal to a fixed constant, $Z$ , independent of the context. This is known as the self-normalization property of log-linear models [AK15]. Furthermore, one can show that the pointwise mutual information of predictions from the model is given by \nWe can therefore fit the RAND-WALK model by matching the model’s predicted values for PMI with the empirical values, i.e., we minimize \nwhere $X _ { w , w ^ { prime } }$ is the number of times $w$ and $w ^ { prime }$ occur next to each other. This objective can be seen as a frequency-weighted version of the SVD loss in Equation (20.138). (See [LG14] for more connections between word embeddings and SVD.) \nFurthermore, some additional approximations can be used to show that the NLL for the RANDWALK model is equivalent to the CBOW and SGNS word2vec objectives. We can also derive the objective for GloVE from this approach. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n20.5.6 Contextual word embeddings \nConsider the sentences “I was eating an apple” and “I bought a new phone from Apple”. The meaning of the word “apple” is different in both cases, but a fixed word embedding, of the type discussed in Section 20.5, would not be able to capture this. In Section 15.7, we discuss contextual word embeddings, where the embedding of a word is a function of all the words in its context (usually a sentence). This can give much improved results, and is currently the standard approach to representing natural language data, as a pre-processing step before doing transfer learning (see Section 19.2). \n20.6 Exercises \nExercise 20.1 [EM for FA] \nDerive the EM updates for the factor analysis model. For simplicity, you can optionally assume $pmb { mu } = mathbf { 0 }$ is fixed. \nExercise 20.2 [EM for mixFA *] \nDerive the EM updates for a mixture of factor analysers. \nExercise 20.3 [Deriving the second principal component] \na. Let \nShow that $begin{array} { r } { frac { partial J } { partial z _ { 2 } } = 0 } end{array}$ yields $z _ { i 2 } = pmb { v } _ { 2 } ^ { I } pmb { x } _ { i }$ \nb. Show that the value of ${ pmb v } _ { 2 }$ that minimizes \nis given by the eigenvector of $mathbf { C }$ with the second largest eigenvalue. Hint: recall that $mathbf { C } { boldsymbol { v } } _ { 1 } = lambda _ { 1 } { boldsymbol { v } } _ { 1 }$ and $begin{array} { r } { frac { partial mathbf { x } ^ { T } mathbf { A } pmb { x } } { partial mathbf { x } } = ( mathbf { A } + mathbf { A } ^ { T } ) pmb { x } } end{array}$ . \nExercise 20.4 [Deriving the residual error for PCA *] \na. Prove that \nHint: first consider the case $K = 2$ . Use the fact that $pmb { v } _ { j } ^ { T } pmb { v } _ { j } = 1$ and ${ pmb v } _ { j } ^ { prime } { pmb v } _ { k } = 0$ for $k neq j$ . Also, recall $z _ { i j } = pmb { x } _ { i } ^ { T } pmb { v } _ { j }$ . \nb. Now show that \nHint: recall vjT Cvj = λjvjT vj = λj. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Word embeddings",
        "subsubsection": "RAND-WALK model of word embeddings"
    },
    {
        "content": "20.5.6 Contextual word embeddings \nConsider the sentences “I was eating an apple” and “I bought a new phone from Apple”. The meaning of the word “apple” is different in both cases, but a fixed word embedding, of the type discussed in Section 20.5, would not be able to capture this. In Section 15.7, we discuss contextual word embeddings, where the embedding of a word is a function of all the words in its context (usually a sentence). This can give much improved results, and is currently the standard approach to representing natural language data, as a pre-processing step before doing transfer learning (see Section 19.2). \n20.6 Exercises \nExercise 20.1 [EM for FA] \nDerive the EM updates for the factor analysis model. For simplicity, you can optionally assume $pmb { mu } = mathbf { 0 }$ is fixed. \nExercise 20.2 [EM for mixFA *] \nDerive the EM updates for a mixture of factor analysers. \nExercise 20.3 [Deriving the second principal component] \na. Let \nShow that $begin{array} { r } { frac { partial J } { partial z _ { 2 } } = 0 } end{array}$ yields $z _ { i 2 } = pmb { v } _ { 2 } ^ { I } pmb { x } _ { i }$ \nb. Show that the value of ${ pmb v } _ { 2 }$ that minimizes \nis given by the eigenvector of $mathbf { C }$ with the second largest eigenvalue. Hint: recall that $mathbf { C } { boldsymbol { v } } _ { 1 } = lambda _ { 1 } { boldsymbol { v } } _ { 1 }$ and $begin{array} { r } { frac { partial mathbf { x } ^ { T } mathbf { A } pmb { x } } { partial mathbf { x } } = ( mathbf { A } + mathbf { A } ^ { T } ) pmb { x } } end{array}$ . \nExercise 20.4 [Deriving the residual error for PCA *] \na. Prove that \nHint: first consider the case $K = 2$ . Use the fact that $pmb { v } _ { j } ^ { T } pmb { v } _ { j } = 1$ and ${ pmb v } _ { j } ^ { prime } { pmb v } _ { k } = 0$ for $k neq j$ . Also, recall $z _ { i j } = pmb { x } _ { i } ^ { T } pmb { v } _ { j }$ . \nb. Now show that \nHint: recall vjT Cvj = λjvjT vj = λj. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Word embeddings",
        "subsubsection": "Contextual word embeddings"
    },
    {
        "content": "20.5.6 Contextual word embeddings \nConsider the sentences “I was eating an apple” and “I bought a new phone from Apple”. The meaning of the word “apple” is different in both cases, but a fixed word embedding, of the type discussed in Section 20.5, would not be able to capture this. In Section 15.7, we discuss contextual word embeddings, where the embedding of a word is a function of all the words in its context (usually a sentence). This can give much improved results, and is currently the standard approach to representing natural language data, as a pre-processing step before doing transfer learning (see Section 19.2). \n20.6 Exercises \nExercise 20.1 [EM for FA] \nDerive the EM updates for the factor analysis model. For simplicity, you can optionally assume $pmb { mu } = mathbf { 0 }$ is fixed. \nExercise 20.2 [EM for mixFA *] \nDerive the EM updates for a mixture of factor analysers. \nExercise 20.3 [Deriving the second principal component] \na. Let \nShow that $begin{array} { r } { frac { partial J } { partial z _ { 2 } } = 0 } end{array}$ yields $z _ { i 2 } = pmb { v } _ { 2 } ^ { I } pmb { x } _ { i }$ \nb. Show that the value of ${ pmb v } _ { 2 }$ that minimizes \nis given by the eigenvector of $mathbf { C }$ with the second largest eigenvalue. Hint: recall that $mathbf { C } { boldsymbol { v } } _ { 1 } = lambda _ { 1 } { boldsymbol { v } } _ { 1 }$ and $begin{array} { r } { frac { partial mathbf { x } ^ { T } mathbf { A } pmb { x } } { partial mathbf { x } } = ( mathbf { A } + mathbf { A } ^ { T } ) pmb { x } } end{array}$ . \nExercise 20.4 [Deriving the residual error for PCA *] \na. Prove that \nHint: first consider the case $K = 2$ . Use the fact that $pmb { v } _ { j } ^ { T } pmb { v } _ { j } = 1$ and ${ pmb v } _ { j } ^ { prime } { pmb v } _ { k } = 0$ for $k neq j$ . Also, recall $z _ { i j } = pmb { x } _ { i } ^ { T } pmb { v } _ { j }$ . \nb. Now show that \nHint: recall vjT Cvj = λjvjT vj = λj. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nc. If $K = d$ there is no truncation, so $J _ { d } = 0$ . Use this to show that the error from only using $K < d$ terms is given by \nHint: partition the sum $textstyle sum _ { j = 1 } ^ { d } lambda _ { j }$ into $textstyle sum _ { j = 1 } ^ { K } lambda _ { j }$ and $textstyle sum _ { j = K + 1 } ^ { d } lambda _ { j }$ . \nExercise 20.5 [PCA via successive deflation] \nLet $pmb { v } _ { 1 } , pmb { v } _ { 2 } , ldots , pmb { v } _ { k }$ be the first $k$ eigenvectors with largest eigenvalues of $begin{array} { r } { mathbf { C } = frac { 1 } { n } mathbf { X } ^ { T } mathbf { X } } end{array}$ , i.e., the principal basis vectors. These satisfy \nWe will construct a method for finding the ${ pmb v } _ { j }$ sequentially. \nAs we showed in class, $_ { v _ { 1 } }$ is the first principal eigenvector of $mathbf { C }$ , and satisfies $mathbf { C } { boldsymbol { v } } _ { 1 } = lambda _ { 1 } { boldsymbol { v } } _ { 1 }$ . Now define $tilde { mathbf { x } } _ { i }$ as the orthogonal projection of $pmb { x } _ { i }$ onto the space orthogonal to ${ pmb v } _ { 1 }$ : \nDefine $tilde { mathbf { X } } = [ tilde { pmb { x } } _ { 1 } ; . . . ; tilde { pmb { x } } _ { n } ]$ as the deflated matrix of rank $d - 1$ , which is obtained by removing from the $d$ dimensional data the component that lies in the direction of the first principal direction: \na. Using the facts that $mathbf { X } ^ { T } mathbf { X } pmb { v } _ { 1 } = n lambda _ { 1 } pmb { v } _ { 1 }$ (and hence $pmb { v } _ { 1 } ^ { T } mathbf { X } ^ { T } mathbf { X } = n lambda _ { 1 } pmb { v } _ { 1 } ^ { T }$ ) and ${ pmb v } _ { 1 } ^ { T } { pmb v } _ { 1 } = 1$ , show that the covariance of the deflated matrix is given by \nb. Let $textbf { em u }$ be the principal eigenvector of $tilde { mathbf { C } }$ . Explain why ${ pmb u } = { pmb v } _ { 2 }$ . (You may assume $mathbf { Delta } ^ { mathbf { u } }$ is unit norm.) \nc. Suppose we have a simple method for finding the leading eigenvector and eigenvalue of a pd matrix, denoted by $[ lambda , boldsymbol { mathbf { u } } ] = f ( boldsymbol { mathbf { C } } )$ . Write some pseudo code for finding the first $K$ principal basis vectors of $mathbf { X }$ that only uses the special $f$ function and simple vector arithmetic, i.e., your code should not use SVD or the eig function. Hint: this should be a simple iterative routine that takes 2–3 lines to write. The input is $mathbf { C }$ , $K$ and the function $f$ , the output should be ${ pmb v } _ { j }$ and $lambda _ { j }$ for $j = 1 : K$ . \nExercise 20.6 [PPCA variance terms] \nRecall that in the PPCA model, $mathbf { C } = mathbf { W } mathbf { W } ^ { T } + sigma ^ { 2 } mathbf { I }$ . We will show that this model correctly captures the variance of the data along the principal axes, and approximates the variance in all the remaining directions with a single average value $sigma ^ { 2 }$ . \nConsider the variance of the predictive distribution $p ( { pmb x } )$ along some direction specified by the unit vector $_ { v }$ , where $pmb { v } ^ { T } pmb { v } = 1$ , which is given by $boldsymbol { v } ^ { prime } boldsymbol { C } boldsymbol { v }$ . \na. First suppose $_ v$ is orthogonal to the principal subspace. and hence $pmb { v } ^ { T } mathbf { U } = mathbf { 0 }$ . Show that ${ pmb v } ^ { T } { bf C } { pmb v } = sigma ^ { 2 }$ . b. Now suppose $_ v$ is parallel to the principal subspace. and hence $mathbf { nabla } pmb { v } = mathbf { nabla } pmb { u } _ { i }$ for some eigenvector ${ bf { u } } _ { i }$ . Show that $v ^ { T } mathbf { C } v = ( lambda _ { i } - sigma ^ { 2 } ) + sigma ^ { 2 } = lambda _ { i }$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nExercise 20.7 [Posterior inference in PPCA *] Derive $p ( z _ { n } | pmb { x } _ { n } )$ for the PPCA model. \nExercise 20.8 [Imputation in a FA model *] Derive an expression for $p ( pmb { x } _ { h } | pmb { x } _ { v } , pmb { theta } )$ for a FA model, where ${ pmb x } = ( { pmb x } _ { h } , { pmb x } _ { v } )$ is a partition of the data vector. \nExercise 20.9 [Efficiently evaluating the PPCA density] \nDerive an expression for $p ( mathbf { boldsymbol { x } } | hat { mathbf { W } } , hat { boldsymbol { sigma } } ^ { 2 } )$ for the PPCA model based on plugging in the MLEs and using the matrix inversion lemma. \n21 Clustering \n21.1 Introduction \nClustering is a very common form of unsupervised learning. There are two main kinds of methods. In the first approach, the input is a set of data samples $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ , where $pmb { x } _ { n } in mathcal { X }$ , where typically $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ . In the second approach, the input is an $N times N$ pairwise dissimilarity metric $D _ { i j } geq 0$ . In both cases, the goal is to assign similar data points to the same cluster. \nAs is often the case with unsupervised learning, it is hard to evaluate the quality of a clustering algorithm. If we have labeled data for some of the data, we can use the similarity (or equality) between the labels of two data points as a metric for determining if the two inputs “should” be assigned to the same cluster or not. If we don’t have labels, but the method is based on a generative model of the data, we can use log likelihood as a metric. We will see examples of both approaches below. \n21.1.1 Evaluating the output of clustering methods \nThe validation of clustering structures is the most difficult and frustrating part of cluster analysis. Without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage. — Jain and Dubes [JD88] \nClustering is an unsupervised learning technique, so it is hard to evaluate the quality of the output of any given method [Kle02; LWG12]. If we use probabilistic models, we can always evaluate the likelihood of the data, but this has two drawbacks: first, it does not directly assess any clustering that is discovered by the model; and second, it does not apply to non-probabilistic methods. So now we discuss some performance measures not based on likelihood. \nIntuitively, the goal of clustering is to assign points that are similar to the same cluster, and to ensure that points that are dissimilar are in different clusters. There are several ways of measuring these quantities e.g., see [JD88; KR90]. However, these internal criteria may be of limited use. An alternative is to rely on some external form of data with which to validate the method. For example, if we have labels for each object, then we can assume that objects with the same label are similar. We can then use the metrics we discuss below to quantify the quality of the clusters. (If we do not have labels, but we have a reference clustering, we can derive labels from that clustering.)",
        "chapter": "V Beyond Supervised Learning",
        "section": "Dimensionality Reduction",
        "subsection": "Exercises",
        "subsubsection": "N/A"
    },
    {
        "content": "21 Clustering \n21.1 Introduction \nClustering is a very common form of unsupervised learning. There are two main kinds of methods. In the first approach, the input is a set of data samples $mathcal { D } = { pmb { x } _ { n } : n = 1 : N }$ , where $pmb { x } _ { n } in mathcal { X }$ , where typically $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ . In the second approach, the input is an $N times N$ pairwise dissimilarity metric $D _ { i j } geq 0$ . In both cases, the goal is to assign similar data points to the same cluster. \nAs is often the case with unsupervised learning, it is hard to evaluate the quality of a clustering algorithm. If we have labeled data for some of the data, we can use the similarity (or equality) between the labels of two data points as a metric for determining if the two inputs “should” be assigned to the same cluster or not. If we don’t have labels, but the method is based on a generative model of the data, we can use log likelihood as a metric. We will see examples of both approaches below. \n21.1.1 Evaluating the output of clustering methods \nThe validation of clustering structures is the most difficult and frustrating part of cluster analysis. Without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage. — Jain and Dubes [JD88] \nClustering is an unsupervised learning technique, so it is hard to evaluate the quality of the output of any given method [Kle02; LWG12]. If we use probabilistic models, we can always evaluate the likelihood of the data, but this has two drawbacks: first, it does not directly assess any clustering that is discovered by the model; and second, it does not apply to non-probabilistic methods. So now we discuss some performance measures not based on likelihood. \nIntuitively, the goal of clustering is to assign points that are similar to the same cluster, and to ensure that points that are dissimilar are in different clusters. There are several ways of measuring these quantities e.g., see [JD88; KR90]. However, these internal criteria may be of limited use. An alternative is to rely on some external form of data with which to validate the method. For example, if we have labels for each object, then we can assume that objects with the same label are similar. We can then use the metrics we discuss below to quantify the quality of the clusters. (If we do not have labels, but we have a reference clustering, we can derive labels from that clustering.) \nAAA ABB AA AAB BBC ccc \n21.1.1.1 Purity \nLet $N _ { i j }$ be the number of objects in cluster $i$ that belong to class $j$ , and let $begin{array} { r } { N _ { i } = sum _ { j = 1 } ^ { C } N _ { i j } } end{array}$ be the total number of objects in cluster $i$ . Define $p _ { i j } = N _ { i j } / N _ { i }$ ; this is the empirical distr bution over class labels for cluster $i$ . We define the purity of a cluster as $p _ { i } triangleq operatorname* { m a x } _ { j } p _ { i j }$ , and the overall purity of a clustering as \nFor example, in Figure 21.1, we have that the purity is \nThe purity ranges between 0 (bad) and 1 (good). However, we can trivially achieve a purity of $^ { 1 }$ by putting each object into its own cluster, so this measure does not penalize for the number of clusters. \n21.1.1.2 Rand index \nLet $U = { u _ { 1 } , ldots , u _ { R } }$ and $V = { v _ { 1 } , ldots , v _ { C } }$ be two different partitions of the $N$ data points. For example, $U$ might be the estimated clustering and $V$ is reference clustering derived from the class labels. Now define a $2 times 2$ contingency table, containing the following numbers: $T P$ is the number of pairs that are in the same cluster in both $U$ and $V$ (true positives); $T N$ is the number of pairs that are in the different clusters in both $U$ and $V$ (true negatives); $F N$ is the number of pairs that are in the different clusters in $U$ but the same cluster in $V$ (false negatives); and $F P$ is the number of pairs that are in the same cluster in $U$ but different clusters in $V$ (false positives). A common summary statistic is the Rand index: \nThis can be interpreted as the fraction of clustering decisions that are correct. Clearly $0 leq R leq 1$ . For example, consider Figure 21.1, The three clusters contain 6, 6 and 5 points, so the number of “positives” (i.e., pairs of objects put in the same cluster, regardless of label) is \nOf these, the number of true positives is given by \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nwhere the last two terms come from cluster 3: there are $binom { 3 } { 2 }$ pairs labeled $C$ and $binom { 2 } { 2 }$ pairs labeled $A$ . So $F P = 4 0 - 2 0 = 2 0$ . Similarly, one can show $F N = 2 4$ and $T N = 7 2$ . So the Rand index is $( 2 0 + 7 2 ) / ( 2 0 + 2 0 + 2 4 + 7 2 ) = 0 . 6 8$ . \nThe Rand index only achieves its lower bound of $0$ if $T P = T N = 0$ , which is a rare event. One can define an adjusted Rand index [HA85] as follows: \nHere the model of randomness is based on using the generalized hyper-geometric distribution, i.e., the two partitions are picked at random subject to having the original number of classes and objects in each, and then the expected value of $T P + T N$ is computed. This model can be used to compute the statistical significance of the Rand index. \nThe Rand index weights false positives and false negatives equally. Various other summary statistics for binary decision problems, such as the F-score (Section 5.1.4), can also be used. \n21.1.1.3 Mutual information \nAnother way to measure cluster quality is to compute the mutual information between two candidate partitions $U$ and $V$ , as proposed in [VD99]. To do this, let $begin{array} { r } { p _ { U V } ( i , j ) = frac { | u _ { i } cap v _ { j } | } { N } } end{array}$ be the probability that a randomly chosen object belongs to cluster $u _ { i }$ in $U$ and $v _ { j }$ in $V$ . Also, let $p _ { U } ( i ) = | u _ { i } | / N$ be the be the probability that a randomly chosen object belongs to cluster $u _ { i }$ in $U$ ; define $p _ { V } ( j ) = | v _ { j } | / N$ similarly. Then we have \nThis lies between 0 and $operatorname* { m i n } { mathbb { H } left( U right) , mathbb { H } left( V right) }$ . Unfortunately, the maximum value can be achieved by using lots of small clusters, which have low entropy. To compensate for this, we can use the normalized mutual information, \nThis lies between 0 and 1. A version of this that is adjusted for chance (under a particular random data model) is described in [VEB09]. Another variant, called variation of information, is described in [Mei05]. \n21.2 Hierarchical agglomerative clustering \nA common form of clustering is known as hierarchical agglomerative clustering or HAC. The input to the algorithm is an $N times N$ dissimilarity matrix $D _ { i j } geq 0$ , and the output is a tree structure in which groups $i$ and $j$ with small disimilarity are grouped together in a hierarchical fashion. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Introduction",
        "subsubsection": "Evaluating the output of clustering methods"
    },
    {
        "content": "For example, consider the set of 5 inputs points in Figure 21.2(a), $pmb { x } _ { n } in mathbb { R } ^ { 2 }$ . We will use city block distance between the points to define the dissimilarity, i.e., \nWe start with a tree with $N$ leaves, each corresponding to a cluster with a single data point. Next we compute the pair of points that are closest, and merge them. We see that (1,3) and (4,5) are both distance 1 apart, so they get merged first. We then measure the dissimilarity between the sets ${ 1 , 3 }$ , ${ 4 , 5 }$ and ${ 2 }$ using some measure (details below), and group them, and repeat. The result is a binary tree known as a dendogram, as shown in Figure 21.2(b). By cutting this tree at different heights, we can induce a different number of (nested) clusters. We give more details below. \n21.2.1 The algorithm \nAgglomerative clustering starts with $N$ groups, each initially containing one object, and then at each step it merges the two most similar groups until there is a single group, containing all the data. See Algorithm 11 for the pseudocode. Since picking the two most similar clusters to merge takes $O ( N ^ { 2 } )$ time, and there are $O ( N )$ steps in the algorithm, the total running time is $O ( N ^ { 3 } )$ . However, by using a priority queue, this can be reduced to $O ( N ^ { 2 } log N )$ (see e.g., [MRS08, ch. 17] for details). \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nAlgorithm 11: Agglomerative clustering \n1 Initialize clusters as singletons: for $i gets 1$ to $n$ do $C _ { i } gets { i }$   \n2   \n3 Initialize set of clusters available for merging: $S gets { 1 , ldots , n }$ ; repeat   \n4 Pick 2 most similar clusters to merge: $( j , k ) gets arg operatorname* { m i n } _ { j , k in S } d _ { j , k }$   \n5 Create new cluster $C _ { ell } gets C _ { j } cup C _ { k }$   \n6 Mark $j$ and $k$ as unavailable: $S gets S setminus { j , k }$   \n7 if $C _ { ell } neq { 1 , ldots , n }$ then   \n8 Mark $ell$ as available, $S gets S cup { ell }$   \n9 foreach $i in S$ do   \n10 Update dissimilarity matrix $d ( i , ell )$ \n11 until no more clusters are available for merging \nThere are actually three variants of agglomerative clustering, depending on how we define the dissimilarity between groups of objects. We give the details below. \n21.2.1.1 Single link \nIn single link clustering, also called nearest neighbor clustering, the distance between two groups $G$ and $H$ is defined as the distance between the two closest members of each group: \nSee Figure 21.3(a). \nThe tree built using single link clustering is a minimum spanning tree of the data, which is a tree that connects all the objects in a way that minimizes the sum of the edge weights (distances). To see this, note that when we merge two clusters, we connect together the two closest members of the clusters; this adds an edge between the corresponding nodes, and this is guaranteed to be the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license “lightest weight” edge joining these two clusters. And once two clusters have been merged, they will never be considered again, so we cannot create cycles. As a consequence of this, we can actually implement single link clustering in $O ( N ^ { 2 } )$ time, whereas the other variants take $O ( N ^ { 3 } )$ time. \n\n21.2.1.2 Complete link \nIn complete link clustering, also called furthest neighbor clustering, the distance between two groups is defined as the distance between the two most distant pairs: \nSee Figure 21.3(b). \nSingle linkage only requires that a single pair of objects be close for the two groups to be considered close together, regardless of the similarity of the other members of the group. Thus clusters can be formed that violate the compactness property, which says that all the observations within a group should be similar to each other. In particular if we define the diameter of a group as the largest dissimilarity of its members, $d _ { G } = operatorname* { m a x } _ { i in G , i ^ { prime } in G } d _ { i , i ^ { prime } }$ , then we can see that single linkage can produce clusters with large diameters. Complete linkage represents the opposite extreme: two groups are considered close only if all of the observations in their union are relatively similar. This will tend to produce clusterings with small diameter, i.e., compact clusters. (Compare Figure 21.4(a) with Figure 21.4(b).) \n21.2.1.3 Average link \nIn practice, the preferred method is average link clustering, which measures the average distance between all pairs: \nwhere $n _ { G }$ and $n _ { H }$ are the number of elements in groups $G$ and $H$ . See Figure 21.3(c). \nAverage link clustering represents a compromise between single and complete link clustering. It tends to produce relatively compact clusters that are relatively far apart. (See Figure 21.4(c).) However, since it involves averaging of the $d _ { i , i ^ { prime } }$ ’s, any change to the measurement scale can change the result. In contrast, single linkage and complete linkage are invariant to monotonic transformations of $d _ { i , i ^ { prime } }$ , since they leave the relative ordering the same. \n21.2.2 Example \nSuppose we have a set of time series measurements of the expression levels for $N = 3 0 0$ genes at $T = 7$ points. Thus each data sample is a vector $mathbf { boldsymbol { x } } _ { n } in mathbb { R } ^ { 7 }$ . See Figure 21.5 for a visualization of the data. We see that there are several kinds of genes, such as those whose expression level goes up monotonically over time (in response to a given stimulus), those whose expression level goes down monotonically, and those with more complex response patterns. \nSuppose we use Euclidean distance to compute a pairwise dissimilarity matrix, $ { mathbf { D } } in mathbb { R } ^ { 3 0 0 times 3 0 0 }$ , and apply HAC using average linkage. We get the dendogram in Figure 21.6(a). If we cut the tree at \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 a certain height, we get the 16 clusters shown in Figure 21.6(b). The time series assigned to each cluster do indeed “look like” each other.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Hierarchical agglomerative clustering",
        "subsubsection": "The algorithm"
    },
    {
        "content": "21.2.1.2 Complete link \nIn complete link clustering, also called furthest neighbor clustering, the distance between two groups is defined as the distance between the two most distant pairs: \nSee Figure 21.3(b). \nSingle linkage only requires that a single pair of objects be close for the two groups to be considered close together, regardless of the similarity of the other members of the group. Thus clusters can be formed that violate the compactness property, which says that all the observations within a group should be similar to each other. In particular if we define the diameter of a group as the largest dissimilarity of its members, $d _ { G } = operatorname* { m a x } _ { i in G , i ^ { prime } in G } d _ { i , i ^ { prime } }$ , then we can see that single linkage can produce clusters with large diameters. Complete linkage represents the opposite extreme: two groups are considered close only if all of the observations in their union are relatively similar. This will tend to produce clusterings with small diameter, i.e., compact clusters. (Compare Figure 21.4(a) with Figure 21.4(b).) \n21.2.1.3 Average link \nIn practice, the preferred method is average link clustering, which measures the average distance between all pairs: \nwhere $n _ { G }$ and $n _ { H }$ are the number of elements in groups $G$ and $H$ . See Figure 21.3(c). \nAverage link clustering represents a compromise between single and complete link clustering. It tends to produce relatively compact clusters that are relatively far apart. (See Figure 21.4(c).) However, since it involves averaging of the $d _ { i , i ^ { prime } }$ ’s, any change to the measurement scale can change the result. In contrast, single linkage and complete linkage are invariant to monotonic transformations of $d _ { i , i ^ { prime } }$ , since they leave the relative ordering the same. \n21.2.2 Example \nSuppose we have a set of time series measurements of the expression levels for $N = 3 0 0$ genes at $T = 7$ points. Thus each data sample is a vector $mathbf { boldsymbol { x } } _ { n } in mathbb { R } ^ { 7 }$ . See Figure 21.5 for a visualization of the data. We see that there are several kinds of genes, such as those whose expression level goes up monotonically over time (in response to a given stimulus), those whose expression level goes down monotonically, and those with more complex response patterns. \nSuppose we use Euclidean distance to compute a pairwise dissimilarity matrix, $ { mathbf { D } } in mathbb { R } ^ { 3 0 0 times 3 0 0 }$ , and apply HAC using average linkage. We get the dendogram in Figure 21.6(a). If we cut the tree at \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 a certain height, we get the 16 clusters shown in Figure 21.6(b). The time series assigned to each cluster do indeed “look like” each other. \n\n21.2.3 Extensions \nThere are many extensions to the basic HAC algorithm. For example, [Mon+21] present a more scalable version of the bottom up algorithm that builds sub-clusters in parallel. And g [Mon+19] discusses an online version of the algorithm, that can cluster data as it arrives, while reconsidering \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license previous clustering decisions (as opposed to only making greedy decisions). Under certain assumptions, this can provably recover the true underlying structure. This can be useful for clustering “mentions” of “entities” (such as people or things) in streaming text data. (This problem is called entity discovery.)",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Hierarchical agglomerative clustering",
        "subsubsection": "Example"
    },
    {
        "content": "21.2.3 Extensions \nThere are many extensions to the basic HAC algorithm. For example, [Mon+21] present a more scalable version of the bottom up algorithm that builds sub-clusters in parallel. And g [Mon+19] discusses an online version of the algorithm, that can cluster data as it arrives, while reconsidering \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license previous clustering decisions (as opposed to only making greedy decisions). Under certain assumptions, this can provably recover the true underlying structure. This can be useful for clustering “mentions” of “entities” (such as people or things) in streaming text data. (This problem is called entity discovery.) \n\n21.3 K means clustering \nThere are several problems with hierarchical agglomerative clustering (Section 21.2). First, it takes $O ( N ^ { 3 } )$ time (for the average link method), making it hard to apply to big datasets. Second, it assumes that a dissimilarity matrix has already been computed, whereas the notion of “similarity” is often unclear and needs to be learned. Third, it is just an algorithm, not a model, and so it is hard to evaluate how good it is. That is, there is no clear objective that it is optimizing. \nIn this section, we discuss the $mathbf { K }$ -means algorithm [Mac67; Llo82], which addresses these issues. First, it runs in $O ( N K T )$ time, where $T$ is the number of iterations. Second, it computes similarity in terms of Euclidean distance to learned cluster centers $pmb { mu } _ { k } in mathbb { R } ^ { D }$ , rather than requiring a dissimilarity matrix. Third, it optimizes a well-defined cost function, as we will see. \n21.3.1 The algorithm \nWe assume there are $K$ cluster centers $pmb { mu } _ { k } in mathbb { R } ^ { D }$ , so we can cluster the data by assigning each data point $pmb { x } _ { n } in mathbb { R } ^ { D }$ to it closest center: \nOf course, we don’t know the cluster centers, but we can estimate them by computing the average value of all points assigned to them: \nWe can then iterate these steps to convergence. \nMore formally, we can view this as finding a local minimum of the following cost function, known as the distortion: \nwhere $mathbf { X } in mathbb { R } ^ { N times D }$ , $mathbf { Z } in [ 0 , 1 ] ^ { N times K }$ , and $mathbf { M } in mathbb { R } ^ { D times K }$ contains the cluster centers $pmb { mu } _ { k }$ in its columns. K-means optimizes this using alternating minimization. (This is closely related to the EM algorithm for GMMs, as we discuss in Section 21.4.1.1.) \n21.3.2 Examples \nIn this section, we give some examples of K-means clustering. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Hierarchical agglomerative clustering",
        "subsubsection": "Extensions"
    },
    {
        "content": "21.3 K means clustering \nThere are several problems with hierarchical agglomerative clustering (Section 21.2). First, it takes $O ( N ^ { 3 } )$ time (for the average link method), making it hard to apply to big datasets. Second, it assumes that a dissimilarity matrix has already been computed, whereas the notion of “similarity” is often unclear and needs to be learned. Third, it is just an algorithm, not a model, and so it is hard to evaluate how good it is. That is, there is no clear objective that it is optimizing. \nIn this section, we discuss the $mathbf { K }$ -means algorithm [Mac67; Llo82], which addresses these issues. First, it runs in $O ( N K T )$ time, where $T$ is the number of iterations. Second, it computes similarity in terms of Euclidean distance to learned cluster centers $pmb { mu } _ { k } in mathbb { R } ^ { D }$ , rather than requiring a dissimilarity matrix. Third, it optimizes a well-defined cost function, as we will see. \n21.3.1 The algorithm \nWe assume there are $K$ cluster centers $pmb { mu } _ { k } in mathbb { R } ^ { D }$ , so we can cluster the data by assigning each data point $pmb { x } _ { n } in mathbb { R } ^ { D }$ to it closest center: \nOf course, we don’t know the cluster centers, but we can estimate them by computing the average value of all points assigned to them: \nWe can then iterate these steps to convergence. \nMore formally, we can view this as finding a local minimum of the following cost function, known as the distortion: \nwhere $mathbf { X } in mathbb { R } ^ { N times D }$ , $mathbf { Z } in [ 0 , 1 ] ^ { N times K }$ , and $mathbf { M } in mathbb { R } ^ { D times K }$ contains the cluster centers $pmb { mu } _ { k }$ in its columns. K-means optimizes this using alternating minimization. (This is closely related to the EM algorithm for GMMs, as we discuss in Section 21.4.1.1.) \n21.3.2 Examples \nIn this section, we give some examples of K-means clustering. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "The algorithm"
    },
    {
        "content": "21.3 K means clustering \nThere are several problems with hierarchical agglomerative clustering (Section 21.2). First, it takes $O ( N ^ { 3 } )$ time (for the average link method), making it hard to apply to big datasets. Second, it assumes that a dissimilarity matrix has already been computed, whereas the notion of “similarity” is often unclear and needs to be learned. Third, it is just an algorithm, not a model, and so it is hard to evaluate how good it is. That is, there is no clear objective that it is optimizing. \nIn this section, we discuss the $mathbf { K }$ -means algorithm [Mac67; Llo82], which addresses these issues. First, it runs in $O ( N K T )$ time, where $T$ is the number of iterations. Second, it computes similarity in terms of Euclidean distance to learned cluster centers $pmb { mu } _ { k } in mathbb { R } ^ { D }$ , rather than requiring a dissimilarity matrix. Third, it optimizes a well-defined cost function, as we will see. \n21.3.1 The algorithm \nWe assume there are $K$ cluster centers $pmb { mu } _ { k } in mathbb { R } ^ { D }$ , so we can cluster the data by assigning each data point $pmb { x } _ { n } in mathbb { R } ^ { D }$ to it closest center: \nOf course, we don’t know the cluster centers, but we can estimate them by computing the average value of all points assigned to them: \nWe can then iterate these steps to convergence. \nMore formally, we can view this as finding a local minimum of the following cost function, known as the distortion: \nwhere $mathbf { X } in mathbb { R } ^ { N times D }$ , $mathbf { Z } in [ 0 , 1 ] ^ { N times K }$ , and $mathbf { M } in mathbb { R } ^ { D times K }$ contains the cluster centers $pmb { mu } _ { k }$ in its columns. K-means optimizes this using alternating minimization. (This is closely related to the EM algorithm for GMMs, as we discuss in Section 21.4.1.1.) \n21.3.2 Examples \nIn this section, we give some examples of K-means clustering. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n21.3.2.1 Clustering points in the 2d plane \nFigure 21.7 gives an illustration of K-means clustering applied to some points in the 2d plane. We see that the method induces a Voronoi tessellation of the points. The resulting clustering is sensitive to the initialization. Indeed, we see that the lower quality clustering on the right has higher distortion. By default, sklearn uses 10 random restarts (combined with the K-means $^ { + + }$ initialization described in Section 21.3.4) and returns the clustering with lowest distortion. (In sklearn, the distortion is called the “inertia”.) \n21.3.2.2 Clustering gene expression time series data from yeast cells \nIn Figure 21.8, we show the result of applying K-means clustering with $K = 1 6$ to the $3 0 0 times 7$ yeast time series matrix shown in Figure 21.5. We see that time series that “look similar” to each other are \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license assigned to the same cluster. We also see that the centroid of each cluster is a reasonabe summary all the data points assigned to that cluster. Finally we notice that group 6 was not used, since no points were assigned to it. However, this is just an accident of the initialization process, and we are not guaranteed to get the same clustering, or number of clusters, if we repeat the algorithm. (We discuss good ways to initialize the method in Section 21.3.4, and ways to choose $K$ in Section 21.3.7.) \n\n21.3.3 Vector quantization \nSuppose we want to perform lossy compression of some real-valued vectors, $pmb { x } _ { n } in mathbb { R } ^ { D }$ . A very simple approach to this is to use vector quantization or VQ. The basic idea is to replace each real-valued vector $pmb { x } _ { n } in mathbb { R } ^ { D }$ with a discrete symbol $z _ { n } in { 1 , ldots , K }$ , which is an index into a codebook of $K$ prototypes, $pmb { mu } _ { k } in mathbb { R } ^ { D }$ . Each data vector is encoded by using the index of the most similar prototype, where similarity is measured in terms of Euclidean distance: \nWe can define a cost function that measures the quality of a codebook by computing the reconstruction error or distortion it induces: \nwhere $operatorname* { d e c o d e } ( k ) = pmb { mu } _ { k }$ . This is exactly the cost function that is minimized by the K-means algorithm. Of course, we can achieve zero distortion if we assign one prototype to every data vector, by using   \n$K = N$ and assigning . However, this does not compress the data at all. In particular, it ${ pmb { mu } } _ { n } = { pmb x } _ { n }$   \ntakes $O ( N D B )$ bits, where $N$ is the number of real-valued data vectors, each of length $D$ , and $B$ is   \nthe number of bits needed to represent a real-valued scalar (the quantization accuracy to represent   \neach ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ ). \nWe can do better by detecting similar vectors in the data, creating prototypes or centroids for them, and then representing the data as deviations from these prototypes. This reduces the space \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 requirement to $O ( N log _ { 2 } K + K D B )$ bits. The $O ( N log _ { 2 } K )$ term arises because each of the $N$ data vectors needs to specify which of the $K$ codewords it is using; and the $O ( K D B )$ term arises because we have to store each codebook entry, each of which is a $D$ -dimensional vector. When $N$ is large, the first term dominates the second, so we can approximate the rate of the encoding scheme (number of bits needed per object) as $O ( log _ { 2 } K )$ , which is typically much less than $O ( D B )$ .",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "Examples"
    },
    {
        "content": "21.3.3 Vector quantization \nSuppose we want to perform lossy compression of some real-valued vectors, $pmb { x } _ { n } in mathbb { R } ^ { D }$ . A very simple approach to this is to use vector quantization or VQ. The basic idea is to replace each real-valued vector $pmb { x } _ { n } in mathbb { R } ^ { D }$ with a discrete symbol $z _ { n } in { 1 , ldots , K }$ , which is an index into a codebook of $K$ prototypes, $pmb { mu } _ { k } in mathbb { R } ^ { D }$ . Each data vector is encoded by using the index of the most similar prototype, where similarity is measured in terms of Euclidean distance: \nWe can define a cost function that measures the quality of a codebook by computing the reconstruction error or distortion it induces: \nwhere $operatorname* { d e c o d e } ( k ) = pmb { mu } _ { k }$ . This is exactly the cost function that is minimized by the K-means algorithm. Of course, we can achieve zero distortion if we assign one prototype to every data vector, by using   \n$K = N$ and assigning . However, this does not compress the data at all. In particular, it ${ pmb { mu } } _ { n } = { pmb x } _ { n }$   \ntakes $O ( N D B )$ bits, where $N$ is the number of real-valued data vectors, each of length $D$ , and $B$ is   \nthe number of bits needed to represent a real-valued scalar (the quantization accuracy to represent   \neach ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ ). \nWe can do better by detecting similar vectors in the data, creating prototypes or centroids for them, and then representing the data as deviations from these prototypes. This reduces the space \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 requirement to $O ( N log _ { 2 } K + K D B )$ bits. The $O ( N log _ { 2 } K )$ term arises because each of the $N$ data vectors needs to specify which of the $K$ codewords it is using; and the $O ( K D B )$ term arises because we have to store each codebook entry, each of which is a $D$ -dimensional vector. When $N$ is large, the first term dominates the second, so we can approximate the rate of the encoding scheme (number of bits needed per object) as $O ( log _ { 2 } K )$ , which is typically much less than $O ( D B )$ . \n\nOne application of VQ is to image compression. Consider the $2 0 0 times 3 2 0$ pixel image in Figure 21.9; we will treat this as a set of $N = 6 4 , 0 0 0$ scalars. If we use one byte to represent each pixel (a gray-scale intensity of 0 to 255), then $B = 8$ , so we need $N B = 5 1 2 , 0 0 0$ bits to represent the image in uncompressed form. For the compressed image, we need $O ( N log _ { 2 } K )$ bits. For $K = 4$ , this is about 128kb, a factor of 4 compression, yet it results in negligible perceptual loss (see Figure 21.9(b)). \nGreater compression could be achieved if we modeled spatial correlation between the pixels, e.g., if we encoded 5x5 blocks (as used by JPEG). This is because the residual errors (differences from the model’s predictions) would be smaller, and would take fewer bits to encode. This shows the deep connection between data compression and density estimation. See the sequel to this book, [Mur23], for more information. \n21.3.4 The K-means++ algorithm \nK-means is optimizing a non-convex objective, and hence needs to be initialized carefully. A simple approach is to pick $K$ data points at random, and to use these as the initial values for $pmb { mu } _ { k }$ . We can improve on this by using multiple restarts, i.e., we run the algorithm multiple times from different random starting points, and then pick the best solution. However, this can be slow. \nA better approach is to pick the centers sequentially so as to try to “cover” the data. That is, we pick the initial point uniformly at random, and then each subsequent point is picked from the remaining points, with probability proportional to its squared distance to the point’s closest cluster center. That is, at iteration $t$ , we pick the next cluster center to be ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ with probability \nwhere \nis the squared distance of $_ { x }$ to the closest existing centroid. Thus points that are far away from a centroid are more likely to be picked, thus reducing the distortion. This is known as farthest point clustering [Gon85], or K-means $^ { + + }$ [AV07; Bah+12; Bac+16; BLK17; LS19a]. Surprisingly, this simple trick can be shown to guarantee that the recontruction error is never more than $O ( log K )$ worse than optimal [AV07]. \n21.3.5 The K-medoids algorithm \nThere is a variant of K-means called K-medoids algorithm, in which we estimate each cluster center $pmb { mu } _ { k }$ by choosing the data example $pmb { x } _ { n } in mathcal { X }$ whose average dissimilarity to all other points in that cluster is minimal; such a point is known as a medoid. By contrast, in K-means, we take averages over points ${ pmb x } _ { n } in mathbb { R } ^ { D }$ assigned to the cluster to compute the center. K-medoids can be more robust to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license outliers (although that issue can also be tackled by using mixtures of Student distributions, instead of mixtures of Gaussians). More importantly, K-medoids can be applied to data that does not live in $mathbb { R } ^ { D }$ , where averaging may not be well defined. In K-medoids, the input to the algorithm is $N times N$ pairwise distance matrix, $D ( n , n ^ { prime } )$ , not an $N times D$ feature matrix.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "Vector quantization"
    },
    {
        "content": "One application of VQ is to image compression. Consider the $2 0 0 times 3 2 0$ pixel image in Figure 21.9; we will treat this as a set of $N = 6 4 , 0 0 0$ scalars. If we use one byte to represent each pixel (a gray-scale intensity of 0 to 255), then $B = 8$ , so we need $N B = 5 1 2 , 0 0 0$ bits to represent the image in uncompressed form. For the compressed image, we need $O ( N log _ { 2 } K )$ bits. For $K = 4$ , this is about 128kb, a factor of 4 compression, yet it results in negligible perceptual loss (see Figure 21.9(b)). \nGreater compression could be achieved if we modeled spatial correlation between the pixels, e.g., if we encoded 5x5 blocks (as used by JPEG). This is because the residual errors (differences from the model’s predictions) would be smaller, and would take fewer bits to encode. This shows the deep connection between data compression and density estimation. See the sequel to this book, [Mur23], for more information. \n21.3.4 The K-means++ algorithm \nK-means is optimizing a non-convex objective, and hence needs to be initialized carefully. A simple approach is to pick $K$ data points at random, and to use these as the initial values for $pmb { mu } _ { k }$ . We can improve on this by using multiple restarts, i.e., we run the algorithm multiple times from different random starting points, and then pick the best solution. However, this can be slow. \nA better approach is to pick the centers sequentially so as to try to “cover” the data. That is, we pick the initial point uniformly at random, and then each subsequent point is picked from the remaining points, with probability proportional to its squared distance to the point’s closest cluster center. That is, at iteration $t$ , we pick the next cluster center to be ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ with probability \nwhere \nis the squared distance of $_ { x }$ to the closest existing centroid. Thus points that are far away from a centroid are more likely to be picked, thus reducing the distortion. This is known as farthest point clustering [Gon85], or K-means $^ { + + }$ [AV07; Bah+12; Bac+16; BLK17; LS19a]. Surprisingly, this simple trick can be shown to guarantee that the recontruction error is never more than $O ( log K )$ worse than optimal [AV07]. \n21.3.5 The K-medoids algorithm \nThere is a variant of K-means called K-medoids algorithm, in which we estimate each cluster center $pmb { mu } _ { k }$ by choosing the data example $pmb { x } _ { n } in mathcal { X }$ whose average dissimilarity to all other points in that cluster is minimal; such a point is known as a medoid. By contrast, in K-means, we take averages over points ${ pmb x } _ { n } in mathbb { R } ^ { D }$ assigned to the cluster to compute the center. K-medoids can be more robust to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license outliers (although that issue can also be tackled by using mixtures of Student distributions, instead of mixtures of Gaussians). More importantly, K-medoids can be applied to data that does not live in $mathbb { R } ^ { D }$ , where averaging may not be well defined. In K-medoids, the input to the algorithm is $N times N$ pairwise distance matrix, $D ( n , n ^ { prime } )$ , not an $N times D$ feature matrix.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "The K-means++ algorithm"
    },
    {
        "content": "One application of VQ is to image compression. Consider the $2 0 0 times 3 2 0$ pixel image in Figure 21.9; we will treat this as a set of $N = 6 4 , 0 0 0$ scalars. If we use one byte to represent each pixel (a gray-scale intensity of 0 to 255), then $B = 8$ , so we need $N B = 5 1 2 , 0 0 0$ bits to represent the image in uncompressed form. For the compressed image, we need $O ( N log _ { 2 } K )$ bits. For $K = 4$ , this is about 128kb, a factor of 4 compression, yet it results in negligible perceptual loss (see Figure 21.9(b)). \nGreater compression could be achieved if we modeled spatial correlation between the pixels, e.g., if we encoded 5x5 blocks (as used by JPEG). This is because the residual errors (differences from the model’s predictions) would be smaller, and would take fewer bits to encode. This shows the deep connection between data compression and density estimation. See the sequel to this book, [Mur23], for more information. \n21.3.4 The K-means++ algorithm \nK-means is optimizing a non-convex objective, and hence needs to be initialized carefully. A simple approach is to pick $K$ data points at random, and to use these as the initial values for $pmb { mu } _ { k }$ . We can improve on this by using multiple restarts, i.e., we run the algorithm multiple times from different random starting points, and then pick the best solution. However, this can be slow. \nA better approach is to pick the centers sequentially so as to try to “cover” the data. That is, we pick the initial point uniformly at random, and then each subsequent point is picked from the remaining points, with probability proportional to its squared distance to the point’s closest cluster center. That is, at iteration $t$ , we pick the next cluster center to be ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ with probability \nwhere \nis the squared distance of $_ { x }$ to the closest existing centroid. Thus points that are far away from a centroid are more likely to be picked, thus reducing the distortion. This is known as farthest point clustering [Gon85], or K-means $^ { + + }$ [AV07; Bah+12; Bac+16; BLK17; LS19a]. Surprisingly, this simple trick can be shown to guarantee that the recontruction error is never more than $O ( log K )$ worse than optimal [AV07]. \n21.3.5 The K-medoids algorithm \nThere is a variant of K-means called K-medoids algorithm, in which we estimate each cluster center $pmb { mu } _ { k }$ by choosing the data example $pmb { x } _ { n } in mathcal { X }$ whose average dissimilarity to all other points in that cluster is minimal; such a point is known as a medoid. By contrast, in K-means, we take averages over points ${ pmb x } _ { n } in mathbb { R } ^ { D }$ assigned to the cluster to compute the center. K-medoids can be more robust to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license outliers (although that issue can also be tackled by using mixtures of Student distributions, instead of mixtures of Gaussians). More importantly, K-medoids can be applied to data that does not live in $mathbb { R } ^ { D }$ , where averaging may not be well defined. In K-medoids, the input to the algorithm is $N times N$ pairwise distance matrix, $D ( n , n ^ { prime } )$ , not an $N times D$ feature matrix. \n\nThe classic algorithm for solving the K-medoids is the partitioning around medoids or PAM method [KR87]. In this approach, at each iteration, we loop over all $K$ medoids. For each medoid $m$ , we consider each non-medoid point $o$ , swap $m$ and $o$ , and recompute the cost (sum of all the distances of points to their medoid). If the cost has decreased, we keep this swap. The running time of this algorithm is $O ( N ^ { 2 } K T )$ , where $T$ is the number of iterations. \nThere is also a simpler and faster method, known as the Voronoi iteration method due to [PJ09]. In this approach, at each iteration, we have two steps, similar to K-means. First, for each cluster $k$ , look at all the points currently assigned to that cluster, $S _ { k } = { n : z _ { n } = k }$ , and then set $m _ { k }$ to be the index of the medoid of that set. (To find the medoid requires examining all $vert S _ { k } vert$ candidate points, and choosing the one that has the smallest sum of distances to all the other points in $S _ { k }$ .) Second, for each point $n$ , assign it to its closest medoid, $z _ { n } = mathrm { a r g m i n } _ { k } D ( n , k )$ . The pseudo-code is given in Algorithm 12. \n21.3.6 Speedup tricks \nK-means clustering takes $O ( N K I )$ time, where $I$ is the number of iterations, but we can reduce the constant factors using various tricks. For example, [Elk03] shows how to use the triangle inequality to keep track of lower and upper bounds for the distances between inputs and the centroids; this can be used to eliminate some redundant computations. Another approach is to use a minibatch approximation, as proposed in [Scu10]. This can be significantly faster, although can result in slightly worse loss (see Figure 21.10). \n21.3.7 Choosing the number of clusters $pmb { K }$ \nIn this section, we discuss how to choose the number of clusters $K$ in the K-means algorithm and other related methods. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "The K-medoids algorithm"
    },
    {
        "content": "The classic algorithm for solving the K-medoids is the partitioning around medoids or PAM method [KR87]. In this approach, at each iteration, we loop over all $K$ medoids. For each medoid $m$ , we consider each non-medoid point $o$ , swap $m$ and $o$ , and recompute the cost (sum of all the distances of points to their medoid). If the cost has decreased, we keep this swap. The running time of this algorithm is $O ( N ^ { 2 } K T )$ , where $T$ is the number of iterations. \nThere is also a simpler and faster method, known as the Voronoi iteration method due to [PJ09]. In this approach, at each iteration, we have two steps, similar to K-means. First, for each cluster $k$ , look at all the points currently assigned to that cluster, $S _ { k } = { n : z _ { n } = k }$ , and then set $m _ { k }$ to be the index of the medoid of that set. (To find the medoid requires examining all $vert S _ { k } vert$ candidate points, and choosing the one that has the smallest sum of distances to all the other points in $S _ { k }$ .) Second, for each point $n$ , assign it to its closest medoid, $z _ { n } = mathrm { a r g m i n } _ { k } D ( n , k )$ . The pseudo-code is given in Algorithm 12. \n21.3.6 Speedup tricks \nK-means clustering takes $O ( N K I )$ time, where $I$ is the number of iterations, but we can reduce the constant factors using various tricks. For example, [Elk03] shows how to use the triangle inequality to keep track of lower and upper bounds for the distances between inputs and the centroids; this can be used to eliminate some redundant computations. Another approach is to use a minibatch approximation, as proposed in [Scu10]. This can be significantly faster, although can result in slightly worse loss (see Figure 21.10). \n21.3.7 Choosing the number of clusters $pmb { K }$ \nIn this section, we discuss how to choose the number of clusters $K$ in the K-means algorithm and other related methods. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "Speedup tricks"
    },
    {
        "content": "The classic algorithm for solving the K-medoids is the partitioning around medoids or PAM method [KR87]. In this approach, at each iteration, we loop over all $K$ medoids. For each medoid $m$ , we consider each non-medoid point $o$ , swap $m$ and $o$ , and recompute the cost (sum of all the distances of points to their medoid). If the cost has decreased, we keep this swap. The running time of this algorithm is $O ( N ^ { 2 } K T )$ , where $T$ is the number of iterations. \nThere is also a simpler and faster method, known as the Voronoi iteration method due to [PJ09]. In this approach, at each iteration, we have two steps, similar to K-means. First, for each cluster $k$ , look at all the points currently assigned to that cluster, $S _ { k } = { n : z _ { n } = k }$ , and then set $m _ { k }$ to be the index of the medoid of that set. (To find the medoid requires examining all $vert S _ { k } vert$ candidate points, and choosing the one that has the smallest sum of distances to all the other points in $S _ { k }$ .) Second, for each point $n$ , assign it to its closest medoid, $z _ { n } = mathrm { a r g m i n } _ { k } D ( n , k )$ . The pseudo-code is given in Algorithm 12. \n21.3.6 Speedup tricks \nK-means clustering takes $O ( N K I )$ time, where $I$ is the number of iterations, but we can reduce the constant factors using various tricks. For example, [Elk03] shows how to use the triangle inequality to keep track of lower and upper bounds for the distances between inputs and the centroids; this can be used to eliminate some redundant computations. Another approach is to use a minibatch approximation, as proposed in [Scu10]. This can be significantly faster, although can result in slightly worse loss (see Figure 21.10). \n21.3.7 Choosing the number of clusters $pmb { K }$ \nIn this section, we discuss how to choose the number of clusters $K$ in the K-means algorithm and other related methods. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n21.3.7.1 Minimizing the distortion \nBased on our experience with supervised learning, a natural choice for picking $K$ is to pick the value that minimizes the reconstruction error on a validation set, defined as follows: \nwhere $hat { pmb x } _ { n } = mathrm { d e c o d e } ( mathrm { e n c o d e } ( { pmb x } _ { n } ) )$ is the reconstruction of ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ . \nUnfortunately, this technique will not work. Indeed, as we see in Figure 21.11a, the distortion monotonically decreases with $K$ . To see why, note that the K-means model is a degenerate density model which consists of $K$ “spikes” at the centers. As we increase $K$ , we “cover” more of the input $pmb { mu } _ { k }$ space. Hence any given input point is more likely to find a close prototype to accurately represent it as $K$ increases, thus decreasing reconstruction error. Thus unlike with supervised learning, we cannot use reconstruction error on a validation set as a way to select the best unsupervised model. (This comment also applies to picking the dimensionality for PCA, see Section 20.1.4.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n21.3.7.2 Maximizing the marginal likelihood \nA method that does work is to use a proper probabilistic model, such as a GMM, as we describe in Section 21.4.1. We can then use the log marginal likelihood (LML) of the data to perform model selection. \nWe can approximate the LML using the BIC score as we discussed in Section 5.2.5.1. From Equation (5.59), we have \nwhere $D _ { K }$ is the number of parameters in a model with $K$ clusters, and $hat { pmb { theta } } _ { K }$ is the MLE. We see from Figure 21.11b that this exhibits the typical U-shaped curve, where the penalty decreases and then increases. \nThe reason this works is that each cluster is associated with a Gaussian distribution that fills a volume of the input space, rather than being a degenerate spike. Once we have enough clusters to cover the true modes of the distribution, the Bayesian Occam’s razor (Section 5.2.3) kicks in, and starts penalizing the model for being unncessarily complex. \nSee Section 21.4.1.3 for more discussion of Bayesian model selection for mixture models. \n21.3.7.3 Silhouette coefficient \nIn this section, we describe a common heuristic method for picking the number of clusters in a K-means clustering model. This is designed to work for spherical (not elongated) clusters. First we define the silhouette coefficient of an instance $i$ to be $s c ( i ) = ( b _ { i } - a _ { i } ) / operatorname* { m a x } ( a _ { i } , b _ { i } )$ , where $a _ { i }$ is the mean distance to the other instances in cluster $k _ { i } = mathrm { a r g m i n } _ { k } left| left| pmb { mu } _ { k } - pmb { x } _ { i } right| right|$ , and $b _ { i }$ is the mean distance to the other instances in the next closest cluster, $k _ { i } ^ { prime } = mathrm { a r g m i n } _ { k neq k _ { i } } vert vert pmb { mu } _ { k } - pmb { x } _ { i } vert vert$ . Thus $a _ { i }$ is a measure of compactness of $i$ ’s cluster, and $b _ { i }$ is a measure of distance between the clusters. The silhouette coefficient varies from -1 to $+ 1$ . A value of $+ 1$ means the instance is close to all the members of its cluster, and far from other clusters; a value of 0 means it is close to a cluster boundary; and a value of -1 means it may be in the wrong cluster. We define the silhouette score of a clustering $K$ to be the mean silhouette coefficient over all instances. \nIn Figure 21.11a, we plot the distortion vs $K$ for the data in Figure 21.7. As we explained above, it goes down monotonically with $K$ . There is a slight “kink” or “elbow” in the curve at $K = 3$ , but this is hard to detect. In Figure 21.11c, we plot the silhouette score vs $K$ . Now we see a more prominent peak at $K = 3$ , although it seems $K = 7$ is almost as good. See Figure 21.12 for a comparison of some of these clusterings. \nIt can be informative to look at the individual silhouette coefficients, and not just the mean score. We can plot these in a silhouette diagram, as shown in Figure 21.13, where each colored region corresponds to a different cluster. The dotted vertical line is the average coefficient. Clusters with many points to the left of this line are likely to be of low quality. We can also use the silhouette diagram to look at the size of each cluster, even if the data is not 2d. \n21.3.7.4 Incrementally growing the number of mixture components \nAn alternative to searching for the best value of $K$ is to incrementally “grow” GMMs. We can start with a small value of $K$ , and after each round of training, we consider splitting the cluster with the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 highest mixing weight into two, with the new centroids being random perturbations of the original centroid, and the new scores being half of the old scores. If a new cluster has too small a score, or too narrow a variance, it is removed. We continue in this way until the desired number of clusters is reached. See [FJ02] for details. \n\n21.3.7.5 Sparse estimation methods \nAnother approach is to pick a large value of $K$ , and then to use some kind of sparsity-promoting prior or inference method to “kill off” unneeded mixture components, such as variational Bayes. See the sequel to this book, [Mur23], for details. \n21.4 Clustering using mixture models \nWe have seen how the K-means algorithm can be used to cluster data vectors in $mathbb { R } ^ { D }$ . However, this method assumes that all clusters have the same spherical shape, which is a very restrictive assumption. In addition, K-means assumes that all clusters can be described by Gaussians in the input space, so it cannot be applied to discrete data. By using mixture models (Section 3.5), we can overcome both of these problems, as we illustrate below. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "K means clustering",
        "subsubsection": "Choosing the number of clusters K"
    },
    {
        "content": "21.4.1 Mixtures of Gaussians \nRecall from Section 3.5.1 that a Gaussian mixture model (GMM) is a model of the form \nIf we know the model parameters $pmb { theta } = ( pi , { pmb { mu } _ { k } , pmb { Sigma } _ { k } } )$ , we can use Bayes rule to compute the responsibility (posterior membership probability) of cluster $k$ for data point ${ boldsymbol { mathbf { mathit { x } } } } _ { n }$ : \nGiven the responsibilities, we can compute the most probable cluster assignment as follows: \nThis is known as hard clustering. \n21.4.1.1 K-means is a special case of EM \nWe can estimate the parameters of a GMM using the EM algorithm (Section 8.7.3). It turns out that the K-means algorithm is a special case of this algorithm, in which we make two approximations: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license we fix $boldsymbol { Sigma } _ { k } = mathbf { I }$ and $pi _ { k } = 1 / K$ for all the clusters (so we just have to estimate the means $pmb { mu } _ { k }$ ), .a90nd we approximate the $mathrm { E }$ step, by replacing the soft responsibilities with hard cluster assignments, .i88.e., we compute . $z _ { n } ^ { * } = mathrm { a r g m a x } _ { k } r _ { n k }$ , and s0e.8t $r _ { n k } approx mathbb { I } left( k = z _ { n } ^ { * } right)$ i10n0st0e.89ad o20f 0using 93t100he s 0o.9f2t 0r0es0.p93onsibilitie0s, $r _ { n k } = p ( z _ { n } = k | mathbf { x } _ { n } , pmb theta )$ . With this approximation, the weighted MLE problem in Equation (8.165) of the M step reduces to Equation (21.14), so we recover K-means. \n\nHowever, the assumption that all the clusters have the same spherical shape is very restrictive. For example, Figure 21.14 shows the marginal density and clustering induced using different shaped covariance matrices for some 2d data. We see that modeling this particular dataset needs the ability to capture off-diagonal covariance for some clusters (top row). \n21.4.1.2 Unidentifiability and label switching \nNote that we are free to permute the labels in a mixture model without changing the likelihood. This is called the label switching problem, and is an example of non-identifiability of the parameters. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nThis can cause problems if we wish to perform posterior inference over the parameters (as opposed to just computing the MLE or a MAP estimate). For example, suppose we fit a GMM with $K = 2$ components to the data in Figure 21.15 using HMC. The posterior over the means, $p ( mu _ { 1 } , mu _ { 2 } | mathcal { D } )$ , is shown in Figure 21.16a. We see that the marginal posterior for each component, $p ( mu _ { k } vert mathcal { D } )$ , is bimodal. This reflects the fact that there are two equally good explanations of the data: either $mu _ { 1 } approx 4 7$ and $mu _ { 2 } approx 5 7$ , or vice versa. \nTo break symmetry, we can add an ordering constraint on the centers, so that $mu _ { 1 } < mu _ { 2 }$ . We can do this by adding a penalty or potential function to the objective if the penalty is violated. More precisely, the penalized log joint becomes \nwhere \nThis has the desired effect, as shown in Figure 21.16b. \nA more general approach is to apply a transformation to the parameters, to ensure identifiability. That is, we sample the parameters $pmb theta$ from a proposal, and then apply an invertible transformation $theta ^ { prime } = f ( theta )$ to them before computing the log joint, $log p ( mathcal { D } , pmb theta ^ { prime } )$ . To account for the change of variables (Section 2.8.3), we add the log of the determinant of the Jacobian. In the case of a 1d ordering transformation, which just sorts its inputs, the determinant of the Jacobian is 1, so the log-det-Jacobian term vanishes. \nUnfortunately, this approach does not scale to more than 1 dimensional problems, because there is no obvious way to enforce an ordering constraint on the centers $pmb { mu } _ { k }$ . \n21.4.1.3 Bayesian model selection \nOnce we have a reliable way to ensure identifiability, we can use Bayesian model selection techniques from Section 5.2.2 to select the number of clusters $K$ . In Figure 21.17, we illustrate the results of fitting a GMM with $K = 3 - 6$ components to the data in Figure 21.15. We use the ordering transform on the means, and perform inference using HMC. We compare the resulting GMM model fits to the fit of a kernel density estimate (Section 16.3), which often over-smooths the data. We see fairly strong evidence for two bumps, corresponding to different subpopulations. \nWe can compare these models more quantitatively by computing their WAIC scores (widely applicable information criterion) which is an approximation to the log marginal likelihood (see [Wat10; Wat13; VGG17] for details). The results are shown in Figure 21.18. (This kind of visualization was proposed in [McE20, p228].) We see that the model with $K = 6$ scores significantly higher than for the other models, although $K = 5$ is a close second. This is consistent with the plot in Figure 21.17. \n21.4.2 Mixtures of Bernoullis \nAs we discussed in Section 3.5.2, we can use a mixtures of Bernoullis to cluster binary data. The model has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Clustering using mixture models",
        "subsubsection": "Mixtures of Gaussians"
    },
    {
        "content": "This can cause problems if we wish to perform posterior inference over the parameters (as opposed to just computing the MLE or a MAP estimate). For example, suppose we fit a GMM with $K = 2$ components to the data in Figure 21.15 using HMC. The posterior over the means, $p ( mu _ { 1 } , mu _ { 2 } | mathcal { D } )$ , is shown in Figure 21.16a. We see that the marginal posterior for each component, $p ( mu _ { k } vert mathcal { D } )$ , is bimodal. This reflects the fact that there are two equally good explanations of the data: either $mu _ { 1 } approx 4 7$ and $mu _ { 2 } approx 5 7$ , or vice versa. \nTo break symmetry, we can add an ordering constraint on the centers, so that $mu _ { 1 } < mu _ { 2 }$ . We can do this by adding a penalty or potential function to the objective if the penalty is violated. More precisely, the penalized log joint becomes \nwhere \nThis has the desired effect, as shown in Figure 21.16b. \nA more general approach is to apply a transformation to the parameters, to ensure identifiability. That is, we sample the parameters $pmb theta$ from a proposal, and then apply an invertible transformation $theta ^ { prime } = f ( theta )$ to them before computing the log joint, $log p ( mathcal { D } , pmb theta ^ { prime } )$ . To account for the change of variables (Section 2.8.3), we add the log of the determinant of the Jacobian. In the case of a 1d ordering transformation, which just sorts its inputs, the determinant of the Jacobian is 1, so the log-det-Jacobian term vanishes. \nUnfortunately, this approach does not scale to more than 1 dimensional problems, because there is no obvious way to enforce an ordering constraint on the centers $pmb { mu } _ { k }$ . \n21.4.1.3 Bayesian model selection \nOnce we have a reliable way to ensure identifiability, we can use Bayesian model selection techniques from Section 5.2.2 to select the number of clusters $K$ . In Figure 21.17, we illustrate the results of fitting a GMM with $K = 3 - 6$ components to the data in Figure 21.15. We use the ordering transform on the means, and perform inference using HMC. We compare the resulting GMM model fits to the fit of a kernel density estimate (Section 16.3), which often over-smooths the data. We see fairly strong evidence for two bumps, corresponding to different subpopulations. \nWe can compare these models more quantitatively by computing their WAIC scores (widely applicable information criterion) which is an approximation to the log marginal likelihood (see [Wat10; Wat13; VGG17] for details). The results are shown in Figure 21.18. (This kind of visualization was proposed in [McE20, p228].) We see that the model with $K = 6$ scores significantly higher than for the other models, although $K = 5$ is a close second. This is consistent with the plot in Figure 21.17. \n21.4.2 Mixtures of Bernoullis \nAs we discussed in Section 3.5.2, we can use a mixtures of Bernoullis to cluster binary data. The model has the form \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nHere is the probability that bit $d$ turns on in cluster $k$ . We can fit this model with EM, SGD, $mu _ { d k }$ MCMC, etc. See Figure 3.13 for an example, where we cluster some binarized MNIST digits. \n21.5 Spectral clustering * \nIn this section, we discuss an approach to clustering based on eigenvalue analysis of a pairwise similarity matrix. It uses the eigenvectors to derive feature vectors for each datapoint, which are then clustered using a feature-based clustering method, such as K-means (Section 21.3). This is known as spectral clustering [SM00; Lux07]. \n21.5.1 Normalized cuts \nWe start by creating a weighted undirected graph W, where each data vector is a node, and the strength of the $i - j$ edge is a measure of similarity. Typically we only connected a node to its most similar neighbors, to ensure the graph is sparse, which speeds computation. \nOur goal is to find $K$ clusters of similar points. That is, we want to find a graph partition into $S _ { 1 } , ldots , S _ { K }$ disjoint sets of nodes so as to minimize some kind of cost. \nOur first attempt at a cost function is to compute the weight of connections between nodes in each cluster to nodes outside each cluster: \nwhere $begin{array} { r } { W ( A , B ) triangleq sum _ { i in A , j in B } w _ { i j } } end{array}$ and ${ overline { { S } } } _ { k } = V setminus S _ { k }$ is the complement of $S _ { k }$ , where $V = { 1 , ldots , N }$ . Unfortunately the optimal solution to this often just partitions off a single node from the rest, since that minimizes the weight of the cut. To prevent this, we can divide by the size of each set, to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Clustering using mixture models",
        "subsubsection": "Mixtures of Bernoullis"
    },
    {
        "content": "Here is the probability that bit $d$ turns on in cluster $k$ . We can fit this model with EM, SGD, $mu _ { d k }$ MCMC, etc. See Figure 3.13 for an example, where we cluster some binarized MNIST digits. \n21.5 Spectral clustering * \nIn this section, we discuss an approach to clustering based on eigenvalue analysis of a pairwise similarity matrix. It uses the eigenvectors to derive feature vectors for each datapoint, which are then clustered using a feature-based clustering method, such as K-means (Section 21.3). This is known as spectral clustering [SM00; Lux07]. \n21.5.1 Normalized cuts \nWe start by creating a weighted undirected graph W, where each data vector is a node, and the strength of the $i - j$ edge is a measure of similarity. Typically we only connected a node to its most similar neighbors, to ensure the graph is sparse, which speeds computation. \nOur goal is to find $K$ clusters of similar points. That is, we want to find a graph partition into $S _ { 1 } , ldots , S _ { K }$ disjoint sets of nodes so as to minimize some kind of cost. \nOur first attempt at a cost function is to compute the weight of connections between nodes in each cluster to nodes outside each cluster: \nwhere $begin{array} { r } { W ( A , B ) triangleq sum _ { i in A , j in B } w _ { i j } } end{array}$ and ${ overline { { S } } } _ { k } = V setminus S _ { k }$ is the complement of $S _ { k }$ , where $V = { 1 , ldots , N }$ . Unfortunately the optimal solution to this often just partitions off a single node from the rest, since that minimizes the weight of the cut. To prevent this, we can divide by the size of each set, to \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nget the following objective, known as the normalized cut: \nnwohdeer $operatorname { v o l } ( A ) triangleq textstyle sum _ { i in A } d _ { i }$ aisphthientto clwuesitgehrts osfucshet $A$ atanod $begin{array} { r } { d _ { i } = sum _ { j = 1 } ^ { N } w _ { i j } } end{array}$ cilsutshterwaeriegshitmeidladretgoreeacohf $i$ $K$   \nother, but are different to nodes in other clusters. \nWe can formulate the Ncut problem in terms of searching for binary vectors $pmb { c } _ { i } in { 0 , 1 } ^ { N }$ that minimizes the above objective, where $c _ { i k } = 1$ iff point $i$ belongs to cluster $k$ . Unfortunately this is NP-hard [WW93]. Below we discuss a continuous relaxation of the problem based on eigenvector methods that is easier to solve. \n21.5.2 Eigenvectors of the graph Laplacian encode the clustering \nIn Section 20.4.9.2, we discussed the graph Laplacian, which is defined as $mathbf { L } triangleq mathbf { D } - mathbf { W }$ , where W is a symmetric weight matrix for the graph, and $mathbf { D } = mathrm { d i a g } ( d _ { i } )$ is a diagonal matrix containing the weighted degree of each node, $d _ { i } = sum _ { j } w _ { i j }$ . To get some intuition as to why $mathbf { L }$ might be useful for graph-based clustering, we note the following result. \nTheorem 21.5.1. The set of eigenvectors of $mathbf { L }$ with eigenvalue 0 is spanned by the indicator vectors $mathbf { 1 } _ { S _ { 1 } } , dotsc , mathbf { 1 } _ { S _ { K } }$ , where $S _ { k }$ are the $K$ connected components of the graph. \nProof. Let us start with the case $K = 1$ . If $f$ is an eigenvector with eigenvalue 0, then $0 =$ $begin{array} { r } { sum _ { i j } w _ { i j } ( f _ { i } - f _ { j } ) ^ { 2 } } end{array}$ . If two nodes are connected, so $w _ { i j } > 0$ , we must have that $f _ { i } = f _ { j }$ . Hence $f$ is constant for all vertices which are connected by a path in the graph. Now suppose $K > 1$ . In this \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license case, $mathbf { L }$ will be block diagonal. A similar argument to the above shows that we will have $K$ indicator functions, which “select out” the connected components. □",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Spectral clustering *",
        "subsubsection": "Normalized cuts"
    },
    {
        "content": "get the following objective, known as the normalized cut: \nnwohdeer $operatorname { v o l } ( A ) triangleq textstyle sum _ { i in A } d _ { i }$ aisphthientto clwuesitgehrts osfucshet $A$ atanod $begin{array} { r } { d _ { i } = sum _ { j = 1 } ^ { N } w _ { i j } } end{array}$ cilsutshterwaeriegshitmeidladretgoreeacohf $i$ $K$   \nother, but are different to nodes in other clusters. \nWe can formulate the Ncut problem in terms of searching for binary vectors $pmb { c } _ { i } in { 0 , 1 } ^ { N }$ that minimizes the above objective, where $c _ { i k } = 1$ iff point $i$ belongs to cluster $k$ . Unfortunately this is NP-hard [WW93]. Below we discuss a continuous relaxation of the problem based on eigenvector methods that is easier to solve. \n21.5.2 Eigenvectors of the graph Laplacian encode the clustering \nIn Section 20.4.9.2, we discussed the graph Laplacian, which is defined as $mathbf { L } triangleq mathbf { D } - mathbf { W }$ , where W is a symmetric weight matrix for the graph, and $mathbf { D } = mathrm { d i a g } ( d _ { i } )$ is a diagonal matrix containing the weighted degree of each node, $d _ { i } = sum _ { j } w _ { i j }$ . To get some intuition as to why $mathbf { L }$ might be useful for graph-based clustering, we note the following result. \nTheorem 21.5.1. The set of eigenvectors of $mathbf { L }$ with eigenvalue 0 is spanned by the indicator vectors $mathbf { 1 } _ { S _ { 1 } } , dotsc , mathbf { 1 } _ { S _ { K } }$ , where $S _ { k }$ are the $K$ connected components of the graph. \nProof. Let us start with the case $K = 1$ . If $f$ is an eigenvector with eigenvalue 0, then $0 =$ $begin{array} { r } { sum _ { i j } w _ { i j } ( f _ { i } - f _ { j } ) ^ { 2 } } end{array}$ . If two nodes are connected, so $w _ { i j } > 0$ , we must have that $f _ { i } = f _ { j }$ . Hence $f$ is constant for all vertices which are connected by a path in the graph. Now suppose $K > 1$ . In this \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license case, $mathbf { L }$ will be block diagonal. A similar argument to the above shows that we will have $K$ indicator functions, which “select out” the connected components. □ \n\nThis suggests the following clustering algorithm. Compute the eigenvectors and values of $mathbf { L }$ , and let $mathbf { U }$ be an $N times K$ matrix with the $K$ eigenvectors with smallest eigenvalue in its columns. (Fast methods for computing such “bottom” eigenvectors are discussed in [YHJ09]). Let $pmb { u } _ { i } in mathbb { R } ^ { K }$ be the i’th row of U. Since these ${ pmb u } _ { i }$ will be piecewise constant, we can apply K-means clustering (Section 21.3) to them to recover the connected components. (Note that the vectors ${ pmb u } _ { i }$ are the same as those computed by Laplacian eigenmaps discussed in Section 20.4.9.) \nReal data may not exhibit such clean block structure, but one can show, using results from perturbation theory, that the eigenvectors of a “perturbed” Laplacian will be close to these ideal indicator functions [NJW01]. \nIn practice, it is important to normalize the graph Laplacian, to account for the fact that some nodes are more highly connected than others. One way to do this (proposed in [NJW01]) is to create a symmetric matrix \nThis time the eigenspace of $0$ is spanned by $mathbf { D } ^ { frac { 1 } { 2 } } mathbf { 1 } _ { S _ { k } }$ . This suggests the following algorithm: find the smallest $K$ eigenvectors of $mathbf { L } _ { boldsymbol { s y m } }$ , stack them into the matrix $mathbf { U }$ , normalize each row to unit norm by creating $t _ { i j } = u _ { i j } / sqrt { ( sum _ { k } u _ { i k } ^ { 2 } ) }$ to make the matrix $mathbf { T }$ , cluster the rows of $mathbf { T }$ using K-means, then infer the partitioning of the original points. \n21.5.3 Example \nFigure 21.19 illustrates the method in action. In Figure 21.19(a), we see that K-means does a poor job of clustering, since it implicitly assumes each cluster corresponds to a spherical Gaussian. Next we try spectral clustering. We compute a dense similarity matrix W using a Gaussian kernel, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Spectral clustering *",
        "subsubsection": "Eigenvectors of the graph Laplacian encode the clustering"
    },
    {
        "content": "This suggests the following clustering algorithm. Compute the eigenvectors and values of $mathbf { L }$ , and let $mathbf { U }$ be an $N times K$ matrix with the $K$ eigenvectors with smallest eigenvalue in its columns. (Fast methods for computing such “bottom” eigenvectors are discussed in [YHJ09]). Let $pmb { u } _ { i } in mathbb { R } ^ { K }$ be the i’th row of U. Since these ${ pmb u } _ { i }$ will be piecewise constant, we can apply K-means clustering (Section 21.3) to them to recover the connected components. (Note that the vectors ${ pmb u } _ { i }$ are the same as those computed by Laplacian eigenmaps discussed in Section 20.4.9.) \nReal data may not exhibit such clean block structure, but one can show, using results from perturbation theory, that the eigenvectors of a “perturbed” Laplacian will be close to these ideal indicator functions [NJW01]. \nIn practice, it is important to normalize the graph Laplacian, to account for the fact that some nodes are more highly connected than others. One way to do this (proposed in [NJW01]) is to create a symmetric matrix \nThis time the eigenspace of $0$ is spanned by $mathbf { D } ^ { frac { 1 } { 2 } } mathbf { 1 } _ { S _ { k } }$ . This suggests the following algorithm: find the smallest $K$ eigenvectors of $mathbf { L } _ { boldsymbol { s y m } }$ , stack them into the matrix $mathbf { U }$ , normalize each row to unit norm by creating $t _ { i j } = u _ { i j } / sqrt { ( sum _ { k } u _ { i k } ^ { 2 } ) }$ to make the matrix $mathbf { T }$ , cluster the rows of $mathbf { T }$ using K-means, then infer the partitioning of the original points. \n21.5.3 Example \nFigure 21.19 illustrates the method in action. In Figure 21.19(a), we see that K-means does a poor job of clustering, since it implicitly assumes each cluster corresponds to a spherical Gaussian. Next we try spectral clustering. We compute a dense similarity matrix W using a Gaussian kernel, \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n$begin{array} { r } { W _ { i j } = exp bigl ( { - frac { 1 } { 2 sigma ^ { 2 } } | | pmb { x } _ { i } - pmb { x } _ { j } | | _ { 2 } ^ { 2 } } bigr ) } end{array}$ . We then compute the first two eigenvectors of the normalized Laplacian $L _ { mathrm { s y m } }$ . From this we infer the clustering using K-means, with $K = 2$ ; the results are shown in Figure 21.19(b). \n21.5.4 Connection with other methods \nSpectral clustering is closely related to several other methods for unsupervised learning, some of which we discuss below. \n21.5.4.1 Connection with kPCA \nSpectral clustering is closely related to kernel PCA (Section 20.4.6). In particular, kPCA uses the largest eigenvectors of $mathbf { W }$ ; these are equivalent to the smallest eigenvectors of $mathbf { I } - mathbf { W }$ . This is similar to the above method, which computes the smallest eigenvectors of $mathbf { L } = mathbf { D } - mathbf { W }$ . See [Ben+04a] for details. In practice, spectral clustering tends to give better results than kPCA. \n21.5.4.2 Connection with random walk analysis \nIn practice we get better results by computing the eigenvectors of the normalized graph Laplacian. One way to normalize the graph Laplacian, which is used in [SM00; Mei01], is to define \nOne can show that for $mathbf { L } _ { r w }$ , the eigenspace of $0$ is again spanned by the indicator vectors $mathbf { 1 } _ { S _ { k } }$ [Lux07], so we can perform clustering directly on the $K$ smallest eigenvectors $mathbf { U }$ . \nThere is an interesting connection between this approach and random walks on a graph. First note that ${ mathbf { P } } = { mathbf { D } } ^ { - 1 } { mathbf { W } } = { mathbf { I } } - { mathbf { L } } _ { r w }$ is a stochastic matrix, where $p _ { i j } = w _ { i j } / d _ { i }$ can be interpreted as the probability of going from $i$ to $j$ . If the graph is connected and non-bipartite, it possesses a unique stationary distribution $pmb { pi } = ( pmb { pi } _ { 1 } , ldots , pmb { pi } _ { N } )$ , where $pi _ { i } = d _ { i } / mathrm { v o l } ( V )$ , and $operatorname { v o l } ( V ) = textstyle sum _ { i } d _ { i }$ is the sum of all the node degrees. Furthermore, one can show that for a partition of size 2, \nThis means that we are looking for a cut such that a random walk spends more time transitioning to similar points, and rarely makes transitions from $S$ to $overline { S }$ or vice versa. This analysis can be extended to $K > 2$ ; for details, see [Mei01]. \n21.6 Biclustering * \nIn some cases, we have a data matrix $mathbf { X } in mathbb { R } ^ { N _ { r } times N _ { c } }$ and we want to cluster the rows and the columns; this is known as biclustering or coclustering. This is widely used in bioinformatics, where the rows often represent genes and the columns represent conditions. It can also be used for collaborative filtering, where the rows represent users and the columns represent movies. \nA variety of ad hoc methods for biclustering have been proposed; see [MO04] for a review. In Section 21.6.1, we present a simple probabilistic generative model in which we assign a latent cluster id to each row, and a differnet latent cluster id to each column. In Section 21.6.2, we extend this to the case where each row can belong to multiple clusters, depending on which groups of features (columns) we choose to use to define the different groups of objects (rows). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Spectral clustering *",
        "subsubsection": "Example"
    },
    {
        "content": "$begin{array} { r } { W _ { i j } = exp bigl ( { - frac { 1 } { 2 sigma ^ { 2 } } | | pmb { x } _ { i } - pmb { x } _ { j } | | _ { 2 } ^ { 2 } } bigr ) } end{array}$ . We then compute the first two eigenvectors of the normalized Laplacian $L _ { mathrm { s y m } }$ . From this we infer the clustering using K-means, with $K = 2$ ; the results are shown in Figure 21.19(b). \n21.5.4 Connection with other methods \nSpectral clustering is closely related to several other methods for unsupervised learning, some of which we discuss below. \n21.5.4.1 Connection with kPCA \nSpectral clustering is closely related to kernel PCA (Section 20.4.6). In particular, kPCA uses the largest eigenvectors of $mathbf { W }$ ; these are equivalent to the smallest eigenvectors of $mathbf { I } - mathbf { W }$ . This is similar to the above method, which computes the smallest eigenvectors of $mathbf { L } = mathbf { D } - mathbf { W }$ . See [Ben+04a] for details. In practice, spectral clustering tends to give better results than kPCA. \n21.5.4.2 Connection with random walk analysis \nIn practice we get better results by computing the eigenvectors of the normalized graph Laplacian. One way to normalize the graph Laplacian, which is used in [SM00; Mei01], is to define \nOne can show that for $mathbf { L } _ { r w }$ , the eigenspace of $0$ is again spanned by the indicator vectors $mathbf { 1 } _ { S _ { k } }$ [Lux07], so we can perform clustering directly on the $K$ smallest eigenvectors $mathbf { U }$ . \nThere is an interesting connection between this approach and random walks on a graph. First note that ${ mathbf { P } } = { mathbf { D } } ^ { - 1 } { mathbf { W } } = { mathbf { I } } - { mathbf { L } } _ { r w }$ is a stochastic matrix, where $p _ { i j } = w _ { i j } / d _ { i }$ can be interpreted as the probability of going from $i$ to $j$ . If the graph is connected and non-bipartite, it possesses a unique stationary distribution $pmb { pi } = ( pmb { pi } _ { 1 } , ldots , pmb { pi } _ { N } )$ , where $pi _ { i } = d _ { i } / mathrm { v o l } ( V )$ , and $operatorname { v o l } ( V ) = textstyle sum _ { i } d _ { i }$ is the sum of all the node degrees. Furthermore, one can show that for a partition of size 2, \nThis means that we are looking for a cut such that a random walk spends more time transitioning to similar points, and rarely makes transitions from $S$ to $overline { S }$ or vice versa. This analysis can be extended to $K > 2$ ; for details, see [Mei01]. \n21.6 Biclustering * \nIn some cases, we have a data matrix $mathbf { X } in mathbb { R } ^ { N _ { r } times N _ { c } }$ and we want to cluster the rows and the columns; this is known as biclustering or coclustering. This is widely used in bioinformatics, where the rows often represent genes and the columns represent conditions. It can also be used for collaborative filtering, where the rows represent users and the columns represent movies. \nA variety of ad hoc methods for biclustering have been proposed; see [MO04] for a review. In Section 21.6.1, we present a simple probabilistic generative model in which we assign a latent cluster id to each row, and a differnet latent cluster id to each column. In Section 21.6.2, we extend this to the case where each row can belong to multiple clusters, depending on which groups of features (columns) we choose to use to define the different groups of objects (rows). \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Spectral clustering *",
        "subsubsection": "Connection with other methods"
    },
    {
        "content": "O1 killer whale, blue whale, humpback, seal, walrus, dolphin   \nO2 antelope, horse, giraffe, zebra, deer   \nO3 monkey, gorilla, chimp   \nO4 hippo, elephant, rhino   \nO5 grizzly bear, polar bear   \nF1 flippers, strain teeth, swims, arctic, coastal, ocean, water   \nF2 hooves, long neck, horns   \nF3 hands, bipedal, jungle, tree   \nF4 bulbous body shape, slow, inactive   \nF5 meat teeth, eats meat, hunter, fierce   \nF6 walks, quadrapedal, ground \nO1  \nO2  \nO3  \nO4  \nO5   \n中\n21.6.1 Basic biclustering \nHere we present a simple probabilistic generative model for biclustering based on [Kem+06] (see also [SMM03] for a related approach). The idea is to associate each row and each column with a latent indicator, $u _ { i } in { 1 , ldots , N _ { u } }$ , $v _ { j } in { 1 , ldots , N _ { v } }$ , where $N _ { u }$ is the number of row clusters, and $N _ { v }$ is the number of column clusters. We then use the following generative model: \nwhere $theta _ { a , b }$ are the parameters for row cluster $a$ and column cluster $b$ . \nFigure 21.20 shows a simple example. The data has the form $X _ { i j } = 1$ iff animal $i$ has feature $j$ , where $i ~ = ~ 1 : ~ 5 0$ and $j ~ = ~ 1 ~ : ~ 8 5$ . The animals represent whales, bears, horses, etc. The features represent properties of the habitat (jungle, tree, coastal), or anatomical properties (has teeth, quadripedal), or behavioral properties (swims, eats meat), etc. The method discovered 12 animal clusters and 33 feature clusters. ([Kem+06] use a Bayesian nonparametric method to infer the number of clusters.) For example, the O2 cluster is ${$ { antelope, horse, giraffe, zebra, deer $}$ , which is characterized by feature clusters $mathrm { F 2 } = left{ begin{array} { r l r l } end{array} right.$ { hooves, long neck, horns} and ${ mathrm { F 6 } } = left{ begin{array} { r l r l } end{array} right.$ { walks, quadripedal, ground $}$ , whereas the O4 cluster is ${$ { hippo, elephant, rhino $}$ , which is characterized by feature clusters $mathrm { F 4 } = left{ begin{array} { r l r } end{array} right.$ bulbous body shape, slow, inactive $}$ and F6. \n21.6.2 Nested partition models (Crosscat) \nThe problem with basic biclustering (Section 21.6.1) is that each object (row) can only belong to one cluster. Intuitively, an object can have multiple roles, and can be assigned to different clusters depending on which subset of features you use. For example, in the animal dataset, we may want to group the animals on the basis of anatomical features (e.g., mammals are warm blooded, reptiles are \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 not), or on the basis of behavioral features (e.g., predators vs prey).",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Biclustering *",
        "subsubsection": "Basic biclustering"
    },
    {
        "content": "O1 killer whale, blue whale, humpback, seal, walrus, dolphin   \nO2 antelope, horse, giraffe, zebra, deer   \nO3 monkey, gorilla, chimp   \nO4 hippo, elephant, rhino   \nO5 grizzly bear, polar bear   \nF1 flippers, strain teeth, swims, arctic, coastal, ocean, water   \nF2 hooves, long neck, horns   \nF3 hands, bipedal, jungle, tree   \nF4 bulbous body shape, slow, inactive   \nF5 meat teeth, eats meat, hunter, fierce   \nF6 walks, quadrapedal, ground \nO1  \nO2  \nO3  \nO4  \nO5   \n中\n21.6.1 Basic biclustering \nHere we present a simple probabilistic generative model for biclustering based on [Kem+06] (see also [SMM03] for a related approach). The idea is to associate each row and each column with a latent indicator, $u _ { i } in { 1 , ldots , N _ { u } }$ , $v _ { j } in { 1 , ldots , N _ { v } }$ , where $N _ { u }$ is the number of row clusters, and $N _ { v }$ is the number of column clusters. We then use the following generative model: \nwhere $theta _ { a , b }$ are the parameters for row cluster $a$ and column cluster $b$ . \nFigure 21.20 shows a simple example. The data has the form $X _ { i j } = 1$ iff animal $i$ has feature $j$ , where $i ~ = ~ 1 : ~ 5 0$ and $j ~ = ~ 1 ~ : ~ 8 5$ . The animals represent whales, bears, horses, etc. The features represent properties of the habitat (jungle, tree, coastal), or anatomical properties (has teeth, quadripedal), or behavioral properties (swims, eats meat), etc. The method discovered 12 animal clusters and 33 feature clusters. ([Kem+06] use a Bayesian nonparametric method to infer the number of clusters.) For example, the O2 cluster is ${$ { antelope, horse, giraffe, zebra, deer $}$ , which is characterized by feature clusters $mathrm { F 2 } = left{ begin{array} { r l r l } end{array} right.$ { hooves, long neck, horns} and ${ mathrm { F 6 } } = left{ begin{array} { r l r l } end{array} right.$ { walks, quadripedal, ground $}$ , whereas the O4 cluster is ${$ { hippo, elephant, rhino $}$ , which is characterized by feature clusters $mathrm { F 4 } = left{ begin{array} { r l r } end{array} right.$ bulbous body shape, slow, inactive $}$ and F6. \n21.6.2 Nested partition models (Crosscat) \nThe problem with basic biclustering (Section 21.6.1) is that each object (row) can only belong to one cluster. Intuitively, an object can have multiple roles, and can be assigned to different clusters depending on which subset of features you use. For example, in the animal dataset, we may want to group the animals on the basis of anatomical features (e.g., mammals are warm blooded, reptiles are \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 not), or on the basis of behavioral features (e.g., predators vs prey). \n\nWe now present a model that can capture this phenomenon. We illustrate the method with an example. Suppose we have a $6 times 6$ matrix, with $N _ { u } = 2$ row clusters and $N _ { v } = 3$ column clusters. Furthermore, suppose the latent column assignments are as follows: $pmb { v } = [ 1 , 1 , 2 , 3 , 3 , 3 ]$ . This means we put columns 1 and 2 into group 1, column 3 into group 2, and columns 4 to 6 into group 3. For the columns that get clustered into group 1, we cluster the rows as follows: $pmb { u } _ { : , 1 } = [ 1 , 1 , 1 , 2 , 2 , 2 ]$ ; For the columns that get clustered into group 2, we cluster the rows as follows: $pmb { u } _ { : , 2 } = [ 1 , 1 , 2 , 2 , 2 , 2 ]$ ; and for the columns that get clustered into group 3, we cluster the rows as follows: $pmb { u } _ { : , 3 } = left[ 1 , 1 , 1 , 1 , 1 , 2 right]$ . The resulting partition is shown in Figure 21.21(b). We see that the clustering of the rows depends on which group of columns we choose to focus on. \nFormally, we can define the model as follows: \nwhere $theta _ { k , l }$ are the parameters for cocluster $k in { 1 , ldots , N _ { u } }$ and $l in { 1 , ldots , N _ { v } }$ . \nThis model was independently proposed in [Sha+06; Man+16] who call it crosscat (for crosscategorization), in [Gua+10; CFD10], who call it multi-clust, and in [RG11], who call it nested partitioning. In all of these papers, the authors propose to use Dirichlet processes, to avoid the problem of estimating the number of clusters. Here we assume the number of clusters is known, and show the parameters explicitly, for notational simplicity. \nFigure 21.22 illustrates the model applied to some binary data containing 22 animals and 106 features. The figure shows the (approximate) MAP partition. The first partition of the columns contains taxonomic features, such as “has bones”, “is warm-blooded”, “lays eggs”, etc. This divides the \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nA B C Leopard Leopard Sheep Alligator Seal Python Dolphin Seal Monkey Dolphin Bat Frog Alligator Jellyfish Iguana Octopus Frog Penguin Python Finch Finch Seagull Ostrich Owl Seagull Eagle Owl Dragonfly Penguin Bat Eagle Grasshopper Grasshopper Ant Ant Bee Bee Sheep Jellyfish Frog Monkey Octopus Iguana Dragonfly ■ Ostrich 聘野畔 谢盟 animals into birds, reptiles/ amphibians, mammals, and invertebrates. The second partition of the columns contains features that are treated as noise, with no apparent structure (except for the single row labeled “frog”). The third partition of the columns contains ecological features like “dangerous”, “carnivorous”, “lives in water”, etc. This divides the animals into prey, land predators, sea predators and air predators. Thus each animal (row) can belong to a different cluster depending on what set of features are considered. \n\n22 Recommender Systems \nRecommender systems are systems which recommend items (such as movies, books, ads) to users based on various information, such as their past viewing/ purchasing behavior (e.g., which movies they rated high or low, which ads they clicked on), as well as optional “side information” such as demographics about the user, or information about the content of the item (e.g., its title, genre or price). Such systems are widely used by various internet companies, such as Facebook, Amazon, Netflix, Google, etc. In this chapter, we give a brief introduction to the topic. More details can be found in e.g., [DKK12; Pat12; Yan+14; AC16; Agg16; Zha+19b].. \n22.1 Explicit feedback \nIn this section, we consider the simplest setting in which the user gives explicit feedback to the system in terms of a rating, such as $+ 1$ or -1 (for like/dislike) or a score from 1 to 5. Let $Y _ { u i } in mathbb { R }$ be the rating that user $u$ gives to item $i$ . We can represent this as an $M times N$ matrix, where $M$ is the number of users, and $N$ is the number of items. Typically this matrix will be very large but very sparse, since most users will not provide any feedback on most items. See Figure 22.1(a) for an example. We can also view this sparse matrix as a bipartite graph, where the weight of the $u - i$ edge is $Y _ { u i }$ . This reflects the fact that we are dealing with relational data, i.e., the values of $u$ and $i$ have no intrinsic meaning (they are just arbitrary indices), it is the fact that $u$ and $i$ are connected that matters. \nIf $Y _ { u i }$ is missing, it could be because user $u$ has not interacted with item $i$ , or it could be that they knew they wouldn’t like it and so they chose not to engage with it. In the former case, some of the data is missing at random; in the latter case, the missingness is informative about the true value of $Y _ { u i }$ . (See e.g., [Mar+11] for further discussion of this point.) We will assume the data is missing at random, for simplicity. \n22.1.1 Datasets \nA famous example of an explicit ratings matrix was made available by the movie streaming company Netflix. In 2006, they released a large dataset of 100,480,507 movie ratings (on a scale of 1 to 5) from 480,189 users of 17,770 movies. Despite the large size of the training set, the ratings matrix is still $9 9 %$ sparse (unknown). Along with the data, they offered a prize of $$ 100,000$ , known as the Netflix Prize, to any team that could predict the true ratings of a set of test (user, item) pairs more accurately than their incumbent system. The prize was claimed on September 21, 2009 by a team known as “Pragmatic Chaos”. They used an ensemble of different methods, as described in [Kor09; BK07; FHK12]. However, a key component in their ensemble was the method described in Section 22.1.3.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Clustering",
        "subsection": "Biclustering *",
        "subsubsection": "Nested partition models (Crosscat)"
    },
    {
        "content": "22 Recommender Systems \nRecommender systems are systems which recommend items (such as movies, books, ads) to users based on various information, such as their past viewing/ purchasing behavior (e.g., which movies they rated high or low, which ads they clicked on), as well as optional “side information” such as demographics about the user, or information about the content of the item (e.g., its title, genre or price). Such systems are widely used by various internet companies, such as Facebook, Amazon, Netflix, Google, etc. In this chapter, we give a brief introduction to the topic. More details can be found in e.g., [DKK12; Pat12; Yan+14; AC16; Agg16; Zha+19b].. \n22.1 Explicit feedback \nIn this section, we consider the simplest setting in which the user gives explicit feedback to the system in terms of a rating, such as $+ 1$ or -1 (for like/dislike) or a score from 1 to 5. Let $Y _ { u i } in mathbb { R }$ be the rating that user $u$ gives to item $i$ . We can represent this as an $M times N$ matrix, where $M$ is the number of users, and $N$ is the number of items. Typically this matrix will be very large but very sparse, since most users will not provide any feedback on most items. See Figure 22.1(a) for an example. We can also view this sparse matrix as a bipartite graph, where the weight of the $u - i$ edge is $Y _ { u i }$ . This reflects the fact that we are dealing with relational data, i.e., the values of $u$ and $i$ have no intrinsic meaning (they are just arbitrary indices), it is the fact that $u$ and $i$ are connected that matters. \nIf $Y _ { u i }$ is missing, it could be because user $u$ has not interacted with item $i$ , or it could be that they knew they wouldn’t like it and so they chose not to engage with it. In the former case, some of the data is missing at random; in the latter case, the missingness is informative about the true value of $Y _ { u i }$ . (See e.g., [Mar+11] for further discussion of this point.) We will assume the data is missing at random, for simplicity. \n22.1.1 Datasets \nA famous example of an explicit ratings matrix was made available by the movie streaming company Netflix. In 2006, they released a large dataset of 100,480,507 movie ratings (on a scale of 1 to 5) from 480,189 users of 17,770 movies. Despite the large size of the training set, the ratings matrix is still $9 9 %$ sparse (unknown). Along with the data, they offered a prize of $$ 100,000$ , known as the Netflix Prize, to any team that could predict the true ratings of a set of test (user, item) pairs more accurately than their incumbent system. The prize was claimed on September 21, 2009 by a team known as “Pragmatic Chaos”. They used an ensemble of different methods, as described in [Kor09; BK07; FHK12]. However, a key component in their ensemble was the method described in Section 22.1.3. \n\nUnfortunately the Netflix data is no longer available due to privacy concerns. Fortunately the MovieLens group at the University of Minnesota have released an anonymized public dataset of movie ratings, on a scale of 1-5, that can be used for research [HK15]. There are also various other public explicit ratings datasets, such as the Jester jokes dataset from [Gol+01] and the BookCrossing dataset from [Zie+05]. \n22.1.2 Collaborative filtering \nThe original approach to the recommendation problem is called collaborative filtering [Gol+92]. The idea is that users collaborate on recommending items by sharing their ratings with other users; then if $u$ wants to know if they interact with $i$ , they can see what ratings other users $u ^ { prime }$ have given to $i$ , and take a weighted average: \nwhere we assume $Y _ { u ^ { prime } , i } =$ ? if the entry is unknown. The traditional approach measured the similarity of two users by comparing the sets $S _ { u } = { Y _ { u , i } neq ? : i in mathcal { I } }$ and $S _ { u ^ { prime } } = { Y _ { u ^ { prime } , i } neq ? : i in mathcal { T } }$ , where $mathcal { T }$ is the set of items. However, this can suffer from data sparsity. In Section 22.1.3 we discuss an approach based on learning dense embedding vectors for each item and each user, so we can compute similarity in a low dimensional feature space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Explicit feedback",
        "subsubsection": "Datasets"
    },
    {
        "content": "Unfortunately the Netflix data is no longer available due to privacy concerns. Fortunately the MovieLens group at the University of Minnesota have released an anonymized public dataset of movie ratings, on a scale of 1-5, that can be used for research [HK15]. There are also various other public explicit ratings datasets, such as the Jester jokes dataset from [Gol+01] and the BookCrossing dataset from [Zie+05]. \n22.1.2 Collaborative filtering \nThe original approach to the recommendation problem is called collaborative filtering [Gol+92]. The idea is that users collaborate on recommending items by sharing their ratings with other users; then if $u$ wants to know if they interact with $i$ , they can see what ratings other users $u ^ { prime }$ have given to $i$ , and take a weighted average: \nwhere we assume $Y _ { u ^ { prime } , i } =$ ? if the entry is unknown. The traditional approach measured the similarity of two users by comparing the sets $S _ { u } = { Y _ { u , i } neq ? : i in mathcal { I } }$ and $S _ { u ^ { prime } } = { Y _ { u ^ { prime } , i } neq ? : i in mathcal { T } }$ , where $mathcal { T }$ is the set of items. However, this can suffer from data sparsity. In Section 22.1.3 we discuss an approach based on learning dense embedding vectors for each item and each user, so we can compute similarity in a low dimensional feature space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n22.1.3 Matrix factorization \nWe can view the recommender problem as one of matrix completion, in which we wish to predict all the missing entries of $mathbf { Y }$ . We can formulate this as the following optimization problem: \nHowever, this is an under-specified problem, since there are an infinite number of ways of filling in the missing entries of $mathbf { Z }$ . \nWe need to add some constraints. Suppose we assume that $mathbf { Y }$ is low rank. Then we can write it in the form $mathbf { Z } = mathbf { U } mathbf { V } ^ { parallel } approx mathbf { Y }$ , where $mathbf { U }$ is an $M times K$ matrix, $mathbf { V }$ is a $N times K$ matrix, $K$ is the rank of the matrix, $M$ is the number of users, and $N$ is the number of items. This corresponds to a prediction of the form by writing \nThis is called matrix factorization. \nIf we observe all the $Y _ { i j }$ entries, we can find the optimal $mathbf { Z }$ using SVD (Section 7.5). However, when $mathbf { Y }$ has missing entries, the corresponding objective is no longer convex, and does not have a unique optimum [SJ03]. We can fit this using alternating least squares (ALS), where we estimate $mathbf { U }$ given $mathbf { V }$ and then estimate $mathbf { V }$ given $mathbf { U }$ (for details, see e.g., [KBV09]). Alternatively we can just use SGD. \nIn practice, it is important to also allow for user-specific and item-specific baselines, by writing \nThis can capture the fact that some users might always tend to give low ratings and others may give high ratings; in addition, some items (e.g., very popular movies) might have unusually high ratings. In addition, we can add some $ell _ { 2 }$ regularization to the parameters to get the objective \nWe can optimize this using SGD by sampling a random $( u , i )$ entry from the set of observed values, and performing the following updates: \nwhere $e _ { u i } = y _ { u i } - { hat { y } } _ { u i }$ is the error term, and $eta geq 0$ is the learning rate. This approach was first proposed by Simon Funk, who was one of the first to do well in the early days of the Netflix competition.1",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Explicit feedback",
        "subsubsection": "Collaborative filtering"
    },
    {
        "content": "22.1.3 Matrix factorization \nWe can view the recommender problem as one of matrix completion, in which we wish to predict all the missing entries of $mathbf { Y }$ . We can formulate this as the following optimization problem: \nHowever, this is an under-specified problem, since there are an infinite number of ways of filling in the missing entries of $mathbf { Z }$ . \nWe need to add some constraints. Suppose we assume that $mathbf { Y }$ is low rank. Then we can write it in the form $mathbf { Z } = mathbf { U } mathbf { V } ^ { parallel } approx mathbf { Y }$ , where $mathbf { U }$ is an $M times K$ matrix, $mathbf { V }$ is a $N times K$ matrix, $K$ is the rank of the matrix, $M$ is the number of users, and $N$ is the number of items. This corresponds to a prediction of the form by writing \nThis is called matrix factorization. \nIf we observe all the $Y _ { i j }$ entries, we can find the optimal $mathbf { Z }$ using SVD (Section 7.5). However, when $mathbf { Y }$ has missing entries, the corresponding objective is no longer convex, and does not have a unique optimum [SJ03]. We can fit this using alternating least squares (ALS), where we estimate $mathbf { U }$ given $mathbf { V }$ and then estimate $mathbf { V }$ given $mathbf { U }$ (for details, see e.g., [KBV09]). Alternatively we can just use SGD. \nIn practice, it is important to also allow for user-specific and item-specific baselines, by writing \nThis can capture the fact that some users might always tend to give low ratings and others may give high ratings; in addition, some items (e.g., very popular movies) might have unusually high ratings. In addition, we can add some $ell _ { 2 }$ regularization to the parameters to get the objective \nWe can optimize this using SGD by sampling a random $( u , i )$ entry from the set of observed values, and performing the following updates: \nwhere $e _ { u i } = y _ { u i } - { hat { y } } _ { u i }$ is the error term, and $eta geq 0$ is the learning rate. This approach was first proposed by Simon Funk, who was one of the first to do well in the early days of the Netflix competition.1 \n22.1.3.1 Probabilistic matrix factorization (PMF) \nWe can convert matrix factorization into a probabilistic model by defining \nThis is known as probabilistic matrix factorization (PMF) [SM08]. The NLL of this model is equivalent to the matrix factorization objective in Equation (22.2). However, the probabilistic perspective allows us to generalize the model more easily. For example, we can capture the fact that the ratings are integers (often mostly 0s), and not reals, using a Poisson or negative Binomial likelihood (see e.g., [GOF18]). This is similar to exponential family PCA (Section 20.2.7), except that we view rows and columns symmetrically. \n22.1.3.2 Example: Netflix \nSuppose we apply PMF to the Netflix dataset using $K = 2$ latent factors. Figure 22.2 visualizes the learned embedding vectors $mathbf { Delta } mathbf { u } _ { i }$ for a few movies. On the left of the plot we have low-brow humor and horror movies (Half Baked, Freddy vs Jason), and on the right we have more serious dramas (Sophie’s Choice, Moonstruck). On the top we have critically acclaimed independent movies (Punch-Drunk Love, I Heart Huckabees), and on the bottom we have mainstream Hollywood blockbusters (Armageddon, Runway Bride). The Wizard of $O z$ is right in the middle of these axes, since it is in some senses an “average movie”. \nUsers are embedded into the same spaces as movies. We can then predict the rating for any user-video pair using proximity in the latent embedding space. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n22.1.3.3 Example: MovieLens \nNow suppose we apply PMF to the MovieLens-1M dataset with 6040 users, 3706 movies, and 1,000,209 ratings. We will use $K = 5 0$ factors. For simplicity, we fit this using SVD applied to the dense ratings matrix, where we replace missing values with 0. (This is just a simple approximation to keep the demo code simple.) In Figure 22.3 we show a snippet of the true and predicted ratings matrix. (We truncate the predictions to lie in the range [1,5].) We see that the model is not particularly accurate, but does capture some structure in the data. \nFurthermore, it seems to behave in a qualitatively sensible way. For example, in Figure 22.4 we show the top 10 movies rated by a given user as well as the top 10 predictions for movies they had not seen. The model seems to have “picked up” on the underlying preferences of the user. For example, we see that many of the predicted movies are action or film-noir, and both of these genres feature in the user’s own top-10 list, even though explicit genre information is not used during model training. \n22.1.4 Autoencoders \nMatrix factorization is a (bi)linear model. We can make a nonlinear version using autoencoders. Let $pmb { y } _ { : , i } in mathbb { R } ^ { M }$ be the $i$ ’th column of the ratings matrix, where unknown ratings are set to $0$ . We can predict this ratings vector using an autoencoder of the form \nwhere $mathbf { V } in mathbb { R } ^ { K M }$ maps the ratings to an embedding space, $mathbf { W } in mathbb { R } ^ { K M }$ maps the embedding space to a distribution over ratings, $pmb { mu } in mathbb { R } ^ { K }$ are the biases of the hidden units, and $pmb { b } in mathbb { R } ^ { M }$ are the biases of the output units. This is called the (item-based) version of the AutoRec model [Sed+15]. This has $2 M K + M + K$ parameters. There is also a user-based version, that can be derived in a similar manner, which has $2 N K + N + K$ parameters. (On MovieLens and Netflix, the authors find that the item-based method works better.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Explicit feedback",
        "subsubsection": "Matrix factorization"
    },
    {
        "content": "22.1.3.3 Example: MovieLens \nNow suppose we apply PMF to the MovieLens-1M dataset with 6040 users, 3706 movies, and 1,000,209 ratings. We will use $K = 5 0$ factors. For simplicity, we fit this using SVD applied to the dense ratings matrix, where we replace missing values with 0. (This is just a simple approximation to keep the demo code simple.) In Figure 22.3 we show a snippet of the true and predicted ratings matrix. (We truncate the predictions to lie in the range [1,5].) We see that the model is not particularly accurate, but does capture some structure in the data. \nFurthermore, it seems to behave in a qualitatively sensible way. For example, in Figure 22.4 we show the top 10 movies rated by a given user as well as the top 10 predictions for movies they had not seen. The model seems to have “picked up” on the underlying preferences of the user. For example, we see that many of the predicted movies are action or film-noir, and both of these genres feature in the user’s own top-10 list, even though explicit genre information is not used during model training. \n22.1.4 Autoencoders \nMatrix factorization is a (bi)linear model. We can make a nonlinear version using autoencoders. Let $pmb { y } _ { : , i } in mathbb { R } ^ { M }$ be the $i$ ’th column of the ratings matrix, where unknown ratings are set to $0$ . We can predict this ratings vector using an autoencoder of the form \nwhere $mathbf { V } in mathbb { R } ^ { K M }$ maps the ratings to an embedding space, $mathbf { W } in mathbb { R } ^ { K M }$ maps the embedding space to a distribution over ratings, $pmb { mu } in mathbb { R } ^ { K }$ are the biases of the hidden units, and $pmb { b } in mathbb { R } ^ { M }$ are the biases of the output units. This is called the (item-based) version of the AutoRec model [Sed+15]. This has $2 M K + M + K$ parameters. There is also a user-based version, that can be derived in a similar manner, which has $2 N K + N + K$ parameters. (On MovieLens and Netflix, the authors find that the item-based method works better.) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nWe can fit this by only updating parameters that are associated with the observed entries of $mathbf { it _ y } _ { : , i }$ Furthermore, we can add an $ell _ { 2 }$ regularizer to the weight matrices to get the objective \nDespite the simplicity of this method, the authors find that this does better than more complex methods such as restricted Boltzmann machines (RBMs, [SMH07]) and local low-rank matrix \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \napproximation (LLORMA, [Lee+13]). \n22.2 Implicit feedback \nSo far, we have assumed that the user gives explicit ratings for each item that they interact with. This is a very restrictive assumption. More generally, we would like to learn from the implicit feedback that users give just by interacting with a system. For example, we can treat the list of movies that user $u$ watches as positives, and regard all the other movies as negatives. Thus we get a sparse, positive-only ratings matrix. \nAlternatively, we can view the fact that they watched movie $i$ but did not watch movie $j$ as an implicit signal that they prefer $i$ to $j$ . The resulting data can be represented as a set of tuples of the form $y _ { n } = ( u , i , j )$ , where $( u , i )$ is a positive pair, and $( u , j )$ is a negative (or unlabeled) pair. \n22.2.1 Bayesian personalized ranking \nTo fit a model to data of the form $( u , i , j )$ , we need to use a ranking loss, so that the model ranks $i$ ahead of $j$ for user $u$ . A simple way to do this is to use a Bernoulli model of the form \nIf we combine this with a Gaussian prior for $pmb theta$ , we get the following MAP estimation problem: \nwhere $mathcal { D } = { ( u , i , j ) : i in mathcal { T } _ { u } ^ { + } , j in mathcal { T } setminus mathcal { T } _ { u } ^ { + } }$ , where $mathcal { I } _ { u } ^ { + }$ are the set of all items that user $u$ selected, and $mathcal { T } backslash mathcal { T } _ { u } ^ { + }$ are all the other items (which they may dislike, or simply may not have seen). This is known as Bayesian personalized ranking or BPR [Ren+09]. \nLet us consider this example from [Zha+20, Sec 16.5]. There are 4 items in total, $mathcal { T } = { i _ { 1 } , i _ { 2 } , i _ { 3 } , i _ { 4 } }$ , and user $u$ chose to interact with $mathcal { T } _ { u } ^ { + } = { i _ { 2 } , i _ { 3 } }$ . In this case, the implicit item-item preference matrix for user $u$ has the form \nwhere $Y _ { u , i , i ^ { prime } } = +$ means user $u$ prefers $i ^ { prime }$ to $i$ , $Y _ { u , i , i ^ { prime } } = -$ means user $u$ prefers $i$ to $i ^ { prime }$ , and $Y _ { u , i , i ^ { prime } } = :$ means we cannot tell what the user’s preference is. For example, focusing on the second column, we see that this user rates $i _ { 2 }$ higher than $i _ { 1 }$ and $i _ { 4 }$ , since they selected $i _ { 2 }$ but not $i _ { 1 }$ or $i _ { 4 }$ ; however, we cannot tell if they prefer $i _ { 2 }$ over $i _ { 3 }$ or vice versa. \nWhen the set of posssible items is large, the number of negatives in $mathcal { T } backslash mathcal { T } _ { u } ^ { + }$ can be very large. Fortunately we can approximate the loss by subsampling negatives. \nNote that an alternative to the log-loss above is to use a hinge loss, similar to the approach used in SVMs (Section 17.3). This has the form \nwhere $m geq 0$ is the safety margin. This tries to ensure the negative items $j$ never score more than $m$ higher than the positive items $i$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Explicit feedback",
        "subsubsection": "Autoencoders"
    },
    {
        "content": "approximation (LLORMA, [Lee+13]). \n22.2 Implicit feedback \nSo far, we have assumed that the user gives explicit ratings for each item that they interact with. This is a very restrictive assumption. More generally, we would like to learn from the implicit feedback that users give just by interacting with a system. For example, we can treat the list of movies that user $u$ watches as positives, and regard all the other movies as negatives. Thus we get a sparse, positive-only ratings matrix. \nAlternatively, we can view the fact that they watched movie $i$ but did not watch movie $j$ as an implicit signal that they prefer $i$ to $j$ . The resulting data can be represented as a set of tuples of the form $y _ { n } = ( u , i , j )$ , where $( u , i )$ is a positive pair, and $( u , j )$ is a negative (or unlabeled) pair. \n22.2.1 Bayesian personalized ranking \nTo fit a model to data of the form $( u , i , j )$ , we need to use a ranking loss, so that the model ranks $i$ ahead of $j$ for user $u$ . A simple way to do this is to use a Bernoulli model of the form \nIf we combine this with a Gaussian prior for $pmb theta$ , we get the following MAP estimation problem: \nwhere $mathcal { D } = { ( u , i , j ) : i in mathcal { T } _ { u } ^ { + } , j in mathcal { T } setminus mathcal { T } _ { u } ^ { + } }$ , where $mathcal { I } _ { u } ^ { + }$ are the set of all items that user $u$ selected, and $mathcal { T } backslash mathcal { T } _ { u } ^ { + }$ are all the other items (which they may dislike, or simply may not have seen). This is known as Bayesian personalized ranking or BPR [Ren+09]. \nLet us consider this example from [Zha+20, Sec 16.5]. There are 4 items in total, $mathcal { T } = { i _ { 1 } , i _ { 2 } , i _ { 3 } , i _ { 4 } }$ , and user $u$ chose to interact with $mathcal { T } _ { u } ^ { + } = { i _ { 2 } , i _ { 3 } }$ . In this case, the implicit item-item preference matrix for user $u$ has the form \nwhere $Y _ { u , i , i ^ { prime } } = +$ means user $u$ prefers $i ^ { prime }$ to $i$ , $Y _ { u , i , i ^ { prime } } = -$ means user $u$ prefers $i$ to $i ^ { prime }$ , and $Y _ { u , i , i ^ { prime } } = :$ means we cannot tell what the user’s preference is. For example, focusing on the second column, we see that this user rates $i _ { 2 }$ higher than $i _ { 1 }$ and $i _ { 4 }$ , since they selected $i _ { 2 }$ but not $i _ { 1 }$ or $i _ { 4 }$ ; however, we cannot tell if they prefer $i _ { 2 }$ over $i _ { 3 }$ or vice versa. \nWhen the set of posssible items is large, the number of negatives in $mathcal { T } backslash mathcal { T } _ { u } ^ { + }$ can be very large. Fortunately we can approximate the loss by subsampling negatives. \nNote that an alternative to the log-loss above is to use a hinge loss, similar to the approach used in SVMs (Section 17.3). This has the form \nwhere $m geq 0$ is the safety margin. This tries to ensure the negative items $j$ never score more than $m$ higher than the positive items $i$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n22.2.2 Factorization machines \nThe AutoRec approach of Section 22.1.4 is nonlinear, but treats users and items asymmetrically. In this section, we discuss a more symmetric discriminative modeling approach. We start with a linear version. The basic idea is to predict the output (such as a rating) for any given user-item pair, $pmb { x } = [ mathrm { o n e - h o t } ( u )$ , one-hot $( i ) ]$ , using \nwhere $pmb { x } in mathbb { R } ^ { D }$ where $D = ( M + N )$ is the number of inputs, $mathbf { V } in mathbb { R } ^ { D times K }$ is a weight matrix, ${ pmb w } in mathbb { R } ^ { D }$ is a weight vector, and $mu in mathbb { R }$ is a global offset. This is known as a factorization machine (FM) [Ren12]. \nThe term $( { pmb v } _ { i } ^ { 1 } { pmb v } _ { j } ) x _ { i } x _ { j }$ measures the interaction between feature $i$ and $j$ in the input. This generalizes the matrix factorization model of Equation (22.4), since it can handle other kinds of information in the input $_ { x }$ , beyond just user and item, as we discuss in Section 22.3. \nComputing Equation (22.17) takes $O ( K D ^ { 2 } )$ time, since it considers all possible pairwise interactions between every user and every item. Fortunately we can rewrite this so that we can compute it in $O ( K D )$ time as follows: \nFor sparse vectors, the overall complexity is linear in the number of non-zero components. So if we use one-hot encodings of the user and item id, the complexity is just $O ( K )$ , analogous to the original matrix factorization objective of Equation (22.4). \nWe can fit this model to minimize any loss we want. For example, if we have explicit feedback, we may choose MSE loss, and if we have implicit feedback, we may choosing ranking loss. \nIn [Guo+17], they propose a model called deep factorization machines, which combines the above method with an MLP applied to a concatenation of the embedding vectors, instead of the inner product. More precisely, it is a model of the form \nThis is closely related to the wide and deep model proposed in [Che+16]. The idea is that the bilinear FM model captures explicit interactions between specific users and items (a form of memorization), whereas the MLP captures implicit interactions between user features and item features, which allows the model to generalize. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Implicit feedback",
        "subsubsection": "Bayesian personalized ranking"
    },
    {
        "content": "22.2.2 Factorization machines \nThe AutoRec approach of Section 22.1.4 is nonlinear, but treats users and items asymmetrically. In this section, we discuss a more symmetric discriminative modeling approach. We start with a linear version. The basic idea is to predict the output (such as a rating) for any given user-item pair, $pmb { x } = [ mathrm { o n e - h o t } ( u )$ , one-hot $( i ) ]$ , using \nwhere $pmb { x } in mathbb { R } ^ { D }$ where $D = ( M + N )$ is the number of inputs, $mathbf { V } in mathbb { R } ^ { D times K }$ is a weight matrix, ${ pmb w } in mathbb { R } ^ { D }$ is a weight vector, and $mu in mathbb { R }$ is a global offset. This is known as a factorization machine (FM) [Ren12]. \nThe term $( { pmb v } _ { i } ^ { 1 } { pmb v } _ { j } ) x _ { i } x _ { j }$ measures the interaction between feature $i$ and $j$ in the input. This generalizes the matrix factorization model of Equation (22.4), since it can handle other kinds of information in the input $_ { x }$ , beyond just user and item, as we discuss in Section 22.3. \nComputing Equation (22.17) takes $O ( K D ^ { 2 } )$ time, since it considers all possible pairwise interactions between every user and every item. Fortunately we can rewrite this so that we can compute it in $O ( K D )$ time as follows: \nFor sparse vectors, the overall complexity is linear in the number of non-zero components. So if we use one-hot encodings of the user and item id, the complexity is just $O ( K )$ , analogous to the original matrix factorization objective of Equation (22.4). \nWe can fit this model to minimize any loss we want. For example, if we have explicit feedback, we may choose MSE loss, and if we have implicit feedback, we may choosing ranking loss. \nIn [Guo+17], they propose a model called deep factorization machines, which combines the above method with an MLP applied to a concatenation of the embedding vectors, instead of the inner product. More precisely, it is a model of the form \nThis is closely related to the wide and deep model proposed in [Che+16]. The idea is that the bilinear FM model captures explicit interactions between specific users and items (a form of memorization), whereas the MLP captures implicit interactions between user features and item features, which allows the model to generalize. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n22.2.3 Neural matrix factorization \nIn this section, we describe the neural matrix factorization model of [He+17]. This is another way to combine bilinear models with deep neural networks. The bilinear part is used to define the following generalized matrix factorization (GMF) pathway, which computes the following feature vector for user $u$ and item $i$ : \nwhere $mathbf { P } in mathbb { R } ^ { M K }$ is a user embedding matrix, and $mathbf { Q } in mathbb { R } ^ { N K }$ is an item embedding matrix. The DNN part is just an MLP applied to a concatenation of the embedding vectors (using different embedding matrices): \nFinally, the model combines these to get \nSee Figure 22.5 for an illustration. \nIn [He+17], the model is trained on implicit feedback, where $y _ { u i } = 1$ if the interaction of user $u$ with item $i$ is observed, and $y _ { u i } = 0$ otherwise. However, it could be trained to minimize BPR loss. \n22.3 Leveraging side information \nSo far, we have assumed that the only information available to the predictor are the integer id of the user and the integer id of the item. This is an extremely impoverished representation, and will fail to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Implicit feedback",
        "subsubsection": "Factorization machines"
    },
    {
        "content": "22.2.3 Neural matrix factorization \nIn this section, we describe the neural matrix factorization model of [He+17]. This is another way to combine bilinear models with deep neural networks. The bilinear part is used to define the following generalized matrix factorization (GMF) pathway, which computes the following feature vector for user $u$ and item $i$ : \nwhere $mathbf { P } in mathbb { R } ^ { M K }$ is a user embedding matrix, and $mathbf { Q } in mathbb { R } ^ { N K }$ is an item embedding matrix. The DNN part is just an MLP applied to a concatenation of the embedding vectors (using different embedding matrices): \nFinally, the model combines these to get \nSee Figure 22.5 for an illustration. \nIn [He+17], the model is trained on implicit feedback, where $y _ { u i } = 1$ if the interaction of user $u$ with item $i$ is observed, and $y _ { u i } = 0$ otherwise. However, it could be trained to minimize BPR loss. \n22.3 Leveraging side information \nSo far, we have assumed that the only information available to the predictor are the integer id of the user and the integer id of the item. This is an extremely impoverished representation, and will fail to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Implicit feedback",
        "subsubsection": "Neural matrix factorization"
    },
    {
        "content": "22.2.3 Neural matrix factorization \nIn this section, we describe the neural matrix factorization model of [He+17]. This is another way to combine bilinear models with deep neural networks. The bilinear part is used to define the following generalized matrix factorization (GMF) pathway, which computes the following feature vector for user $u$ and item $i$ : \nwhere $mathbf { P } in mathbb { R } ^ { M K }$ is a user embedding matrix, and $mathbf { Q } in mathbb { R } ^ { N K }$ is an item embedding matrix. The DNN part is just an MLP applied to a concatenation of the embedding vectors (using different embedding matrices): \nFinally, the model combines these to get \nSee Figure 22.5 for an illustration. \nIn [He+17], the model is trained on implicit feedback, where $y _ { u i } = 1$ if the interaction of user $u$ with item $i$ is observed, and $y _ { u i } = 0$ otherwise. However, it could be trained to minimize BPR loss. \n22.3 Leveraging side information \nSo far, we have assumed that the only information available to the predictor are the integer id of the user and the integer id of the item. This is an extremely impoverished representation, and will fail to \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nFeature vector x Targety   \nX 1 0 0 1 0 0 0 0.30.30.3 0 13 0 0 0 0 5 y   \n$mathbf { x } _ { 2 }$ 1 0 0 0 1 0 0 0.30.30.3 0 14 1 0 0 0 3 ${ tt y } _ { 2 }$   \n$mathbf { x } _ { 3 }$ 1 0 0 0 0 1 0 0.30.30.3 0 16 0 0 0 1 ${ tt y } _ { 3 }$   \n$mathbf { x } _ { 4 }$ 0 1 0 0 0 0 0 0 0.50.5 5 0 0 0 0 4 y   \n$pmb { x } _ { 5 }$ 0 1 0 0 0 0 1 0 0 0.50.5 8 0 0 1 0 · 5 y5   \n$mathbf { x } _ { 6 }$ 0 0 1 1 0 0 0 * 0.5 0 0.5 0 9 0 0 0 0 # 1 y6   \nX7 0 0 1 0 0 0 0.5 0 0.5 0 12 1 0 0 0 5 y7 B SW TI NH SW ST User Movie Time Last Movierated \nwork if we encounter a new user or new item (the so-called cold start problem). To overcome this, we need to leverage “side information”, beyond just the id of the user/item. \nThere are many forms of side information we can use. For items, we often have rich meta-data, such text (e.g., title), images (e.g., cover), high-dimensional categorical variables (e.g., location), or just scalars (eg., price). For users, the side information available depends on the specific form of the interactive system. For search engines, it is the list of queries the user has issued, and (if they are logged in), information derived from websites they have visited (which is tracked via cookies). For online shopping sites, it is the list of searches plus past viewing and purchasing behavior. For social networking sites, there is information about the friendship graph of each user. \nIt is very easy to capture this side information in the factorization machines framework, by expanding our definition of $_ { x }$ beyond the two one-hot vectors, as illustrated in Figure 22.6. The same input encoding can of course be fed into other kinds of models, such as deepFM or neuralMF. \nIn addition to features about the user and item, there may be other contextual features, such as the time of the interaction (e.g., the day or evening). The order (sequence) of the most recently viewed items is often also a useful signal. The “Convolutional Sequence Embedding Recommendation” or Caser model proposed in [TW18] captures this by embedding the last $M$ items, and then treating the $M times K$ input as an image, by using a convolutional layer as part of the model. \nMany other kinds of neural models can be designed for the recommender task. See e.g., [Zha+19b] for a review. \n22.4 Exploration-exploitation tradeoff \nAn interesting “twist” to recommender systems that does not arise in other kinds of prediction problems is the fact that the data that the system is trained on is a consequence of recommendations made by earlier versions of the system. Thus there is a feedback loop [Bot+13]. For example, consider the YouTube video recommendation system [CAS16]. There are millions of videos on the site, so the system must come up with a shortlist, or “slate”, of videos to show the user, to help them find what they want (see e.g., [Ie+19]). If the user watches one of these videos, the system can consider this positive feedback that it made a good recommendation, and it can update the model parameters accordingly. However, maybe there was some other video that the user would have liked even more? \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Leveraging side information",
        "subsubsection": "N/A"
    },
    {
        "content": "Feature vector x Targety   \nX 1 0 0 1 0 0 0 0.30.30.3 0 13 0 0 0 0 5 y   \n$mathbf { x } _ { 2 }$ 1 0 0 0 1 0 0 0.30.30.3 0 14 1 0 0 0 3 ${ tt y } _ { 2 }$   \n$mathbf { x } _ { 3 }$ 1 0 0 0 0 1 0 0.30.30.3 0 16 0 0 0 1 ${ tt y } _ { 3 }$   \n$mathbf { x } _ { 4 }$ 0 1 0 0 0 0 0 0 0.50.5 5 0 0 0 0 4 y   \n$pmb { x } _ { 5 }$ 0 1 0 0 0 0 1 0 0 0.50.5 8 0 0 1 0 · 5 y5   \n$mathbf { x } _ { 6 }$ 0 0 1 1 0 0 0 * 0.5 0 0.5 0 9 0 0 0 0 # 1 y6   \nX7 0 0 1 0 0 0 0.5 0 0.5 0 12 1 0 0 0 5 y7 B SW TI NH SW ST User Movie Time Last Movierated \nwork if we encounter a new user or new item (the so-called cold start problem). To overcome this, we need to leverage “side information”, beyond just the id of the user/item. \nThere are many forms of side information we can use. For items, we often have rich meta-data, such text (e.g., title), images (e.g., cover), high-dimensional categorical variables (e.g., location), or just scalars (eg., price). For users, the side information available depends on the specific form of the interactive system. For search engines, it is the list of queries the user has issued, and (if they are logged in), information derived from websites they have visited (which is tracked via cookies). For online shopping sites, it is the list of searches plus past viewing and purchasing behavior. For social networking sites, there is information about the friendship graph of each user. \nIt is very easy to capture this side information in the factorization machines framework, by expanding our definition of $_ { x }$ beyond the two one-hot vectors, as illustrated in Figure 22.6. The same input encoding can of course be fed into other kinds of models, such as deepFM or neuralMF. \nIn addition to features about the user and item, there may be other contextual features, such as the time of the interaction (e.g., the day or evening). The order (sequence) of the most recently viewed items is often also a useful signal. The “Convolutional Sequence Embedding Recommendation” or Caser model proposed in [TW18] captures this by embedding the last $M$ items, and then treating the $M times K$ input as an image, by using a convolutional layer as part of the model. \nMany other kinds of neural models can be designed for the recommender task. See e.g., [Zha+19b] for a review. \n22.4 Exploration-exploitation tradeoff \nAn interesting “twist” to recommender systems that does not arise in other kinds of prediction problems is the fact that the data that the system is trained on is a consequence of recommendations made by earlier versions of the system. Thus there is a feedback loop [Bot+13]. For example, consider the YouTube video recommendation system [CAS16]. There are millions of videos on the site, so the system must come up with a shortlist, or “slate”, of videos to show the user, to help them find what they want (see e.g., [Ie+19]). If the user watches one of these videos, the system can consider this positive feedback that it made a good recommendation, and it can update the model parameters accordingly. However, maybe there was some other video that the user would have liked even more? \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIt is impossible to answer this counterfactual unless the system takes a chance and shows some items for which the user response is uncertain. This is an example of the exploration-exploitation tradeoff. \nIn addition to needing to explore, the system may have to wait for a long time until it can detect if a change it made its recommendation policies was beneficial. It is common to use reinforcement learning to learn policies which optimize long-term reward. See the sequel to this book, [Mur23], for details. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n23 Graph Embeddings * \nThis chapter is coauthored with Bryan Perozzi, Sami Abu-El-Haija and Ines Chami, and is based on [Cha+21]. \n23.1 Introduction \nWe now turn our focus to data which has semantic relationships between training samples ${ { bf x } _ { n } } _ { n = 1 } ^ { N }$ . The relationships (known as edges) connect training samples (nodes) with an application specific meaning (commonly similarity). Graphs provide the mathematical foundations for reasoning about these kind of relationships \nGraphs are universal data structures that can represent complex relational data (composed of nodes and edges), and appear in multiple domains such as social networks, computational chemistry [Gil+17], biology [Sta+06], recommendation systems [KSJ09], semi-supervised learning [GB18], and others. \nLet ${ bf A } in { 0 , 1 } ^ { N times N }$ be the adjacency matrix, where $N$ is the number of nodes, and let $mathbf { W } in mathbb { R } ^ { N times N }$ be a weighted version. In the methods we discuss below, some set $mathbf { W } = mathbf { A }$ while others set $mathbf { W }$ to a transformation of A, such as row-wise normalization. Finally, let $mathbf { X } in mathbb { R } ^ { N times D }$ be a matrix of node features. \nWhen designing and training a neural network model over graph data, we desire the designed method be applicable to nodes which participate in different graph settings (e.g. have differing connections and community structure). Contrast this with a neural network model designed for images, where each pixel (node) has the same neighborhood structure. By contrast, an arbitrary graph has no specified alignment of nodes, and further, each node might have a different neighborhood structure. See Figure 23.1 for a comparison. Consequently, operations like Euclidean spatial convolution cannot be directly applied on irregular graphs: Euclidean convolutions strongly rely on geometric priors (such as shift invariance), which don’t generalize to non-Euclidean domains. \nThese challenges led to the development of Geometric Deep Learning (GDL) research [Bro+17b], which aims at applying deep learning techniques to non-Euclidean data. In particular, given the widespread prevalence of graphs in real-world applications, there has been a surge of interest in applying machine learning methods to graph-structured data. Among these, Graph Representation Learning (GRL) [Cha+21] methods aim at learning low-dimensional continuous vector representations for graph-structured data, also called embeddings. \nWe divide GRL here into two classes of problems: unsupervised and supervised (or semisupervised) GRL. The first class aims at learning low-dimensional Euclidean representations optimizing an objective, e.g. one that preserve the structure of an input graph. The second class also learns low-dimensional Euclidean representations but for a specific downstream prediction task such as node or graph classification. Further, the graph structure can be fixed throughout training and testing, which is known as the transductive learning setting (e.g. predicting user properties in a large social network), or alternatively the model is expected to answer questions about graphs not seen during training, known as the inductive learning setting (e.g. classifying molecular structures). Finally, while most supervised and unsupervised methods learn representations in Euclidean vector spaces, there recently has been interest for non-Euclidean representation learning, which aims at learning non-Euclidean embedding spaces such as hyperbolic or spherical spaces. The main motivations for this body of work is to use a continuous embedding space that resembles the underlying discrete structure of the input data it tries to embed (e.g. the hyperbolic space is a continuous version of trees [Sar11]).",
        "chapter": "V Beyond Supervised Learning",
        "section": "Recommender Systems",
        "subsection": "Exploration-exploitation tradeoff",
        "subsubsection": "N/A"
    },
    {
        "content": "23 Graph Embeddings * \nThis chapter is coauthored with Bryan Perozzi, Sami Abu-El-Haija and Ines Chami, and is based on [Cha+21]. \n23.1 Introduction \nWe now turn our focus to data which has semantic relationships between training samples ${ { bf x } _ { n } } _ { n = 1 } ^ { N }$ . The relationships (known as edges) connect training samples (nodes) with an application specific meaning (commonly similarity). Graphs provide the mathematical foundations for reasoning about these kind of relationships \nGraphs are universal data structures that can represent complex relational data (composed of nodes and edges), and appear in multiple domains such as social networks, computational chemistry [Gil+17], biology [Sta+06], recommendation systems [KSJ09], semi-supervised learning [GB18], and others. \nLet ${ bf A } in { 0 , 1 } ^ { N times N }$ be the adjacency matrix, where $N$ is the number of nodes, and let $mathbf { W } in mathbb { R } ^ { N times N }$ be a weighted version. In the methods we discuss below, some set $mathbf { W } = mathbf { A }$ while others set $mathbf { W }$ to a transformation of A, such as row-wise normalization. Finally, let $mathbf { X } in mathbb { R } ^ { N times D }$ be a matrix of node features. \nWhen designing and training a neural network model over graph data, we desire the designed method be applicable to nodes which participate in different graph settings (e.g. have differing connections and community structure). Contrast this with a neural network model designed for images, where each pixel (node) has the same neighborhood structure. By contrast, an arbitrary graph has no specified alignment of nodes, and further, each node might have a different neighborhood structure. See Figure 23.1 for a comparison. Consequently, operations like Euclidean spatial convolution cannot be directly applied on irregular graphs: Euclidean convolutions strongly rely on geometric priors (such as shift invariance), which don’t generalize to non-Euclidean domains. \nThese challenges led to the development of Geometric Deep Learning (GDL) research [Bro+17b], which aims at applying deep learning techniques to non-Euclidean data. In particular, given the widespread prevalence of graphs in real-world applications, there has been a surge of interest in applying machine learning methods to graph-structured data. Among these, Graph Representation Learning (GRL) [Cha+21] methods aim at learning low-dimensional continuous vector representations for graph-structured data, also called embeddings. \nWe divide GRL here into two classes of problems: unsupervised and supervised (or semisupervised) GRL. The first class aims at learning low-dimensional Euclidean representations optimizing an objective, e.g. one that preserve the structure of an input graph. The second class also learns low-dimensional Euclidean representations but for a specific downstream prediction task such as node or graph classification. Further, the graph structure can be fixed throughout training and testing, which is known as the transductive learning setting (e.g. predicting user properties in a large social network), or alternatively the model is expected to answer questions about graphs not seen during training, known as the inductive learning setting (e.g. classifying molecular structures). Finally, while most supervised and unsupervised methods learn representations in Euclidean vector spaces, there recently has been interest for non-Euclidean representation learning, which aims at learning non-Euclidean embedding spaces such as hyperbolic or spherical spaces. The main motivations for this body of work is to use a continuous embedding space that resembles the underlying discrete structure of the input data it tries to embed (e.g. the hyperbolic space is a continuous version of trees [Sar11]). \n\n23.2 Graph Embedding as an Encoder/Decoder Problem \nWhile there are many approaches to GRL, many methods follow a similar pattern. First, the network input (node features $mathbf { X } in mathbb { R } ^ { N times D }$ and graph edges in A or $mathbf { W } in mathbb { R } ^ { N times N }$ ) is encoded from the discrete domain of the graph into a continuous representation (embedding), $mathbf { Z } in mathbb { R } ^ { N times L }$ . Next, the learned representation $mathbf { Z }$ is used to optimize a particular objective (such as reconstructing the links of the graph). In this section we will use the graph encoder-decoder model (GraphEDM) proposed by Chami et al. [Cha+21] to analyze popular families of GRL methods. \nThe GraphEDM framework (Figure 23.2, [Cha+21]) provides a general framework that encapsulates a wide variety of supervised and unsupervised graph embedding methods: including ones utilizing the graph as a regularizer (e.g. [ZG02]), positional embeddings(e.g. [PARS14]), and graph neural networks such as ones based on message passing [Gil+17; Sca+09] or graph convolutions \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "23.2 Graph Embedding as an Encoder/Decoder Problem \nWhile there are many approaches to GRL, many methods follow a similar pattern. First, the network input (node features $mathbf { X } in mathbb { R } ^ { N times D }$ and graph edges in A or $mathbf { W } in mathbb { R } ^ { N times N }$ ) is encoded from the discrete domain of the graph into a continuous representation (embedding), $mathbf { Z } in mathbb { R } ^ { N times L }$ . Next, the learned representation $mathbf { Z }$ is used to optimize a particular objective (such as reconstructing the links of the graph). In this section we will use the graph encoder-decoder model (GraphEDM) proposed by Chami et al. [Cha+21] to analyze popular families of GRL methods. \nThe GraphEDM framework (Figure 23.2, [Cha+21]) provides a general framework that encapsulates a wide variety of supervised and unsupervised graph embedding methods: including ones utilizing the graph as a regularizer (e.g. [ZG02]), positional embeddings(e.g. [PARS14]), and graph neural networks such as ones based on message passing [Gil+17; Sca+09] or graph convolutions \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n[Bru+14; KW16a]). \nThe GraphEDM framework takes as input a weighted graph $mathbf { W } in mathbb { R } ^ { N times N }$ , and optional node features $mathbf { X } in mathbb { R } ^ { N times D }$ . In (semi-)supervised settings, we assume that we are given training target labels for nodes (denoted $N$ ), edges (denoted $E$ ), and/or for the entire graph (denoted $G$ ). We denote the supervision signal as $S in { N , E , G }$ , as presented below. \nThe GraphEDM model itself can be decomposed into the following components: \n• Graph encoder network $mathrm { E N C } _ { Theta ^ { E } } : mathbb { R } ^ { N times N } times mathbb { R } ^ { N times D }  mathbb { R } ^ { N times L }$ , parameterized by $Theta ^ { E }$ , which combines the graph structure with optional node features to produce a node embedding matrix $mathbf { Z } in mathbb { R } ^ { N times L }$ as follows: \nAs we shall see next, this node embedding matrix might capture different graph properties depending on the supervision used for training. \n• Graph decoder network ${ mathrm { D E C } } _ { Theta ^ { D } } : mathbb { R } ^ { N times L } to mathbb { R } ^ { N times N }$ , parameterized by $Theta ^ { D }$ , which uses the node embeddings $Z$ to compute similarity scores for all node pairs in matrix $widehat { mathbf { W } } in mathbb { R } ^ { N times N }$ as follows: \n• Classification network ${ mathrm { D E C } } _ { ominus } s : mathbb { R } ^ { N times L }  mathbb { R } ^ { N times | mathcal { V } | }$ , where $y$ is the label space. This network is used in (semi-)supervised settings and parameterized by $Theta ^ { S }$ . The output is a distribution over the labels $hat { y } ^ { S }$ , using node embeddings, as follows: \nSpecific choices of the aforementioned (encoder and decoder) networks allows GraphEDM to realize specific graph embedding methods, as we explain in the next subsections. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nThe output of a model, as described by GraphEDM framework, is a reconstructed graph similarity matrix $widehat { W }$ (often used to train unsupervised embedding algorithms), and/or labels $widehat { boldsymbol { y } } ^ { S }$ for supervised applicaticons. The label output space $_ { mathcal { V } }$ is application dependent. For instancbe, in node-level classification, $widehat { y } ^ { N } in mathcal { V } ^ { N }$ , with $mathcal { V }$ representing the node label space. Alternately, for edge-level labeling, $widehat { y } ^ { E } in mathcal { V } ^ { N times N }$ , wbith $mathcal { V }$ representing the edge label space. Finally, we note that other kinds of labeling abre possible, such as graph-level labeling (where we would say $widehat { boldsymbol { y } } ^ { G } in mathcal { V }$ , with $mathcal { V }$ representing the graph label space). \nFinally, a loss must be specified. This can be used to optimize the parameters $Theta = { Theta ^ { E } , Theta ^ { D } , Theta ^ { S } }$ . GraphEDM models can be optimized using a combination of three different terms. First, a supervised lroescsontsetrrmu,c $mathcal { L } _ { mathrm { S U P } } ^ { S }$ ,oscsotmepramr,e he predi,ctmedaylalbeveles $hat { y } ^ { S }$ tohethgeragprohusntdrutcrtutrhe taobeilms $y ^ { S }$ . rNegxutl,ara zgartaiponh $mathcal { L } _ { G }$ constraints on the model parameters. Finally, a weight regularization loss term, $mathcal { L } _ { mathrm { R E G } }$ , allows representing priors on trainable model parameters for reducing overfitting. Models realizable by GraphEDM framework are trained by minimizing the total loss $mathcal { L }$ defined as: \nwhere $alpha$ , $beta$ and $gamma$ are hyper-parameters, that can be tuned or set to zero. Note that graph embedding methods can be trained in a supervised ( $alpha neq 0$ ) or unsupervised ( $alpha = 0$ ) fashion. Supervised graph embedding approaches leverage an additional source of information to learn embeddings such as node or graph labels. On the other hand, unsupervised network embedding approaches rely on the graph structure only to learn node embeddings. \n23.3 Shallow graph embeddings \nShallow embedding methods are transductive graph embedding methods, where the encoder function maps categorical node IDs onto a Euclidean space through an embedding matrix. Each node $v _ { i } in V$ has a corresponding low-dimensional learnable embedding vector $mathbf { Z } _ { i } in mathbb { R } ^ { L }$ and the shallow encoder function is \nCrucially, the embedding dictionary $mathbf { Z }$ is directly learned as model parameters. In the unsupervised case, embeddings $mathbf { Z }$ are optimized to recover some information about the input graph (e.g., the adjacency matrix $mathbf { W }$ , or some transformation of it). This is somewhat similar to dimensionality reduction methods, such as PCA (Section 20.1), but for graph data structures. In the supervised case, the embeddings are optimized to predict some labels, for nodes, edges and/or the whole graph. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Graph Embedding as an Encoder/Decoder Problem",
        "subsubsection": "N/A"
    },
    {
        "content": "23.3.1 Unsupervised embeddings \nIn the unsupervised case, we will consider two main types of shallow graph embedding methods: distance-based and outer product-based. Distance-based methods optimize the embedding dictionary $mathbf { Z } = boldsymbol { Theta } ^ { E } in mathbb { R } ^ { N times L }$ such that nodes $i$ and $j$ which are close in the graph (as measured by some graph distance function) are embedded in $mathbf { Z }$ such that $d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ is small, where $d _ { 2 } ( . , . )$ is a pairwise distance function between embedding vectors. The distance function $d _ { 2 } ( cdot , cdot )$ can be customized, which can lead to Euclidean (Section 23.3.2) or non-Euclidean (Section 23.3.3) embeddings. The decoder outputs a node-to-node matrix $widehat { mathbf { W } } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } )$ , with $widehat { W } _ { i j } = d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ . \nAlternatively, some method crely on pairwise dot-procducts to compute node similarities. The ecoder network can be written as: $widehat { W } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } ) = mathbf { Z } mathbf { Z } ^ { top }$ . \nIn both cases, unsupervised emb dcdings for distance- and product-based methods are learned by minimizing the graph regularization loss: \nwhere $s ( mathbf { W } )$ is an optional transformation of the adjacency matrix $mathbf { W }$ , and $d _ { 1 }$ is pairwise distance function between matrices, which does not need to be of the same form as $d _ { 2 }$ . As we shall see, there are many plausible choices for $s , d _ { 1 } , d _ { 2 }$ . For instance, we can let $s$ be the adjacency matrix itself, $s ( mathbf { W } ) = mathbf { W }$ or a power of it e.g. $s ( mathbf { W } ) = mathbf { W } ^ { 2 }$ . If the input is a weighted binary matrix $mathbf { W } = mathbf { A }$ , we can set $s ( mathbf { W } ) = 1 - mathbf { W }$ , so that connected nodes with $A _ { i j } = 1$ get a weight (distance) of $0$ . \n23.3.2 Distance-based: Euclidean methods \nDistance-based methods minimize Euclidean distances between similar (connected) nodes. We give some examples below. \nMulti-dimensional scaling (MDS, Section 20.4.4) is equivalent to setting $s ( mathbf { W } )$ to some distance matrix measuring the dissimilarity between nodes (e.g. proportional to pairwise shortest distance) and then defining \nwhere $widehat { W } _ { i j } = d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } ) = | | mathbf { Z } _ { i } - mathbf { Z } _ { j } | |$ (although other distance metrics are plausible). \nLaplcacian eigenmaps (Section 20.4.9) learn embeddings by solving the generalized eigenvector problem \nwhere $mathbf { L } = mathbf { D } - mathbf { W }$ is the graph Laplacian (Section 20.4.9.2), and $mathbf { D }$ is a diagonal matrix containing the sum across columns for each row. The first constraint removes an arbitrary scaling factor in the embedding and the second one removes trivial solutions corresponding to the constant eigenvector (with eigenvalue zero for connected graphs). Further, note that $begin{array} { r } { mathrm { t r } ( mathbf { Z } ^ { mathsf { T } } mathbf { L } mathbf { Z } ) = frac { 1 } { 2 } sum _ { i , j } W _ { i j } vert vert mathbf { Z } _ { i } - mathbf { Z } _ { j } vert vert _ { 2 } ^ { 2 } } end{array}$ , where $mathbf { Z } _ { i }$ is the $i$ ’th row of $mathbf { Z }$ ; therefore the minimization objective can be equivalently written as a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license graph reconstruction term, as follows:",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Shallow graph embeddings",
        "subsubsection": "Unsupervised embeddings"
    },
    {
        "content": "23.3.1 Unsupervised embeddings \nIn the unsupervised case, we will consider two main types of shallow graph embedding methods: distance-based and outer product-based. Distance-based methods optimize the embedding dictionary $mathbf { Z } = boldsymbol { Theta } ^ { E } in mathbb { R } ^ { N times L }$ such that nodes $i$ and $j$ which are close in the graph (as measured by some graph distance function) are embedded in $mathbf { Z }$ such that $d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ is small, where $d _ { 2 } ( . , . )$ is a pairwise distance function between embedding vectors. The distance function $d _ { 2 } ( cdot , cdot )$ can be customized, which can lead to Euclidean (Section 23.3.2) or non-Euclidean (Section 23.3.3) embeddings. The decoder outputs a node-to-node matrix $widehat { mathbf { W } } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } )$ , with $widehat { W } _ { i j } = d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ . \nAlternatively, some method crely on pairwise dot-procducts to compute node similarities. The ecoder network can be written as: $widehat { W } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } ) = mathbf { Z } mathbf { Z } ^ { top }$ . \nIn both cases, unsupervised emb dcdings for distance- and product-based methods are learned by minimizing the graph regularization loss: \nwhere $s ( mathbf { W } )$ is an optional transformation of the adjacency matrix $mathbf { W }$ , and $d _ { 1 }$ is pairwise distance function between matrices, which does not need to be of the same form as $d _ { 2 }$ . As we shall see, there are many plausible choices for $s , d _ { 1 } , d _ { 2 }$ . For instance, we can let $s$ be the adjacency matrix itself, $s ( mathbf { W } ) = mathbf { W }$ or a power of it e.g. $s ( mathbf { W } ) = mathbf { W } ^ { 2 }$ . If the input is a weighted binary matrix $mathbf { W } = mathbf { A }$ , we can set $s ( mathbf { W } ) = 1 - mathbf { W }$ , so that connected nodes with $A _ { i j } = 1$ get a weight (distance) of $0$ . \n23.3.2 Distance-based: Euclidean methods \nDistance-based methods minimize Euclidean distances between similar (connected) nodes. We give some examples below. \nMulti-dimensional scaling (MDS, Section 20.4.4) is equivalent to setting $s ( mathbf { W } )$ to some distance matrix measuring the dissimilarity between nodes (e.g. proportional to pairwise shortest distance) and then defining \nwhere $widehat { W } _ { i j } = d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } ) = | | mathbf { Z } _ { i } - mathbf { Z } _ { j } | |$ (although other distance metrics are plausible). \nLaplcacian eigenmaps (Section 20.4.9) learn embeddings by solving the generalized eigenvector problem \nwhere $mathbf { L } = mathbf { D } - mathbf { W }$ is the graph Laplacian (Section 20.4.9.2), and $mathbf { D }$ is a diagonal matrix containing the sum across columns for each row. The first constraint removes an arbitrary scaling factor in the embedding and the second one removes trivial solutions corresponding to the constant eigenvector (with eigenvalue zero for connected graphs). Further, note that $begin{array} { r } { mathrm { t r } ( mathbf { Z } ^ { mathsf { T } } mathbf { L } mathbf { Z } ) = frac { 1 } { 2 } sum _ { i , j } W _ { i j } vert vert mathbf { Z } _ { i } - mathbf { Z } _ { j } vert vert _ { 2 } ^ { 2 } } end{array}$ , where $mathbf { Z } _ { i }$ is the $i$ ’th row of $mathbf { Z }$ ; therefore the minimization objective can be equivalently written as a \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license graph reconstruction term, as follows: \n\nwhere $s ( mathbf { W } ) = mathbf { W }$ . \n23.3.3 Distance-based: non-Euclidean methods \nSo far, we have discussed methods which assume that embeddings lie in an Euclidean Space. However, recent work has considered hyperbolic geometry for graph embedding. In particular, hyperbolic embeddings are ideal for embedding trees and offer an exciting alternative to Euclidean geometry for graphs that exhibit hierarchical structures. We give some examples below. \nNickel and Kiela [NK17] learn embeddings of hierarchical graphs using the Poincaré model of hyperbolic space. This is simple to represent in our notation as we only need to change $d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ to the Poincaré distance function: \nThe optimization then learns embeddings which minimize distances between connected nodes while maximizing distances between disconnected nodes: \nwhere the denominator is approximated using negative sampling. Note that since the hyperbolic space has a manifold structure, care needs to be taken to ensure that the embeddings remain on the manifold (using Riemannian optimization techniques [Bon13]). \nOther variants of these methods have been proposed. Nickel and Kiela [NK18] explore the Lorentz model of hyperbolic space , and show that it provides better numerical stability than the Poincaré model. Another line of work extends non-Euclidean embeddings to mixed-curvature product spaces [Gu+18], which provide more flexibility for other types of graphs (e.g. ring of trees). Finally, work by Chamberlain, Clough, and Deisenroth [CCD17] extends Poincaré embeddings using skip-gram losses with hyperbolic inner products. \n23.3.4 Outer product-based: Matrix factorization methods \nMatrix factorization approaches learn embeddings that lead to a low rank representation of some similarity matrix $s ( mathbf { W } )$ , with $s : mathbb { R } ^ { N times N } to mathbb { R } ^ { N times N }$ . The following are frequent choices: $s ( mathbf { W } ) = mathbf { W }$ , $s ( mathbf { W } ) = L$ (Graph Laplacian), or other proximity measure such as the Katz centrality index, Common Neighbors or Adamic/Adar index. \nThe decoder function in matrix factorization methods is just a dot product: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Shallow graph embeddings",
        "subsubsection": "Distance-based: Euclidean methods"
    },
    {
        "content": "where $s ( mathbf { W } ) = mathbf { W }$ . \n23.3.3 Distance-based: non-Euclidean methods \nSo far, we have discussed methods which assume that embeddings lie in an Euclidean Space. However, recent work has considered hyperbolic geometry for graph embedding. In particular, hyperbolic embeddings are ideal for embedding trees and offer an exciting alternative to Euclidean geometry for graphs that exhibit hierarchical structures. We give some examples below. \nNickel and Kiela [NK17] learn embeddings of hierarchical graphs using the Poincaré model of hyperbolic space. This is simple to represent in our notation as we only need to change $d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ to the Poincaré distance function: \nThe optimization then learns embeddings which minimize distances between connected nodes while maximizing distances between disconnected nodes: \nwhere the denominator is approximated using negative sampling. Note that since the hyperbolic space has a manifold structure, care needs to be taken to ensure that the embeddings remain on the manifold (using Riemannian optimization techniques [Bon13]). \nOther variants of these methods have been proposed. Nickel and Kiela [NK18] explore the Lorentz model of hyperbolic space , and show that it provides better numerical stability than the Poincaré model. Another line of work extends non-Euclidean embeddings to mixed-curvature product spaces [Gu+18], which provide more flexibility for other types of graphs (e.g. ring of trees). Finally, work by Chamberlain, Clough, and Deisenroth [CCD17] extends Poincaré embeddings using skip-gram losses with hyperbolic inner products. \n23.3.4 Outer product-based: Matrix factorization methods \nMatrix factorization approaches learn embeddings that lead to a low rank representation of some similarity matrix $s ( mathbf { W } )$ , with $s : mathbb { R } ^ { N times N } to mathbb { R } ^ { N times N }$ . The following are frequent choices: $s ( mathbf { W } ) = mathbf { W }$ , $s ( mathbf { W } ) = L$ (Graph Laplacian), or other proximity measure such as the Katz centrality index, Common Neighbors or Adamic/Adar index. \nThe decoder function in matrix factorization methods is just a dot product: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Shallow graph embeddings",
        "subsubsection": "Distance-based: non-Euclidean methods"
    },
    {
        "content": "where $s ( mathbf { W } ) = mathbf { W }$ . \n23.3.3 Distance-based: non-Euclidean methods \nSo far, we have discussed methods which assume that embeddings lie in an Euclidean Space. However, recent work has considered hyperbolic geometry for graph embedding. In particular, hyperbolic embeddings are ideal for embedding trees and offer an exciting alternative to Euclidean geometry for graphs that exhibit hierarchical structures. We give some examples below. \nNickel and Kiela [NK17] learn embeddings of hierarchical graphs using the Poincaré model of hyperbolic space. This is simple to represent in our notation as we only need to change $d _ { 2 } ( mathbf { Z } _ { i } , mathbf { Z } _ { j } )$ to the Poincaré distance function: \nThe optimization then learns embeddings which minimize distances between connected nodes while maximizing distances between disconnected nodes: \nwhere the denominator is approximated using negative sampling. Note that since the hyperbolic space has a manifold structure, care needs to be taken to ensure that the embeddings remain on the manifold (using Riemannian optimization techniques [Bon13]). \nOther variants of these methods have been proposed. Nickel and Kiela [NK18] explore the Lorentz model of hyperbolic space , and show that it provides better numerical stability than the Poincaré model. Another line of work extends non-Euclidean embeddings to mixed-curvature product spaces [Gu+18], which provide more flexibility for other types of graphs (e.g. ring of trees). Finally, work by Chamberlain, Clough, and Deisenroth [CCD17] extends Poincaré embeddings using skip-gram losses with hyperbolic inner products. \n23.3.4 Outer product-based: Matrix factorization methods \nMatrix factorization approaches learn embeddings that lead to a low rank representation of some similarity matrix $s ( mathbf { W } )$ , with $s : mathbb { R } ^ { N times N } to mathbb { R } ^ { N times N }$ . The following are frequent choices: $s ( mathbf { W } ) = mathbf { W }$ , $s ( mathbf { W } ) = L$ (Graph Laplacian), or other proximity measure such as the Katz centrality index, Common Neighbors or Adamic/Adar index. \nThe decoder function in matrix factorization methods is just a dot product: \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nMatrix factorization methods learn $mathbf { Z }$ by minimizing a regularization loss $mathcal { L } _ { G , mathrm { R E C O N } } ( mathbf { W } , widehat { mathbf { W } } ; Theta ) =$ $| | s ( mathbf { W } ) - widehat { mathbf { W } } | | _ { F } ^ { 2 }$ . \nThe gracph factorization method of [Ahm+13] learns a low-rank factorization of a graph by minimizing the graph regularization loss $begin{array} { r } { mathcal { L } _ { G , mathrm { R E C O N } } ( mathbf { W } , widehat { mathbf { W } } ; boldsymbol { Theta } ) = sum _ { ( v _ { i } , v _ { j } ) in E } ( mathbf { W } _ { i j } - widehat { mathbf { W } } _ { i j } ) ^ { 2 } } end{array}$ . \nNote that if $mathbf { A }$ is the binary adjacency matrix, ( ${ bf A } _ { i j } = 1$ iff $( v _ { i } , v _ { j } ) in E$ and $mathbf { A } _ { i j } = 0$ otherwise), the graph regularization loss can be expressed in terms of the Frobenius norm: \nwhere $odot$ is the element-wise matrix multiplication operator. Therefore, GF also learns a low-rank factorization of the adjacency matrix $W$ measured in Frobenuis norm. We note that this is a sparse operation (summing only over edges which exist in the graph), and so the method has computational complexity $O ( M )$ . \nThe methods described so far are all symmetric, that is, they assume that $mathbf { W } _ { i j } = mathbf { W } _ { j i }$ . This is a limiting assumption when working with directed graphs as some relationships are not reciprocal. The GraRep method of [CLX15] overcomes this limitation by learning two embeddings per node, a source embedding $mathbf { Z } ^ { s }$ and a target embedding $mathbf { Z } ^ { t }$ , which capture asymmetric proximity in directed networks. In addition to asymmetry, GraRep learns embeddings that preserve $k$ -hop neighborhoods via powers of the adjacency matrix and minimizes a graph reconstruction loss with: \nfor each $1 leq k leq K$ . GraRep concatenates all representations to get source embeddings ${ bf Z } ^ { s } = { }$ $[ mathbf { Z } ^ { ( 1 ) , s } | ldots | mathbf { Z } ^ { ( K ) , s } ]$ and target embeddings $mathbf { Z } ^ { t } = [ mathbf { Z } ^ { ( 1 ) , t } | ldots | mathbf { Z } ^ { ( K ) , t } ]$ . Unfortunately, GraRep is not very scalable, since it uses a matrix power, $mathbf { D } ^ { - 1 } mathbf { W }$ , making it increasingly more dense. This limitation can be circumvented by using implicit matrix factorization [Per+17] as discussed below. \n23.3.5 Outer product-based: Skip-gram methods \nSkip-gram graph embedding models were inspired by research in natural language processing to model the distributional behavior of words [Mik+13c; PSM14b]. Skip-gram word embeddings are optimized to predict words in their context (the surrounding words) for each target word in a sentence. Given \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Shallow graph embeddings",
        "subsubsection": "Outer product-based: Matrix factorization methods"
    },
    {
        "content": "Matrix factorization methods learn $mathbf { Z }$ by minimizing a regularization loss $mathcal { L } _ { G , mathrm { R E C O N } } ( mathbf { W } , widehat { mathbf { W } } ; Theta ) =$ $| | s ( mathbf { W } ) - widehat { mathbf { W } } | | _ { F } ^ { 2 }$ . \nThe gracph factorization method of [Ahm+13] learns a low-rank factorization of a graph by minimizing the graph regularization loss $begin{array} { r } { mathcal { L } _ { G , mathrm { R E C O N } } ( mathbf { W } , widehat { mathbf { W } } ; boldsymbol { Theta } ) = sum _ { ( v _ { i } , v _ { j } ) in E } ( mathbf { W } _ { i j } - widehat { mathbf { W } } _ { i j } ) ^ { 2 } } end{array}$ . \nNote that if $mathbf { A }$ is the binary adjacency matrix, ( ${ bf A } _ { i j } = 1$ iff $( v _ { i } , v _ { j } ) in E$ and $mathbf { A } _ { i j } = 0$ otherwise), the graph regularization loss can be expressed in terms of the Frobenius norm: \nwhere $odot$ is the element-wise matrix multiplication operator. Therefore, GF also learns a low-rank factorization of the adjacency matrix $W$ measured in Frobenuis norm. We note that this is a sparse operation (summing only over edges which exist in the graph), and so the method has computational complexity $O ( M )$ . \nThe methods described so far are all symmetric, that is, they assume that $mathbf { W } _ { i j } = mathbf { W } _ { j i }$ . This is a limiting assumption when working with directed graphs as some relationships are not reciprocal. The GraRep method of [CLX15] overcomes this limitation by learning two embeddings per node, a source embedding $mathbf { Z } ^ { s }$ and a target embedding $mathbf { Z } ^ { t }$ , which capture asymmetric proximity in directed networks. In addition to asymmetry, GraRep learns embeddings that preserve $k$ -hop neighborhoods via powers of the adjacency matrix and minimizes a graph reconstruction loss with: \nfor each $1 leq k leq K$ . GraRep concatenates all representations to get source embeddings ${ bf Z } ^ { s } = { }$ $[ mathbf { Z } ^ { ( 1 ) , s } | ldots | mathbf { Z } ^ { ( K ) , s } ]$ and target embeddings $mathbf { Z } ^ { t } = [ mathbf { Z } ^ { ( 1 ) , t } | ldots | mathbf { Z } ^ { ( K ) , t } ]$ . Unfortunately, GraRep is not very scalable, since it uses a matrix power, $mathbf { D } ^ { - 1 } mathbf { W }$ , making it increasingly more dense. This limitation can be circumvented by using implicit matrix factorization [Per+17] as discussed below. \n23.3.5 Outer product-based: Skip-gram methods \nSkip-gram graph embedding models were inspired by research in natural language processing to model the distributional behavior of words [Mik+13c; PSM14b]. Skip-gram word embeddings are optimized to predict words in their context (the surrounding words) for each target word in a sentence. Given \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \na sequence of words $( w _ { 1 } , dots , w _ { T } )$ , skip-gram will minimize the objective: \nfor each target words $w _ { k }$ . These conditional probabilities can be efficiently estimated using neural networks. See Section 20.5.2.2 for details. \nThis idea has been leveraged for graph embeddings in the DeepWalk framework of [PARS14]. They justified this by showing empirically how the frequency statistics induced by random walks in real graphs follow a distribution similar to that of words used in natural language. In terms of GraphEDM, skip-gram graph embedding methods use an outer product (Equation 23.13) as their decoder function and a graph reconstruction term computed over random walks on the graph. \nIn more detail, DeepWalk trains node embeddings to maximize the probability of predicting context nodes for each center node. The context nodes are nodes appearing adjacent to the center node, in simulated random walks on A. To train embeddings, DeepWalk generates sequences of nodes using truncated unbiased random walks on the graph—which can be compared to sentences in natural language models—and then maximize their log-likelihood. Each random walk starts with a node $v _ { i _ { 1 } } in V$ and repeatedly samples the next node uniformly at random: $v _ { i _ { j + 1 } } in { v in V mid ( v _ { i _ { j } } , v ) in E }$ . The walk length is a hyperparameter. All generated random-walks can then be encoded by a sequence model. This two-step paradigm introduced by [PARS14] has been followed by many subsequent works, such as node2vec [GL16]. \nWe note that it is common for underlying implementations to use two distinct representations for each node, one for when a node is center of a truncated random walk, and one when it is in the context. The implications of this modeling choice is studied further in [AEHPAR17]. \nTo present DeepWalk in the GraphEDM framework, we can set: \nwhere $begin{array} { r } { P ( Q = q ) = frac { T _ { mathrm { m a x } } - 1 + q } { T _ { mathrm { m a x } } } } end{array}$ Tmax−1+q (see [AEH+18] for the derivation). Training DeepWalk is equivalent to minimizing: \nwhere $widehat { mathbf { W } } = mathbf { Z } mathbf { Z } ^ { mathsf { T } }$ , and the partition function is given by $begin{array} { r } { Z ( mathbf { Z } ) = prod _ { i } sum _ { j } exp ( widehat { mathbf { W } } _ { i j } ) } end{array}$ can be approximated in $O ( N )$ time via hierarchical softmax (see Section 20.5.2). (It is also commoncto model $widehat { bf W } = { bf Z } _ { mathrm { o u t } } { bf Z } _ { mathrm { i n } } ^ { mathrm { ~ tiny ~ T ~ } }$ for directed graphs using embedding dictionaries $mathbf { Z } _ { mathrm { o u t } } , mathbf { Z } _ { mathrm { i n } } in mathbb { R } ^ { N times L }$ .) \nAs noted by [LG14], Skip-gram methods can be viewed as implicit matrix factorization, and the methods discussed here are related to those of Matrix Factorization (see Section 23.3.4). This relationship is discussed in depth by [Qiu+18], who propose a general matrix factorization framework, NetMF, which uses the same underlying graph proximity information as DeepWalk, LINE [Tan+15], and node2vec [GL16]. Casting the node embedding problem as matrix factorization can inherit benefits of efficient sparse matrix operations [Qiu+19a]. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n23.3.6 Supervised embeddings \nIn many applications, we have labeled data in addition to node features and graph structure. While it is possible to tackle a supervised task by first learning unsupervised representations and then using them as features in a secondary model, this is not the ideal workflow. Unsupervised node embeddings might not preserve important properties of graphs (e.g., node neighborhoods or attributes), that are most useful for a downstream supervised task. \nIn light of this limitation, a number of methods combining these two steps, namely learning embeddings and predicting node or graph labels, have been proposed. Here, we focus on simple shallow methods. We discuss deep, nonlinear embeddings later on. \n23.3.6.1 Label propagation \nLabel propagation (LP) [ZG02] is a very popular algorithm for graph-based semi-supervised node classification. The encoder is a shallow model represented by a lookup table $mathbf { Z }$ . LP uses the label space to represent the node embeddings directly (i.e. the decoder in LP is simply the identity function): \nIn particular, LP uses the graph structure to smooth the label distribution over the graph by adding a regularization term to the loss function, using the underlying assumption that neighbor nodes should have similar labels (i.e. there exist some label consistency between connected nodes). Laplacian eigenmaps are utilized in the regularization to enforce this smoothness: \nLP minimizes this energy function over the space of functions that take fixed values on labeled nodes (i.e. $hat { y } _ { i } ^ { N } = y _ { i } ^ { N } forall i | v _ { i } in V _ { L } backslash$ ) using an iterative algorithm that updates an unlabeled node’s label distribution via the weighted average of its neighbors’ labels. \nLabel spreading (LS) [Zho+04] is a variant of label propagation which minimizes the following energy function: \nwhere $begin{array} { r } { D _ { i } = sum _ { j } W _ { i j } } end{array}$ is the degree of node $v _ { i }$ . \nIn both methods, the supervised loss is simply the sum of distances between predicted labels and ground truth labels (one-hot vectors): \nNote that while the regularization term is computed over all nodes in the graph, the supervised loss is computed over labeled nodes only. These methods are expected to work well with consistent graphs, that is graphs where node proximity in the graph is positively correlated with label similarity. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Shallow graph embeddings",
        "subsubsection": "Outer product-based: Skip-gram methods"
    },
    {
        "content": "23.3.6 Supervised embeddings \nIn many applications, we have labeled data in addition to node features and graph structure. While it is possible to tackle a supervised task by first learning unsupervised representations and then using them as features in a secondary model, this is not the ideal workflow. Unsupervised node embeddings might not preserve important properties of graphs (e.g., node neighborhoods or attributes), that are most useful for a downstream supervised task. \nIn light of this limitation, a number of methods combining these two steps, namely learning embeddings and predicting node or graph labels, have been proposed. Here, we focus on simple shallow methods. We discuss deep, nonlinear embeddings later on. \n23.3.6.1 Label propagation \nLabel propagation (LP) [ZG02] is a very popular algorithm for graph-based semi-supervised node classification. The encoder is a shallow model represented by a lookup table $mathbf { Z }$ . LP uses the label space to represent the node embeddings directly (i.e. the decoder in LP is simply the identity function): \nIn particular, LP uses the graph structure to smooth the label distribution over the graph by adding a regularization term to the loss function, using the underlying assumption that neighbor nodes should have similar labels (i.e. there exist some label consistency between connected nodes). Laplacian eigenmaps are utilized in the regularization to enforce this smoothness: \nLP minimizes this energy function over the space of functions that take fixed values on labeled nodes (i.e. $hat { y } _ { i } ^ { N } = y _ { i } ^ { N } forall i | v _ { i } in V _ { L } backslash$ ) using an iterative algorithm that updates an unlabeled node’s label distribution via the weighted average of its neighbors’ labels. \nLabel spreading (LS) [Zho+04] is a variant of label propagation which minimizes the following energy function: \nwhere $begin{array} { r } { D _ { i } = sum _ { j } W _ { i j } } end{array}$ is the degree of node $v _ { i }$ . \nIn both methods, the supervised loss is simply the sum of distances between predicted labels and ground truth labels (one-hot vectors): \nNote that while the regularization term is computed over all nodes in the graph, the supervised loss is computed over labeled nodes only. These methods are expected to work well with consistent graphs, that is graphs where node proximity in the graph is positively correlated with label similarity. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n23.4 Graph Neural Networks \nAn extensive area of research focuses on defining convolutions over graph data. In the notation of Chami et al. [Cha+21], these (semi-)supervised neighborhood aggregation methods can be represented by an encoder of the form $mathbf { Z } = mathrm { E N C } ( mathbf { X } , mathbf { W } ; Theta ^ { E } )$ , and decoders of the form $widehat { mathbf { W } } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } )$ and/or $widehat { boldsymbol { y } } ^ { S } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { S } )$ . There are many models in this family; we review somecof them below. \n23.4.1 Message passing GNNs \nThe original graph neural network (GNN) model of [GMS05; Sca+09] was the first formulation of deep learning methods for graph-structured data. It views the supervised graph embedding problem as an information diffusion mechanism, where nodes send information to their neighbors until some stable equilibrium state is reached. More concretely, given randomly initialized node embeddings $mathbf { Z } ^ { 0 }$ , it applies the following recursion: \nwhere parameters $Theta ^ { E }$ are reused at every iteration. After convergence ( $t = T$ ), the node embeddings $mathbf { Z } ^ { T }$ are used to predict the final output such as node or graph labels: \nThis process is repeated several times and the GNN parameters $Theta ^ { E }$ and $Theta ^ { D }$ are learned with backpropagation via the Almeda-Pineda algorithm [Alm87; Pin88]. By Banach’s fixed point theorem, this process is guaranteed to converge to a unique solution when the recursion provides a contraction mapping. In light of this, Scarselli et al. [Sca+09] explore maps that can be expressed using message passing networks: \nwhere $f ( cdot )$ is a multi-layer perception (MLP) constrained to be a contraction mapping. The decoder function, however, has no constraints and can be any MLP. \nLi et al. [Li+15] propose Gated Graph Sequence Neural Networks (GGSNNs), which remove the contraction mapping requirement from GNNs. In GGSNNs, the recursive algorithm in Equation 23.22 is relaxed by applying mapping functions for a fixed number of steps, where each mapping function is a gated recurrent unit [Cho+14b] with parameters shared for every iteration. The GGSNN model outputs predictions at every step, and so is particularly useful for tasks which have sequential structure (such as temporal graphs). \nGilmer et al. [Gil+17] provide a framework for graph neural networks called message passing neural networks (MPNNs), which encapsulates many recent models. In contrast with the GNN model which runs for an indefinite number of iterations, MPNNs provide an abstraction for modern approaches, which consist of multi-layer neural networks with a fixed number of layers. At every layer $ell$ , message functions $f ^ { ell } ( . )$ receive messages from neighbors (based on neighbor’s hidden state), \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 which are then passed to aggregation functions $h ^ { ell } ( . )$ :",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Shallow graph embeddings",
        "subsubsection": "Supervised embeddings"
    },
    {
        "content": "23.4 Graph Neural Networks \nAn extensive area of research focuses on defining convolutions over graph data. In the notation of Chami et al. [Cha+21], these (semi-)supervised neighborhood aggregation methods can be represented by an encoder of the form $mathbf { Z } = mathrm { E N C } ( mathbf { X } , mathbf { W } ; Theta ^ { E } )$ , and decoders of the form $widehat { mathbf { W } } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } )$ and/or $widehat { boldsymbol { y } } ^ { S } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { S } )$ . There are many models in this family; we review somecof them below. \n23.4.1 Message passing GNNs \nThe original graph neural network (GNN) model of [GMS05; Sca+09] was the first formulation of deep learning methods for graph-structured data. It views the supervised graph embedding problem as an information diffusion mechanism, where nodes send information to their neighbors until some stable equilibrium state is reached. More concretely, given randomly initialized node embeddings $mathbf { Z } ^ { 0 }$ , it applies the following recursion: \nwhere parameters $Theta ^ { E }$ are reused at every iteration. After convergence ( $t = T$ ), the node embeddings $mathbf { Z } ^ { T }$ are used to predict the final output such as node or graph labels: \nThis process is repeated several times and the GNN parameters $Theta ^ { E }$ and $Theta ^ { D }$ are learned with backpropagation via the Almeda-Pineda algorithm [Alm87; Pin88]. By Banach’s fixed point theorem, this process is guaranteed to converge to a unique solution when the recursion provides a contraction mapping. In light of this, Scarselli et al. [Sca+09] explore maps that can be expressed using message passing networks: \nwhere $f ( cdot )$ is a multi-layer perception (MLP) constrained to be a contraction mapping. The decoder function, however, has no constraints and can be any MLP. \nLi et al. [Li+15] propose Gated Graph Sequence Neural Networks (GGSNNs), which remove the contraction mapping requirement from GNNs. In GGSNNs, the recursive algorithm in Equation 23.22 is relaxed by applying mapping functions for a fixed number of steps, where each mapping function is a gated recurrent unit [Cho+14b] with parameters shared for every iteration. The GGSNN model outputs predictions at every step, and so is particularly useful for tasks which have sequential structure (such as temporal graphs). \nGilmer et al. [Gil+17] provide a framework for graph neural networks called message passing neural networks (MPNNs), which encapsulates many recent models. In contrast with the GNN model which runs for an indefinite number of iterations, MPNNs provide an abstraction for modern approaches, which consist of multi-layer neural networks with a fixed number of layers. At every layer $ell$ , message functions $f ^ { ell } ( . )$ receive messages from neighbors (based on neighbor’s hidden state), \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 which are then passed to aggregation functions $h ^ { ell } ( . )$ : \n\nwhere $mathbf { H } ^ { 0 } = mathbf { X }$ . After $ell$ layers of message passing, nodes’ hidden representations encode information within $ell$ -hop neighborhoods. \nBattaglia et al. [Bat+18] propose GraphNet, which further extends the MPNN framework to learn representations for edges, nodes and the entire graph using message passing functions. The explicit addition of edge and graph representations adds additional expressivity to the MPNN model, and allows the application of graph models to additional domains. \n23.4.2 Spectral Graph Convolutions \nSpectral methods define graph convolutions using the spectral domain of the graph Laplacian matrix. These methods broadly fall into two categories: spectrum-based methods, which explicitly compute an eigendecomposition of the Laplacian (e.g., spectral CNNs [Bru+14]) and spectrum-free methods, which are motivated by spectral graph theory but do not actually perform a spectral decomposition (e.g., Graph convolutional networks or GCN [KW16a]). \nA major disadvantage of spectrum-based methods is that they rely on the spectrum of the graph Laplacian and are therefore domain-dependent (i.e. cannot generalize to new graphs). Moreover, computing the Laplacian’s spectral decomposition is computationally expensive. Spectrum-free methods overcome these limitations by utilizing approximations of these spectral filters. However, spectrum-free methods require using the whole graph W, and so do not scale well. \nFor more details on spectral approaches, see e.g., [Bro+17b; Cha+21]. \n23.4.3 Spatial Graph Convolutions \nSpectrum-based methods have an inherent domain dependency which limits the application of a model trained on one graph to a new dataset. Additionally, spectrum-free methods (e.g. GCNs) require using the entire graph A, which can quickly become unfeasible as the size of the graph grows. \nTo overcome these limitations, another branch of graph convolutions (spatial methods) borrow ideas from standard CNNs – applying convolutions in the spatial domain as defined by the graph topology. For instance, in computer vision, convolutional filters are spatially localized by using fixed rectangular patches around each pixel. Combined with the natural ordering of pixels in images (top, left, bottom, right), it is possible to reuse filters’ weights at every location. This process significantly reduces the total number of parameters needed for a model. While such spatial convolutions cannot directly be applied in graph domains, spatial graph convolutions take inspiration from them. The core idea is to use neighborhood sampling and attention mechanisms to create fixed-size graph patches, overcoming the irregularity of graphs. \n23.4.3.1 Sampling-based spatial methods \nTo overcome the domain dependency and storage limitations of GCNs, Hamilton, Ying, and Leskovec [HYL17] propose GraphSAGE, a framework to learn inductive node embeddings. Instead of \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license averaging signals from all one-hop neighbors (via multiplications with the Laplacian matrix), SAGE samples fixed neighborhoods (of size $q$ ) for each node. This removes the strong dependency on fixed graph structure and allows generalization to new graphs. At every SAGE layer, nodes aggregate information from nodes sampled from their neighborhood (see Figure 23.5). In the GraphEDM notation, the propagation rule can be written as:",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Graph Neural Networks",
        "subsubsection": "Message passing GNNs"
    },
    {
        "content": "where $mathbf { H } ^ { 0 } = mathbf { X }$ . After $ell$ layers of message passing, nodes’ hidden representations encode information within $ell$ -hop neighborhoods. \nBattaglia et al. [Bat+18] propose GraphNet, which further extends the MPNN framework to learn representations for edges, nodes and the entire graph using message passing functions. The explicit addition of edge and graph representations adds additional expressivity to the MPNN model, and allows the application of graph models to additional domains. \n23.4.2 Spectral Graph Convolutions \nSpectral methods define graph convolutions using the spectral domain of the graph Laplacian matrix. These methods broadly fall into two categories: spectrum-based methods, which explicitly compute an eigendecomposition of the Laplacian (e.g., spectral CNNs [Bru+14]) and spectrum-free methods, which are motivated by spectral graph theory but do not actually perform a spectral decomposition (e.g., Graph convolutional networks or GCN [KW16a]). \nA major disadvantage of spectrum-based methods is that they rely on the spectrum of the graph Laplacian and are therefore domain-dependent (i.e. cannot generalize to new graphs). Moreover, computing the Laplacian’s spectral decomposition is computationally expensive. Spectrum-free methods overcome these limitations by utilizing approximations of these spectral filters. However, spectrum-free methods require using the whole graph W, and so do not scale well. \nFor more details on spectral approaches, see e.g., [Bro+17b; Cha+21]. \n23.4.3 Spatial Graph Convolutions \nSpectrum-based methods have an inherent domain dependency which limits the application of a model trained on one graph to a new dataset. Additionally, spectrum-free methods (e.g. GCNs) require using the entire graph A, which can quickly become unfeasible as the size of the graph grows. \nTo overcome these limitations, another branch of graph convolutions (spatial methods) borrow ideas from standard CNNs – applying convolutions in the spatial domain as defined by the graph topology. For instance, in computer vision, convolutional filters are spatially localized by using fixed rectangular patches around each pixel. Combined with the natural ordering of pixels in images (top, left, bottom, right), it is possible to reuse filters’ weights at every location. This process significantly reduces the total number of parameters needed for a model. While such spatial convolutions cannot directly be applied in graph domains, spatial graph convolutions take inspiration from them. The core idea is to use neighborhood sampling and attention mechanisms to create fixed-size graph patches, overcoming the irregularity of graphs. \n23.4.3.1 Sampling-based spatial methods \nTo overcome the domain dependency and storage limitations of GCNs, Hamilton, Ying, and Leskovec [HYL17] propose GraphSAGE, a framework to learn inductive node embeddings. Instead of \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license averaging signals from all one-hop neighbors (via multiplications with the Laplacian matrix), SAGE samples fixed neighborhoods (of size $q$ ) for each node. This removes the strong dependency on fixed graph structure and allows generalization to new graphs. At every SAGE layer, nodes aggregate information from nodes sampled from their neighborhood (see Figure 23.5). In the GraphEDM notation, the propagation rule can be written as:",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Graph Neural Networks",
        "subsubsection": "Spectral Graph Convolutions"
    },
    {
        "content": "where $mathbf { H } ^ { 0 } = mathbf { X }$ . After $ell$ layers of message passing, nodes’ hidden representations encode information within $ell$ -hop neighborhoods. \nBattaglia et al. [Bat+18] propose GraphNet, which further extends the MPNN framework to learn representations for edges, nodes and the entire graph using message passing functions. The explicit addition of edge and graph representations adds additional expressivity to the MPNN model, and allows the application of graph models to additional domains. \n23.4.2 Spectral Graph Convolutions \nSpectral methods define graph convolutions using the spectral domain of the graph Laplacian matrix. These methods broadly fall into two categories: spectrum-based methods, which explicitly compute an eigendecomposition of the Laplacian (e.g., spectral CNNs [Bru+14]) and spectrum-free methods, which are motivated by spectral graph theory but do not actually perform a spectral decomposition (e.g., Graph convolutional networks or GCN [KW16a]). \nA major disadvantage of spectrum-based methods is that they rely on the spectrum of the graph Laplacian and are therefore domain-dependent (i.e. cannot generalize to new graphs). Moreover, computing the Laplacian’s spectral decomposition is computationally expensive. Spectrum-free methods overcome these limitations by utilizing approximations of these spectral filters. However, spectrum-free methods require using the whole graph W, and so do not scale well. \nFor more details on spectral approaches, see e.g., [Bro+17b; Cha+21]. \n23.4.3 Spatial Graph Convolutions \nSpectrum-based methods have an inherent domain dependency which limits the application of a model trained on one graph to a new dataset. Additionally, spectrum-free methods (e.g. GCNs) require using the entire graph A, which can quickly become unfeasible as the size of the graph grows. \nTo overcome these limitations, another branch of graph convolutions (spatial methods) borrow ideas from standard CNNs – applying convolutions in the spatial domain as defined by the graph topology. For instance, in computer vision, convolutional filters are spatially localized by using fixed rectangular patches around each pixel. Combined with the natural ordering of pixels in images (top, left, bottom, right), it is possible to reuse filters’ weights at every location. This process significantly reduces the total number of parameters needed for a model. While such spatial convolutions cannot directly be applied in graph domains, spatial graph convolutions take inspiration from them. The core idea is to use neighborhood sampling and attention mechanisms to create fixed-size graph patches, overcoming the irregularity of graphs. \n23.4.3.1 Sampling-based spatial methods \nTo overcome the domain dependency and storage limitations of GCNs, Hamilton, Ying, and Leskovec [HYL17] propose GraphSAGE, a framework to learn inductive node embeddings. Instead of \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license averaging signals from all one-hop neighbors (via multiplications with the Laplacian matrix), SAGE samples fixed neighborhoods (of size $q$ ) for each node. This removes the strong dependency on fixed graph structure and allows generalization to new graphs. At every SAGE layer, nodes aggregate information from nodes sampled from their neighborhood (see Figure 23.5). In the GraphEDM notation, the propagation rule can be written as: \n\nwhere AGG $( cdot )$ is an aggregation function. This aggregation function can be any permutation invariant operator such as averaging (SAGE-mean) or max-pooling (SAGE-pool). As SAGE works with fixed size neighborhoods (and not the entire adjacency matrix), it also reduces the computational complexity of training GCNs. \n23.4.3.2 Attention-based spatial methods \nAttention mechanisms (Section 15.4) have been successfully used in language models where they, for example, allow models to identify relevant parts of long sequence inputs. Inspired by their success in language, similar ideas have been proposed for graph convolution networks. Such graph-based attention models learn to focus their attention on important neighbors during the message passing step via parametric patches which are learned on top of node features. This provides more flexibility in inductive settings, compared to methods that rely on fixed weights such as GCNs. \nThe Graph attention network (GAT) model of [Vel+18] is an attention-based version of GCNs. At every GAT layer, it attends over the neighborhood of each node and learns to selectively pick nodes which lead to the best performance for some downstream task. The intuition behind this is similar to SAGE [HYL17] and makes GAT suitable for inductive and transductive problems. However unlike SAGE, which limits the convolution step to fixed size-neighborhoods, GAT allows each node to attend over the entirety of its neighbors – assigning each of them different weights. The attention parameters are trained through backpropagation, and the attention scores are then row-normalized with a softmax activation. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n23.4.3.3 Geometric spatial methods \nMonti et al. [Mon+17] propose MoNet, a general framework that works particularly well when the node features lie in a geometric space, such as 3D point clouds or meshes. MoNet learns attention patches using parametric functions in a pre-defined spatial domain (e.g. spatial coordinates), and then applies convolution filters in the resulting graph domain. \nMoNet generalizes spatial approaches which introduce constructions for convolutions on manifolds, such as the Geodesic CNN (GCNN) [Mas+15] and the Anisotropic CNN (ACNN) [Bos+16]. Both GCNN and ACNN use fixed patches that are defined on a specific coordinate system and therefore cannot generalize to graph-structured data. However, the MoNet framework is more general; any pseudo-coordinates (i.e. node features) can be used to induce the patches. More formally, if $mathbf { U } ^ { s }$ are pseudo-coordinates and $mathbf { H } ^ { ell }$ are features from another domain, the MoNet layer can be expressed in our notation as: \nwhere $g _ { k } ( U ^ { s } )$ are the learned parametric patches, which are $N times N$ matrices. In practice, MoNet uses Gaussian kernels to learn patches, such that: \nwhere $pmb { mu } _ { k }$ and $Sigma _ { k }$ are learned parameters, and $Sigma _ { k }$ is restricted to be diagonal. \n23.4.4 Non-Euclidean Graph Convolutions \nAs we discussed in Section 23.3.3, hyperbolic geometry enables learning of shallow embeddings of hierarchical graphs which have smaller distortion than Euclidean embeddings. However, one major downside of shallow embeddings is that they do not generalize well (if at all) across graphs. On the other hand, Graph Neural Networks, which leverage node features, have achieved good results on many inductive graph embedding tasks. \nIt is natural then, that there has been recent interest in extending Graph Neural Networks to learn non-Euclidean embeddings. One major challenge in doing so again revolves around the nature of convolution itself. How should we perform convolutions in a non-Euclidean space, where standard operations such as inner products and matrix multiplications are not defined? \nHyperbolic Graph Convolution Networks (HGCN) [Cha+19a] and Hyperbolic Graph Neural Networks (HGNN) [LNK19] apply graph convolutions in hyperbolic space by leveraging the Euclidean tangent space, which provides a first-order approximation of the hyperbolic manifold at a point. For every graph convolution step, node embeddings are mapped to the Euclidean tangent space at the origin, where convolutions are applied, and then mapped back to the hyperbolic space. These approaches yield significant improvements on graphs that exhibit hierarchical structure (Figure 23.6). \n23.5 Deep graph embeddings \nIn this section, we use graph neural networks to devise graph embeddings in the unsupervised and semi-supervised cases. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Graph Neural Networks",
        "subsubsection": "Spatial Graph Convolutions"
    },
    {
        "content": "23.4.3.3 Geometric spatial methods \nMonti et al. [Mon+17] propose MoNet, a general framework that works particularly well when the node features lie in a geometric space, such as 3D point clouds or meshes. MoNet learns attention patches using parametric functions in a pre-defined spatial domain (e.g. spatial coordinates), and then applies convolution filters in the resulting graph domain. \nMoNet generalizes spatial approaches which introduce constructions for convolutions on manifolds, such as the Geodesic CNN (GCNN) [Mas+15] and the Anisotropic CNN (ACNN) [Bos+16]. Both GCNN and ACNN use fixed patches that are defined on a specific coordinate system and therefore cannot generalize to graph-structured data. However, the MoNet framework is more general; any pseudo-coordinates (i.e. node features) can be used to induce the patches. More formally, if $mathbf { U } ^ { s }$ are pseudo-coordinates and $mathbf { H } ^ { ell }$ are features from another domain, the MoNet layer can be expressed in our notation as: \nwhere $g _ { k } ( U ^ { s } )$ are the learned parametric patches, which are $N times N$ matrices. In practice, MoNet uses Gaussian kernels to learn patches, such that: \nwhere $pmb { mu } _ { k }$ and $Sigma _ { k }$ are learned parameters, and $Sigma _ { k }$ is restricted to be diagonal. \n23.4.4 Non-Euclidean Graph Convolutions \nAs we discussed in Section 23.3.3, hyperbolic geometry enables learning of shallow embeddings of hierarchical graphs which have smaller distortion than Euclidean embeddings. However, one major downside of shallow embeddings is that they do not generalize well (if at all) across graphs. On the other hand, Graph Neural Networks, which leverage node features, have achieved good results on many inductive graph embedding tasks. \nIt is natural then, that there has been recent interest in extending Graph Neural Networks to learn non-Euclidean embeddings. One major challenge in doing so again revolves around the nature of convolution itself. How should we perform convolutions in a non-Euclidean space, where standard operations such as inner products and matrix multiplications are not defined? \nHyperbolic Graph Convolution Networks (HGCN) [Cha+19a] and Hyperbolic Graph Neural Networks (HGNN) [LNK19] apply graph convolutions in hyperbolic space by leveraging the Euclidean tangent space, which provides a first-order approximation of the hyperbolic manifold at a point. For every graph convolution step, node embeddings are mapped to the Euclidean tangent space at the origin, where convolutions are applied, and then mapped back to the hyperbolic space. These approaches yield significant improvements on graphs that exhibit hierarchical structure (Figure 23.6). \n23.5 Deep graph embeddings \nIn this section, we use graph neural networks to devise graph embeddings in the unsupervised and semi-supervised cases. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Graph Neural Networks",
        "subsubsection": "Non-Euclidean Graph Convolutions"
    },
    {
        "content": "23.5.1 Unsupervised embeddings \nIn this section, we discuss unsupervised losses for GNNs, as illustrated in Figure 23.7. \n23.5.1.1 Structural deep network embedding \nThe structural deep network embedding (SDNE) method of [WCZ16] uses auto-encoders which preserve first and second-order node proximity. The SDNE encoder takes a row of the adjacency matrix as input (setting $s ( mathbf { W } ) = mathbf { W }$ ) and produces node embeddings $mathbf { Z } = mathrm { E N C } ( mathbf { W } ; theta ^ { E } )$ . (Note that this ignores any node features.) The SDNE decoder returns $widehat { mathbf { W } } = mathrm { D E C } ( mathbf { Z } ; boldsymbol { Theta } ^ { D } )$ , a reconstruction trained to recover the original graph adjacency matrix. SDNE prceserves second order node proximity by minimizing the following loss: \nThe first term is similar to the matrix factorization regularization objective, except that $widehat { bf W }$ is not computed using outer products. The second term is used by distance-based shallow emcbedding methods. \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \n23.5.1.2 (Variational) graph auto-encoders\nKipf and Welling [KW16b] use graph convolutions (Section 23.4.2) to learn node embeddings $mathbf { Z } =$ $mathrm { G C N } ( mathbf { W } , mathbf { X } ; Theta ^ { E } )$ . The decoder is an outer product: $operatorname { D E C } ( mathbf { Z } ; Theta ^ { D } ) = mathbf { Z } mathbf { Z } ^ { scriptscriptstyle 1 }$ . The graph reconstruction term is the sigmoid cross entropy between the true adjacency and the predicted edge similarity scores: \nComputing the regularization term over all possible nodes pairs is computationally challenging in practice, so the Graph Auto Encoders (GAE) model uses negative sampling to overcome this challenge. \nWhereas GAE is a deterministic model, the authors also introduce variational graph auto-encoders (VGAE), which relies on variational auto-encoders (as in Section 20.3.5) to encode and decode the graph structure. In VGAE, the embedding $mathbf { Z }$ is modeled as a latent variable with a standard multivariate normal prior $p ( mathbf { Z } ) = mathcal { N } ( mathbf { Z } | mathbf { 0 } , mathbf { I } )$ and a graph convolution is used as the amortized inference network, $q _ { Phi } ( mathbf { Z } | mathbf { W } , mathbf { X } )$ . The model is trained by minimizing the corresponding negative evidence lower bound: \n23.5.1.3 Iterative generative modelling of graphs (Graphite) \nThe graphite model of [GZE19] extends GAE and VGAE by introducing a more complex decoder. This decoder iterates between pairwise decoding functions and graph convolutions, as follows: \nwhere $mathbf { Z } ^ { ( 0 ) }$ is initialized using the output of the encoder network. This process allows Graphite to learn more expressive decoders. Finally, similar to GAE, Graphite can be deterministic or variational. \n23.5.1.4 Methods based on contrastive losses \nThe deep graph infomax method of [Vel+19] is a GAN-like method for creating graph-level embeddings. Given one or more real (positive) graphs, each with its adjacency matrix $mathbf { W } in mathbb { R } ^ { N times N }$ and node features $mathbf { X } in mathbb { R } ^ { N times D }$ , this method creates fake (negative) adjacency matrices $mathbf { W } ^ { - } in mathbb { R } ^ { N ^ { - } times N ^ { - } }$ and their features $X ^ { - } in mathbb { R } ^ { N ^ { - } times D }$ . It trains (i) an encoder that processes both real and fake samples, respectively giving $Z = mathrm { E N C } ( mathbf { X } , mathbf { W } ; boldsymbol { Theta } ^ { E } ) in mathbb { R } ^ { N times L }$ and $mathbf { Z } ^ { - } = mathrm { E N C } ( mathbf { X } ^ { - } , mathbf { W } ^ { - } ; boldsymbol { Theta } ^ { E } ) in mathbb { R } ^ { N ^ { - } times L }$ , (ii) a (readout) graph pooling function $mathcal { R } : mathbb { R } ^ { N times L }  mathbb { R } ^ { L }$ , and (iii) a descriminator function $mathcal { D } : mathbb { R } ^ { L } times mathbb { R } ^ { L } to$ $[ 0 , 1 ]$ which is trained to output $mathcal { D } ( mathbf { Z } _ { i } , mathcal { R } ( mathbf { Z } ) ) approx 1$ and $begin{array} { r } { mathcal { D } ( mathbf { Z } _ { j } ^ { - } , mathcal { R } ( mathbf { Z } ^ { - } ) ) approx 0 } end{array}$ , respectively, for nodes \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license corresponding to given graph $i in V$ and fake graph $j in V ^ { - }$ . Specifically, DGI optimizes: \n\nwhere $Theta$ contains $Theta ^ { E }$ and the parameters of $mathcal { R } , mathcal { D }$ . In the first expectation, DGI samples from the real (positive) graphs. If only one graph is given, it could sample some subgraphs from it (e.g. connected components). The second expectation samples fake (negative) graphs. In DGI, fake samples use the real adjacency $W ^ { - } : = W$ but fake features $X ^ { - }$ are a row-wise random permutation of real $X$ . The ENC used in DGI is a graph convolutional network, though any GNN can be used. The readout $mathcal { R }$ summarizes an entire (variable-size) graph to a single (fixed-dimension) vector. Veličković et al. [Vel+19] use $mathcal { R }$ as a row-wise mean, though other graph pooling might be used e.g. ones aware of the adjacency. \nThe optimization of Equation (23.34) is shown by [Vel+19] to maximize a lower-bound on the mutual information between the outputs of the encoder and the graph pooling function, i.e., between individual node representations and the graph representation. \nIn [Pen+20] they present a variant called Graphical Mutual Information. Rather than maximizing MI of node information and an entire graph, GMI maximizes the MI between the representation of a node and its neighbors. \n23.5.2 Semi-supervised embeddings \nIn this section, we discuss semi-supervised losses for GNNs. We consider the simple special case in which we use a nonlinear encoder of the node features, but ignore the graph structure, i.e., we use $mathbf { Z } = mathrm { E N C } ( mathbf { X } ; boldsymbol { Theta } ^ { E } )$ . \n23.5.2.1 SemiEmb \n[WRC08] propose an approach called semi-supervised embeddings (SemiEmb) They use an MLP for the encoder of $mathbf { X }$ . For the decoder, we can use a distance-based graph decoder: $widehat { bf W } _ { i j } =$ $mathrm { D E C } ( mathbf { Z } ; Theta ^ { D } ) _ { i j } = | | mathbf { Z } _ { i } - mathbf { Z } _ { j } | | ^ { 2 }$ , where $| | cdot | |$ can be the L2 or L1 norm. \nSemiEmb regularizes intermediate or auxiliary layers in the network using the same regularizer as the label propagation loss in Equation (23.19). SemiEmb uses a feed forward network to predict labels from intermediate embeddings, which are then compared to ground truth labels using the Hinge loss. \n23.5.2.2 Planetoid \nUnsupervised skip-gram methods like DeepWalk and node2vec learn embeddings in a multi-step pipeline, where random walks are first generated from the graph and then used to learn embeddings. These embeddings are likely not optimal for downstream classification tasks. The Planetoid method of [YCS16] extends such random walk methods to leverage node label information during the embedding algorithm. \nPlanetoid first maps nodes to embeddings ${ bf Z } = [ { bf Z } ^ { c } | | { bf Z } ^ { F } ] = mathrm { E N C } ( { bf X } ; Theta ^ { E } )$ using a neural network (again ignoring graph structure). The node embeddings ${ bf Z } ^ { c }$ capture structural information while the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 node embeddings ${ bf Z } ^ { F }$ capture feature information. There are two variants, a transductive version that directly learns ${ bf Z } ^ { c }$ (as an embedding lookup), and an inductive model where ${ bf Z } ^ { c }$ is computed with parametric mappings that act on input features $mathbf { X }$ . The Planetoid objective contains both a supervised loss and a graph regularization loss. The graph regularization loss measures the ability to predict context using nodes embeddings:",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Deep graph embeddings",
        "subsubsection": "Unsupervised embeddings"
    },
    {
        "content": "where $Theta$ contains $Theta ^ { E }$ and the parameters of $mathcal { R } , mathcal { D }$ . In the first expectation, DGI samples from the real (positive) graphs. If only one graph is given, it could sample some subgraphs from it (e.g. connected components). The second expectation samples fake (negative) graphs. In DGI, fake samples use the real adjacency $W ^ { - } : = W$ but fake features $X ^ { - }$ are a row-wise random permutation of real $X$ . The ENC used in DGI is a graph convolutional network, though any GNN can be used. The readout $mathcal { R }$ summarizes an entire (variable-size) graph to a single (fixed-dimension) vector. Veličković et al. [Vel+19] use $mathcal { R }$ as a row-wise mean, though other graph pooling might be used e.g. ones aware of the adjacency. \nThe optimization of Equation (23.34) is shown by [Vel+19] to maximize a lower-bound on the mutual information between the outputs of the encoder and the graph pooling function, i.e., between individual node representations and the graph representation. \nIn [Pen+20] they present a variant called Graphical Mutual Information. Rather than maximizing MI of node information and an entire graph, GMI maximizes the MI between the representation of a node and its neighbors. \n23.5.2 Semi-supervised embeddings \nIn this section, we discuss semi-supervised losses for GNNs. We consider the simple special case in which we use a nonlinear encoder of the node features, but ignore the graph structure, i.e., we use $mathbf { Z } = mathrm { E N C } ( mathbf { X } ; boldsymbol { Theta } ^ { E } )$ . \n23.5.2.1 SemiEmb \n[WRC08] propose an approach called semi-supervised embeddings (SemiEmb) They use an MLP for the encoder of $mathbf { X }$ . For the decoder, we can use a distance-based graph decoder: $widehat { bf W } _ { i j } =$ $mathrm { D E C } ( mathbf { Z } ; Theta ^ { D } ) _ { i j } = | | mathbf { Z } _ { i } - mathbf { Z } _ { j } | | ^ { 2 }$ , where $| | cdot | |$ can be the L2 or L1 norm. \nSemiEmb regularizes intermediate or auxiliary layers in the network using the same regularizer as the label propagation loss in Equation (23.19). SemiEmb uses a feed forward network to predict labels from intermediate embeddings, which are then compared to ground truth labels using the Hinge loss. \n23.5.2.2 Planetoid \nUnsupervised skip-gram methods like DeepWalk and node2vec learn embeddings in a multi-step pipeline, where random walks are first generated from the graph and then used to learn embeddings. These embeddings are likely not optimal for downstream classification tasks. The Planetoid method of [YCS16] extends such random walk methods to leverage node label information during the embedding algorithm. \nPlanetoid first maps nodes to embeddings ${ bf Z } = [ { bf Z } ^ { c } | | { bf Z } ^ { F } ] = mathrm { E N C } ( { bf X } ; Theta ^ { E } )$ using a neural network (again ignoring graph structure). The node embeddings ${ bf Z } ^ { c }$ capture structural information while the \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 node embeddings ${ bf Z } ^ { F }$ capture feature information. There are two variants, a transductive version that directly learns ${ bf Z } ^ { c }$ (as an embedding lookup), and an inductive model where ${ bf Z } ^ { c }$ is computed with parametric mappings that act on input features $mathbf { X }$ . The Planetoid objective contains both a supervised loss and a graph regularization loss. The graph regularization loss measures the ability to predict context using nodes embeddings: \n\nwith $widehat { mathbf { W } } _ { i j } = mathbf { Z } _ { i } ^ { top } mathbf { Z } _ { j }$ and $gamma in { - 1 , 1 }$ with $gamma = 1$ if $( v _ { i } , v _ { j } ) in E$ is a positive pair and $gamma = - 1$ if $( v _ { i } , v _ { j } )$ is a necgative pair. The distribution under the expectation is directly defined through a sampling process \nThe supervised loss in Planetoid is the negative log-likelihood of predicting the correct labels: \nwhere $i$ is a node’s index while $k$ indicates label classes, and $widehat { y } _ { i } ^ { N }$ are computed using a neural network followed by a softmax activation, mapping $mathbf { Z } _ { i }$ to predicted lbabels. \n23.6 Applications \nThere are many applications of graph embeddings, both unsupervised and supervised. We give some examples in the sections below. \n23.6.1 Unsupervised applications \nIn this section, we discuss common unsupervised applications. \n23.6.1.1 Graph reconstruction \nA popular unsupervised graph application is graph reconstruction. In this setting, the goal is to learn mapping functions (which can be parametric or not) that map nodes onto a manifold which can reconstruct the graph. This is regarded as unsupervised in the sense that there is no supervision beyond the graph structure. Models can be trained by minimizing a reconstruction error, which is the error in recovering the original graph from learned embeddings. Several algorithms were designed specifically for this task, and we refer to Section 23.3.1 and Section 23.5.1 for some examples of reconstruction objectives. At a high level, graph reconstruction is similar to dimensionality reduction in the sense that the main goal is to summarize some input data into a low-dimensional embedding. Instead of compressing high dimensional vectors into low-dimensional ones as standard dimensionality reduction methods (e.g. PCA) do, the goal of graph reconstruction models is to compress data defined on graphs into low-dimensional vectors. \n23.6.1.2 Link prediction \nThe goal in link prediction is to predict missing or unobserved links (e.g., links that may appear in the future for dynamic and temporal networks). Link prediction can also help identify spurious \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license links and remove them. It is a major application of graph learning models in industry, and common example of applications include predicting friendships in social networks predicting user-product interactions in recommendation systems, predicting suspicious links in a fraud detection system (see Figure 23.8), or predicting missing relationships between entities in a knowledge graph (see e.g., [Nic+15]).",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Deep graph embeddings",
        "subsubsection": "Semi-supervised embeddings"
    },
    {
        "content": "with $widehat { mathbf { W } } _ { i j } = mathbf { Z } _ { i } ^ { top } mathbf { Z } _ { j }$ and $gamma in { - 1 , 1 }$ with $gamma = 1$ if $( v _ { i } , v _ { j } ) in E$ is a positive pair and $gamma = - 1$ if $( v _ { i } , v _ { j } )$ is a necgative pair. The distribution under the expectation is directly defined through a sampling process \nThe supervised loss in Planetoid is the negative log-likelihood of predicting the correct labels: \nwhere $i$ is a node’s index while $k$ indicates label classes, and $widehat { y } _ { i } ^ { N }$ are computed using a neural network followed by a softmax activation, mapping $mathbf { Z } _ { i }$ to predicted lbabels. \n23.6 Applications \nThere are many applications of graph embeddings, both unsupervised and supervised. We give some examples in the sections below. \n23.6.1 Unsupervised applications \nIn this section, we discuss common unsupervised applications. \n23.6.1.1 Graph reconstruction \nA popular unsupervised graph application is graph reconstruction. In this setting, the goal is to learn mapping functions (which can be parametric or not) that map nodes onto a manifold which can reconstruct the graph. This is regarded as unsupervised in the sense that there is no supervision beyond the graph structure. Models can be trained by minimizing a reconstruction error, which is the error in recovering the original graph from learned embeddings. Several algorithms were designed specifically for this task, and we refer to Section 23.3.1 and Section 23.5.1 for some examples of reconstruction objectives. At a high level, graph reconstruction is similar to dimensionality reduction in the sense that the main goal is to summarize some input data into a low-dimensional embedding. Instead of compressing high dimensional vectors into low-dimensional ones as standard dimensionality reduction methods (e.g. PCA) do, the goal of graph reconstruction models is to compress data defined on graphs into low-dimensional vectors. \n23.6.1.2 Link prediction \nThe goal in link prediction is to predict missing or unobserved links (e.g., links that may appear in the future for dynamic and temporal networks). Link prediction can also help identify spurious \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license links and remove them. It is a major application of graph learning models in industry, and common example of applications include predicting friendships in social networks predicting user-product interactions in recommendation systems, predicting suspicious links in a fraud detection system (see Figure 23.8), or predicting missing relationships between entities in a knowledge graph (see e.g., [Nic+15]). \n\nA common approach for training link prediction models is to mask some edges in the graph (positive and negative edges), train a model with the remaining edges and then test it on the masked set of edges. Note that link prediction is different from graph reconstruction. In link prediction, we aim at predicting links that are not observed in the original graph while in graph reconstruction, we only want to compute embeddings that preserve the graph structure through reconstruction error minimization. \nFinally, while link prediction has similarities with supervised tasks in the sense that we have labels for edges (positive, negative, unobserved), we group it under the unsupervised class of applications since edge labels are usually not used during training, but only used to measure the predictive quality of embeddings. \n23.6.1.3 Clustering \nClustering is particularly useful for discovering communities and has many real-world applications. For instance, clusters exist in biological networks (e.g. as groups of proteins with similar properties), or in social networks (e.g. as groups of people with similar interests). \nThe unsupervised methods introduced in this chapter can be used to solve clustering problems \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nby applying the clustering algorithm (e.g. k-means) to embeddings that are output by an encoder. Further, clustering can be joined with the learning algorithm while learning a shallow [Roz+19] or Graph Convolution [Chi+19a; CEL19] embedding model. \n23.6.1.4 Visualization \nThere are many off-the-shelf tools for mapping graph nodes onto two-dimensional manifolds for the purpose of visualization. Visualizations allow network scientists to qualitatively understand graph properties, understand relationships between nodes or visualize node clusters. Among the popular tools are methods based on Force-Directed Layouts, with various web-app Javascript implementations. \nUnsupervised graph embedding methods are also used for visualization purposes: by first training an encoder-decoder model (corresponding to a shallow embedding or graph convolution network), and then mapping every node representation onto a two-dimensional space using t-SNE (Section 20.4.10) or PCA (Section 20.1). Such a process (embedding $$ dimensionality reduction) is commonly used to qualitatively evaluate the performance of graph learning algorithms. If nodes have attributes, one can use these attributes to color the nodes on 2D visualization plots. Good embedding algorithms embed nodes that have similar attributes nearby in the embedding space, as demonstrated in visualizations of various methods [PARS14; KW16a; AEH+18]. Finally, beyond mapping every node to a 2D coordinate, methods which map every graph to a representation [ARZP19] can similarly be projected into two dimensions to visualize and qualitatively analyze graph-level properties. \n23.6.2 Supervised applications \nIn this section, we discuss common supervised applications. \n23.6.2.1 Node classification \nNode classification is an important supervised graph application, where the goal is to learn node representations that can accurately predict node labels. (This is sometimes called statistical relational learning [GT07].) For instance, node labels could be scientific topics in citation networks, or gender and other attributes in social networks. \nSince labeling large graphs can be time-consuming and expensive, semi-supervised node classification is a particularly common application. In semi-supervised settings, only a fraction of nodes are labeled and the goal is to leverage links between nodes to predict attributes of unlabeled nodes. This setting is transductive since there is only one partially labeled fixed graph. It is also possible to do inductive node classification, which corresponds to the task of classifying nodes in multiple graphs. \nNote that node features can significantly boost the performance on node classification tasks if these are descriptive for the target label. Indeed, recent methods such as GCN (Section 23.4.2) GraphSAGE (Section 23.4.3.1) have achieved state-of-the-art performance on multiple node classification benchmarks due to their ability to combine structural information and semantics coming from features. On the other hand, other methods such as random walks on graphs fail to leverage feature information and therefore achieve lower performance on these tasks. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Applications",
        "subsubsection": "Unsupervised applications"
    },
    {
        "content": "by applying the clustering algorithm (e.g. k-means) to embeddings that are output by an encoder. Further, clustering can be joined with the learning algorithm while learning a shallow [Roz+19] or Graph Convolution [Chi+19a; CEL19] embedding model. \n23.6.1.4 Visualization \nThere are many off-the-shelf tools for mapping graph nodes onto two-dimensional manifolds for the purpose of visualization. Visualizations allow network scientists to qualitatively understand graph properties, understand relationships between nodes or visualize node clusters. Among the popular tools are methods based on Force-Directed Layouts, with various web-app Javascript implementations. \nUnsupervised graph embedding methods are also used for visualization purposes: by first training an encoder-decoder model (corresponding to a shallow embedding or graph convolution network), and then mapping every node representation onto a two-dimensional space using t-SNE (Section 20.4.10) or PCA (Section 20.1). Such a process (embedding $$ dimensionality reduction) is commonly used to qualitatively evaluate the performance of graph learning algorithms. If nodes have attributes, one can use these attributes to color the nodes on 2D visualization plots. Good embedding algorithms embed nodes that have similar attributes nearby in the embedding space, as demonstrated in visualizations of various methods [PARS14; KW16a; AEH+18]. Finally, beyond mapping every node to a 2D coordinate, methods which map every graph to a representation [ARZP19] can similarly be projected into two dimensions to visualize and qualitatively analyze graph-level properties. \n23.6.2 Supervised applications \nIn this section, we discuss common supervised applications. \n23.6.2.1 Node classification \nNode classification is an important supervised graph application, where the goal is to learn node representations that can accurately predict node labels. (This is sometimes called statistical relational learning [GT07].) For instance, node labels could be scientific topics in citation networks, or gender and other attributes in social networks. \nSince labeling large graphs can be time-consuming and expensive, semi-supervised node classification is a particularly common application. In semi-supervised settings, only a fraction of nodes are labeled and the goal is to leverage links between nodes to predict attributes of unlabeled nodes. This setting is transductive since there is only one partially labeled fixed graph. It is also possible to do inductive node classification, which corresponds to the task of classifying nodes in multiple graphs. \nNote that node features can significantly boost the performance on node classification tasks if these are descriptive for the target label. Indeed, recent methods such as GCN (Section 23.4.2) GraphSAGE (Section 23.4.3.1) have achieved state-of-the-art performance on multiple node classification benchmarks due to their ability to combine structural information and semantics coming from features. On the other hand, other methods such as random walks on graphs fail to leverage feature information and therefore achieve lower performance on these tasks. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n23.6.2.2 Graph classification \nGraph classification is a supervised application where the goal is to predict graph labels. Graph classification problems are inductive and a common example is classifying chemical compounds (e.g. predicting toxicity or odor from a molecule, as shown in Figure 23.9). \nGraph classification requires some notion of pooling, in order to aggregate node-level information into graph-level information. As discussed earlier, generalizing this notion of pooling to arbitrary graphs is non trivial because of the lack of regularity in the graph structure making graph pooling an active research area. In addition to the supervised methods discussed above, a number of unsupervised methods for learning graph-level representations have been proposed [Tsi+18; ARZP19; TMP20]. \nA Notation \nA.1 Introduction \nIt is very difficult to come up with a single, consistent notation to cover the wide variety of data, models and algorithms that we discuss in this book. Furthermore, conventions differ between different fields (such as machine learning, statistics and optimization), and between different books and papers within the same field. Nevertheless, we have tried to be as consistent as possible. Below we summarize most of the notation used in this book, although individual sections may introduce new notation. Note also that the same symbol may have different meanings depending on the context, although we try to avoid this where possible. \nA.2 Common mathematical symbols \nWe list some common symbols below.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Graph Embeddings *",
        "subsection": "Applications",
        "subsubsection": "Supervised applications"
    },
    {
        "content": "A Notation \nA.1 Introduction \nIt is very difficult to come up with a single, consistent notation to cover the wide variety of data, models and algorithms that we discuss in this book. Furthermore, conventions differ between different fields (such as machine learning, statistics and optimization), and between different books and papers within the same field. Nevertheless, we have tried to be as consistent as possible. Below we summarize most of the notation used in this book, although individual sections may introduce new notation. Note also that the same symbol may have different meanings depending on the context, although we try to avoid this where possible. \nA.2 Common mathematical symbols \nWe list some common symbols below.",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Introduction",
        "subsubsection": "N/A"
    },
    {
        "content": "A Notation \nA.1 Introduction \nIt is very difficult to come up with a single, consistent notation to cover the wide variety of data, models and algorithms that we discuss in this book. Furthermore, conventions differ between different fields (such as machine learning, statistics and optimization), and between different books and papers within the same field. Nevertheless, we have tried to be as consistent as possible. Below we summarize most of the notation used in this book, although individual sections may introduce new notation. Note also that the same symbol may have different meanings depending on the context, although we try to avoid this where possible. \nA.2 Common mathematical symbols \nWe list some common symbols below. \nA.3 Functions \nGeneric functions will be denoted by $f$ (and sometimes $g$ or $h$ ). We will encounter many named functions, such as $operatorname { t a n h } ( x )$ or $sigma ( x )$ . A scalar function applied to a vector is assumed to be applied elementwise, e.g., $pmb { x } ^ { 2 } = [ x _ { 1 } ^ { 2 } , dots , x _ { D } ^ { 2 } ]$ . Functionals (functions of a function) are written using “blackboard” font, e.g., $mathbb H ( p )$ for the entropy of a distribution $p$ . A function parameterized by fixed parameters $pmb theta$ will be denoted by $f ( { pmb x } ; { pmb theta } )$ or sometimes $f _ { boldsymbol { theta } } ( boldsymbol { x } )$ . We list some common functions (with no free parameters) below. \nA.3.1 Common functions of one argument \nA.3.2 Common functions of two arguments \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Common mathematical symbols",
        "subsubsection": "N/A"
    },
    {
        "content": "A.3 Functions \nGeneric functions will be denoted by $f$ (and sometimes $g$ or $h$ ). We will encounter many named functions, such as $operatorname { t a n h } ( x )$ or $sigma ( x )$ . A scalar function applied to a vector is assumed to be applied elementwise, e.g., $pmb { x } ^ { 2 } = [ x _ { 1 } ^ { 2 } , dots , x _ { D } ^ { 2 } ]$ . Functionals (functions of a function) are written using “blackboard” font, e.g., $mathbb H ( p )$ for the entropy of a distribution $p$ . A function parameterized by fixed parameters $pmb theta$ will be denoted by $f ( { pmb x } ; { pmb theta } )$ or sometimes $f _ { boldsymbol { theta } } ( boldsymbol { x } )$ . We list some common functions (with no free parameters) below. \nA.3.1 Common functions of one argument \nA.3.2 Common functions of two arguments \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Functions",
        "subsubsection": "Common functions of one argument"
    },
    {
        "content": "A.3 Functions \nGeneric functions will be denoted by $f$ (and sometimes $g$ or $h$ ). We will encounter many named functions, such as $operatorname { t a n h } ( x )$ or $sigma ( x )$ . A scalar function applied to a vector is assumed to be applied elementwise, e.g., $pmb { x } ^ { 2 } = [ x _ { 1 } ^ { 2 } , dots , x _ { D } ^ { 2 } ]$ . Functionals (functions of a function) are written using “blackboard” font, e.g., $mathbb H ( p )$ for the entropy of a distribution $p$ . A function parameterized by fixed parameters $pmb theta$ will be denoted by $f ( { pmb x } ; { pmb theta } )$ or sometimes $f _ { boldsymbol { theta } } ( boldsymbol { x } )$ . We list some common functions (with no free parameters) below. \nA.3.1 Common functions of one argument \nA.3.2 Common functions of two arguments \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Functions",
        "subsubsection": "Common functions of two arguments"
    },
    {
        "content": "A.3 Functions \nGeneric functions will be denoted by $f$ (and sometimes $g$ or $h$ ). We will encounter many named functions, such as $operatorname { t a n h } ( x )$ or $sigma ( x )$ . A scalar function applied to a vector is assumed to be applied elementwise, e.g., $pmb { x } ^ { 2 } = [ x _ { 1 } ^ { 2 } , dots , x _ { D } ^ { 2 } ]$ . Functionals (functions of a function) are written using “blackboard” font, e.g., $mathbb H ( p )$ for the entropy of a distribution $p$ . A function parameterized by fixed parameters $pmb theta$ will be denoted by $f ( { pmb x } ; { pmb theta } )$ or sometimes $f _ { boldsymbol { theta } } ( boldsymbol { x } )$ . We list some common functions (with no free parameters) below. \nA.3.1 Common functions of one argument \nA.3.2 Common functions of two arguments \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nsoftmax(x) Softmax function, $big [ frac { e ^ { x _ { c } } } { sum _ { c ^ { prime } = 1 } ^ { C } e ^ { x _ { c ^ { prime } } } } big ] _ { c = 1 } ^ { C }$ \nA.4 Linear algebra \nIn this section, we summarize the notation we use for linear algebra (see Chapter 7 for details). \nA.4.1 General notation \nVectors are bold lower case letters such as $_ { x }$ , $mathbf { boldsymbol { w } }$ . Matrices are bold upper case letters, such as $mathbf { X }$ , $mathbf { W }$ . Scalars are non-bold lower case. When creating a vector from a list of $N$ scalars, we write $pmb { x } = [ x _ { 1 } , dots , x _ { N } ]$ ; this may be a column vector or a row vector, depending on the context. (Vectors are assumed to be column vectors, unless noted otherwise.) When creating an $M times N$ matrix from a list of vectors, we write $mathbf { X } = [ pmb { x } _ { 1 } , dots , pmb { x } _ { N } ]$ if we stack along the columns, or $mathbf { X } = [ pmb { x } _ { 1 } ; dots ; pmb { x } _ { M } ]$ if we stack along the rows. \nA.4.2 Vectors \nHere is some standard notation for vectors. (We assume $mathbf { Delta } _ { mathbf { u } }$ and $mathbf { nabla } _ { mathbf { v } }$ are both $N$ -dimensional vectors.) \nA.4.3 Matrices \nHere is some standard notation for matrices. (We assume $mathbf { s }$ is a square $N times N$ matrix, $mathbf { X }$ and $mathbf { Y }$ are of size $M times N$ , and $mathbf { Z }$ is of size $M ^ { prime } times N ^ { prime }$ .) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Functions",
        "subsubsection": "Common functions of >2 arguments"
    },
    {
        "content": "softmax(x) Softmax function, $big [ frac { e ^ { x _ { c } } } { sum _ { c ^ { prime } = 1 } ^ { C } e ^ { x _ { c ^ { prime } } } } big ] _ { c = 1 } ^ { C }$ \nA.4 Linear algebra \nIn this section, we summarize the notation we use for linear algebra (see Chapter 7 for details). \nA.4.1 General notation \nVectors are bold lower case letters such as $_ { x }$ , $mathbf { boldsymbol { w } }$ . Matrices are bold upper case letters, such as $mathbf { X }$ , $mathbf { W }$ . Scalars are non-bold lower case. When creating a vector from a list of $N$ scalars, we write $pmb { x } = [ x _ { 1 } , dots , x _ { N } ]$ ; this may be a column vector or a row vector, depending on the context. (Vectors are assumed to be column vectors, unless noted otherwise.) When creating an $M times N$ matrix from a list of vectors, we write $mathbf { X } = [ pmb { x } _ { 1 } , dots , pmb { x } _ { N } ]$ if we stack along the columns, or $mathbf { X } = [ pmb { x } _ { 1 } ; dots ; pmb { x } _ { M } ]$ if we stack along the rows. \nA.4.2 Vectors \nHere is some standard notation for vectors. (We assume $mathbf { Delta } _ { mathbf { u } }$ and $mathbf { nabla } _ { mathbf { v } }$ are both $N$ -dimensional vectors.) \nA.4.3 Matrices \nHere is some standard notation for matrices. (We assume $mathbf { s }$ is a square $N times N$ matrix, $mathbf { X }$ and $mathbf { Y }$ are of size $M times N$ , and $mathbf { Z }$ is of size $M ^ { prime } times N ^ { prime }$ .) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Linear algebra",
        "subsubsection": "General notation"
    },
    {
        "content": "softmax(x) Softmax function, $big [ frac { e ^ { x _ { c } } } { sum _ { c ^ { prime } = 1 } ^ { C } e ^ { x _ { c ^ { prime } } } } big ] _ { c = 1 } ^ { C }$ \nA.4 Linear algebra \nIn this section, we summarize the notation we use for linear algebra (see Chapter 7 for details). \nA.4.1 General notation \nVectors are bold lower case letters such as $_ { x }$ , $mathbf { boldsymbol { w } }$ . Matrices are bold upper case letters, such as $mathbf { X }$ , $mathbf { W }$ . Scalars are non-bold lower case. When creating a vector from a list of $N$ scalars, we write $pmb { x } = [ x _ { 1 } , dots , x _ { N } ]$ ; this may be a column vector or a row vector, depending on the context. (Vectors are assumed to be column vectors, unless noted otherwise.) When creating an $M times N$ matrix from a list of vectors, we write $mathbf { X } = [ pmb { x } _ { 1 } , dots , pmb { x } _ { N } ]$ if we stack along the columns, or $mathbf { X } = [ pmb { x } _ { 1 } ; dots ; pmb { x } _ { M } ]$ if we stack along the rows. \nA.4.2 Vectors \nHere is some standard notation for vectors. (We assume $mathbf { Delta } _ { mathbf { u } }$ and $mathbf { nabla } _ { mathbf { v } }$ are both $N$ -dimensional vectors.) \nA.4.3 Matrices \nHere is some standard notation for matrices. (We assume $mathbf { s }$ is a square $N times N$ matrix, $mathbf { X }$ and $mathbf { Y }$ are of size $M times N$ , and $mathbf { Z }$ is of size $M ^ { prime } times N ^ { prime }$ .) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Linear algebra",
        "subsubsection": "Vectors"
    },
    {
        "content": "softmax(x) Softmax function, $big [ frac { e ^ { x _ { c } } } { sum _ { c ^ { prime } = 1 } ^ { C } e ^ { x _ { c ^ { prime } } } } big ] _ { c = 1 } ^ { C }$ \nA.4 Linear algebra \nIn this section, we summarize the notation we use for linear algebra (see Chapter 7 for details). \nA.4.1 General notation \nVectors are bold lower case letters such as $_ { x }$ , $mathbf { boldsymbol { w } }$ . Matrices are bold upper case letters, such as $mathbf { X }$ , $mathbf { W }$ . Scalars are non-bold lower case. When creating a vector from a list of $N$ scalars, we write $pmb { x } = [ x _ { 1 } , dots , x _ { N } ]$ ; this may be a column vector or a row vector, depending on the context. (Vectors are assumed to be column vectors, unless noted otherwise.) When creating an $M times N$ matrix from a list of vectors, we write $mathbf { X } = [ pmb { x } _ { 1 } , dots , pmb { x } _ { N } ]$ if we stack along the columns, or $mathbf { X } = [ pmb { x } _ { 1 } ; dots ; pmb { x } _ { M } ]$ if we stack along the rows. \nA.4.2 Vectors \nHere is some standard notation for vectors. (We assume $mathbf { Delta } _ { mathbf { u } }$ and $mathbf { nabla } _ { mathbf { v } }$ are both $N$ -dimensional vectors.) \nA.4.3 Matrices \nHere is some standard notation for matrices. (We assume $mathbf { s }$ is a square $N times N$ matrix, $mathbf { X }$ and $mathbf { Y }$ are of size $M times N$ , and $mathbf { Z }$ is of size $M ^ { prime } times N ^ { prime }$ .) \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \n$mathbf { S } ^ { - 1 }$ Inverse of a square matrix   \n$mathbf { X } ^ { dagger }$ Pseudo-inverse of a matrix   \n$mathbf { X } ^ { top }$ Transpose of a matrix   \n$mathrm { d i a g } ( mathbf { S } )$ Diagonal vector extracted from square matrix   \nI or ${ mathbf { I } } _ { N }$ Identity matrix of size $N times N$   \n$mathbf { X } odot mathbf { Y }$ Elementwise product   \n$mathbf { X } otimes mathbf { Z }$ Kronecker product (see Section 7.2.5) \nA.4.4 Matrix calculus \nIn this section, we summarize the notation we use for matrix calculus (see Section 7.8 for details). Let $pmb theta in mathbb { R } ^ { N }$ be a vector and $f : mathbb { R } ^ { N }  mathbb { R }$ be a scalar valued function. The derivative of $f$ wrt its argument is denoted by the following: \nThe gradient is a vector that must be evaluated at a point in space. To emphasize this, we will sometimes write \nWe can also compute the (symmetric) $N times N$ matrix of second partial derivatives, known as the Hessian: \nThe Hessian is a matrix that must be evaluated at a point in space. To emphasize this, we will sometimes write \nA.5 Optimization \nIn this section, we summarize the notation we use for optimization (see Chapter 8 for details). \nWe will often write an objective or cost function that we wish to minimize as $mathcal { L } ( pmb { theta } )$ , where $pmb theta$ are the variables to be optimized (often thought of as parameters of a statistical model). We denote the parameter value that achieves the minimum as $theta _ { * } = mathrm { a r g m i n } _ { theta in Theta } mathcal { L } ( theta )$ , where $Theta$ is the set we are optimizing over. (Note that there may be more than one such optimal value, so we should really write $theta _ { * } in mathrm { a r g m i n } _ { theta in Theta } mathcal { L } ( theta )$ .) \nWhen performing iterative optimization, we use $t$ to index the iteration number. We use $eta$ as a step size (learning rate) parameter. Thus we can write the gradient descent algorithm (explained in Section 8.4) as follows: $pmb { theta } _ { t + 1 } = pmb { theta } _ { t } - eta _ { t } pmb { g } _ { t }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Linear algebra",
        "subsubsection": "Matrices"
    },
    {
        "content": "$mathbf { S } ^ { - 1 }$ Inverse of a square matrix   \n$mathbf { X } ^ { dagger }$ Pseudo-inverse of a matrix   \n$mathbf { X } ^ { top }$ Transpose of a matrix   \n$mathrm { d i a g } ( mathbf { S } )$ Diagonal vector extracted from square matrix   \nI or ${ mathbf { I } } _ { N }$ Identity matrix of size $N times N$   \n$mathbf { X } odot mathbf { Y }$ Elementwise product   \n$mathbf { X } otimes mathbf { Z }$ Kronecker product (see Section 7.2.5) \nA.4.4 Matrix calculus \nIn this section, we summarize the notation we use for matrix calculus (see Section 7.8 for details). Let $pmb theta in mathbb { R } ^ { N }$ be a vector and $f : mathbb { R } ^ { N }  mathbb { R }$ be a scalar valued function. The derivative of $f$ wrt its argument is denoted by the following: \nThe gradient is a vector that must be evaluated at a point in space. To emphasize this, we will sometimes write \nWe can also compute the (symmetric) $N times N$ matrix of second partial derivatives, known as the Hessian: \nThe Hessian is a matrix that must be evaluated at a point in space. To emphasize this, we will sometimes write \nA.5 Optimization \nIn this section, we summarize the notation we use for optimization (see Chapter 8 for details). \nWe will often write an objective or cost function that we wish to minimize as $mathcal { L } ( pmb { theta } )$ , where $pmb theta$ are the variables to be optimized (often thought of as parameters of a statistical model). We denote the parameter value that achieves the minimum as $theta _ { * } = mathrm { a r g m i n } _ { theta in Theta } mathcal { L } ( theta )$ , where $Theta$ is the set we are optimizing over. (Note that there may be more than one such optimal value, so we should really write $theta _ { * } in mathrm { a r g m i n } _ { theta in Theta } mathcal { L } ( theta )$ .) \nWhen performing iterative optimization, we use $t$ to index the iteration number. We use $eta$ as a step size (learning rate) parameter. Thus we can write the gradient descent algorithm (explained in Section 8.4) as follows: $pmb { theta } _ { t + 1 } = pmb { theta } _ { t } - eta _ { t } pmb { g } _ { t }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Linear algebra",
        "subsubsection": "Matrix calculus"
    },
    {
        "content": "$mathbf { S } ^ { - 1 }$ Inverse of a square matrix   \n$mathbf { X } ^ { dagger }$ Pseudo-inverse of a matrix   \n$mathbf { X } ^ { top }$ Transpose of a matrix   \n$mathrm { d i a g } ( mathbf { S } )$ Diagonal vector extracted from square matrix   \nI or ${ mathbf { I } } _ { N }$ Identity matrix of size $N times N$   \n$mathbf { X } odot mathbf { Y }$ Elementwise product   \n$mathbf { X } otimes mathbf { Z }$ Kronecker product (see Section 7.2.5) \nA.4.4 Matrix calculus \nIn this section, we summarize the notation we use for matrix calculus (see Section 7.8 for details). Let $pmb theta in mathbb { R } ^ { N }$ be a vector and $f : mathbb { R } ^ { N }  mathbb { R }$ be a scalar valued function. The derivative of $f$ wrt its argument is denoted by the following: \nThe gradient is a vector that must be evaluated at a point in space. To emphasize this, we will sometimes write \nWe can also compute the (symmetric) $N times N$ matrix of second partial derivatives, known as the Hessian: \nThe Hessian is a matrix that must be evaluated at a point in space. To emphasize this, we will sometimes write \nA.5 Optimization \nIn this section, we summarize the notation we use for optimization (see Chapter 8 for details). \nWe will often write an objective or cost function that we wish to minimize as $mathcal { L } ( pmb { theta } )$ , where $pmb theta$ are the variables to be optimized (often thought of as parameters of a statistical model). We denote the parameter value that achieves the minimum as $theta _ { * } = mathrm { a r g m i n } _ { theta in Theta } mathcal { L } ( theta )$ , where $Theta$ is the set we are optimizing over. (Note that there may be more than one such optimal value, so we should really write $theta _ { * } in mathrm { a r g m i n } _ { theta in Theta } mathcal { L } ( theta )$ .) \nWhen performing iterative optimization, we use $t$ to index the iteration number. We use $eta$ as a step size (learning rate) parameter. Thus we can write the gradient descent algorithm (explained in Section 8.4) as follows: $pmb { theta } _ { t + 1 } = pmb { theta } _ { t } - eta _ { t } pmb { g } _ { t }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nWe often use a hat symbol to denote an estimate or prediction (e.g., $hat { pmb { theta } }$ , $hat { y }$ ), a star subscript or superscript to denote a true (but usually unknown) value (e.g., $pmb { theta } _ { ast }$ or $pmb { theta } ^ { * }$ ), an overline to denote a mean value (e.g., $overline { { pmb { theta } } }$ ). \nA.6 Probability \nn this section, we summarize the notation we use for probability theory (see Chapter 2 for details) \nWe denote a probability density function (pdf) or probability mass function (pmf) by $p$ , a cumulative distribution function (cdf) by $P$ , and the probability of a binary event by Pr. We write $p ( X )$ for the distribution for random variable $X$ , and $p ( Y )$ for the distribution for random variable $Y$ — these refer to different distributions, even though we use the same $p$ symbol in both cases. (In cases where confusion may arise, we write $p _ { X } ( cdot )$ and $p _ { Y } ( cdot )$ .) Approximations to a distribution $p$ will often be represented by $q$ , or sometimes $hat { p }$ . \nIn some cases, we distinguish between a random variable (rv) and the values it can take on. In this case, we denote the variable in upper case (e.g., $X$ ), and its value in lower case (e.g., $x$ ). However, we often ignore this distinction between variables and values. For example, we sometimes write $p ( x )$ to denote either the scalar value (the distribution evaluated at a point) or the distribution itself, depending on whether $X$ is observed or not. \nWe write $X sim p$ to denote that $X$ is distributed according to distribution $p$ . We write $X perp Y mid Z$ to denote that $X$ is conditionally independent of $Y$ given $Z$ . If $X sim p$ , we denote the expected value of $f ( X )$ using \nIf $f$ is the identity function, we write ${ overline { { X } } } triangleq mathbb { E } left[ X right]$ . Similarly, the variance is denoted by \nIf $_ { x }$ is a random vector, the covariance matrix is denoted \nIf $X sim p$ , the mode of a distribution is denoted by \nWe denote parametric distributions using $p ( { boldsymbol { mathbf { mathit { x } } } } | mathbf { boldsymbol { theta } } )$ , where $_ { x }$ are the random variables, $pmb theta$ are the parameters and $p$ is a pdf or pmf. For example, $mathcal { N } ( x | mu , sigma ^ { 2 } )$ is a Gaussian (normal) distribution with mean $mu$ and standard deviation $sigma$ . \nA.7 Information theory \nIn this section, we summarize the notation we use for information theory (see Chapter 6 for details). If $X sim p$ , we denote the (differential) entropy of the distribution by $mathbb { H } left( X right)$ or $mathbb { H } left( p right)$ . If $Y sim q$ , we denote the KL divergence from distribution $p$ to $q$ by $D _ { mathbb { K L } } left( p parallel q right)$ . If $( X , Y ) sim p$ , we denote the mutual information between $X$ and $Y$ by $mathbb { I } left( X ; Y right)$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Optimization",
        "subsubsection": "N/A"
    },
    {
        "content": "We often use a hat symbol to denote an estimate or prediction (e.g., $hat { pmb { theta } }$ , $hat { y }$ ), a star subscript or superscript to denote a true (but usually unknown) value (e.g., $pmb { theta } _ { ast }$ or $pmb { theta } ^ { * }$ ), an overline to denote a mean value (e.g., $overline { { pmb { theta } } }$ ). \nA.6 Probability \nn this section, we summarize the notation we use for probability theory (see Chapter 2 for details) \nWe denote a probability density function (pdf) or probability mass function (pmf) by $p$ , a cumulative distribution function (cdf) by $P$ , and the probability of a binary event by Pr. We write $p ( X )$ for the distribution for random variable $X$ , and $p ( Y )$ for the distribution for random variable $Y$ — these refer to different distributions, even though we use the same $p$ symbol in both cases. (In cases where confusion may arise, we write $p _ { X } ( cdot )$ and $p _ { Y } ( cdot )$ .) Approximations to a distribution $p$ will often be represented by $q$ , or sometimes $hat { p }$ . \nIn some cases, we distinguish between a random variable (rv) and the values it can take on. In this case, we denote the variable in upper case (e.g., $X$ ), and its value in lower case (e.g., $x$ ). However, we often ignore this distinction between variables and values. For example, we sometimes write $p ( x )$ to denote either the scalar value (the distribution evaluated at a point) or the distribution itself, depending on whether $X$ is observed or not. \nWe write $X sim p$ to denote that $X$ is distributed according to distribution $p$ . We write $X perp Y mid Z$ to denote that $X$ is conditionally independent of $Y$ given $Z$ . If $X sim p$ , we denote the expected value of $f ( X )$ using \nIf $f$ is the identity function, we write ${ overline { { X } } } triangleq mathbb { E } left[ X right]$ . Similarly, the variance is denoted by \nIf $_ { x }$ is a random vector, the covariance matrix is denoted \nIf $X sim p$ , the mode of a distribution is denoted by \nWe denote parametric distributions using $p ( { boldsymbol { mathbf { mathit { x } } } } | mathbf { boldsymbol { theta } } )$ , where $_ { x }$ are the random variables, $pmb theta$ are the parameters and $p$ is a pdf or pmf. For example, $mathcal { N } ( x | mu , sigma ^ { 2 } )$ is a Gaussian (normal) distribution with mean $mu$ and standard deviation $sigma$ . \nA.7 Information theory \nIn this section, we summarize the notation we use for information theory (see Chapter 6 for details). If $X sim p$ , we denote the (differential) entropy of the distribution by $mathbb { H } left( X right)$ or $mathbb { H } left( p right)$ . If $Y sim q$ , we denote the KL divergence from distribution $p$ to $q$ by $D _ { mathbb { K L } } left( p parallel q right)$ . If $( X , Y ) sim p$ , we denote the mutual information between $X$ and $Y$ by $mathbb { I } left( X ; Y right)$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Probability",
        "subsubsection": "N/A"
    },
    {
        "content": "We often use a hat symbol to denote an estimate or prediction (e.g., $hat { pmb { theta } }$ , $hat { y }$ ), a star subscript or superscript to denote a true (but usually unknown) value (e.g., $pmb { theta } _ { ast }$ or $pmb { theta } ^ { * }$ ), an overline to denote a mean value (e.g., $overline { { pmb { theta } } }$ ). \nA.6 Probability \nn this section, we summarize the notation we use for probability theory (see Chapter 2 for details) \nWe denote a probability density function (pdf) or probability mass function (pmf) by $p$ , a cumulative distribution function (cdf) by $P$ , and the probability of a binary event by Pr. We write $p ( X )$ for the distribution for random variable $X$ , and $p ( Y )$ for the distribution for random variable $Y$ — these refer to different distributions, even though we use the same $p$ symbol in both cases. (In cases where confusion may arise, we write $p _ { X } ( cdot )$ and $p _ { Y } ( cdot )$ .) Approximations to a distribution $p$ will often be represented by $q$ , or sometimes $hat { p }$ . \nIn some cases, we distinguish between a random variable (rv) and the values it can take on. In this case, we denote the variable in upper case (e.g., $X$ ), and its value in lower case (e.g., $x$ ). However, we often ignore this distinction between variables and values. For example, we sometimes write $p ( x )$ to denote either the scalar value (the distribution evaluated at a point) or the distribution itself, depending on whether $X$ is observed or not. \nWe write $X sim p$ to denote that $X$ is distributed according to distribution $p$ . We write $X perp Y mid Z$ to denote that $X$ is conditionally independent of $Y$ given $Z$ . If $X sim p$ , we denote the expected value of $f ( X )$ using \nIf $f$ is the identity function, we write ${ overline { { X } } } triangleq mathbb { E } left[ X right]$ . Similarly, the variance is denoted by \nIf $_ { x }$ is a random vector, the covariance matrix is denoted \nIf $X sim p$ , the mode of a distribution is denoted by \nWe denote parametric distributions using $p ( { boldsymbol { mathbf { mathit { x } } } } | mathbf { boldsymbol { theta } } )$ , where $_ { x }$ are the random variables, $pmb theta$ are the parameters and $p$ is a pdf or pmf. For example, $mathcal { N } ( x | mu , sigma ^ { 2 } )$ is a Gaussian (normal) distribution with mean $mu$ and standard deviation $sigma$ . \nA.7 Information theory \nIn this section, we summarize the notation we use for information theory (see Chapter 6 for details). If $X sim p$ , we denote the (differential) entropy of the distribution by $mathbb { H } left( X right)$ or $mathbb { H } left( p right)$ . If $Y sim q$ , we denote the KL divergence from distribution $p$ to $q$ by $D _ { mathbb { K L } } left( p parallel q right)$ . If $( X , Y ) sim p$ , we denote the mutual information between $X$ and $Y$ by $mathbb { I } left( X ; Y right)$ . \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license \nA.8 Statistics and machine learning \nWe briefly summarize the notation we use for statistical learning. \nA.8.1 Supervised learning \nFor supervised learning, we denote the observed features (also called inputs or covariates) by $mathbf { boldsymbol { x } } in mathcal { X }$ . Often $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , meaning the features are real-valued. (Note that this includes the case of discrete-valued inputs, which can be represented as one-hot vectors.) Sometimes we compute manually-specified features of the input; we denote these by $phi ( { pmb x } )$ . We also have outputs (also called targets or response variables) $pmb { y } in mathcal { V }$ that we wish to predict. Our task is to learn a conditional probability distribution $p ( pmb { y } | pmb { x } , pmb theta )$ , where $pmb theta$ are the parameters of the model. If $mathcal { Y } = { 1 , ldots , C }$ , we call this classification. If $mathcal { V } = mathbb { R } ^ { C }$ , we call this regression (often $C = 1$ , so we are just predicting a scalar response). \nThe parameters $pmb theta$ are estimated from training data, denoted by $mathcal { D } = { ( pmb { x } _ { n } , pmb { y } _ { n } ) : n in { 1 , dots , N _ { mathcal { D } } } }$ (so $N _ { mathcal { D } }$ is the number of training cases). If $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , we can store the training inputs in an $N _ { mathcal { D } } times D$ design matrix denoted by $mathbf { X }$ . If $mathcal { V } = mathbb { R } ^ { C }$ , we can store the training outputs in an $N _ { mathcal { D } } times C$ matrix $mathbf { Y }$ . If $mathcal { Y } = { 1 , ldots , C }$ , we can represent each class label as a $C$ -dimensional bit vector, with one element turned on (this is known as a one-hot encoding), so we can store the training outputs in an $N _ { mathit { D } } times C$ binary matrix $mathbf { Y }$ . \nA.8.2 Unsupervised learning and generative models \nUnsupervised learning is usually formalized as the task of unconditional density estimation, namely modeling $p ( { pmb x } | { pmb theta } )$ . In some cases, we want to perform conditional density estimation; we denote the values we are conditioning on by $mathbf { Delta } _ { mathbf { u } }$ , so the model becomes $p ( pmb { x } | pmb { u } , pmb theta )$ . This is similar to supervised learning, except that $_ { x }$ is usually high dimensional (e.g., an image) and $mathbf { Delta } _ { mathbf { u } }$ is usually low dimensional (e.g., a class label or a text description). \nIn some models, we have latent variables, also called hidden variables, which are never observed in the training data. We call such models latent variable models (LVM). We denote the latent variables for data case $n$ by $z _ { n } in { mathcal { Z } }$ . Sometimes latent variables are known as hidden variables, and are denoted by $scriptstyle h _ { n }$ . By contrast, the visible variables will be denoted by ${ pmb v } _ { n }$ . Typically the latent variables are continuous or discrete, i.e., $mathcal { Z } = mathbb { R } ^ { L }$ or $mathcal { Z } = { 1 , ldots , K }$ . \nMost LVMs have the form $p ( pmb { x } _ { n } , z _ { n } | pmb { theta } )$ ; such models can be used for unsupervised learning. However, LVMs can also be used for supervised learning. In particular, we can either create a generative (unconditional) model of the form $p ( pmb { x } _ { n } , pmb { y } _ { n } , z _ { n } | pmb { theta } )$ , or a discriminative (conditional) model of the form $p ( pmb { y } _ { n } , pmb { z } _ { n } | pmb { x } _ { n } , pmb { theta } )$ . \nA.8.3 Bayesian inference \nWhen working with Bayesian inference, we write the prior over the parameters as $p ( pmb theta | phi )$ , where $phi$ are the hyperparameters. For conjugate models, the posterior has the same form as the prior (by definition). We can therefore just update the hyperparameters from their prior value, $breve { phi }$ , to their posterior value, $hat { phi }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Information theory",
        "subsubsection": "N/A"
    },
    {
        "content": "A.8 Statistics and machine learning \nWe briefly summarize the notation we use for statistical learning. \nA.8.1 Supervised learning \nFor supervised learning, we denote the observed features (also called inputs or covariates) by $mathbf { boldsymbol { x } } in mathcal { X }$ . Often $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , meaning the features are real-valued. (Note that this includes the case of discrete-valued inputs, which can be represented as one-hot vectors.) Sometimes we compute manually-specified features of the input; we denote these by $phi ( { pmb x } )$ . We also have outputs (also called targets or response variables) $pmb { y } in mathcal { V }$ that we wish to predict. Our task is to learn a conditional probability distribution $p ( pmb { y } | pmb { x } , pmb theta )$ , where $pmb theta$ are the parameters of the model. If $mathcal { Y } = { 1 , ldots , C }$ , we call this classification. If $mathcal { V } = mathbb { R } ^ { C }$ , we call this regression (often $C = 1$ , so we are just predicting a scalar response). \nThe parameters $pmb theta$ are estimated from training data, denoted by $mathcal { D } = { ( pmb { x } _ { n } , pmb { y } _ { n } ) : n in { 1 , dots , N _ { mathcal { D } } } }$ (so $N _ { mathcal { D } }$ is the number of training cases). If $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , we can store the training inputs in an $N _ { mathcal { D } } times D$ design matrix denoted by $mathbf { X }$ . If $mathcal { V } = mathbb { R } ^ { C }$ , we can store the training outputs in an $N _ { mathcal { D } } times C$ matrix $mathbf { Y }$ . If $mathcal { Y } = { 1 , ldots , C }$ , we can represent each class label as a $C$ -dimensional bit vector, with one element turned on (this is known as a one-hot encoding), so we can store the training outputs in an $N _ { mathit { D } } times C$ binary matrix $mathbf { Y }$ . \nA.8.2 Unsupervised learning and generative models \nUnsupervised learning is usually formalized as the task of unconditional density estimation, namely modeling $p ( { pmb x } | { pmb theta } )$ . In some cases, we want to perform conditional density estimation; we denote the values we are conditioning on by $mathbf { Delta } _ { mathbf { u } }$ , so the model becomes $p ( pmb { x } | pmb { u } , pmb theta )$ . This is similar to supervised learning, except that $_ { x }$ is usually high dimensional (e.g., an image) and $mathbf { Delta } _ { mathbf { u } }$ is usually low dimensional (e.g., a class label or a text description). \nIn some models, we have latent variables, also called hidden variables, which are never observed in the training data. We call such models latent variable models (LVM). We denote the latent variables for data case $n$ by $z _ { n } in { mathcal { Z } }$ . Sometimes latent variables are known as hidden variables, and are denoted by $scriptstyle h _ { n }$ . By contrast, the visible variables will be denoted by ${ pmb v } _ { n }$ . Typically the latent variables are continuous or discrete, i.e., $mathcal { Z } = mathbb { R } ^ { L }$ or $mathcal { Z } = { 1 , ldots , K }$ . \nMost LVMs have the form $p ( pmb { x } _ { n } , z _ { n } | pmb { theta } )$ ; such models can be used for unsupervised learning. However, LVMs can also be used for supervised learning. In particular, we can either create a generative (unconditional) model of the form $p ( pmb { x } _ { n } , pmb { y } _ { n } , z _ { n } | pmb { theta } )$ , or a discriminative (conditional) model of the form $p ( pmb { y } _ { n } , pmb { z } _ { n } | pmb { x } _ { n } , pmb { theta } )$ . \nA.8.3 Bayesian inference \nWhen working with Bayesian inference, we write the prior over the parameters as $p ( pmb theta | phi )$ , where $phi$ are the hyperparameters. For conjugate models, the posterior has the same form as the prior (by definition). We can therefore just update the hyperparameters from their prior value, $breve { phi }$ , to their posterior value, $hat { phi }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Statistics and machine learning",
        "subsubsection": "Supervised learning"
    },
    {
        "content": "A.8 Statistics and machine learning \nWe briefly summarize the notation we use for statistical learning. \nA.8.1 Supervised learning \nFor supervised learning, we denote the observed features (also called inputs or covariates) by $mathbf { boldsymbol { x } } in mathcal { X }$ . Often $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , meaning the features are real-valued. (Note that this includes the case of discrete-valued inputs, which can be represented as one-hot vectors.) Sometimes we compute manually-specified features of the input; we denote these by $phi ( { pmb x } )$ . We also have outputs (also called targets or response variables) $pmb { y } in mathcal { V }$ that we wish to predict. Our task is to learn a conditional probability distribution $p ( pmb { y } | pmb { x } , pmb theta )$ , where $pmb theta$ are the parameters of the model. If $mathcal { Y } = { 1 , ldots , C }$ , we call this classification. If $mathcal { V } = mathbb { R } ^ { C }$ , we call this regression (often $C = 1$ , so we are just predicting a scalar response). \nThe parameters $pmb theta$ are estimated from training data, denoted by $mathcal { D } = { ( pmb { x } _ { n } , pmb { y } _ { n } ) : n in { 1 , dots , N _ { mathcal { D } } } }$ (so $N _ { mathcal { D } }$ is the number of training cases). If $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , we can store the training inputs in an $N _ { mathcal { D } } times D$ design matrix denoted by $mathbf { X }$ . If $mathcal { V } = mathbb { R } ^ { C }$ , we can store the training outputs in an $N _ { mathcal { D } } times C$ matrix $mathbf { Y }$ . If $mathcal { Y } = { 1 , ldots , C }$ , we can represent each class label as a $C$ -dimensional bit vector, with one element turned on (this is known as a one-hot encoding), so we can store the training outputs in an $N _ { mathit { D } } times C$ binary matrix $mathbf { Y }$ . \nA.8.2 Unsupervised learning and generative models \nUnsupervised learning is usually formalized as the task of unconditional density estimation, namely modeling $p ( { pmb x } | { pmb theta } )$ . In some cases, we want to perform conditional density estimation; we denote the values we are conditioning on by $mathbf { Delta } _ { mathbf { u } }$ , so the model becomes $p ( pmb { x } | pmb { u } , pmb theta )$ . This is similar to supervised learning, except that $_ { x }$ is usually high dimensional (e.g., an image) and $mathbf { Delta } _ { mathbf { u } }$ is usually low dimensional (e.g., a class label or a text description). \nIn some models, we have latent variables, also called hidden variables, which are never observed in the training data. We call such models latent variable models (LVM). We denote the latent variables for data case $n$ by $z _ { n } in { mathcal { Z } }$ . Sometimes latent variables are known as hidden variables, and are denoted by $scriptstyle h _ { n }$ . By contrast, the visible variables will be denoted by ${ pmb v } _ { n }$ . Typically the latent variables are continuous or discrete, i.e., $mathcal { Z } = mathbb { R } ^ { L }$ or $mathcal { Z } = { 1 , ldots , K }$ . \nMost LVMs have the form $p ( pmb { x } _ { n } , z _ { n } | pmb { theta } )$ ; such models can be used for unsupervised learning. However, LVMs can also be used for supervised learning. In particular, we can either create a generative (unconditional) model of the form $p ( pmb { x } _ { n } , pmb { y } _ { n } , z _ { n } | pmb { theta } )$ , or a discriminative (conditional) model of the form $p ( pmb { y } _ { n } , pmb { z } _ { n } | pmb { x } _ { n } , pmb { theta } )$ . \nA.8.3 Bayesian inference \nWhen working with Bayesian inference, we write the prior over the parameters as $p ( pmb theta | phi )$ , where $phi$ are the hyperparameters. For conjugate models, the posterior has the same form as the prior (by definition). We can therefore just update the hyperparameters from their prior value, $breve { phi }$ , to their posterior value, $hat { phi }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Statistics and machine learning",
        "subsubsection": "Unsupervised learning and generative models"
    },
    {
        "content": "A.8 Statistics and machine learning \nWe briefly summarize the notation we use for statistical learning. \nA.8.1 Supervised learning \nFor supervised learning, we denote the observed features (also called inputs or covariates) by $mathbf { boldsymbol { x } } in mathcal { X }$ . Often $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , meaning the features are real-valued. (Note that this includes the case of discrete-valued inputs, which can be represented as one-hot vectors.) Sometimes we compute manually-specified features of the input; we denote these by $phi ( { pmb x } )$ . We also have outputs (also called targets or response variables) $pmb { y } in mathcal { V }$ that we wish to predict. Our task is to learn a conditional probability distribution $p ( pmb { y } | pmb { x } , pmb theta )$ , where $pmb theta$ are the parameters of the model. If $mathcal { Y } = { 1 , ldots , C }$ , we call this classification. If $mathcal { V } = mathbb { R } ^ { C }$ , we call this regression (often $C = 1$ , so we are just predicting a scalar response). \nThe parameters $pmb theta$ are estimated from training data, denoted by $mathcal { D } = { ( pmb { x } _ { n } , pmb { y } _ { n } ) : n in { 1 , dots , N _ { mathcal { D } } } }$ (so $N _ { mathcal { D } }$ is the number of training cases). If $boldsymbol { mathcal { X } } = mathbb { R } ^ { D }$ , we can store the training inputs in an $N _ { mathcal { D } } times D$ design matrix denoted by $mathbf { X }$ . If $mathcal { V } = mathbb { R } ^ { C }$ , we can store the training outputs in an $N _ { mathcal { D } } times C$ matrix $mathbf { Y }$ . If $mathcal { Y } = { 1 , ldots , C }$ , we can represent each class label as a $C$ -dimensional bit vector, with one element turned on (this is known as a one-hot encoding), so we can store the training outputs in an $N _ { mathit { D } } times C$ binary matrix $mathbf { Y }$ . \nA.8.2 Unsupervised learning and generative models \nUnsupervised learning is usually formalized as the task of unconditional density estimation, namely modeling $p ( { pmb x } | { pmb theta } )$ . In some cases, we want to perform conditional density estimation; we denote the values we are conditioning on by $mathbf { Delta } _ { mathbf { u } }$ , so the model becomes $p ( pmb { x } | pmb { u } , pmb theta )$ . This is similar to supervised learning, except that $_ { x }$ is usually high dimensional (e.g., an image) and $mathbf { Delta } _ { mathbf { u } }$ is usually low dimensional (e.g., a class label or a text description). \nIn some models, we have latent variables, also called hidden variables, which are never observed in the training data. We call such models latent variable models (LVM). We denote the latent variables for data case $n$ by $z _ { n } in { mathcal { Z } }$ . Sometimes latent variables are known as hidden variables, and are denoted by $scriptstyle h _ { n }$ . By contrast, the visible variables will be denoted by ${ pmb v } _ { n }$ . Typically the latent variables are continuous or discrete, i.e., $mathcal { Z } = mathbb { R } ^ { L }$ or $mathcal { Z } = { 1 , ldots , K }$ . \nMost LVMs have the form $p ( pmb { x } _ { n } , z _ { n } | pmb { theta } )$ ; such models can be used for unsupervised learning. However, LVMs can also be used for supervised learning. In particular, we can either create a generative (unconditional) model of the form $p ( pmb { x } _ { n } , pmb { y } _ { n } , z _ { n } | pmb { theta } )$ , or a discriminative (conditional) model of the form $p ( pmb { y } _ { n } , pmb { z } _ { n } | pmb { x } _ { n } , pmb { theta } )$ . \nA.8.3 Bayesian inference \nWhen working with Bayesian inference, we write the prior over the parameters as $p ( pmb theta | phi )$ , where $phi$ are the hyperparameters. For conjugate models, the posterior has the same form as the prior (by definition). We can therefore just update the hyperparameters from their prior value, $breve { phi }$ , to their posterior value, $hat { phi }$ . \nDraft of “Probabilistic Machine Learning: An Introduction”. August 8, 2022 \nIn variational inference (Section 4.6.8.3), we use $psi$ to represent the parameters of the variational posterior, i.e., $p ( pmb theta | mathcal { D } ) approx q ( pmb theta | psi )$ . We optimize the ELBO wrt $psi$ to make this a good approximation. When performing Monte Carlo sampling, we use a $s$ subscript or superscript to denote a sample (e.g., $pmb { theta } _ { s }$ or $pmb { theta } ^ { s }$ ). \nA.9 Abbreviations \nHere are some of the abbreviations used in the book. \nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license rv Random variable RVM Relevance vector machine SGD Stochastic gradient descent SSE Sum of squared errors SVI Stochastic variational inference SVM Support vector machine VB Variational Bayes w.r.t. With respect to",
        "chapter": "V Beyond Supervised Learning",
        "section": "Notation",
        "subsection": "Statistics and machine learning",
        "subsubsection": "Bayesian inference"
    }
]